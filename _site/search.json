[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Harrie’s Hoekje",
    "section": "",
    "text": "Met deze blog Harrie’s Hoekje wil ik enkele ontwikkelingen op het terrein van moderne data-analyse bijhouden. Op dit terrein gebeurt op dit moment heel veel. Het zo nu en dan erover schrijven is voor mij een manier om een beetje bij te blijven en ontwikkelingen die mij interesseren zichtbaar te maken. Oudere versie (2017-2022) vind je hier, gemaakt met Distill. De blog die je nu leest beslaat de periode 2019-heden, is in een nieuw jasje gestoken en gemaakt met Quarto.\nDr. Harrie Jonkman studeerde sociologie en onderwijskunde en werkte aan de Rijksuniversiteit Groningen, in het onderwijs, het Nationaal Comité 4 en 5 mei en het Nederlands Jeugdinstituut (NJi). Sinds 2008 is hij verbonden aan het Verwey-Jonker Instituut. Zijn werk richt zich op de sociale en cognitieve ontwikkeling van kinderen en jongeren, sociale determinanten en preventie van ontwikkelings- en gedragsproblemen. In 2012 ontving hij van het National Institute on Drug Abuse (VS) een beurs als ‘Distinguished Researcher’. Hij was landelijk projectleider van de Community that Care strategie in Nederland, ondersteunde deze ook in andere landen en schreef zijn promotieonderzoek naar de effecten van de preventiestrategie Communities that Care. Bij het VerweyJonker Instituut is hij betrokken bij experimenten en evaluaties van sociale programma’s, longitudinale studies, en internationaal vergelijkende studies. Hij ondersteunt preventiewerk in verschillende landen, was adviseur van de International Task Force on Prevention van de Society of Prevention Research en werkte tientallen jaren als supervisor in stuurgroepen van onderwijsinstellingen. De laatste jaren specialiseerde hij zich in moderne data-analyse. Zijn interesse gaat daarbij uit naar multilevel-analyse, effectonderzoek en, sinds kort, machine-learning, zowel frequentisch als Bayesiaans. Maar ook vormgeving en visualisatie heeft zijn interesse. Hij ziet zichzelf meer als liefhebber dan als expert.\nDit zijn blogs die hij graag volgt:\nSimply Statistics\nRbloggers\nR4stats\nDataCamp\nJASP\nMLwiN\nSTATA\nIndividuen die hij graag volgt zijn onder andere:\nKieran Healey\nRens van de Schoot\nDavid Spiegelhalter\nAlison Hill\nHier boeken die op dit moment voor hem belangrijk zijn:\nJohnson, A.A., Ott, M., Dogucu, M. (2021). Bayes Rules! Introduction to Bayesian Modeling with R. https://www.bayesrulesbook.com/\nBatra, N. (ed, 2021). The Epidemiologist R Handbook. https://epirhandbook.com/index.html\nBaumer, B.S., Kaplan, D.T. & Horton, N.J. (2018). Modern Data Science with R. Boca Raton: CRC Press.\nFreeman, M. & Ross, J. (2019). Programming Skills for Data Science. Start writing code to wrangle, analyze, and visualize data with R. Boston: Addison Wesley.\nGillespie, C. & Lovelace, R. (2017). Efficient R Programming. (https://csgillespie.github.io/efficientR/)\nGrolemund, G. & Wickham, H. (2019). R for Data Science. (https://r4ds.had.co.nz/)\nHealey, K. (2019). Data Visualization: A Practical Introduction. Princeton: Princeton University Press.\nIrizarry, R.A. (2019). Introduction to Data Science. Data Analysis and Prediction Algorithms with R. (https://rafalab.github.io/dsbook/)\nLovelace, R., Nowosad, J. & Muenchow, J. (2019). Geocomputation with R. (https://geocompr.robinlovelace.net/)\nMcElreath, R. (2019). Statistical Rethinking. A Bayesian Course with Examples in R and Stan. (Second edition). Boca Raton: Chapman and Hall/CRC Vooral met brms, ggplot2 and the tidyverse https://bookdown.org/connect/#/apps/1850/access\nPoldrack, R.A. (2021). Statistical Thinking for the 21st Century.https://statsthinking21.github.io/statsthinking21-core-site/\nSpiegelhalter, D. (2019). The Art of Statistics. Learning from Data. https://github.com/dspiegel29/ArtofStatistics\nXie, Y., Allaire, J.J. & Grolemund, G.(2019). R Markdown: The Definitive Guide. https://bookdown.org/yihui/rmarkdown/\nXie, Y., Dervieux, C. & Riederer, E. (2021). RMarkdown Cookbook. https://bookdown.org/yihui/rmarkdown-cookbook/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Harrie’s Hoekje",
    "section": "",
    "text": "Introductie op multilevel analyse in R met lme4 en tidyverse\n\n\n\n\n\n\n\nmodelleren\n\n\n\n\nDeze blog laat zien hoe je met de R-pakketten lme4 en tidyverse op eigentijdse wijze multilevel analyses kunt uitvoeren.\n\n\n\n\n\n\nAug 3, 2023\n\n\nRaffaele Vacca, bewerking Harrie Jonkman\n\n\n\n\n\n\n  \n\n\n\n\nEnkelvoudige, meervoudige en moderatie regressieanalyse\n\n\n\n\n\n\n\nmodelleren\n\n\n\n\nIn deze blog zet ik enkele verschillende vormen van regressie (enkelvoudige, meervoudige en modererende regressie) op rij en vergelijk ik de modellen. Met bepaalde pakketten worden de effecten inzichtelijk gemaakt.\n\n\n\n\n\n\nJul 3, 2023\n\n\nHarrie Jonkman\n\n\n\n\n\n\n  \n\n\n\n\nMaken en publiceren van een blog\n\n\n\n\n\n\n\ncommuniceren\n\n\n\n\nIn dit blog laat ik zien hoe je zelf met Quarto een blog kunt maken en hoe je dat kunt publiceren en verspreiden. Je kunt hier het blog vinden en de stappen die gezet zijn om het te maken en te publiceren.\n\n\n\n\n\n\nMar 18, 2023\n\n\nHarrie Jonkman\n\n\n\n\n\n\n  \n\n\n\n\nMaken en publiceren van een boek\n\n\n\n\n\n\n\ncommuniceren\n\n\n\n\nIn dit blog laat ik zien hoe je met Quarto een boek kunt maken en hoe je dat kunt publiceren en verspreiden. Je kunt hier het boek vinden en de stappen die gezet zijn om het te maken en te publiceren\n\n\n\n\n\n\nFeb 20, 2023\n\n\nHarrie Jonkman\n\n\n\n\n\n\n  \n\n\n\n\nBegin van mijn Quartoreis, een opzet voor een Quarto presentatie\n\n\n\n\n\n\n\ncommuniceren\n\n\n\n\nMet Quarto, het nieuwe programma om wetenschappelijk te schrijven, kun je ook mooie presentaties maken. Megahn Hall maakte een mooie versie. Hoe dat er uit ziet en hoe je dat doet, daarover vind je informatie in deze blog.\n\n\n\n\n\n\nFeb 13, 2023\n\n\nMeghan Hall, bewerking Harrie Jonkman\n\n\n\n\n\n\n  \n\n\n\n\nWerken met Tidymodels, een suite voor machine learning\n\n\n\n\n\n\n\nmodelleren\n\n\n\n\nTidymodels is het suite-pakket van Posit om met machinelearning te werken. Op de website staan vijf handleidingen die voor deze post wat bewerkt zijn en die de verschillende aspecten van het pakket laten zien. Het is een introductie, het vertelt iets over de belangrijkste onderdelen en er wordt een uitgebreide case-studie gepresenteerd.\n\n\n\n\n\n\nJan 11, 2023\n\n\nMax Kuhn en Julia Silge, bewerkt door HarrieJonkman\n\n\n\n\n\n\n  \n\n\n\n\nBayesiaanse hierarchische logistische regressie\n\n\n\n\n\n\n\nmodelleren\n\n\nBayesiaans\n\n\n\n\nHoe voer je een geclusterde logistische regressie uit vanuit een Bayesiaans perspectief? Aan de hand van data over succes en falen van Himalayaexpedities en leeftijd en zeestofgebruik van klimmers wordt duidelijk gemaakt hoe dit werkt. Deze blog is een bewerking van een deel van een hoofdstuk uit het prachtige Bayes Rules! boek.\n\n\n\n\n\n\nDec 13, 2022\n\n\nAlicia A. Johnson e.a., bewerking HarrieJonkman\n\n\n\n\n\n\n  \n\n\n\n\nWerken met moderne schattingstechnieken\n\n\n\n\n\n\n\nmodelleren\n\n\nBayesiaans\n\n\n\n\nHoe je schattingen maakt van uitkomsten door kennis van experts in te zetten, hoe zet je dit proces uit, hoe bereken je het en hoe combineer je deze kennis weer met data.\n\n\n\n\n\n\nDec 8, 2022\n\n\nHarrieJonkman\n\n\n\n\n\n\n  \n\n\n\n\nVisualisaties\n\n\n\n\n\n\n\nvisualisatie\n\n\ncommunicatie\n\n\n\n\nVisualisaties met hotspot, choropleth en interactieve kaarten\n\n\n\n\n\n\nNov 13, 2022\n\n\nJacob Kap, bewerking HarrieJonkman\n\n\n\n\n\n\n  \n\n\n\n\nStatistisch denken in de 21ste eeuw\n\n\n\n\n\n\n\nvoorbereiding\n\n\nmodelleren\n\n\ncommunicatie\n\n\n\n\nStatistisch denken in de 21ste eeuw met R én Python\n\n\n\n\n\n\nNov 7, 2022\n\n\nRussel Poldrack, bewerking HarrieJonkman\n\n\n\n\n\n\n  \n\n\n\n\nVerhalen vertellen met data\n\n\n\n\n\n\n\nvoorbereiding\n\n\nmodelleren\n\n\ncommunicatie\n\n\n\n\nBespreking van een interessant leerboek moderne dataanalyse Telling stories with data van Rohan Alexander\n\n\n\n\n\n\nOct 7, 2022\n\n\nHarrieJonkman\n\n\n\n\n\n\n  \n\n\n\n\nWerken met Quarto\n\n\n\n\n\n\n\ncommunicatie\n\n\n\n\nOver het werken met Quarto, de opvolger of het equivalent van RMarkdown\n\n\n\n\n\n\nOct 7, 2022\n\n\nHarrieJonkman\n\n\n\n\n\n\n  \n\n\n\n\nWijken plotten\n\n\n\n\n\n\n\nvisualization\n\n\n\n\nOver het maken van kaarten\n\n\n\n\n\n\nAug 18, 2022\n\n\nDaryn Ramsden, bewerking HarrieJonkman\n\n\n\n\n\n\n  \n\n\n\n\nGeografische analyse van risico’s met INLA\n\n\n\n\n\n\n\nvisualisatie\n\n\nmodellen\n\n\nBayesiaans\n\n\n\n\nPaula Moraga laat zien hoe je geografische gegevens kunt analyseren met het programma INLA\n\n\n\n\n\n\nAug 15, 2022\n\n\nPaula Moraga, bewerking HarrieJonkman\n\n\n\n\n\n\n  \n\n\n\n\nBayes en Stan\n\n\n\n\n\n\n\nBayesiaans\n\n\n\n\nOver het werken met Bayesiaanse analyse in Stan.\n\n\n\n\n\n\nJul 6, 2022\n\n\nJim Albert, werking HarrieJonkman\n\n\n\n\n\n\n  \n\n\n\n\nBayes Rules!\n\n\n\n\n\n\n\nBayesiaans\n\n\n\n\nAantekeningen bij een prachtig Bayesiaans studieboek\n\n\n\n\n\n\nJul 6, 2022\n\n\nHarrieJonkman\n\n\n\n\n\n\n  \n\n\n\n\nggplot stap voor stap\n\n\n\n\n\n\n\nvisualisatie\n\n\n\n\nEen GGPLOT2 handleiding om mooi te kunnen plotten in R.\n\n\n\n\n\n\nJun 27, 2022\n\n\nCédric Scherer, bewerking Harrie Jonkman\n\n\n\n\n\n\n  \n\n\n\n\nVan distill naar quarto?\n\n\n\n\n\n\n\nvisualisatie\n\n\n\n\nEen blog maken met quarto.\n\n\n\n\n\n\nMay 31, 2022\n\n\nDanielle Navarro bewerking HarrieJonkman\n\n\n\n\n\n\n  \n\n\n\n\nMissende waarden\n\n\n\n\n\n\n\ntheorie\n\n\nvoorbereiding\n\n\nmodellen\n\n\n\n\nOver het pakket mice en missende waarden\n\n\n\n\n\n\nMay 8, 2022\n\n\nMichy Allice, bewerking HarrieJonkman\n\n\n\n\n\n\n  \n\n\n\n\nKaarten maken met R\n\n\n\n\n\n\n\nvisualisatie\n\n\n\n\nHieronder een korte introductie op hoe je kaarten maakt met R, met name met het pakket sf.\n\n\n\n\n\n\nMay 5, 2022\n\n\nEuginio Petrovich, bewerking Harrie Jonkman\n\n\n\n\n\n\n  \n\n\n\n\nNaïeve Bayesiaanse classificatie\n\n\n\n\n\n\n\nBayesiaans\n\n\n\n\nIn deze blog wordt getoond hoe Naïeve Bayesiaanse classificatieanalyse werkt.\n\n\n\n\n\n\nMar 29, 2022\n\n\nHarrie Jonkman en Mr.X\n\n\n\n\n\n\n  \n\n\n\n\nWat kun je met Bayes?\n\n\n\n\n\n\n\nBayesiaans\n\n\n\n\nIn deze blog wordt getoond wat je met Bayes wetenschappelijk kunt: schatten, testen en voorspellen.\n\n\n\n\n\n\nMar 29, 2022\n\n\nJohnson e.a. en Harrie Jonkman\n\n\n\n\n\n\n  \n\n\n\n\nBayes’ principes\n\n\n\n\n\n\n\nBayesiaans\n\n\n\n\nEen blog over de principes van de Bayesiaanse theorie\n\n\n\n\n\n\nMar 7, 2022\n\n\nJohnson e.a. en Harrie Jonkman\n\n\n\n\n\n\n  \n\n\n\n\nRegressie en nog zo iets\n\n\n\n\n\n\n\nanalyse\n\n\n\n\nDit is een blog naar aanleiding van Gelman/Hill/Vehtari nieuwe boek Regresion and other stories\n\n\n\n\n\n\nFeb 10, 2022\n\n\nHarrie Jonkman\n\n\n\n\n\n\n  \n\n\n\n\nMultilevel modeling met STAN\n\n\n\n\n\n\n\nanalyse\n\n\n\n\nIntroductie op multilevel modelleren met gebruik van rstanarm: Een tutorial voor onderwijsonderzoekers\n\n\n\n\n\n\nFeb 6, 2022\n\n\nJoonHo Lee, Nicholas Sim, Feng Ji, and Sophia Rabe-Hesketh, vertaling HarrieJonkman\n\n\n\n\n\n\n  \n\n\n\n\nMultilevel modeling\n\n\n\n\n\n\n\nanalyse\n\n\n\n\nDit is een post over multilevel analyse van longitudinale data met betrekking tot alcoholgebruik van jongeren.\n\n\n\n\n\n\nJan 4, 2022\n\n\nAlexander Cernat, bewerking Harrie Jonkman\n\n\n\n\n\n\n  \n\n\n\n\nLatente Groei Modeling\n\n\n\n\n\n\n\nanalyse\n\n\n\n\nDit is een blog over Latente Groei Modeling van longitudinale data van alcoholgebruik van jongeren\n\n\n\n\n\n\nJan 3, 2022\n\n\nAlexander Cernat, bewerking Harrie Jonkman\n\n\n\n\n\n\n  \n\n\n\n\nOpschonen en ontrafelen\n\n\n\n\n\n\n\nvoorbereiding\n\n\n\n\nEen post over opschonen en ontrafelen van data\n\n\n\n\n\n\nOct 3, 2021\n\n\nBewerking Harrie Jonkman\n\n\n\n\n\n\n  \n\n\n\n\nRmarkdown en Officedown\n\n\n\n\n\n\n\ncommunicatie\n\n\n\n\nEen blog over documentatie en hoe ze dat in de farmaceutische industrie kunnen doen\n\n\n\n\n\n\nOct 1, 2021\n\n\nJakub Sobolewski, bewerking Harrie Jonkman\n\n\n\n\n\n\n  \n\n\n\n\nKaart van Zwitserland\n\n\n\n\n\n\n\nvisualisatie\n\n\n\n\nDit is een mooie blog van Giulia Ruggeri over kaarten maken met ggplot2 en sf, in dit geval van Zwitserland\n\n\n\n\n\n\nJul 21, 2021\n\n\nGiulia Ruggeri, bewerking Harrie Jonkman\n\n\n\n\n\n\n  \n\n\n\n\nVerzekeringskosten voorspellen\n\n\n\n\n\n\n\nmachine learning\n\n\n\n\nDit is de blog die Arta Seyedan op 14 februari 2021 voor R-bloggers schreef over het voorspellen van verzekeringskosten en die ik wat bewerkt en vertaald heb.\n\n\n\n\n\n\nJul 18, 2021\n\n\nArta Seyadan, bewerking Harrie Jonkman\n\n\n\n\n\n\n  \n\n\n\n\nClassificeren van Palmer penguins\n\n\n\n\n\n\n\nmachine learning\n\n\n\n\nDe laatste tijd heeft Julia Silge een aantal videoopnamen gemaakt die laten zien hoe het tidymodels raamwerk is te gebruiken.Het zijn opnamen over de eerste stappen in het modelleren tot hoe complexe modellen zijn te evalueren. Deze videoopname is goed voor mensen die net beginnen met tidymodels. Ze maakt daarbij gebruik van een #TidyTuesday dataset over pinguïns. Hier gaat het om classificeren.\n\n\n\n\n\n\nJul 18, 2021\n\n\nJulia Silge, bewerking Harrie Jonkman\n\n\n\n\n\n\n  \n\n\n\n\nClassificeren met Tidymodels\n\n\n\n\n\n\n\nmachine learning\n\n\n\n\nDit is bewerking van een blog over classificeren met het pakket Tidymodels die Rahul Raoniar in Towards data science schreef.\n\n\n\n\n\n\nJul 5, 2021\n\n\nRahul Raoniar, bewerking Harrie Jonkman\n\n\n\n\n\n\n  \n\n\n\n\nTidymodels opnieuw\n\n\n\n\n\n\n\nmachine learning\n\n\n\n\nBlog van Rebecca Barter onder de titel ‘Tidymodels: tidy machine learning in R’\n\n\n\n\n\n\nApr 22, 2021\n\n\nRebecca Barter, bewerking Harrie Jonkman\n\n\n\n\n\n\n  \n\n\n\n\nGitHub voor samenwerking\n\n\n\n\n\n\n\ncommunicatie\n\n\n\n\nLisa Lendway heeft een aantal interessante repositories op haar GitHub account staan, zie hier. Ze zijn vaak kort, maar helder en concreet. Haar stijl en de consistentie daarin bevallen mij zeer. Van haar manier van doen leer ik veel. Zij maakt haar stukken vaak voor haar statistieklessen en deelt zo haar kennis met haar studenten en anderen buiten haar klas. Ik heb mij voorgenomen om er een aantal goed te lezen, te vertalen en te bewerken waar nodig, en deze op mijn website over te nemen. Vorige maand deed ik dat al met een blof over Distill en nu een over GitHub.\n\n\n\n\n\n\nMar 10, 2021\n\n\nLisa Lendway, vertaling Harrie Jonkman\n\n\n\n\n\n\n  \n\n\n\n\nWebsite met distill\n\n\n\n\n\n\n\ncommunicatie\n\n\n\n\nWebsite met blog maken\n\n\n\n\n\n\nFeb 19, 2021\n\n\nLisa Lendway, vertaling Harrie Jonkman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-01-03-latente-groei-modeling/latente-groei-modeling.html",
    "href": "posts/2022-01-03-latente-groei-modeling/latente-groei-modeling.html",
    "title": "Latente Groei Modeling",
    "section": "",
    "text": "Longitudinale gegevens zijn zo interessant omdat ze ons in staat stellen te kijken naar verandering in de tijd, je krijgt er een beter inzicht mee in causale verbanden en je kunt gebeurtenissen en hun timing ermee verklaren. Om gebruik te maken van dit soort gegevens, moeten we verder gaan dan de klassieke statistische methoden, zoals OLS regressie en ANOVA. Dan moeten we gebruik maken van modellen die de extra complexiteit van de gegevens ook echt aan kunnen. Alexandru Cernat schreef ook hier een duidelijke blog over dat ik heb bewerkt en waarbij ik ook de alcoholdata van Singer en Willet heb gebruik.\nEen populair model voor de analyse van longitudinale gegevens is het Latente Groei Model (Latent Growth Model, LGM). Hiermee kan de verandering in de tijd worden geschat, terwijl rekening wordt gehouden met de hiërarchische aard van de gegevens (meerdere punten in de tijd die genest zijn binnen individuen). Het is vergelijkbaar met het multilevel model van verandering, maar hier wordt de schatting gedaan met behulp van het Structural Equation Modeling (SEM)-raamwerk. Dit raamwerk maakt gebruik van gegevens in het brede formaat (elke rij is een individu en de diverse metingen in de tijd verschijnen als verschillende kolommen).\nMeer in het bijzonder kan het LGM helpen\n- te begrijpen hoe verandering in de tijd verloopt;\n- verandering verklaren met behulp van tijdvariërende en tijdconstante voorspellers;\n- variantie ontleden in tussen- en binnenvariatie;\n- en het model kan makkelijk worden uitgebreid naar andere analysemodellen.\nHieronder volgt een korte inleiding op LGM, hoe de uitkomsten zijn te schatten en hoe de schattingen van verandering zijn te visualiseren.\nLaten we eerst de benodigde pakketten eens laden. We zullen tidyverse gebruiken voor het opschonen en visualiseren van de gegevens en lavaan voor het uitvoeren van de LGM in R.\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv ggplot2 3.3.6     v purrr   0.3.4\nv tibble  3.1.7     v dplyr   1.0.9\nv tidyr   1.2.0     v stringr 1.4.1\nv readr   2.1.2     v forcats 0.5.1\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'stringr' was built under R version 4.1.3\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(lavaan)\n\nWarning: package 'lavaan' was built under R version 4.1.3\n\n\nThis is lavaan 0.6-11\nlavaan is FREE software! Please report any bugs.\n\n\nLaten we, voordat we aan de LGM beginnen, eens kijken naar het soort gegevens dat we zouden willen analyseren. Hier gebruik ik alcoholdata van jongeren met de drie metingen van Singer en Willet voor die vrij toegankelijk zijn op internet.\nStel dat we geïnteresseerd zijn in hoe alcoholscore in de tijd verandert. Om het preciezer te formuleren willen laten zien hoe alcoholgebruik onder jongeren gemiddeld verandert, en tegelijk willen we een onderscheid maken tussen variatie, hoe jongeren veranderen ten opzichte van anderen. Maar tegelijk willen we ook iets zeggen over binnenvariatie en hoe jongeren veranderen ten opzichte van hun eigen gemiddelde/trend.\nLaten we eerst eens kijken hoe de gegevens eruit zien. Laten we eens kijken naar de brede gegevens, dit zijn de gegevens die gebruikt worden om LGM uit te voeren en laten we ook maar meteen het lange bestand bekijken:\n\nalcohol1 &lt;- read.table(\"https://stats.idre.ucla.edu/stat/r/examples/alda/data/alcohol1_pp.txt\", header=T, sep=\",\")\nattach(alcohol1)\n\nWe beginnen met het lange formaat, waar elke rij een combinatie is van individu en tijd. Dit is het formaat dat we nodig hebben voor visualisatie met ggplot2, en voor andere modellen (zoals het multilevel model voor verandering).\n\nhead(alcohol1)\n\n  id age coa male age_14   alcuse      peer      cpeer  ccoa\n1  1  14   1    0      0 1.732051 1.2649111  0.2469111 0.549\n2  1  15   1    0      1 2.000000 1.2649111  0.2469111 0.549\n3  1  16   1    0      2 2.000000 1.2649111  0.2469111 0.549\n4  2  14   1    1      0 0.000000 0.8944272 -0.1235728 0.549\n5  2  15   1    1      1 0.000000 0.8944272 -0.1235728 0.549\n6  2  16   1    1      2 1.000000 0.8944272 -0.1235728 0.549\n\n\nOm een idee te krijgen van wat we gaan modelleren, maken we een eenvoudige grafiek met de gemiddelde verandering in de tijd en de trend voor elk individu.\n\nggplot(alcohol1, aes(age_14, alcuse, group = id)) + \n  geom_line(alpha = 0.1) + # add individual line with transparency\n  stat_summary( # add average line\n    aes(group = 1),\n    fun = mean,\n    geom = \"line\",\n    size = 1.5,\n    color = \"red\"\n  ) +\n  theme_bw() + # nice theme\n  labs(x = \"Leeftijd\", y = \"Alcoholgebruik\") # nice labels\n\n\n\n\nWe zien hier een gemiddelde verandering in de tijd. Tegelijk willen we ook zichtbaar maken wat variatie is in de manier waarop mensen veranderen. LGM is in staat beide tegelijk te schatten!"
  },
  {
    "objectID": "posts/2022-01-03-latente-groei-modeling/latente-groei-modeling.html#wat-is-latente-groei-modellering",
    "href": "posts/2022-01-03-latente-groei-modeling/latente-groei-modeling.html#wat-is-latente-groei-modellering",
    "title": "Latente Groei Modeling",
    "section": "Wat is Latente Groei Modellering?",
    "text": "Wat is Latente Groei Modellering?\nNu we een idee hebben van de gegevens en het soort onderzoeksvragen dat we zouden kunnen hebben, kunnen we overgaan tot de uitvoering van LGM. De formule voor het LGM is eigenlijk zeer gelijkaardig aan die voor het multilevel model van verandering:\n\\(Y_j=\\alpha_0 + \\alpha_1*\\gamma_j + \\zeta_{00} + \\zeta_{11}*\\gamma_j + \\epsilon_j\\)\nWaarbij:\n\n\\(Y_j\\) is de variabele van belang (alcuse, alchoholgebruik van jongeren) die verandert in tijd, j.\n\n\\(\\alpha_0\\) is de gemiddelde waarde bij het begin van de gegevensverzameling (het beginpunt van de rode lijn hierboven).\n\n\\(\\alpha_1*\\gamma_j\\) is de gemiddelde snelheid van verandering in de tijd (de helling van de rode lijn in de grafiek hierboven). Hier is \\(gamma_j\\) gewoon een maat voor de tijd.\n\n\\(\\zeta_{00}\\) is de tussenvariatie aan het begin van de gegevens. Het vat samen hoe verschillend de individuele startpunten zijn ten opzichte van het gemiddelde startpunt.\n\n\\(\\zeta_{11}\\gamma_j\\) is de tussen variatie in de snelheid van verandering. Samenvattend hoe verschillend de individuele veranderingsversnellingen zijn ten opzichte van de gemiddelde verandering (rode lijn hierboven).\n\nDe \\(\\epsilon_j\\) is de binnenvariatie of hoeveel individuen variëren rond hun voorspelde trend. Met de onderstaande grafiek kunnen we een beter idee krijgen van de verschillende variatiebronnen:\n\n\nalcohol1 %&gt;% \n  filter(id %in% 1:2) %&gt;% # selecteer twee individuen\n  ggplot(aes(age_14, alcuse, color = id)) +\n  geom_point() + # punten voor VeerkrachtTotaal\n  geom_smooth(method = lm, se = FALSE) + # liniaire lijn\n  theme_bw() + # mooi thema\n  labs(x = \"Leeftijd\", y = \"Alcoholgebruik\") # nice labels\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nDe interne variatie wordt weergegeven door de afstand tussen de lijn en de punten. Dit wordt voor elk individu afzonderlijk gedaan (door de kleur in de grafiek). De tussenvariatie verwijst naar hoe verschillend de lijnen zijn. Dit kan zowel het beginpunt als de helling zijn.\nOmdat deze techniek met brede databestanden werkt, zetten we de data over van lang naar wijd\n\nlibrary(tidyr)\nlibrary(dplyr)\n\nalcwide&lt;-alcohol1 %&gt;%\n  select(id, alcuse, age_14) %&gt;%\n  pivot_wider(names_from = age_14, \n              values_from = alcuse) %&gt;%\n  rename(\n    \"age_14\"=\"0\",\n    \"age_15\"=\"1\",         \n    \"age_16\"=\"2\"\n    )\n\n i =~ 1*Meting.0 +  1*Meting.1 +  1*Meting.2 \n  s =~ 0*Meting.0 +  1*Meting.1 +  2*Meting.2 \n  i ~~ s\nStructural Equation Modeling heeft zijn eigen manier om deze statistische relaties weer te geven. Hieronder is afgebeeld hoe we ons het hierboven beschreven model zouden moeten voorstellen:\n\n\n\nFig.1, LGM grafisch verbeeld\n\n\nIn de figuur worden de latente variabelen voorgesteld door cirkels (de twee \\(\\eta\\)-variabelen, intercept en slope), terwijl de waargenomen variabelen worden voorgesteld door vierkanten (de vier y-variabelen). Wij krijgen ook de residuen (kleine cirkels die \\(\\epsilon\\) voorstellen). Voor de latente variabelen hebben wij gemiddelden (\\(\\alpha\\)) en varianties (\\(\\zeta\\)). Deze zijn geschat en hebben de hierboven beschreven interpretatie. De pijlen tussen de latente en de geobserveerde variabelen (die gewoon regressiehellingen of ladingen zijn) liggen van tevoren vast. Voor de latente interceptvariabele (weergegeven door \\(\\eta_0\\)) zijn de ladingen vastgesteld op 1 (daarom is er in bovenstaande formule niets vermenigvuldigd met \\(\\alpha_0\\) en \\(\\eta_{00}\\)). De ladingen voor de hellende latente variabele (weergegeven door \\(\\eta_1\\)) worden vastgesteld naar gelang van de verandering in tijd (\\(\\gamma_j\\) in bovenstaande formule). In dit geval gaat het eenvoudig van 0 naar 2. Er is ook een correlatie tussen het beginpunt en de verandering in tijd, weergegeven door de dubbele pijl \\(\\zeta_{01}\\). Dit wordt niet vaak geïnterpreteerd, maar het geeft je in feite een idee of mensen convergeren (of meer op elkaar gaan lijken in de tijd) of divergeren (meer van elkaar gaan verschillen).\nNu het technische deel duidelijk is gemaakt, kunnen we wat modelleren en meer grafieken maken!\n\n# first LGM \nmodel &lt;- 'i =~ 1*age_14 + 1*age_15 + 1*age_16\n          s =~ 0*age_14 + 1*age_15 + 2*age_16 '\n                  \n\nfit1 &lt;- growth(model, data = alcwide)\n\nsummary(fit1, standardized = TRUE)\n\nlavaan 0.6-11 ended normally after 22 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         8\n                                                      \n  Number of observations                            82\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                                 0.636\n  Degrees of freedom                                 1\n  P-value (Chi-square)                           0.425\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i =~                                                                  \n    age_14            1.000                               0.898    0.963\n    age_15            1.000                               0.898    0.862\n    age_16            1.000                               0.898    0.796\n  s =~                                                                  \n    age_14            0.000                               0.000    0.000\n    age_15            1.000                               0.484    0.464\n    age_16            2.000                               0.967    0.857\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  i ~~                                                                  \n    s                -0.187    0.102   -1.841    0.066   -0.431   -0.431\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .age_14            0.000                               0.000    0.000\n   .age_15            0.000                               0.000    0.000\n   .age_16            0.000                               0.000    0.000\n    i                 0.634    0.103    6.163    0.000    0.706    0.706\n    s                 0.277    0.062    4.481    0.000    0.573    0.573\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .age_14            0.064    0.147    0.436    0.663    0.064    0.073\n   .age_15            0.420    0.094    4.463    0.000    0.420    0.387\n   .age_16            0.280    0.180    1.556    0.120    0.280    0.220\n    i                 0.807    0.193    4.177    0.000    1.000    1.000\n    s                 0.234    0.083    2.803    0.005    1.000    1.000\n\n\nEr zijn in principe zes soorten coëfficiënten die hier interessant zijn:\n\nintercept i: de waarde 0.634 staat voor de gemiddelde verwachte alcoholgebruik aan het begin van het onderzoek voor alle respondenten wanneer ze 14 jaar zijn.\n\nslope s: de waarde 0.277 vertegenwoordigt de gemiddelde verandering voor alle respondenten. Dus bij elke meting stijgt het gebruik van alcohol met met 0.277.\n\nvariantie i: de waarde 0.807 vertegenwoordigt de tussenvariatie aan het begin van het onderzoek. Dus hoe verschillend zijn mensen vergeleken met het gemiddelde.\n\nvariantie s: de waarde 0.234 staat voor de tussenvariatie in de veranderingssnelheid. Het laat zien hoe verschillend veranderingshellingen zijn voor verschillende mensen.\n\nVariantie alcoholgebruik: de waarden tussen 0.064 en 0.420 geven de interne variatie op elk punt in de tijd weer.\n\ncorrelatie tussen i en s: de waarde -0.187 laat zien dat het alcohol niet in de tijd convergeert (althans niet significant).\n\n\nHoe kunnen we de verandering visualiseren?\nEen goede manier om te begrijpen wat je modelleert, is de voorspelde scores van het model visualiseren. We zullen het predict() commando gebruiken om een nieuw object op te slaan met de voorspelde scores op individueel niveau voor het intercept en de helling.\n\n# voorspellen van twee latente variabelen\npred_lgm &lt;- predict(fit1) \n\nDit heeft de voorspelde score voor het intercept en de helling voor elk individu:\n\nhead(pred_lgm)\n\n              i          s\n[1,] 1.67339200 0.15061147\n[2,] 0.01577357 0.41987762\n[3,] 1.03717373 0.89157072\n[4,] 0.15472328 0.80763226\n[5,] 0.01182452 0.09588422\n[6,] 2.85697940 0.05794456\n\n\nDeze zijn gebaseerd op ons model. Wij zouden dus bijvoorbeeld het gemiddelde van deze variabelen kunnen schatten en dat gemiddelde voor intercept en slope zou achtereenvolgens dezelfde resultaten moeten geven als hierboven:\n\n# gemiddelde van het intercept (eerste kolom)\nmean(pred_lgm[, 1]) \n\n[1] 0.634417\n\n\n\n# gemiddelde slope (tweede kolom)\nmean(pred_lgm[, 2])\n\n[1] 0.2773233\n\n\nOm de resultaten te plotten, willen wij deze gegevens (intercept (\\(\\zeta_0\\)) en helling (\\(\\zeta_1\\))) omzetten in verwachte scores bij elke meting (\\(\\gamma_j\\)j). We kunnen deze transformatie doen op basis van het padmodel dat we hierboven hebben gezien:\n\\(Y_1=\\eta_0 + \\eta_1\\gamma_j\\)\nVoor de eerste meting (time=0) is de verwachte waarde dus alleen het intercept (\\(\\eta_0\\)) omdat \\(\\gamma_j\\) gelijk is aan 0. Voor de meting zou de verwachte waarde het intercept (\\(\\eta_0\\)) en de helling (\\(\\eta_1\\)) zijn. Voor meting drie zou het intercept + 2*helling zijn, enzovoort.\nIn R zouden we al deze metingen met de hand kunnen berekenen of we zouden het automatisch kunnen doen met behulp van functioneel programmeren. Op basis van de bovenstaande formule kunnen we een tegenhanger in R maken:\npred_lgm[, 1] + x*pred_lgm[, 2]\nVoor de eerste meting (alc_14=0) krijgen we deze scores\n\npred_lgm[, 1] + 0*pred_lgm[, 2]\n\n [1] 1.67339200 0.01577357 1.03717373 0.15472328 0.01182452 2.85697940\n [7] 1.70002148 0.01182452 0.09353383 0.95999586 0.02431252 0.89485737\n[13] 0.01972262 2.74652531 1.69122663 0.98981031 0.07985392 1.88598973\n[19] 0.01182452 3.16706021 1.01268775 2.68643747 0.96288677 0.07985392\n[25] 2.71888080 0.08543872 2.64724315 0.01182452 0.01577357 1.03091617\n[31] 1.77626546 1.62959225 0.08543872 0.08380297 0.95604681 0.96394491\n[37] 1.09292161 0.13755300 0.01182452 0.01182452 1.25884647 0.01182452\n[43] 0.01182452 0.11987977 2.13501694 0.01740932 0.07985392 0.01182452\n[49] 0.01182452 0.01182452 0.01182452 0.07985392 0.01182452 0.01577357\n[55] 0.01182452 0.11848083 0.01182452 0.99389867 0.02227273 0.08543872\n[61] 1.92422677 0.01182452 0.01182452 0.15671367 0.08543872 1.49845628\n[67] 0.88801741 0.07985392 0.01182452 1.83223971 0.01182452 0.01182452\n[73] 0.01182452 0.13848524 0.01182452 0.01182452 0.11920222 0.01182452\n[79] 1.04572058 0.96288677 0.01182452 0.11198167\n\n\nVoor de tweede meting (alc_14=1) ziet het er zo uit\n\npred_lgm[, 1] + 1*pred_lgm[, 2]\n\n [1] 1.8240035 0.4356512 1.9287445 0.9623555 0.1077087 2.9149240 1.5604805\n [8] 0.1077087 1.3870516 1.0257079 1.1447539 1.1224615 0.7635937 2.8155774\n[15] 2.0052428 1.2209099 0.2510256 1.5773629 0.1077087 2.4340560 1.3706937\n[22] 2.1045835 1.2657784 0.2510256 2.4588992 0.7148063 2.0159411 0.1077087\n[29] 0.4356512 1.4090953 2.3055104 1.4121079 0.7148063 0.5789681 0.6977654\n[36] 1.3536504 1.9968551 1.0118258 0.1077087 0.1077087 1.3953791 0.1077087\n[43] 0.1077087 1.2942168 2.4109430 0.5714894 0.2510256 0.1077087 0.1077087\n[50] 0.1077087 0.1077087 0.2510256 0.1077087 0.4356512 0.1077087 1.1780436\n[57] 0.1077087 1.5604210 0.9753629 0.7148063 1.9775396 0.1077087 0.1077087\n[64] 1.1276442 0.7148063 2.2197859 0.5544485 0.2510256 0.1077087 1.1445052\n[71] 0.1077087 0.1077087 0.1077087 1.0892425 0.1077087 0.1077087 1.2379508\n[78] 0.1077087 1.3386955 1.2657784 0.1077087 0.6383319\n\n\nBij de derde meting ziet het alcoholgebruik er zo uit.\n\npred_lgm[, 1] + 2*pred_lgm[, 2]\n\n [1] 1.9746149 0.8555288 2.8203152 1.7699878 0.2035930 2.9728685 1.4209395\n [8] 0.2035930 2.6805694 1.0914199 2.2651952 1.3500657 1.5074647 2.8846296\n[15] 2.3192590 1.4520095 0.4221974 1.2687361 0.2035930 1.7010517 1.7286996\n[22] 1.5227296 1.5686701 0.4221974 2.1989176 1.3441739 1.3846391 0.2035930\n[29] 0.8555288 1.7872745 2.8347553 1.1946236 1.3441739 1.0741332 0.4394841\n[36] 1.7433558 2.9007885 1.8860986 0.2035930 0.2035930 1.5319118 0.2035930\n[43] 0.2035930 2.4685539 2.6868691 1.1255695 0.4221974 0.2035930 0.2035930\n[50] 0.2035930 0.2035930 0.4221974 0.2035930 0.8555288 0.2035930 2.2376064\n[57] 0.2035930 2.1269432 1.9284531 1.3441739 2.0308524 0.2035930 0.2035930\n[64] 2.0985747 1.3441739 2.9411155 0.2208797 0.4221974 0.2035930 0.4567708\n[71] 0.2035930 0.2035930 0.2035930 2.0399998 0.2035930 0.2035930 2.3566993\n[78] 0.2035930 1.6316705 1.5686701 0.2035930 1.1646821\n\n\nwaarbij x onze codering van tijd voorstelt (of \\(\\gamma_j\\)). We kunnen deze functie meerdere keren toepassen met het map() commando. De onderstaande syntaxis past deze formule toe voor de getallen 0, 1, 2 (onze codering van meting (variabele alc_14).\n\nmap(0:2, # loop over, in ons geval 1,2,3\n    function(x) pred_lgm[, 1] + x * pred_lgm[, 2]) # formule die gebruikt wordt\n\n[[1]]\n [1] 1.67339200 0.01577357 1.03717373 0.15472328 0.01182452 2.85697940\n [7] 1.70002148 0.01182452 0.09353383 0.95999586 0.02431252 0.89485737\n[13] 0.01972262 2.74652531 1.69122663 0.98981031 0.07985392 1.88598973\n[19] 0.01182452 3.16706021 1.01268775 2.68643747 0.96288677 0.07985392\n[25] 2.71888080 0.08543872 2.64724315 0.01182452 0.01577357 1.03091617\n[31] 1.77626546 1.62959225 0.08543872 0.08380297 0.95604681 0.96394491\n[37] 1.09292161 0.13755300 0.01182452 0.01182452 1.25884647 0.01182452\n[43] 0.01182452 0.11987977 2.13501694 0.01740932 0.07985392 0.01182452\n[49] 0.01182452 0.01182452 0.01182452 0.07985392 0.01182452 0.01577357\n[55] 0.01182452 0.11848083 0.01182452 0.99389867 0.02227273 0.08543872\n[61] 1.92422677 0.01182452 0.01182452 0.15671367 0.08543872 1.49845628\n[67] 0.88801741 0.07985392 0.01182452 1.83223971 0.01182452 0.01182452\n[73] 0.01182452 0.13848524 0.01182452 0.01182452 0.11920222 0.01182452\n[79] 1.04572058 0.96288677 0.01182452 0.11198167\n\n[[2]]\n [1] 1.8240035 0.4356512 1.9287445 0.9623555 0.1077087 2.9149240 1.5604805\n [8] 0.1077087 1.3870516 1.0257079 1.1447539 1.1224615 0.7635937 2.8155774\n[15] 2.0052428 1.2209099 0.2510256 1.5773629 0.1077087 2.4340560 1.3706937\n[22] 2.1045835 1.2657784 0.2510256 2.4588992 0.7148063 2.0159411 0.1077087\n[29] 0.4356512 1.4090953 2.3055104 1.4121079 0.7148063 0.5789681 0.6977654\n[36] 1.3536504 1.9968551 1.0118258 0.1077087 0.1077087 1.3953791 0.1077087\n[43] 0.1077087 1.2942168 2.4109430 0.5714894 0.2510256 0.1077087 0.1077087\n[50] 0.1077087 0.1077087 0.2510256 0.1077087 0.4356512 0.1077087 1.1780436\n[57] 0.1077087 1.5604210 0.9753629 0.7148063 1.9775396 0.1077087 0.1077087\n[64] 1.1276442 0.7148063 2.2197859 0.5544485 0.2510256 0.1077087 1.1445052\n[71] 0.1077087 0.1077087 0.1077087 1.0892425 0.1077087 0.1077087 1.2379508\n[78] 0.1077087 1.3386955 1.2657784 0.1077087 0.6383319\n\n[[3]]\n [1] 1.9746149 0.8555288 2.8203152 1.7699878 0.2035930 2.9728685 1.4209395\n [8] 0.2035930 2.6805694 1.0914199 2.2651952 1.3500657 1.5074647 2.8846296\n[15] 2.3192590 1.4520095 0.4221974 1.2687361 0.2035930 1.7010517 1.7286996\n[22] 1.5227296 1.5686701 0.4221974 2.1989176 1.3441739 1.3846391 0.2035930\n[29] 0.8555288 1.7872745 2.8347553 1.1946236 1.3441739 1.0741332 0.4394841\n[36] 1.7433558 2.9007885 1.8860986 0.2035930 0.2035930 1.5319118 0.2035930\n[43] 0.2035930 2.4685539 2.6868691 1.1255695 0.4221974 0.2035930 0.2035930\n[50] 0.2035930 0.2035930 0.4221974 0.2035930 0.8555288 0.2035930 2.2376064\n[57] 0.2035930 2.1269432 1.9284531 1.3441739 2.0308524 0.2035930 0.2035930\n[64] 2.0985747 1.3441739 2.9411155 0.2208797 0.4221974 0.2035930 0.4567708\n[71] 0.2035930 0.2035930 0.2035930 2.0399998 0.2035930 0.2035930 2.3566993\n[78] 0.2035930 1.6316705 1.5686701 0.2035930 1.1646821\n\n\n\n\nConclusies\nHopelijk geeft dit je een idee over wat LGM is, hoe je het kan schatten in R en hoe je deze verandering kan visualiseren.\n[Met dank aan Alexandru Cernat}(https://www.alexcernat.com/estimating-and-visualizing-change-in-time-using-latent-growth-models-with-r/)"
  },
  {
    "objectID": "posts/2022-01-03-multilevel-modeling/multilevel-modeling.html",
    "href": "posts/2022-01-03-multilevel-modeling/multilevel-modeling.html",
    "title": "Multilevel modeling",
    "section": "",
    "text": "Longitudinale gegevens zijn heel boeiend omdat je ermee kunt kijken naar verandering in de tijd, een beter begrip krijgt van causale verbanden en gebeurtenissen en hun timing ermee kunt verklaren. Om dit te kunnen doen, moeten we verder gaan dan de klassieke statistische methoden, zoals OLS regressie en ANOVA, en modellen gebruiken die beter kunnen omgaan met complexiteit van de gegevens. Alexander Cernat schreef er een blog over hier die ik hier in het Nederlands overzet en waarbij ik alcoholdata van Willet en Singer gebruik/\nEen populair model voor de analyse van longitudinale gegevens is het Multilevel Model voor Verandering; MultiLevel Model for Change (MLMC). Dit model maakt de schatting van verandering in de tijd mogelijk, terwijl rekening wordt gehouden met de hiërarchische aard van de gegevens (meerdere punten in de tijd genest binnen individuen). Het is vergelijkbaar met het Latente Groei Model; Latent Growth Model zie deze post, maar hier wordt geschat met behulp van het multilevel model raamwerk (ook bekend als hiërarchische modellering of random effecten). Deze techniek maakt gebruik van het lange dataformaat (elke rij is is een rij gegevens op een specifiek tijdstip voor een individu).\nMeer in het bijzonder kan het MLMC helpen:\n- te begrijpen hoe de individuele en geaggregeerde verandering in de tijd verlopen;\n- verandering te verklaren met behulp van tijdsvariërende (bv. tijd) en tijdsconstante (b.v. geslacht) voorspellers;\n- variantie te ontleden in tussen- en binnen- variatie;\n- gemakkelijk om te gaan met continue tijd, onevenwichtige gegevens (niet alle individuen zijn op alle tijdstippen aanwezig) en verschillende timings (niet iedereen geeft gegevens op precies hetzelfde moment).\nHier volgt een korte inleiding op MLMC, hoe hiermee te werken in R en hoe veranderingen zijn te visualiseren.\nLaten we eerst de R-pakketten laden (deze moeten dus wel geïnstalleerd zijn). We zullen tidyverse gebruiken voor het opschonen en visualiseren van de data, lme4 voor het uitvoeren van de MLMC in R en sjstats voor het schatten van intra class correlation (icc)\nJe kunt pakketten installeren met het install.packages() commando.\nLaten we vervolgens de pakketten binnenhalen die wij bij deze analyse zullen gebruiken.\n\n# pakket voor dataopschonen en visualiseren\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv ggplot2 3.3.6     v purrr   0.3.4\nv tibble  3.1.7     v dplyr   1.0.9\nv tidyr   1.2.0     v stringr 1.4.1\nv readr   2.1.2     v forcats 0.5.1\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'stringr' was built under R version 4.1.3\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\n# pakket voor multilevel-modeleren\nlibrary(lme4)\n\nWarning: package 'lme4' was built under R version 4.1.3\n\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n# pakket om intra class correlation makkelijk vast te stellen\nlibrary(sjstats)\n\nLaten we, voordat we aan de MLMC beginnen, eerst kijken naar de data die we willen analyseren. Hier gebruik ik alcuse (alcoholgebruik) met drie metingen. Dit is een longitudinale dataset van 82 jongeren.\nWe willen weten hoe alcoholgebruik in de tijd verandert en we willen die verandering begrijpen. Hierbij maken we een onderscheid tussen tussenvariatie (hoe de jongeren ten opzichte van elkaar veranderen) en binnenvariatie (hoe jongeren veranderen ten opzichte van hun eigen gemiddelde/trend).\nLaten we deze gegevens eens onderzoeken. We kijken hiervoor naar de gegevens in lang formaat, die we zullen gebruiken voor de modellering en de grafieken. Hieronder zie je de eerste tien gegevens:\n\nalcohol1 &lt;- read.table(\"https://stats.idre.ucla.edu/stat/r/examples/alda/data/alcohol1_pp.txt\", header=T, sep=\",\")\nattach(alcohol1)\nhead(alcohol1, n=10)\n\n   id age coa male age_14   alcuse      peer      cpeer  ccoa\n1   1  14   1    0      0 1.732051 1.2649111  0.2469111 0.549\n2   1  15   1    0      1 2.000000 1.2649111  0.2469111 0.549\n3   1  16   1    0      2 2.000000 1.2649111  0.2469111 0.549\n4   2  14   1    1      0 0.000000 0.8944272 -0.1235728 0.549\n5   2  15   1    1      1 0.000000 0.8944272 -0.1235728 0.549\n6   2  16   1    1      2 1.000000 0.8944272 -0.1235728 0.549\n7   3  14   1    1      0 1.000000 0.8944272 -0.1235728 0.549\n8   3  15   1    1      1 2.000000 0.8944272 -0.1235728 0.549\n9   3  16   1    1      2 3.316625 0.8944272 -0.1235728 0.549\n10  4  14   1    1      0 0.000000 1.7888544  0.7708544 0.549\n\n\nWe zien dat elke rij een combinatie is van een jongere (variabele id (van 1 tot en met 4, die we in de analyse gebruiken)) en tijd (variabele age_14, alcoholgebruik op 14-jarige leeftijd). Dit is ook het formaat dat we nodig hebben voor een visualisatie met ggplot2.\nOm te zien wat we gaan modelleren, kunnen we een eenvoudige grafiek maken met een gemiddelde veranderingslijn in tijd voor de hele dataset en een wirwar van lijnen voor de verandering van elk individu:\n\nggplot(alcohol1, aes(age, alcuse, group = id)) +\n  geom_line(alpha = 0.1) + # voeg individuele lijn met transparantie toe\n  stat_summary( # voeg gemiddelde lijn toe\n    aes(group = 1),\n    fun = mean,\n    geom = \"line\",\n    size = 1.0,\n    color = \"red\"\n  ) +\n  theme_bw() + # goed theme voor visualisatie\n  labs(x = \"Leeftijd\", y = \"Alcoholgebrui\") # de labels\n\n\n\n\nWe zien dus een hele lichte positieve en constante verandering in de tijd, maar vooral ook heel wat variatie in de manier waarop jongeren alcohol gebruiken (grijze lijnen). MLMC is in staat om beide dingen (het structurele en individuele) tegelijk te schatten!"
  },
  {
    "objectID": "posts/2022-01-03-multilevel-modeling/multilevel-modeling.html#wat-is-multilevel-modellering",
    "href": "posts/2022-01-03-multilevel-modeling/multilevel-modeling.html#wat-is-multilevel-modellering",
    "title": "Multilevel modeling",
    "section": "Wat is multilevel modellering?",
    "text": "Wat is multilevel modellering?\nMultilevel modellering is een uitbreiding van regressie modellering (‘just regression’) waarin we als het ware verschillende bronnen van variatie uit elkaar trekken. Waarom is dit belangrijk?\nTraditioneel gaat OLS-regressie er vanuit dat alle gevallen onafhankelijk zijn. Dit impliceert dat er geen correlatie is tussen de geobserveerde metingen als gevolg van zaken als clustering. Dit is vaak niet waar bij sociaal-wetenschappelijke gegevens. Individuen zijn bijvoorbeeld genest in huishoudens, klassen, buurten, regio’s en landen. Studenten zijn genest in klassen, scholen, regio’s en landen. Deze geneste structuur zorgt ervoor dat individuen op elkaar lijken. Zo zullen de gezondheidsuitkomsten waarschijnlijk vergelijkbaar zijn voor mensen die in dezelfde buurt wonen, vanwege bijvoorbeeld zelfselectie (vergelijkbaar inkomen en opleiding), kwaliteit van de gezondheidszorg of luchtkwaliteit. Als dit waar is, dan kunnen we mensen die uit dezelfde buurt komen niet als onafhankelijk behandelen.\nMultilevel modellering lost dit probleem op door willekeurige effecten (d.w.z. variatie) te schatten voor de verschillende niveaus die in de gegevens aanwezig zijn. Op die manier worden de regressiecoëfficiënten (zoals standaardfouten) gecorrigeerd voor deze geneste structuur. Bovendien kan met dit soort modellen worden geschat hoeveel variatie van elk niveau afkomstig is. Dit kan zeer informatief zijn. Door bijvoorbeeld de variatie van de leerlingresultaten uit elkaar te halen op leerlingniveau, klasniveau en schoolniveau, kunnen we begrijpen wat de belangrijkste factoren zijn die de uitkomsten van de leerlingen beïnvloeden. Dit kan informatie opleveren voor theorie en beleid.\n\nMultilevel modellering en longitudinale data\nLongitudinale data zijn ook genest. Metingen op verschillende tijdstippen zijn genest binnen een individu. Deze waarnemingen zijn niet onafhankelijk, omdat stabiele individuele kenmerken (zoals genen, persoonlijkheid of context) leiden tot consistente uitkomsten binnen de individuen en over de tijd. Multilevel modellering kan dus helpen corrigeren voor dit geklusterd karakter, maar helpt ons ook om twee belangrijke bronnen van variatie in longitudinale gegevens uit elkaar te halen: tussen-variatie (tussen individuen) en binnen-variatie (binnen het individu).\nTussen-variatie heeft betrekking op de manier waarop individuen van elkaar verschillen wat de belangrijke uitkomst betreft (bv. in dit geval dat het ene individu gemiddeld een hoger alcuse score heeft dan het andere individu). Terwijl binnen-variatie, de tweede vorm van variatie, betrekking heeft op de manier waarop alcuse op een bepaalde meting verschilt van het individuele gemiddelde (bv. hebben zij een lager of hoger score op alcoholgebruik in vergelijking met hun normale inkomen).\nLaten we, om deze vormen van variatie beter te begrijpen, eens naar de onderstaande grafieken kijken (een voor individu met idn=1 en de ander met idn=10).\n\nggplot(alcohol1, aes(age_14, alcuse)) +\n  stat_summary( # voeg gemiddelde lijn toe\n    aes(id = 1),\n    fun = mean,\n    geom = \"line\",\n    size = 1.0,\n    color = \"red\"\n  ) +\n  theme_bw() + # nice theme\n  labs(x = \"Leeftijd\", y = \"Alcoholgebruik_14\") # nice labels\n\nWarning: Ignoring unknown aesthetics: id\n\n\n\n\n\n\nggplot(alcohol1, aes(age_14, alcuse)) +\n  aes(id=10) + \n  stat_summary( # voeg gemiddelde lijn toe, Harrie: deze klopt nog niet\n    fun = mean,\n    geom = \"line\",\n    size = 1.0,\n    color = \"blue\"\n  ) +\n  theme_bw() + # nice theme\n  labs(x = \"Leeftijd\", y = \"Alcoholgebruik_14\") # nice labels\n\n\n\n\nHier zien we de twee alcuse-scores van twee individuen en de individuele trends over de tijd (drie metingen). We kunnen de variatie tussen de lijnen zien als het verschil tussen de lijnen. Dit vertelt ons of de trend voor het ene individu anders is dan voor het andere. Als deze bron van variatie 0 zou zijn, dan zouden alle lijnen gelijk zijn. Als individuen zeer verschillende lijnen hebben, dan zal dit een belangrijke bron van variatie zijn. Met interne variatie wordt het verschil bedoeld tussen de individuele trend en de bijbehorende waargenomen scores. Als deze bron van variatie 0 zou zijn, zouden we alcuse bij elke meting perfect voorspellen en zou er geen interne variatie zijn. variatie zijn. Hoe groter deze bron van variatie, hoe meer de individuele waarden buiten hun eigen trend schommelen.\n\n\nHet onvoorwaardelijke verandermodel (ook wel random effects genoemd)\nNu we wat basiskennis hebben van multilevel modellering en longitudinale gegevens kunnen we het uitproberen. Wij gaan gewoonlijk uit van een eenvoudig model dat enkel de binnen- en tussenvariatie wil scheiden. We kunnen het model definiëren als:\n\\(Y_ij=\\gamma_{00}+u_{01}+e_{ij}\\)\nWaarbij:\n\n\\(Y_{ij}\\) is de variabele van belang (bijvoorbeeld alcuse) en varieert per individu (i) en tijd (j)\n\n\\(\\gamma_{00}\\) is het intercept in de regressie. We kunnen dit interpreteren als het grote gemiddelde of het gemiddelde van de uitkomst over alle individuen en tijdstippen.\n\n\\(u_{0i}\\) is de tussenvariatie en vertelt ons hoe verschillend jongeren van elkaar zijn in hun alcoholgebruik-score. Als iedereen hetzelfde scoort op alcuse, zou dit 0 zijn. Hoe groter de verschillen, hoe groter deze coëfficiënt zal zijn en dus de variatie is.\n\n\\(e_{ij}\\) dit is het residu, maar heeft ook hier de interpretatie van binnenvariatie en vertelt ons hoeveel elk individu varieert rond zijn eigen gemiddelde. Hoe groter deze coëfficiënt, hoe meer individuen in hun uitkomsten op alcoholgebruik schommelen.\n\nNu we weten wat we willen modelleren, laten we eens kijken hoe we dat kunnen doen in R en met het pakket lme4. Voor het schatten van multilevel modellen zullen we het lmer()commando gebruiken. We moeten de data en de formule opgeven. Onze uitkomst is alcuse, dus dat staat aan de linkerkant van ~. Aan de rechterkant hebben we “1”, dat staat voor het intercept. Tussen haakjes definiëren we de willekeurige effecten. Hier zeggen we dat we het intercept (“1”) willen laten variëren per individu (| id). Dit alles leidt tot deze syntaxis:\n\n# random intercept model \nm0 &lt;- lmer(data = alcohol1, alcuse ~ 1 + (1 | id))\n\n# resultaten checken\nsummary(m0)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: alcuse ~ 1 + (1 | id)\n   Data: alcohol1\n\nREML criterion at convergence: 673\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.8892 -0.3079 -0.3029  0.6111  2.8562 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n id       (Intercept) 0.5731   0.7571  \n Residual             0.5617   0.7495  \nNumber of obs: 246, groups:  id, 82\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   0.9220     0.0963   9.574\n\n\nLaten we de belangrijkste coëfficienten eens interpreteren:\n\nonder “Fixed effects” hebben we het “(Intercept)”, dat het grote gemiddelde is (\\(\\gamma_{00}\\) en ons vertelt dat over alle tijdstippen en individuen de gemiddelde score 0.922 is.\n\n“Random effects” vertegenwoordigt alles dat varieert met id, de tussen-variatie. In dit geval is de tussenvariatie voor het intercept (\\(\\beta_{0i}\\)) 0.573\n\nonder “Random effects” vertegenwoordigt de “Residual”-coëfficiënt de binnen-variatie. In dit geval is de binnen variatie voor het intercept \\(e_{ij}\\) 0.562\n\nOm de tussen en binnen-variantie beter te begrijpen, berekenen we InterClass Coefficient (ICC), dat is de tussen-variatie gedeeld door de totale variatie (tussen en binnen-variatie opgeteld). Dit berekenen we met het sjstats-pakket waarmee we het percentage tussenvariatie berekenen van het m0-model (51%), het percentage variatie dat door de groep kan worden verklaard.\n\nicc(m0)\n\nWarning: 'icc' is deprecated.\nUse 'performance::icc()' instead.\nSee help(\"Deprecated\")\n\n\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.505\n  Unadjusted ICC: 0.505\n\n\nDit is dus gewoon een verhouding van de tussen-variatie op de totale variatie en vertelt ons welk deel van de variatie tussen individuen is. In ons geval blijkt dat ongeveer 51% van de variatie in alcuse tussen jongeren komt, terwijl de resterende (~ 49%) binnen jongeren ligt. In wezen zou dit erop wijzen dat de verschillen op alcoholgebruik tussen jongeren net iets belangrijker zijn dan de verschillen op alcoholgebruik binnen jongeren.\nOm beter te begrijpen wat het model doet, kunnen we de scores voorspellen en een grafiek maken met de voorspelde individuele score (lijnen) en de waargenomen scores (punten) voor vijf individuen:\n\n# basis voor voorspelling\nalcohol1$pred_m0 &lt;- predict(m0)\n\n\nalcohol1 %&gt;% \n  filter(id %in% 1:5) %&gt;% # haal er vijf jongeren uit, kunnen ook andere jongeren zijn\n  ggplot(aes(age, pred_m0, color = id)) +\n  geom_point(aes(age, alcuse)) + # punten voor de geobserveerde alcgebruik-Totaal-scores\n  geom_smooth(method = lm, se = FALSE) + # liniaire lijn voor voorspellen\n  theme_bw() + # mooi thema\n  labs(x = \"Leeftijd\", y = \"Alcoholgebruik\") + # Labels toevoegen\n  theme(legend.position = \"none\") # geen legenda\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\n\nHet model van onvoorwaardelijke verandering\nHet vorige model is nuttig om ons een idee te geven van hoeveel variatie we op elk niveau hebben, maar we willen ook kijken naar verandering in de tijd! Laten we het model dus uitbreiden:\n\\(Y_{ij}=\\gamma_{00}+ \\gamma_{10}*Meting_{ij}+ u_{0i}+e_{ij}\\)\nIn dit model voegen we tijd (metingen van veertienjarige leeftijd gemeten) toe als voorspeller. Nu stelt \\(\\gamma_{00}\\) de gemiddelde score op alcoholgebruik voor wanneer de tijd 0 (alcoholgebruik op veertien jarige leeftijd) is, terwijl \\(\\gamma_{10}\\) het veranderingspercentage van alcoholgebruik voorstelt wanneer de tijd met 1 toeneemt.\nOm de zaken gemakkelijker te interpreteren, is het belangrijk te beginnen bij 0 zodat de \\(\\gamma_{00}\\) de mooie interpretatie heeft van verwacht alcoholgebruik aan het begin van de studie. De tijdsvariabele ‘age_14’ (alcoholgebruik op 14-jarige leeftijd) kent al de waarden 0, 1 en 2\nNu kunnen we ons model uitvoeren. We voegen gewoon de nieuwe tijdsvariabele toe als een fixed effect:\n\n# unconditionele veranderings model nu (a.k.a. MLMC)\nm1 &lt;- lmer(data = alcohol1, alcuse ~ 1 + age_14 + (1 | id))\nsummary(m1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: alcuse ~ 1 + age_14 + (1 | id)\n   Data: alcohol1\n\nREML criterion at convergence: 654.1\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.19816 -0.66940  0.03001  0.44728  2.66167 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n id       (Intercept) 0.5966   0.7724  \n Residual             0.4915   0.7011  \nNumber of obs: 246, groups:  id, 82\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  0.65130    0.11077   5.880\nage_14       0.27065    0.05474   4.944\n\nCorrelation of Fixed Effects:\n       (Intr)\nage_14 -0.494\n\n\nEen belangrijk verschil met het vorig model zit in het fixed effects-deel. Nu interpreteren wij het intercept (0.651) als het verwachte alcoholgebruik-score aan het begin van de studie (wanneer ze veertien jaar zijn, age_14=0). Het effect van een jaar extra, 0.271, vertelt ons de gemiddelde veranderingssnelheid bij ieder van de twee metingen. De score op alcoholgebruik neemt dus elk jaar met 0.271 toe.\nLaten we de resultaten opnieuw visualiseren op basis van het nieuwe model:\n\n# De basis voor voorspelling van dit model\nalcohol1$pred_m1 &lt;- predict(m1)\n\n\nalcohol1 %&gt;% \n  filter(id %in% 1:5) %&gt;% # selecteer weer 5 individuen\n  ggplot(aes(age_14, pred_m1, color = id)) +\n  geom_point(aes(age_14, alcuse)) + # observatiepunten voor VeerkrachtTotaal\n  geom_smooth(method = lm, se = FALSE) + # lineaire lijn gebaseerd op voorspelling\n  theme_bw() + # mooi thema\n  labs(x = \"Leeftijd\", y = \"Alcoholgebruik\") + # goede labels\n  theme(legend.position = \"none\") # legenda verstoppen\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nNu zien we dat we twee vormen van tussenvariatie hebben. De coëfficiënt \\(u_{0i}\\) vertegenwoordigt de tussenvariatie aan het begin van het onderzoek terwijl \\(u_{1i}\\) de tussenvariatie in het tempo van de verandering vertegenwoordigt. Dit betekent dat we toestaan dat individuen verschillende alcoholgebruik-scores aan het begin hebben (age_14=0), maar ook verschillende trends laten zien.\nDit model willen we met lme4 uitvoeren. We kunnen eenvoudigweg “meting” of in dit geval “jaartal” toevoegen aan het willekeurige deel van het model:\nWe zien nu dus een positieve trend die te wijten is aan de tijdscoëfficiënt. De individuele lijnen lopen, algemeen gezegd, hier parallel aan. We nemen nu aan dat de verandering in de tijd voor alle individuen gelijk is. In meer technische termen nemen we aan dat er geen tussenvariatie is in de snelheid van verandering. Dat is een vrij sterke veronderstelling. In ons geval zouden we dat niet verwachten gezien de eerste grafiek die we hebben gemaakt (en die individuele lijnen die verschillende kanten opgingen). Laten we het model dus uitbreiden met de tussenvariatie in de veranderingssnelheid:\n\\(Y_{ij}=\\gamma_{00}+\\gamma_{10}*Meting_{ij}+u_{0i}+u_{1i}*Meting_{ij}+e{ij}\\)\nNu zien we dat we twee bronnen van tussenvariatie hebben. De coëfficiënt \\(u_0i\\) vertegenwoordigt de tussenvariatie aan het begin van de studie terwijl \\(u_1i\\) de tussenvariatie in het tempo van verandering vertegenwoordigt. Dit betekent dat we toestaan dat individuen verschillen in alcoholgebruik aan het begin, maar ook verschillende trends kunnen kunnen laten zien.\nOm een dergelijk model in lme4 uit te voeren. We kunnen eenvoudigweg “tijd” (age_14) toevoegen aan het willekeurige deel van het model:\n\n# unconditional change model (a.k.a. MLMC) with re for change\nm2 &lt;- lmer(data = alcohol1, alcuse ~ 1 + age_14 + (1 + age_14 | id))\nsummary(m2)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: alcuse ~ 1 + age_14 + (1 + age_14 | id)\n   Data: alcohol1\n\nREML criterion at convergence: 643.2\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.48287 -0.37933 -0.07858  0.38876  2.49284 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n id       (Intercept) 0.6355   0.7972        \n          age_14      0.1552   0.3939   -0.23\n Residual             0.3373   0.5808        \nNumber of obs: 246, groups:  id, 82\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  0.65130    0.10573   6.160\nage_14       0.27065    0.06284   4.307\n\nCorrelation of Fixed Effects:\n       (Intr)\nage_14 -0.441\n\n\nIn de resultaten zien we dat het random deel van het model nu twee coëfficiënten heeft die variëren naar id. Het “(Intercept)”, 0.65130, staat voor de variatie tussen het beginpunt van het onderzoek (\\(\\gamma_{0i})\\), terwijl de coëfficiënt voor age_14, 0.271, staat voor de variatie tussen de veranderingssnelheden ( \\(\\gamma_{1i}\\) ).\nAls wij nu de voorspellingen onderzoeken, zien wij dat individuen zowel verschillende beginpunten als verschillende trends mogen hebben:\n\n# laten we basis voor voorspelling definieren\nalcohol1$pred_m2 &lt;- predict(m2)\n\n\nalcohol1 %&gt;% \n  filter(id %in% 1:5) %&gt;% # selecteer enkele individuen\n  ggplot(aes(age_14, pred_m2, color = id)) +\n  geom_point(aes(age_14, alcuse)) + # punten van geobserveerde VeerkrachtTotaal\n  geom_smooth(method = lm, se = FALSE) + # liniaire lijn voor voorspelling\n  theme_bw() + # mooi thema\n  labs(x = \"Leeftijd\", y = \"Alcoholgebruik\") + # goede labels\n  theme(legend.position = \"none\") # legenda verbergen\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nHoe groter de \\(e_{0i}\\)-coëfficiënt, hoe groter het verschil tussen de mensen aan het begin van het onderzoek, terwijl een grotere \\(e_{1i}\\) op meer uiteenlopende veranderingssnelheden wijst.\n\n\nConclusies\nDit geeft een idee wat multilevel model voor verandering ons in onderzoek kan bieden, hoe je het kunt schatten in R en hoe je deze verandering kunt visualiseren. Dit model is vergelijkbaar met het Latent Growth Model (Latent Groei Model) en daar zal ik ook een stukje over schrijven.\nMet dank aan Alexander Cernat"
  },
  {
    "objectID": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#data-voorbeeld",
    "href": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#data-voorbeeld",
    "title": "Multilevel modeling met STAN",
    "section": "Data voorbeeld",
    "text": "Data voorbeeld\nZe analyseren de Gcsemv dataset (Rasbash et al. 2000) uit het mlmRev pakket in R. De gegevens omvatten de GCSE-examenscores (General Certificate of Secondary Education) van 1.905 leerlingen van 73 scholen in Engeland op een natuurwetenschappelijk vak. De Gcsemv-dataset bestaat uit de volgende 5 variabelen:\n\nschool: schoolidentificatiecode\n\nstudent: identificatiecode student\n\ngender: geslacht van een leerling (M: Man, F: Vrouw)\n\nwritten: totaalscore op schriftelijk werkstuk\ncourse: totaalscore op schriftelijk werkstuk\n\n\n# Use example dataset from mlmRev package: GCSE exam score\ndata(Gcsemv, package = \"mlmRev\")\nsummary(Gcsemv)\n\n     school        student     gender      written          course      \n 68137  : 104   77     :  14   F:1128   Min.   : 0.60   Min.   :  9.25  \n 68411  :  84   83     :  14   M: 777   1st Qu.:37.00   1st Qu.: 62.90  \n 68107  :  79   53     :  13            Median :46.00   Median : 75.90  \n 68809  :  73   66     :  13            Mean   :46.37   Mean   : 73.39  \n 22520  :  65   27     :  12            3rd Qu.:55.00   3rd Qu.: 86.10  \n 60457  :  54   110    :  12            Max.   :90.00   Max.   :100.00  \n (Other):1446   (Other):1827            NA's   :202     NA's   :180     \n\n\nTwee onderdelen van het examen werden geregistreerd als uitkomstvariabelen: schriftelijk werkstuk (written) en cursuswerkstuk (course). In deze tutorial wordt alleen de totaalscore op het cursuswerkstuk (course) geanalyseerd. Zoals hierboven te zien is, ontbreken er bij sommige waarnemingen waarden voor bepaalde covariaten. Hoewel we de data niet subsetten om alleen volledige gevallen op te nemen om aan te tonen dat rstanarm deze waarnemingen automatisch laat vallen, is het over het algemeen een goed gebruik om dit handmatig te doen indien nodig.\n\n# Maak Male dereferentiecategorie en hernoem de variabele\nGcsemv$female &lt;- relevel(Gcsemv$gender, \"M\")\n\n\n# Gebruik alleen de totaalscore op course \nGCSE &lt;- subset(x = Gcsemv, \n               select = c(school, student, female, course))\n\n# Tel unieke scholen en studenten\nJ &lt;- length(unique(GCSE$school))\nN &lt;- nrow(GCSE)\n\nHet pakket rstanarm automatiseert verschillende stappen van datavoorbewerking, waardoor het gebruik ervan sterk lijkt op dat van lme4 en wel op de volgende manier.\n\nInput - rstanarm kan een dataframe als input nemen2.\nOntbrekende gegevens - rstanarm verwijdert automatisch waarnemingen met NA waarden voor elke variabele gebruikt in het model3.\nIdentifiers - rstanarm vereist niet dat identifiers opeenvolgend zijn4. We stellen voor dat het een goede gewoonte is om alle cluster- en unit-identifiers, evenals categorische variabelen als factoren op te slaan. Dit geldt evenzeer voor lme4 als voor rstanarm. Men kan de structuur van de variabelen controleren met behulp van de str() functie.\n\n\n# Check structure of data frame\nstr(GCSE)\n\n'data.frame':   1905 obs. of  4 variables:\n $ school : Factor w/ 73 levels \"20920\",\"22520\",..: 1 1 1 1 1 1 1 1 1 2 ...\n $ student: Factor w/ 649 levels \"1\",\"2\",\"3\",\"4\",..: 16 25 27 31 42 62 101 113 146 1 ...\n $ female : Factor w/ 2 levels \"M\",\"F\": 1 2 2 2 1 2 2 1 1 2 ...\n $ course : num  NA 71.2 76.8 87.9 44.4 NA 89.8 17.5 32.4 84.2 ..."
  },
  {
    "objectID": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#model-1-variërend-intercept-model-zonder-voorspellers-variantiecomponentenmodel",
    "href": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#model-1-variërend-intercept-model-zonder-voorspellers-variantiecomponentenmodel",
    "title": "Multilevel modeling met STAN",
    "section": "Model 1: Variërend intercept model zonder voorspellers (Variantiecomponentenmodel)",
    "text": "Model 1: Variërend intercept model zonder voorspellers (Variantiecomponentenmodel)\nBeschouw het eenvoudigste multilevel model voor leerlingen \\(i=1,...,n\\) genest binnen scholen \\(j=1,...,J\\) en voor wie we examenresultaten als respons hebben. We kunnen een variabel interceptmodel met twee niveaus zonder voorspellers schrijven met de gebruikelijke tweedelige formulering als\n\\[Y_{ij}=\\alpha_{j} + \\epsilon_{ij}, \\text{ where } \\epsilon{ij} \\sim N(0,\\sigma^2_{y}) \\] \\[\\alpha{_j}=u_{\\alpha} + u_{j}, \\text { where } \\epsilon{ij} \\sim N(0,\\sigma^2_{y})\\]\nwaarin \\(yij\\) de examenscore is voor de \\(ith\\) leerling op de \\(jth\\) school, \\(\\alpha_{j}\\) de variërende intercept voor de jde school, en \\(u_{a}\\) het algemene gemiddelde voor alle scholen. Als alternatief kan het model in verkorte vorm worden uitgedrukt als\n\\[y_{ij}=u_{a} + u_{j} = \\epsilon_{ij}\\] Als we verder aannemen dat de fouten op leerlingniveau \\(\\epsilon_{ij}\\) normaal verdeeld zijn met gemiddelde 0 en variantie \\(\\sigma^2_{y}\\), en dat de variërende intercepten op schoolniveau \\(\\alpha_j\\) normaal verdeeld zijn met gemiddelde \\(u_{a}\\) en variantie \\(\\sigma^2_{a}\\), dan kan het model worden uitgedrukt als\n\\[y_{ij}∼ N(\\alpha_{j},\\sigma^2_{y})\\]\n\\[a_{j}∼ N(u_{a},\\sigma^2_{a})\\]\nDit model kan dan worden aangepast met lmer(). We specificeren een intercept (de voorspeller “1”) en laten deze variëren met de niveau-2-identifier (school). We specificeren ook de REML = FALSE optie om maximum likelihood (ML) schattingen te krijgen in plaats van de standaard restricted maximum likelihood (REML) schattingen.\n\nM1 &lt;- lmer(formula = course ~ 1 + (1 | school), \n           data = GCSE, \n           REML = FALSE)\nsummary(M1)\n\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: course ~ 1 + (1 | school)\n   Data: GCSE\n\n     AIC      BIC   logLik deviance df.resid \n 14111.4  14127.7  -7052.7  14105.4     1722 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.9693 -0.5101  0.1116  0.6741  2.7613 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n school   (Intercept)  75.24    8.674  \n Residual             190.77   13.812  \nNumber of obs: 1725, groups:  school, 73\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)    73.72       1.11    66.4\n\n\nOnder Fixed effects zien we dat het intercept \\(u_{a}\\), gemiddeld over de populatie van scholen, wordt geschat op 73,72. Onder Random-effects zien we dat de standaardafwijking tussen de scholen \\(\\sigma_{a}\\) wordt geschat op \\(8.67\\) en de standaardafwijking binnen de scholen \\(\\sigma_{y}\\) op \\(13.81\\)."
  },
  {
    "objectID": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#model-2-variërend-intercept-model-met-een-enkele-voorspeller",
    "href": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#model-2-variërend-intercept-model-met-een-enkele-voorspeller",
    "title": "Multilevel modeling met STAN",
    "section": "Model 2: Variërend intercept model met een enkele voorspeller",
    "text": "Model 2: Variërend intercept model met een enkele voorspeller\nHet variërende interceptmodel5 met een indicatorvariabele voor het vrouwzijn xij kan worden geschreven als\n\\[Y_{ij}∼N(a_{j}+\\beta x{ij}, \\sigma^2_{y}),\\]\n\\[a_{j}∼N(u_{a}, \\sigma^2_{a})\\].\nDe vergelijking van de gemiddelde regressielijn voor alle scholen is \\(u_{ij}=u_{α}+βxij\\). De regressielijnen voor specifieke scholen zullen evenwijdig zijn aan de gemiddelde regressielijn (met dezelfde helling β), maar verschillen wat betreft het intercept \\(a_{j}\\). Dit model kan geschat worden door vrouwelijk toe te voegen aan de formule in de lmer() functie, waardoor alleen het intercept per school varieert en de “helling” voor vrouwelijkheid constant blijft voor alle scholen\n\nM2 &lt;- lmer(formula = course ~ 1 + female + (1 | school), \n           data = GCSE, \n           REML = FALSE)\nsummary(M2)\n\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: course ~ 1 + female + (1 | school)\n   Data: GCSE\n\n     AIC      BIC   logLik deviance df.resid \n 14017.4  14039.2  -7004.7  14009.4     1721 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.7809 -0.5401  0.1259  0.6795  2.6753 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n school   (Intercept)  76.65    8.755  \n Residual             179.96   13.415  \nNumber of obs: 1725, groups:  school, 73\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   69.730      1.185   58.87\nfemaleF        6.739      0.678    9.94\n\nCorrelation of Fixed Effects:\n        (Intr)\nfemaleF -0.338\n\n\nDe gemiddelde regressielijn over de scholen wordt dus geschat als \\(\\hat{\\mu}_{ij}=69.73+ 6.74 x_{ij}\\), waarbij \\(\\sigma_\\_alpha\\) en \\(\\sigma_y\\) worden geschat als respectievelijk \\(8.76\\) en \\(13.41\\). Als we deze schattingen van \\(\\mu_alpha\\), \\(\\beta\\), \\(\\sigma^2_{y}\\), en \\(\\sigma^2_{alpha}\\) als de ware parameterwaarden beschouwen, kunnen we de **Best Linear Unbiased Predictions (BLUPs) voor de fouten op schoolniveau \\(\\hat{u}_j = \\hat{\\alpha}_{j} - \\hat{\\mu}_{\\alpha}\\) verkrijgen.\nDe BLUPs zijn equivalent met de zogenaamde Empirical Bayes (EB)-voorspelling, die het gemiddelde is van de posterieure verdeling van \\(u_{j}\\) gegeven alle geschatte parameters, alsmede de willekeurige variabelen \\(y_{ij}\\) en \\(x_{ij}\\) voor het cluster. Deze voorspellingen worden “Bayes” genoemd omdat ze gebruik maken van de vooraf gespecificeerde prioriteitsverdeling1 \\(u_j \\sim N(\\mu_\\alpha, \\sigma^2_\\alpha)\\), en bij uitbreiding \\(u_j \\sim N(0, \\sigma^2_\\alpha)\\), en “Empirisch” genoemd omdat de parameters van deze prior, \\(\\mu_\\alpha\\) en \\(\\sigma^2_{\\alpha}\\), naast \\(\\beta\\) en \\(\\sigma^2_{y}\\), geschat worden uit de data.\nIn vergelijking met de ML-benadering (Maximum Likelihood - maximale waarschijnlijkheid), waarbij waarden voor \\(u_j\\) worden voorspeld door alleen de geschatte parameters en gegevens van cluster \\(j\\) te gebruiken, houdt de EB-benadering bovendien rekening met de voorafgaande verdeling van \\(u_{j}\\), en levert zij voorspelde waarden op die dichter bij \\(0\\) liggen (een verschijnsel dat wordt beschreven als shrinkage of partial pooling). Om te zien waarom dit verschijnsel shrinkage wordt genoemd, drukken we de uit EB voorspelling verkregen schattingen voor \\(u_j\\) gewoonlijk uit als \\(\\hat{u}_j^{text{EB}} = \\hat{R}_j\\hat{u}_j^{\\text{ML}}\\) waarbij \\(\\hat{u}_j^{\\text{ML}}\\) de ML schattingen zijn, en \\(\\hat{R}_j = \\frac{\\sigma_alpha^2}{n_j}\\) de zogenaamde Shrinkage factor is.\n\nhead(ranef(M2)$school)\n\n      (Intercept)\n20920 -10.1702110\n22520 -17.0578149\n22710   7.8007260\n22738   0.4871012\n22908  -8.1940346\n23208   4.4304453\n\n\nDeze waarden schatten hoeveel het intercept naar boven of beneden verschoven is in bepaalde scholen. Bijvoorbeeld, in de eerste school in de dataset is het geschatte intercept ongeveer 10.17 lager dan gemiddeld, zodat de schoolspecifieke regressielijn \\((69.73 - 10.17) + 6.74 x_{ij}\\) is.\nGelman&Hill (2006) karakteriseren multilevel modellering als partial pooling (ook wel shrinkage genoemd), wat een compromis is tussen twee uitersten: complete pooling, waarbij de clustering helemaal niet in het model wordt meegenomen, en no pooling, waarbij voor elke school aparte intercepten worden geschat als coëfficiënten van dummy-variabelen. De geschatte schoolspecifieke regressielijnen in het bovenstaande model zijn gebaseerd op partial pooling schattingen. Om dit aan te tonen, schatten we eerst de intercept en de helling voor elke school op drie manieren:\n\n# Complete-pooling regression\npooled &lt;- lm(formula = course ~ female,\n             data = GCSE)\na_pooled &lt;- coef(pooled)[1]   # complete-pooling intercept\nb_pooled &lt;- coef(pooled)[2]   # complete-pooling slope\n\n# No-pooling regression\nnopooled &lt;- lm(formula = course ~ 0 + school + female,\n               data = GCSE)\na_nopooled &lt;- coef(nopooled)[1:J]   # 73 no-pooling intercepts              \nb_nopooled &lt;- coef(nopooled)[J+1]\n\n# Partial pooling (multilevel) regression\na_part_pooled &lt;- coef(M2)$school[, 1]\nb_part_pooled &lt;- coef(M2)$school[, 2]\n\nVervolgens plotten we de gegevens en schoolspecifieke regressielijnen voor een selectie van acht scholen met behulp van de volgende commando’s.2:\n\n# (0) Assen plaatsen & scholen kiezen\ny &lt;- GCSE$course\nx &lt;- as.numeric(GCSE$female) - 1 + runif(N, -.05, .05)\nschid &lt;- GCSE$school\nsel.sch &lt;- c(\"65385\",\n             \"68207\",\n             \"60729\",\n             \"67051\",\n             \"50631\",\n             \"60427\",\n             \"64321\",\n             \"68137\")\n\n# (1) Subset 8 van de scholen; genereer data frame\ndf &lt;- data.frame(y, x, schid)\ndf8 &lt;- subset(df, schid %in% sel.sch)\n\n# (2) Aangeven van schattingen van volledige-pooling, geen-pooling, gedeeltelijke pooling \ndf8$a_pooled &lt;- a_pooled \ndf8$b_pooled &lt;- b_pooled\ndf8$a_nopooled &lt;- a_nopooled[df8$schid]\ndf8$b_nopooled &lt;- b_nopooled\ndf8$a_part_pooled &lt;- a_part_pooled[df8$schid]\ndf8$b_part_pooled &lt;- b_part_pooled[df8$schid]\n\n# (3) Plot van hoe regressie fit voor de 8 scholen\nggplot(data = df8, \n       aes(x = x, y = y)) + \n  facet_wrap(facets = ~ schid, \n             ncol = 4) + \n  theme_bw() +\n  geom_jitter(position = position_jitter(width = .05, \n                                         height = 0)) +\n  geom_abline(aes(intercept = a_pooled, \n                  slope = b_pooled), \n              linetype = \"solid\", \n              color = \"blue\", \n              size = 0.5) +\n  geom_abline(aes(intercept = a_nopooled, \n                  slope = b_nopooled), \n              linetype = \"longdash\", \n              color = \"red\", \n              size = 0.5) + \n  geom_abline(aes(intercept = a_part_pooled, \n                  slope = b_part_pooled), \n              linetype = \"dotted\", \n              color = \"purple\", \n              size = 0.7) + \n  scale_x_continuous(breaks = c(0, 1), \n                     labels = c(\"male\", \"female\")) + \n  labs(title = \"Schatting van Volledige-pooling, Geen-pooling en Gedeeltelijke pooling\",\n       x = \"\", \n       y = \"Totale score op cursuswerk paper\")+theme_bw( base_family = \"serif\")\n\n\n\n\nDe blauw-gestreepte, rood-gestreepte en paars-gestippelde lijnen geven respectievelijk de volledige pooling, no-pooling en gedeeltelijke pooling schattingen weer. We zien dat de geschatte schoolspecifieke regressielijn van de gedeeltelijke pooling-schattingen tussen de volledige pooling- en de geenpooling-regressielijn ligt. Er is meer pooling (paarse stippellijn dichter bij blauwe ononderbroken lijn) op scholen met een kleine steekproefomvang."
  },
  {
    "objectID": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#model-3-variërend-intercept-en-slope-model-met-een-enkele-voorspeller",
    "href": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#model-3-variërend-intercept-en-slope-model-met-een-enkele-voorspeller",
    "title": "Multilevel modeling met STAN",
    "section": "Model 3: Variërend intercept en slope model met een enkele voorspeller",
    "text": "Model 3: Variërend intercept en slope model met een enkele voorspeller\nWe breiden nu het variërende interceptmodel met één voorspeller uit om zowel het intercept als de helling willekeurig te laten variëren tussen scholen met behulp van het volgende model \\[y_{ij} = \\alpha_j + \\beta_j x_{ij} +\\epsilon_{ij},\\] \\[\\alpha_j = \\mu_\\alpha + u_j,\\] \\[\\beta_j = \\mu_\\beta + v_j,\\] or in a reduced form as \\[y_{ij} = \\mu_\\alpha + \\mu_\\beta x_{ij} + u_j + v_j x_{ij} + \\epsilon_{ij}\\] where \\(\\epsilon_{ij} \\sim N(0, \\sigma_{y}^{2})\\) and \\(\\left( \\begin{matrix} u_j \\\\ v_j \\end{matrix} \\right) \\sim N\\left( \\left( \\begin{matrix} 0 \\\\ 0 \\end{matrix} \\right) ,\\left( \\begin{matrix} { \\sigma }_{ \\alpha }^{ 2 } & \\rho { \\sigma }_{ \\alpha }{ \\sigma }_{ \\beta } \\\\ \\rho { \\sigma }_{ \\alpha }{ \\sigma }_{ \\beta } & { \\sigma }_{ \\beta }^{ 2 } \\end{matrix} \\right) \\right)\\).]:\n\\[y_{ij}\\sim N(\\alpha_{j}+\\beta_{j}x_{ij} , \\sigma_y ^2 ),\\] \\[\\left( \\begin{matrix} \\alpha _{ j } \\\\ \\beta _{ j } \\end{matrix} \\right) \\sim N\\left( \\left( \\begin{matrix} { \\mu  }_{ \\alpha  } \\\\ { \\mu  }_{ \\beta  } \\end{matrix} \\right) , \\left( \\begin{matrix} { \\sigma  }_{ \\alpha  }^{ 2 } & \\rho { \\sigma  }_{ \\alpha  }{ \\sigma  }_{ \\beta  } \\\\ \\rho { \\sigma  }_{ \\alpha  }{ \\sigma  }_{ \\beta  } & { \\sigma  }_{ \\beta  }^{ 2 } \\end{matrix} \\right)  \\right).\\]\nMerk op dat we nu variatie hebben in de \\(\\alpha_{j}\\)’s en de \\(\\beta_{j}\\)’s, en ook een correlatie parameter \\(\\rho\\) tussen \\(\\alpha_{j}\\) en \\(\\beta_{j}\\). Dit model kan gefit worden met gebruik van lmer() en wel als volgt:\n\nM3 &lt;- lmer(formula = course ~ 1 + female + (1 + female | school), \n           data = GCSE, \n           REML = FALSE)\nsummary(M3) \n\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: course ~ 1 + female + (1 + female | school)\n   Data: GCSE\n\n     AIC      BIC   logLik deviance df.resid \n 13983.4  14016.2  -6985.7  13971.4     1719 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.6886 -0.5222  0.1261  0.6529  2.6729 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n school   (Intercept) 102.93   10.146        \n          femaleF      47.94    6.924   -0.52\n Residual             169.79   13.030        \nNumber of obs: 1725, groups:  school, 73\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   69.425      1.352  51.338\nfemaleF        7.128      1.131   6.302\n\nCorrelation of Fixed Effects:\n        (Intr)\nfemaleF -0.574\n\n\nIn dit model wordt de residuele standaardafwijking binnen de school geschat als \\(\\hat{\\sigma}_{y}=\\) 13.03. De geschatte standaardafwijkingen van de schoolintercepten en de schoolhellingen zijn respectievelijk \\(\\hat{\\sigma}_{\\alpha}= 10.15\\) en \\(\\hat{\\sigma}_{\\beta}= 6.92\\). De geschatte correlatie tussen variërende intercepts en hellingen is \\(\\hat{\\rho} = -0.52\\). We kunnen een soortgelijke code als die in paragraaf 2.2 gebruiken om de gegevens en de schoolspecifieke regressielijnen voor een selectie van acht scholen te plotten."
  },
  {
    "objectID": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#gebruik-van-het-rstanarm-pakket",
    "href": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#gebruik-van-het-rstanarm-pakket",
    "title": "Multilevel modeling met STAN",
    "section": "Gebruik van het rstanarm pakket",
    "text": "Gebruik van het rstanarm pakket\nVeel relatief eenvoudige modellen kunnen worden aangepast met behulp van het rstanarm pakket zonder enige code te schrijven in de Stan taal. Het rstanarm pakket is een “wrapper” voor het rstan pakket waarmee de meest gebruikte regressiemodellen kunnen worden geschat met behulp van Markov Chain Monte Carlo (MCMC) en toch kunnen worden gespecificeerd met de gebruikelijke R modelleersyntaxis. Onderwijs onderzoekers kunnen Bayesiaanse schatting gebruiken voor multilevel modellen met slechts minimale veranderingen in hun bestaande code met lmer().\nBijvoorbeeld, Model 1 met standaard prior verdelingen voor \\(\\mu_{\\alpha}\\), \\(\\sigma_{\\alpha}\\), en \\(\\sigma_{y}\\) kan worden gespecificeerd met het rstanarm pakket door stan_ toe te voegen aan de lmer aanroep:\n\nM1_stanlmer &lt;- stan_lmer(formula = course ~ 1 + (1 | school), \n                         data = GCSE,\n                         seed = 349)\n\nDeze stan_lmer() functie is qua syntax gelijk aan lmer(), maar in plaats van een maximum likelihood schatting uit te voeren, wordt een Bayesiaanse schatting uitgevoerd via MCMC. Omdat elke stap in de MCMC schatting random trekkingen uit de parameter ruimte inhoudt, voegen we een seed optie toe om ervoor te zorgen dat stan_lmer elke keer dat de code wordt uitgevoerd, dezelfde resultaten geeft."
  },
  {
    "objectID": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#prior-distributies",
    "href": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#prior-distributies",
    "title": "Multilevel modeling met STAN",
    "section": "Prior distributies",
    "text": "Prior distributies\nModel 1 is een variabel interceptiemodel met normaal verdeelde leerlingresiduen en intercepten op schoolniveau: \\(y_{ij} \\sim N(\\alpha_{j}, \\sigma_{y}^{2}),\\) en \\(\\alpha_{j},\\sim N(\\mu_{alpha}, \\sigma_{alpha}^{2})\\). De normale verdeling voor de \\(\\alpha{j}\\)’s kan worden beschouwd als een prioriteitsverdeling voor deze variërende intercepten. De parameters van deze prior verdeling, \\(\\mu_{\\alpha}\\) en \\(\\sigma_{\\alpha}\\), worden geschat uit de gegevens bij gebruik van maximum likelihood schatting. Bij volledige Bayesiaanse inferentie hebben alle hyperparameters (\\(\\mu_{\\alpha}\\) en \\(\\sigma_{\\alpha}\\)), samen met de andere niet-gemodelleerde parameters (in dit geval, \\(\\sigma_{y}\\)) ook een priorverdeling nodig.\nHier gebruiken we de standaard prior verdelingen voor de hyperparameters in stan_lmer door geen prior opties op te geven in stan_lmer() functie. De standaard priors zijn bedoeld als zwak informatief in de zin dat ze gematigde regularisatie bieden [Regularisatie kan worden beschouwd als een techniek om ervoor te zorgen dat schattingen binnen een acceptabel bereik van waarden worden begrensd] en helpen bij het stabiliseren van de berekening. Opgemerkt moet worden dat de auteurs van rstanarm suggereren om niet te vertrouwen op rstanarm om de standaard prior voor een model te specificeren, maar eerder om de priors expliciet te specificeren, zelfs als ze inderdaad de huidige standaard zijn, aangezien updates van het pakket andere defaults mee kunnen krijgen.\nTen eerste wordt, alvorens rekening te houden met de schaal van de variabelen, \\(\\mu_{alpha}\\) een normale priorverdeling gegeven met gemiddelde 0 en standaardafwijking 10. Dat wil zeggen, \\(mu_{alpha} \\sim N(0, 10^2)\\). De standaardafwijking van deze prioriteitsverdeling, 10, is vijf keer zo groot als de standaardafwijking van de respons indien deze gestandaardiseerd zou zijn. Dit zou een dichte benadering moeten zijn van een niet-informatieve prior over het door de waarschijnlijkheid ondersteunde bereik, die in gevolgtrekkingen zou moeten geven die vergelijkbaar zijn met die verkregen met maximale waarschijnlijkheidsmethoden indien even zwakke priors worden gebruikt voor de andere parameters.\nTen tweede wordt de (ongeschaalde) prior voor \\(\\sigma_{y}\\) ingesteld op een exponentiële verdeling met de ‘rate’-parameter op 1.\nTen derde, om een prior voor de varianties en covarianties van de variërende (of “willekeurige”) effecten te specificeren, zal rstanarm deze matrix ontbinden in een correlatiematrix van de variërende effecten en een functie van hun varianties. Omdat er in dit voorbeeld slechts één variërend effect is, reduceert de standaard (ongeschaalde) prior voor \\(\\sigma_{\\alpha}\\) die rstanarm gebruikt tot een exponentiële verdeling met de rate parameter op 1.\nOok moet worden opgemerkt dat rstanarm de priors zal schalen tenzij de autoscale = FALSE optie wordt gebruikt. Na het fitten van een model met stan_lmer, kunnen we de gebruikte priors controleren door de prior_summary() functie op te roepen.\n\n# Een samenvatting krijgen van de priors die gebruikt worden \nprior_summary(object = M1_stanlmer)\n\nPriors for model 'M1_stanlmer' \n------\nIntercept (after predictors centered)\n  Specified prior:\n    ~ normal(location = 73, scale = 2.5)\n  Adjusted prior:\n    ~ normal(location = 73, scale = 41)\n\nAuxiliary (sigma)\n  Specified prior:\n    ~ exponential(rate = 1)\n  Adjusted prior:\n    ~ exponential(rate = 0.061)\n\nCovariance\n ~ decov(reg. = 1, conc. = 1, shape = 1, scale = 1)\n------\nSee help('prior_summary.stanreg') for more details\n\n\n\n# Hiermee krijgen we de SD van de uitkomst \nsd(GCSE$course, na.rm = TRUE)\n\n[1] 16.32096\n\n\nZoals hierboven te zien is, worden de schalen van de priors voor \\(\\mu_{\\alpha}\\) en \\(\\sigma_y\\) op respectievelijk \\(163,21\\) en \\(16,32\\) gezet na herschaling. Aangezien de standaard prior voor het intercept normaal is met een schaalparameter van \\(10\\), is de herschaalde prior ook normaal maar met een schaalparameter van \\(\\text{scale} \\times \\text{SD}(y) = 10 \\times 16.321= 163.21\\). Aangezien de standaard prior voor \\(\\sigma_y\\) exponentieel is met een snelheidsparameter van \\(1\\) (of gelijkwaardig, de schaalparameter \\(\\text{scale} = \\frac{1}{text{rate} = 1\\)), is de herschaalde prior eveneens exponentieel met een schaalparameter van \\(\\text{scale} \\maal \\text{SD}(y) = 1 maal 16,321= 16,32\\)."
  },
  {
    "objectID": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#directe-output-van-stan_lmer",
    "href": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#directe-output-van-stan_lmer",
    "title": "Multilevel modeling met STAN",
    "section": "Directe output van stan_lmer",
    "text": "Directe output van stan_lmer\n\nPosterior medianen en posterior mediaan absolute deviaties\nWe kunnen een snelle samenvatting van de fit van Model 1 weergeven door de print methode op de volgende manier te gebruiken:\n\nprint(M1_stanlmer, digits = 2)\n\nstan_lmer\n family:       gaussian [identity]\n formula:      course ~ 1 + (1 | school)\n observations: 1725\n------\n            Median MAD_SD\n(Intercept) 73.68   1.14 \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 13.82   0.24 \n\nError terms:\n Groups   Name        Std.Dev.\n school   (Intercept)  8.87   \n Residual             13.82   \nNum. levels: school 73 \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nHier is de puntschatting van \\(\\mu_{\\alpha}\\) uit stan_lmer \\(73.75\\) en dit komt overeen met de mediaan van de posterior trekkingen. Dit is vergelijkbaar met de ML schatting uit lmer. De puntschatting voor \\(`sigma_{_alpha}\\) van stan_lmer is \\(8.87\\), die groter is dan de ML schatting (\\(8.67\\)). Dit verschil kan deels komen doordat de ML benadering in lmer() geen rekening houdt met de onzekerheid in \\(\\mu_{\\alpha}\\) bij het schatten van \\(\\sigma_{\\alpha}\\). De REML benadering (\\(8.75\\)) in lmer() houdt, zoals eerder vermeld, wel rekening met deze onzekerheid.\nBij gebruik van stan_lmer worden standaardfouten verkregen door de mediaan absolute afwijking (MAD) van elke trekking ten opzichte van de mediaan van die trekkingen te beschouwen. Het is bekend dat ML de neiging heeft om onzekerheden te onderschatten, omdat het gebaseerd is op puntschattingen van hyperparameters. Full Bayes daarentegen propageert de onzekerheid in de hyperparameters over alle niveaus van het model en levert adequatere onzekerheidsschattingen op. Zie ook Brown e.a. (2006) voor verdere discussie.\n\n\nPosterior gemiddelden, posterior standaard deviaties, 95% credible interval en Monte Carlo fouten\n\nsummary(M1_stanlmer, \n        pars = c(\"(Intercept)\", \"sigma\", \"Sigma[school:(Intercept),(Intercept)]\"),\n        probs = c(0.025, 0.975),\n        digits = 2)\n\n\nModel Info:\n function:     stan_lmer\n family:       gaussian [identity]\n formula:      course ~ 1 + (1 | school)\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 1725\n groups:       school (73)\n\nEstimates:\n                                        mean   sd     2.5%   97.5%\n(Intercept)                            73.67   1.12  71.49  75.92 \nsigma                                  13.82   0.24  13.36  14.30 \nSigma[school:(Intercept),(Intercept)]  78.67  15.55  53.46 114.10 \n\nMCMC diagnostics\n                                      mcse Rhat n_eff\n(Intercept)                           0.05 1.01  572 \nsigma                                 0.00 1.00 4579 \nSigma[school:(Intercept),(Intercept)] 0.61 1.00  641 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\nHet is de moeite waard om op te merken dat bij gebruik van de summary methode, de schatting voor de standaardafwijking \\(sigma_y\\) het gemiddelde is van de posterior trekkingen van de parameter. Dit in tegenstelling tot de mediaan van de posterior trekkingen die we krijgen bij gebruik van de print methode. Een voordeel van het gebruik van de mediaan is dat de schatting voor \\(\\sigma_y^2\\) gewoon het kwadraat is van de schatting voor \\(\\sigma_y\\) als het aantal steekproeven oneven is. Dit is niet het geval bij gebruik van het gemiddelde. In dit geval, en meer algemeen wanneer we andere functies van de parameters moeten evalueren, moeten we de posterior trekkingen rechtstreeks benaderen. Dit wordt beschreven in het volgende deel.\nOnder Diagnostics, verwijzen we de lezer naar Paragraaf 5 voor meer informatie over Rhat en n_eff. De waarden onder mcse vertegenwoordigen schattingen voor de Monte Carlo standaardfouten, die de willekeurigheid vertegenwoordigen die geassocieerd is met elke MCMC schattingsrun. Dat wil zeggen, met dezelfde dataset, herhaaldelijk gebruik van een MCMC benadering om een parameter te schatten levert schattingen op met een standaardafwijking gelijk aan de Monte Carlo standaardfout."
  },
  {
    "objectID": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#andere-output-van-stan_lmer",
    "href": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#andere-output-van-stan_lmer",
    "title": "Multilevel modeling met STAN",
    "section": "Andere output van stan_lmer",
    "text": "Andere output van stan_lmer\nZoals gezegd, kunnen gebruikers er de voorkeur aan geven om direct met de posterior trekkingen te werken om schattingen van meer complexe parameters te verkrijgen. Om dit te doen, moeten gebruikers ze handmatig benaderen vanuit het stan_lmer object. We laten zien hoe dit moet in de context van het maken van vergelijkingen tussen individuele scholen.\n\nToegang tot de simulaties en samenvattende resultaten\nGebaseerd op de standaard instellingen, genereert stan_lmer 4 MCMC-ketens van 2.000 iteraties elk. De helft van deze iteraties in elke keten wordt gebruikt als warming-up/burn-in (om de keten te laten convergeren naar de posterior verdeling), en daarom gebruiken we slechts 1.000 steekproeven per keten. Deze door MCMC gegenereerde steekproeven worden geacht getrokken te zijn uit de posterior verdelingen van de parameters in het model. Wij kunnen deze steekproeven gebruiken voor voorspellingen, om de onzekerheid samen te vatten en ‘çredible intervals’ (geloofwaardige intervallen) te schatten voor elke functie van de parameters.\nOm toegang te krijgen tot de posterior trekkingen voor alle parameters, passen we de methode as.matrix() toe op het stanreg object M1_stanlmer. Dit geeft een \\(S\\) bij \\(P\\) matrix, waarbij \\(S\\) de grootte is van de posterior steekproef (of gelijkwaardig, het aantal MCMC iteraties na warm-up) en \\(P\\) het aantal parameters/kwantiteiten. Door deze matrix te manipuleren kunnen we een matrix genereren voor de variërende intercepts \\(\\alpha_{j}\\) en vectoren met de trekkingen voor de within standaardafwijking en de between variantie. Merk op dat om de juiste kolommen voor de parameter van belang te selecteren, het nuttig is om de kolomnamen van de matrix sims te onderzoeken.\nEen meer directe benadering voor het verkrijgen van de posterior trekkingen voor specifieke parameters is gebruik te maken van de ingebouwde functionaliteit van de as.matrix methode voor stanreg objecten. Wanneer de as.matrix methode wordt toegepast op een stanreg object, kan de gebruiker ofwel een optionele karaktervector van parameternamen specificeren, of een optionele karaktervector van reguliere expressies3 om de posterior trekkingen van alleen de parameters waarin ze geïnteresseerd zijn te extraheren. Bijvoorbeeld, omdat de parameter die het totale gemiddelde representeert is gelabeld met (Intercept), kunnen we de posterior trekkingen van alleen deze parameter extraheren door de optie pars = \"(Intercept)\" op te nemen. En omdat de parameters die de 73 schoolfouten representeren allemaal de string b[(Intercept) school: bevatten, kunnen we alle parameters die deze string bevatten extraheren door de optie regex_pars = \"b[(Intercept) school:” te gebruiken.\n\n# Extraheren van de posterior trekkingen voor alle parameters\nsims &lt;- as.matrix(M1_stanlmer)\ndim(sims)\n\n[1] 4000   76\n\npara_name &lt;- colnames(sims)\npara_name\n\n [1] \"(Intercept)\"                          \n [2] \"b[(Intercept) school:20920]\"          \n [3] \"b[(Intercept) school:22520]\"          \n [4] \"b[(Intercept) school:22710]\"          \n [5] \"b[(Intercept) school:22738]\"          \n [6] \"b[(Intercept) school:22908]\"          \n [7] \"b[(Intercept) school:23208]\"          \n [8] \"b[(Intercept) school:25241]\"          \n [9] \"b[(Intercept) school:30474]\"          \n[10] \"b[(Intercept) school:35270]\"          \n[11] \"b[(Intercept) school:37224]\"          \n[12] \"b[(Intercept) school:47627]\"          \n[13] \"b[(Intercept) school:50627]\"          \n[14] \"b[(Intercept) school:50631]\"          \n[15] \"b[(Intercept) school:60421]\"          \n[16] \"b[(Intercept) school:60427]\"          \n[17] \"b[(Intercept) school:60437]\"          \n[18] \"b[(Intercept) school:60439]\"          \n[19] \"b[(Intercept) school:60441]\"          \n[20] \"b[(Intercept) school:60455]\"          \n[21] \"b[(Intercept) school:60457]\"          \n[22] \"b[(Intercept) school:60501]\"          \n[23] \"b[(Intercept) school:60729]\"          \n[24] \"b[(Intercept) school:60741]\"          \n[25] \"b[(Intercept) school:63619]\"          \n[26] \"b[(Intercept) school:63833]\"          \n[27] \"b[(Intercept) school:64251]\"          \n[28] \"b[(Intercept) school:64321]\"          \n[29] \"b[(Intercept) school:64327]\"          \n[30] \"b[(Intercept) school:64343]\"          \n[31] \"b[(Intercept) school:64359]\"          \n[32] \"b[(Intercept) school:64428]\"          \n[33] \"b[(Intercept) school:65385]\"          \n[34] \"b[(Intercept) school:66365]\"          \n[35] \"b[(Intercept) school:67051]\"          \n[36] \"b[(Intercept) school:67105]\"          \n[37] \"b[(Intercept) school:67311]\"          \n[38] \"b[(Intercept) school:68107]\"          \n[39] \"b[(Intercept) school:68111]\"          \n[40] \"b[(Intercept) school:68121]\"          \n[41] \"b[(Intercept) school:68125]\"          \n[42] \"b[(Intercept) school:68133]\"          \n[43] \"b[(Intercept) school:68137]\"          \n[44] \"b[(Intercept) school:68201]\"          \n[45] \"b[(Intercept) school:68207]\"          \n[46] \"b[(Intercept) school:68217]\"          \n[47] \"b[(Intercept) school:68227]\"          \n[48] \"b[(Intercept) school:68233]\"          \n[49] \"b[(Intercept) school:68237]\"          \n[50] \"b[(Intercept) school:68241]\"          \n[51] \"b[(Intercept) school:68255]\"          \n[52] \"b[(Intercept) school:68271]\"          \n[53] \"b[(Intercept) school:68303]\"          \n[54] \"b[(Intercept) school:68321]\"          \n[55] \"b[(Intercept) school:68329]\"          \n[56] \"b[(Intercept) school:68405]\"          \n[57] \"b[(Intercept) school:68411]\"          \n[58] \"b[(Intercept) school:68417]\"          \n[59] \"b[(Intercept) school:68531]\"          \n[60] \"b[(Intercept) school:68611]\"          \n[61] \"b[(Intercept) school:68629]\"          \n[62] \"b[(Intercept) school:68711]\"          \n[63] \"b[(Intercept) school:68723]\"          \n[64] \"b[(Intercept) school:68805]\"          \n[65] \"b[(Intercept) school:68809]\"          \n[66] \"b[(Intercept) school:71927]\"          \n[67] \"b[(Intercept) school:74330]\"          \n[68] \"b[(Intercept) school:74862]\"          \n[69] \"b[(Intercept) school:74874]\"          \n[70] \"b[(Intercept) school:76531]\"          \n[71] \"b[(Intercept) school:76631]\"          \n[72] \"b[(Intercept) school:77207]\"          \n[73] \"b[(Intercept) school:84707]\"          \n[74] \"b[(Intercept) school:84772]\"          \n[75] \"sigma\"                                \n[76] \"Sigma[school:(Intercept),(Intercept)]\"\n\n# Verkrijgen van school-niveau varyiërende intercept a_j\n# trekking voor het algemeen gemiddelde\nmu_a_sims &lt;- as.matrix(M1_stanlmer, \n                       pars = \"(Intercept)\")\n# trekkingen voor 73 scholen van de school-niveua fouten \nu_sims &lt;- as.matrix(M1_stanlmer, \n                    regex_pars = \"b\\\\[\\\\(Intercept\\\\) school\\\\:\")\n# trekkingen van alle 73 school variërende intercepten               \na_sims &lt;- as.numeric(mu_a_sims) + u_sims          \n\n# Verkrijgen van sigma_y en sigma_alpha^2\n# trekkingen van sigma_y\ns_y_sims &lt;- as.matrix(M1_stanlmer, \n                       pars = \"sigma\")\n# trekkingen van sigma_alpha^2\ns__alpha_sims &lt;- as.matrix(M1_stanlmer, \n                       pars = \"Sigma[school:(Intercept),(Intercept)]\")\n\n\n\nVerkrijgen van gemiddelden, standaard deviaties, medianen en 95% geloofwaardigheids intervallen\nIn a_sims hebben we 4.000 posterior trekkingen (van alle 4 ketens) voor de variërende intercepten \\(\\alpha_{j}\\) van de 73 scholen opgeslagen. De eerste kolom van de matrix van 4.000 bij 73 is bijvoorbeeld een vector van 4.000 posterior simulatietrekkingen voor de variërende intercept van de eerste school (School 20920). Een kwantitatieve manier om de posterior kansverdeling van deze 4.000 schattingen voor \\(1,1pha_{1}\\) samen te vatten is het onderzoeken van hun quantielen.\n\n# Computeer gemiddelde, SD, mediaan en 95% geloofwaardigheids interval van de varyiërende intercepten\n\n# Posterior gemiddelde en SD van elke alpha\na_mean &lt;- apply(X = a_sims,     # posterior gemiddelde\n                MARGIN = 2,\n                FUN = mean)\na_sd &lt;- apply(X = a_sims,       # posterior SD\n              MARGIN = 2,\n              FUN = sd)\n\n# Posterior mediaan en 95% geloofwaardigheids interval\na_quant &lt;- apply(X = a_sims, \n                 MARGIN = 2, \n                 FUN = quantile, \n                 probs = c(0.025, 0.50, 0.975))\na_quant &lt;- data.frame(t(a_quant))\nnames(a_quant) &lt;- c(\"Q2.5\", \"Q50\", \"Q97.5\")\n\n# Combineer samenvattende statistieken van posterior simulatie trekkingen\na_df &lt;- data.frame(a_mean, a_sd, a_quant)\nround(head(a_df), 2)\n\n                            a_mean a_sd  Q2.5   Q50 Q97.5\nb[(Intercept) school:20920]  63.63 4.40 54.97 63.57 72.17\nb[(Intercept) school:22520]  57.15 1.79 53.72 57.15 60.60\nb[(Intercept) school:22710]  81.64 3.15 75.47 81.64 87.78\nb[(Intercept) school:22738]  73.03 4.27 64.48 73.11 81.50\nb[(Intercept) school:22908]  66.51 5.28 56.47 66.47 76.70\nb[(Intercept) school:23208]  79.23 2.81 73.60 79.22 85.02\n\n\nWij kunnen een rupsplot maken om de volledige Bayes-schattingen voor de schoolafhankelijke intercepts in rangorde te tonen, samen met hun 95% credible intervallen.\n\n# Sorteer dataframe die een geschatte alfa gemiddelde en sd voor elke school omvatten\na_df &lt;- a_df[order(a_df$a_mean), ]\na_df$a_rank &lt;- c(1 : dim(a_df)[1])  # een vector van de schoolranking \n\n# Plot school-niveau alfas posterior gemiddelde en 95% credible interval\nggplot(data = a_df, \n       aes(x = a_rank, \n           y = a_mean)) +\n  geom_pointrange(aes(ymin = Q2.5, \n                      ymax = Q97.5),\n                  position = position_jitter(width = 0.1, \n                                             height = 0)) + \n  geom_hline(yintercept = mean(a_df$a_mean), \n             size = 0.5, \n             col = \"red\") + \n  scale_x_continuous(\"Rank\", \n                     breaks = seq(from = 0, \n                                  to = 80, \n                                  by = 5)) + \n  scale_y_continuous(expression(paste(\"varying intercept, \", alpha[j]))) + \n  theme_bw( base_family = \"serif\")\n\n\n\n\nDezelfde aanpak kan natuurlijk worden gevolgd om 95% geloofwaardige intervallen te genereren voor \\(\\sigma_y\\) en \\(\\sigma_\\alpha\\).\n\n\nVergelijkingen maken tussen individuele scholen\nHet hebben van steekproeven van alle parameters en variërende intercepten uit hun gezamenlijke posterior verdeling maakt het gemakkelijk om inferenties te trekken over functies van deze parameters.\nIn onderwijsonderzoek en in de onderwijspraktijk is het vaak interessant om de scholen in de data met elkaar te vergelijken. Relevante vragen zijn bijvoorbeeld (1) wat is het verschil tussen de gemiddelden van school A en school B, (2) presteert school A beter dan school B en (3) wat is de rangorde van deze scholen binnen de steekproef. Wanneer niet-Bayesiaanse methoden worden gebruikt, kunnen wij proberen dergelijke vergelijkingen te maken op basis van empirische Bayes- (of Best Linear Unbiased-) voorspellingen van de variërende intercepten. Maar het zal in het algemeen onmogelijk zijn om de onzekerheid uit te drukken voor niet-lineaire functies zoals rangschikkingen. Zie ook Goldstein en Spiegelhalter (2006) voor verdere discussie.\nHier zullen we twee scholen vergelijken als voorbeeld: Scholen 60501 (de 21ste school) en 68271 (de 51ste school). We hebben al 4.000 posterior simulatietrekkingen voor beide scholen. Om conclusies te trekken over het verschil tussen de gemiddelde scores van de twee scholen, kunnen we eenvoudigweg het verschil nemen tussen de twee vectoren van trekkingen \\(\\alpha_{51} - \\alpha_{21}\\).\n\n# Het verschil tussen de twee schoolgemiddelden (school #21 en #51)\nschool_diff &lt;- a_sims[, 21] - a_sims[, 51]\n\nWij kunnen de posterior verdeling van het verschil als volgt onderzoeken met beschrijvende statistieken en een histogram:\n\n# Onderzoek verschillen van twee distributies \nmean &lt;- mean(school_diff)\nsd &lt;- sd(school_diff)\nquantile &lt;- quantile(school_diff, probs = c(0.025, 0.50, 0.975))\nquantile &lt;- data.frame(t(quantile))\nnames(quantile) &lt;- c(\"Q2.5\", \"Q50\", \"Q97.5\")\ndiff_df &lt;- data.frame(mean, sd, quantile)\nround(diff_df, 2)\n\n  mean   sd  Q2.5  Q50 Q97.5\n1 5.12 4.48 -3.53 5.12 14.02\n\n\n\n# Histogram van de verschillen \nggplot(data = data.frame(school_diff), \n       aes(x = school_diff)) + \n  geom_histogram(color = \"black\", \n                 fill = \"gray\", \n                 binwidth = 0.75) + \n  scale_x_continuous(\"Score verschil tussen twee scholen: #21, #51\",\n                     breaks = seq(from = -20, \n                                  to = 20, \n                                  by = 10)) + \n  geom_vline(xintercept = c(mean(school_diff),\n                            quantile(school_diff, \n                                     probs = c(0.025, 0.975))),\n             colour = \"red\", \n             linetype = \"longdash\") + \n  geom_text(aes(5.11, 20, label = \"mean = 5.11\"), \n            color = \"red\", \n            size = 4) + \n  geom_text(aes(9, 50, label = \"SD = 4.46\"), \n            color = \"blue\", \n            size = 4) + \n  theme_bw( base_family = \"serif\") \n\n\n\n\n\nprop.table(table(a_sims[, 21] &gt; a_sims[, 51]))\n\n\n  FALSE    TRUE \n0.12425 0.87575 \n\n\nHet verwachte verschil komt uit op 5,11 met een standaardafwijking van 4,46 en een grote bandbreedte van onzekerheid. Het 95% geloofwaardigheidsinterval is [-3.64, 13.66], dus we zijn er 95% zeker van dat de ware waarde van het verschil tussen de twee scholen binnen het bereik ligt, gegeven de gegevens.\nWe kunnen ook het deel van de tijd bepalen dat School 60501 een hoger gemiddelde heeft dan School 68271:\n\nprop.table(table(a_sims[, 21] &gt; a_sims[, 51]))\n\n\n  FALSE    TRUE \n0.12425 0.87575 \n\n\nDit betekent dat de posterior waarschijnlijkheid dat School 60501 beter is dan School 68271 87.6% is. Elk paar scholen binnen de steekproef van scholen kan op deze manier vergeleken worden."
  },
  {
    "objectID": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#model-2-een-voorspeller-op-studentenniveau-toevoegen",
    "href": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#model-2-een-voorspeller-op-studentenniveau-toevoegen",
    "title": "Multilevel modeling met STAN",
    "section": "Model 2: Een voorspeller op studentenniveau toevoegen",
    "text": "Model 2: Een voorspeller op studentenniveau toevoegen\nOnderzoekers zouden de variërende interceptmodellen kunnen uitbreiden met waargenomen verklarende variabelen op het niveau van de leerling \\(x_{ij}\\), in dit voorbeeld een indicatorvariabele voor vrouw. Een eenvoudig variërend interceptmodel met één voorspeller op het niveau van de leerling kan worden geschreven als \\(y_{ij} \\N(\\alpha_{j} + \\beta x_{ij}, \\sigma_{y}^{2})\\) en \\(\\alpha_{j} \\N(\\mu_{alpha}, \\sigma_{alpha}^{2})\\). We gebruiken niet-informatieve prioriteitsverdelingen voor de hyperparameters (\\(\\mu_{\\alpha}\\) en \\(\\sigma_{\\alpha}\\)) zoals gespecificeerd in het variërende interceptmodel zonder voorspellers. Bovendien krijgt de regressiecoëfficiënt \\(\\beta\\) een normale prioriteitsverdeling met gemiddelde 0 en standaardafwijking 100. Dit betekent, ruwweg, dat we verwachten dat deze coëfficiënt in het bereik \\((-100, 100)\\) ligt, en als de ML schatting in dit bereik ligt, geeft de prior verdeling zeer weinig informatie voor de inferentie.\nHet bovenstaande model kan als volgt worden gefit met de stan_lmer() functie in het rstanarm pakket:\n\nM2_stanlmer &lt;- stan_lmer(formula = course ~ female + (1 | school), \n                         data = GCSE, \n                         prior = normal(location = 0, \n                                        scale = 100,\n                                        autoscale = FALSE),\n                         prior_intercept = normal(location = 0, \n                                                  scale = 100, \n                                                  autoscale = FALSE),\n                         seed = 349)\n\n\nprior_summary(object = M2_stanlmer)\n\nPriors for model 'M2_stanlmer' \n------\nIntercept (after predictors centered)\n ~ normal(location = 0, scale = 100)\n\nCoefficients\n ~ normal(location = 0, scale = 100)\n\nAuxiliary (sigma)\n  Specified prior:\n    ~ exponential(rate = 1)\n  Adjusted prior:\n    ~ exponential(rate = 0.061)\n\nCovariance\n ~ decov(reg. = 1, conc. = 1, shape = 1, scale = 1)\n------\nSee help('prior_summary.stanreg') for more details\n\n\n\nM2_stanlmer\n\nstan_lmer\n family:       gaussian [identity]\n formula:      course ~ female + (1 | school)\n observations: 1725\n------\n            Median MAD_SD\n(Intercept) 69.7    1.3  \nfemaleF      6.7    0.7  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 13.4    0.2  \n\nError terms:\n Groups   Name        Std.Dev.\n school   (Intercept)  9      \n Residual             13      \nNum. levels: school 73 \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nMerk op dat in plaats van de standaard priors in stan_lmer, \\(\\mu_{\\alpha}\\) en \\(\\beta\\) normale prior verdelingen krijgen met gemiddelde 0 en standaardafwijking 100 door de argumenten prior en prior_intercept op te geven als normal(location = 0, scale = 100, autoscale = FALSE). Om te voorkomen dat stan_lmer de prior schaalt, moeten we ervoor zorgen dat het argument autoscale = FALSE wordt toegevoegd.\nDe puntschattingen van \\(_mu_{alpha}\\), \\(\\beta\\), en \\(\\sigma_{y}\\) zijn bijna identiek aan de ML-schattingen van de lmer() fit. Echter, deels omdat ML de onzekerheid over \\(\\mu_{alpha}\\) negeert bij het schatten van \\(\\sigma_{alpha}\\), is de Bayesiaanse schatting voor \\(\\sigma_{alpha}\\) (\\(9,0\\)) groter dan de ML-schatting (\\(8,8\\)), net als bij model 1.\n\nModel 3: Variërende slopes over scholen toevoegen\nWe also use stan_lmer to fit Model 3 using the command below. Note that here, we use the default priors which are mostly similar to what was done in Model 1. Additionally, we are also required to specify a prior for the covariance matrix \\(\\Sigma\\) for \\(\\alpha_j\\) and \\(\\beta_j\\) in this Model. stan_lmer decomposes this covariance matrix (up to a factor of \\(\\sigma_y\\)) into (i) a correlation matrix \\(R\\) and (ii) a matrix of variances \\(V\\), and assigns them separate priors as shown below.\n$$\n\\[\\begin{aligned}\n\\Sigma &=\n\\left(\\begin{matrix}\n\\sigma_\\alpha^2 & \\rho\\sigma_\\alpha \\sigma_\\beta \\\\\n\\rho\\sigma_\\alpha\\sigma_\\beta&\\sigma_\\beta^2\n\\end{matrix} \\right)\\\\ &=\n\\sigma_y^2\\left(\\begin{matrix}\n\\sigma_\\alpha^2/\\sigma_y^2 & \\rho\\sigma_\\alpha \\sigma_\\beta/\\sigma_y^2 \\\\\n\\rho\\sigma_\\alpha\\sigma_\\beta/\\sigma_y^2 & \\sigma_\\beta^2/\\sigma_y^2\n\\end{matrix} \\right)\\\\ &=\n\\sigma_y^2\\left(\\begin{matrix}\n\\sigma_\\alpha/\\sigma_y & 0 \\\\\n0&\\sigma_\\beta/\\sigma_y\n\\end{matrix} \\right)\n\\left(\\begin{matrix}\n1 & \\rho\\\\\n\\rho&1\n\\end{matrix} \\right)\n\\left(\\begin{matrix}\n\\sigma_\\alpha/\\sigma_y & 0 \\\\\n0&\\sigma_\\beta/\\sigma_y\n\\end{matrix} \\right)\\\\\n&= \\sigma_y^2VRV.\n\\end{aligned}\\]\n$$\nDe correlatiematrix \\(R\\) is een 2 bij 2 matrix met 1-en op de diagonaal en \\(rho\\)’s op de off-diagonaal. stan_lmer kent er een LKJ^[Voor meer details over de LKJ verdeling, zie hier en hier prior aan toe, met regularisatieparameter 1 (Lewandowski et all., 2009). Dit komt overeen met het toekennen van een uniforme prior voor \\(rho\\). Hoe groter de regularisatieparameter is dan 1, hoe meer de verdeling voor \\(\\rho\\) de waarde 0 aanneemt.\nDe matrix van (geschaalde) varianties \\(V\\) kan eerst worden samengevat in een vector van (geschaalde) varianties, en vervolgens ontleed in drie delen, \\(J\\), \\(\\tau^2\\) en \\(\\pi\\) zoals hieronder getoond. $$\n(\n\\[\\begin{matrix}\n\\sigma_\\alpha^2/\\sigma_y^2 \\\\\n\\sigma_\\beta^2/\\sigma_y^2\n\\end{matrix}\\]\n) = 2()(\n\\[\\begin{matrix}\n\\frac{\\sigma_\\alpha^2/\\sigma_y^2}{\\sigma_\\alpha^2/\\sigma_y^2 + \\sigma_\\beta^2/\\sigma_y^2} \\\\\n\\frac{\\sigma_\\beta^2/\\sigma_y^2}{\\sigma_\\alpha^2/\\sigma_y^2 + \\sigma_\\beta^2/\\sigma_y^2}\n\\end{matrix}\\]\n)= J^2 .\n$$\nIn deze formulering is \\(J\\) het aantal variërende effecten in het model (hier \\(J=2\\)), kan \\(Jtau^2\\) worden beschouwd als een gemiddelde (geschaalde) variantie over de variërende effecten \\(Jalpha_j\\) en \\(Jbeta_j\\), en is \\(Jpi\\) een niet-negatieve vector die sommeert tot 1 (een zogenaamde simplex/probabiliteitsvector). Een symmetrische Dirichlet4 verdeling met concentratieparameter ingesteld op 1 wordt dan gebruikt als de prior voor \\(\\pi\\). Standaard impliceert dit een gezamenlijk uniforme prior over alle simplexvectoren van dezelfde grootte. Een schaalinvariante Gamma-voorrang met vorm- en schaalparameters beide op 1 wordt dan toegekend voor \\(\\tau\\). Dit komt overeen met het toekennen van de exponentiële verdeling met de snelheidsparameter op 1 die consistent is met de prioriteit toegekend aan \\(\\sigma_y\\) als prioriteit.\n\nM3_stanlmer &lt;- stan_lmer(formula = course ~ female + (1 + female | school), \n                         data = GCSE,\n                         seed = 349)\nprior_summary(object = M3_stanlmer)\n\nPriors for model 'M3_stanlmer' \n------\nIntercept (after predictors centered)\n  Specified prior:\n    ~ normal(location = 73, scale = 2.5)\n  Adjusted prior:\n    ~ normal(location = 73, scale = 41)\n\nCoefficients\n  Specified prior:\n    ~ normal(location = 0, scale = 2.5)\n  Adjusted prior:\n    ~ normal(location = 0, scale = 83)\n\nAuxiliary (sigma)\n  Specified prior:\n    ~ exponential(rate = 1)\n  Adjusted prior:\n    ~ exponential(rate = 0.061)\n\nCovariance\n ~ decov(reg. = 1, conc. = 1, shape = 1, scale = 1)\n------\nSee help('prior_summary.stanreg') for more details\n\nM3_stanlmer\n\nstan_lmer\n family:       gaussian [identity]\n formula:      course ~ female + (1 + female | school)\n observations: 1725\n------\n            Median MAD_SD\n(Intercept) 69.4    1.3  \nfemaleF      7.1    1.1  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 13.0    0.2  \n\nError terms:\n Groups   Name        Std.Dev. Corr \n school   (Intercept) 10.3          \n          femaleF      7.2     -0.49\n Residual             13.0          \nNum. levels: school 73 \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nHier zien we dat de puntschattingen voor \\(\\mu_{\\alpha}\\) en \\(\\sigma_{y}\\) identiek zijn aan de ML-schattingen uit lmer() fit. De puntschatting voor $\\(_bèta\\) is iets anders in dit model (7.14 vergeleken met 7.13). Verder is, net als bij de vorige twee modellen, de Bayesiaanse schatting voor \\(\\sigma_{\\alpha}\\) (10.3) groter dan de ML schatting (10.15). Daarnaast zijn de Bayesiaanse schattingen voor \\(\\sigma_{\\beta}\\) (7.2) en \\(\\rho\\) (-0.49) groter dan de overeenkomstige ML schattingen (respectievelijk 6.92 en -0.52)."
  },
  {
    "objectID": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html",
    "href": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html",
    "title": "Regressie en nog zo iets",
    "section": "",
    "text": "Vijftien jaar geleden schreven Gelman en Hill Data analysis using regression and multilevel/hierarchical models, een klassieker over moderne data-analyse. Ze gebruikte R en WinBugs voor lineaire en logistische, hierarchische regressieanalyse en causale inferentie. Ze lieten zien hoe je dat op de frequentistische en Bayesiaanse manier kunt doen. Het boek werd voor mij een naslagwerk dat ik steeds maar weer uit de kast trok. Vorig jaar dacht ik, laat ik eens zien of Gelman al weer iets nieuws heeft geschreven en toen zag ik dat Regression and other stories hiernet uit was is. Dat heeft Andrew Gelman weer met Jennifer Hill geschreven maar nu ook met de Fin Aki Vehtari. Ik was er nog niet aan toe gekomen om het te lezen. Dat heb ik deze maand gedaan. Ook dit boek zal ik vaker uit de kast trekken. Dit boek gaat over allerlei aspecten van regressie. Het is een theoretisch én praktisch boek. Je leert, wat ze noemen, voorspellende modellen beter begrijpen, toepassen in verschillende praktische problemen en je leert het simuleren. Je leert het opbouwen vanaf de basis en daarna kun je het in verschillende situaties toepassen. Het wil kritisch zijn, zonder nihilistisch te worden en vooral laten zien dat je van statistische analyse kunt leren. Ook dit boek staat op twee benen: frequentisch en Bayesiaans en laat zien hoe informatie wordt gebruikt in het schattingsproces, de assumpties die eraan ten grondslag liggen en hoe schattingen en voorspellingen kunnen worden geïnterpreteerd in beide raamwerken. Beide kunnen worden gebruikt, maar het is ook duidelijk dat de voorkeur bij Bayesiaanse benadering ligt. Dan kun je ook andere informatie gebruiken om te schatten of te voorspellen. En omdat je simuleert (het model duizenden keren draait) kunt je met de Bayesiaanse techniek meer zeggen over onzekerheid. Dat maakt deze techniek zeer geschikt voor regressieanalyses zoals in dit boek gepresenteerd. Wat ik zelf van dit boek heb geleerd zijn de mogelijkheden om op basis van gegevens te voorspellen. Vooral hoofdstuk 9 (Voorspellen en Bayesiaanse inferentie) vond ik interessant. Maar het boek zit vol informatie en kennis en laat zich amper samenvatten. Het lijkt erop dat het een eerste deel is en ik verwacht dat er later nog een tweede deel komt dat de nadruk legt op multilevel analyse. We zullen zien Bij het boek zit ook nog een website met data en scripts om zelf uit te proberen, prachtig onderwijsmateriaal opgesteld door Aki Vehtari hier."
  },
  {
    "objectID": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#interssante-blog",
    "href": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#interssante-blog",
    "title": "Regressie en nog zo iets",
    "section": "Interssante blog",
    "text": "Interssante blog\nToen ik het boek uit had kwam ik een een blog tegen op R-bloggers. Het verscheen op 1 september 2021 hier, maar ik kon niet zien van wie het is (Mister X, sorry. Hij of zij schreef het nadat deze persoon Regression and other stories had gelezen. Het vat heel goed samen hoe moderne regressieanalyse werkt en daarom heb ik het voor hier vertaald.\n\n# Eerst de pakketten inladen die we nodig hebben\nlibrary(plyr); library(dplyr)\n\nWarning: package 'plyr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:plyr':\n\n    arrange, count, desc, failwith, id, mutate, rename, summarise,\n    summarize\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(rstanarm)\n\nWarning: package 'rstanarm' was built under R version 4.1.3\n\n\nLoading required package: Rcpp\n\n\nWarning: package 'Rcpp' was built under R version 4.1.3\n\n\nThis is rstanarm version 2.21.3\n\n\n- See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n\n\n- Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n\n\n- For execution on a local, multicore CPU with excess RAM we recommend calling\n\n\n  options(mc.cores = parallel::detectCores())\n\nlibrary(bayesplot)\n\nWarning: package 'bayesplot' was built under R version 4.1.3\n\n\nThis is bayesplot version 1.9.0\n\n\n- Online documentation and vignettes at mc-stan.org/bayesplot\n\n\n- bayesplot theme set to bayesplot::theme_default()\n\n\n   * Does _not_ affect other ggplot2 plots\n\n\n   * See ?bayesplot_theme_set for details on theme setting\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\nlibrary(readr)\n\nWarning: package 'readr' was built under R version 4.1.3"
  },
  {
    "objectID": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#bayesiaanse-regressieanalyse-met-rstanarm",
    "href": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#bayesiaanse-regressieanalyse-met-rstanarm",
    "title": "Regressie en nog zo iets",
    "section": "Bayesiaanse regressieanalyse met Rstanarm",
    "text": "Bayesiaanse regressieanalyse met Rstanarm\nIn deze post zullen we een eenvoudig voorbeeld van Bayesiaanse regressieanalyse doornemen met het rstanarm pakket in R. Ik heb Gelman, Hill en Vehtari’s recente boek Regression and Other Stories” gelezen, en deze blog post is mijn poging om enkele van de dingen die ik heb geleerd toe te passen. Ik heb de afgelopen jaren stukjes en beetjes van de Bayesiaanse benadering opgevangen, en ik vind het een heel interessante manier om over gegevensanalyse na te denken en ze uit te voeren. Ik heb met veel plezier het nieuwe boek van Gelman en collega’s doorgewerkt en geëxperimenteerd met deze technieken, en ik ben blij dat ik hier iets kan delen van wat ik heb geleerd.\nJe kunt de gegevens en alle code van deze blogpost hier op Github vinden."
  },
  {
    "objectID": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#de-data",
    "href": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#de-data",
    "title": "Regressie en nog zo iets",
    "section": "De data",
    "text": "De data\nDe gegevens die we in deze blog zullen onderzoeken bestaan uit de dagelijkse totale stappentellingen van verschillende fitnesstrackers die ik de afgelopen 6 jaar heb gehad. De eerste waarneming werd geregistreerd op 2015-03-04 en de laatste op 2021-03-15. Gedurende deze periode bevat de dataset de dagelijkse totale stappentellingen voor 2.181 dagen.\nNaast de dagelijkse totale stappentelling bevat de dataset informatie over de dag van de week (bijv. maandag, dinsdag, etc.), het apparaat dat is gebruikt om de stappentelling vast te leggen (door de jaren heen heb ik er 3 gehad - Accupedo, Fitbit en Mi-Band), en het weer voor elke datum (de gemiddelde dagelijkse temperatuur in graden Celsius en de totale dagelijkse neerslag in millimeters, verkregen via het GSODR pakket in R).\nDe dataset (genaamd steps_weather) ziet er als volgt uit:\n\n# Data inladen\nlibrary(readr)\nsteps_weather &lt;- read_csv(\"C:/FilesHarrie/Stanexample/Rstanarmexample/bayesian_regression_rstanarm/Data/steps_weather.csv\")\n\nRows: 2181 Columns: 7\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr  (3): dow, week_weekend, device\ndbl  (3): daily_total, temp, prcp\ndate (1): date\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(steps_weather)\n\n# A tibble: 6 x 7\n  date       daily_total dow   week_weekend device    temp  prcp\n  &lt;date&gt;           &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 2015-03-04       14136 Wed   Weekday      Accupedo   4.3   1.3\n2 2015-03-05       11248 Thu   Weekday      Accupedo   4.7   0  \n3 2015-03-06       12803 Fri   Weekday      Accupedo   5.4   0  \n4 2015-03-07       15011 Sat   Weekend      Accupedo   7.9   0  \n5 2015-03-08        9222 Sun   Weekend      Accupedo  10.2   0  \n6 2015-03-09       21452 Mon   Weekday      Accupedo   8.8   0  \n\n\nHieronder zie je de eerste en laatste data.\n\nmin(steps_weather$date)\n\n[1] \"2015-03-04\"\n\nmax(steps_weather$date)\n\n[1] \"2021-03-15\"\n\n\nHieronder zie je histogrammen van twee variabelen, namelijk dagelijks totale aantal stappen en de gemiddelde temperatuur over deze periode.\n\nhist(steps_weather$daily_total, breaks = 50)\n\n\n\nhist(steps_weather$temp, breaks = 50)"
  },
  {
    "objectID": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#regressie-analyse",
    "href": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#regressie-analyse",
    "title": "Regressie en nog zo iets",
    "section": "Regressie Analyse",
    "text": "Regressie Analyse\nHet doel van deze blogbijdrage is Bayesiaanse regressiemodellering te verkennen met behulp van het rstanarm pakket. Daarom zullen we de gegevens gebruiken om een zeer eenvoudig model te maken en ons te concentreren op het begrijpen van de modelfit en verschillende regressiediagnoses.\nOns model hier is een lineair regressiemodel dat de gemiddelde temperatuur in graden Celsius gebruikt om het totale dagelijkse aantal stappen te voorspellen. We gebruiken het stan_glm commando om de regressie-analyse uit te voeren. We kunnen het model uitvoeren en een samenvatting van de resultaten zien die de volgende tabel oplevert.\nHet draait 4.000 iteraties en daarna worden de resultaten gepresenteerd.\n\nfit_1 &lt;- stan_glm(daily_total ~ temp, data = steps_weather) \n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.473 seconds (Warm-up)\nChain 1:                0.184 seconds (Sampling)\nChain 1:                0.657 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.277 seconds (Warm-up)\nChain 2:                0.174 seconds (Sampling)\nChain 2:                0.451 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.584 seconds (Warm-up)\nChain 3:                0.168 seconds (Sampling)\nChain 3:                0.752 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.884 seconds (Warm-up)\nChain 4:                0.174 seconds (Sampling)\nChain 4:                1.058 seconds (Total)\nChain 4: \n\nsummary(fit_1)\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      daily_total ~ temp\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 2181\n predictors:   2\n\nEstimates:\n              mean    sd      10%     50%     90%  \n(Intercept) 16218.9   276.0 15869.3 16215.8 16580.9\ntemp           26.6    21.0    -0.9    26.4    53.5\nsigma        6199.8    96.4  6082.2  6198.2  6322.6\n\nFit Diagnostics:\n           mean    sd      10%     50%     90%  \nmean_PPD 16519.2   190.8 16279.1 16518.9 16767.0\n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   4.3  1.0  4169 \ntemp          0.3  1.0  4436 \nsigma         1.7  1.0  3317 \nmean_PPD      3.1  1.0  3896 \nlog-posterior 0.0  1.0  1716 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\nDeze tabel bevat de volgende variabelen:\n\nIntercept: Dit cijfer geeft het verwachte dagelijkse aantal stappen weer wanneer de gemiddelde dagtemperatuur 0 is. Met andere woorden, het model voorspelt dat, wanneer de gemiddelde dagtemperatuur 0 graden Celsius is, ik op die dag 16211,7 stappen zal lopen.\ntemp: Dit is de geschatte toename van het dagelijkse aantal stappen per 1 eenheid stijging van de gemiddelde dagelijkse temperatuur in graden Celsius. Met andere woorden, het model voorspelt dat voor elke 1 graad stijging van de gemiddelde dagtemperatuur, ik die dag 26,8 extra stappen zal zetten.\nsigma: Dit is de geschatte standaardafwijking van de residuen van het regressiemodel. (Het residu is het verschil tussen de modelvoorspelling en de waargenomen waarde voor het dagelijkse totale aantal stappen). De verdeling van de residuele waarden heeft een standaardafwijking van 6195,1.\ngeman_PPD: De mean_ppd is de gemiddelde posterior predictive distributie van de door het model geïmpliceerde uitkomstvariabele (we zullen dit verder bespreken in het gedeelte over posterior predictive checques hieronder).\n\nDe uitvoer in de samenvattende tabel hierboven lijkt vrij veel op de uitvoer van een standaard gewone kleinste kwadratenregressie. In de Bayesiaanse benadering van regressiemodellering krijgen we echter niet gewoon puntschattingen van coëfficiënten, maar eerder volledige verdelingen van simulaties die mogelijke waarden van de coëfficiënten vertegenwoordigen, gegeven het model. Met andere woorden, de getallen in de bovenstaande tabel zijn gewoon samenvattingen van verdelingen van coëfficiënten die de relatie tussen de voorspellers en de uitkomstvariabele beschrijven.\nStandaard geven de rstanarm regressiemodellen 4.000 simulaties van de posterior verdeling voor elke modelparameter. We kunnen de simulaties uit het modelobject halen en ze als volgt bekijken:\n\n# extraheer de simulaties van het modelobject\nsims &lt;- as.matrix(fit_1)\n\nEn dat geeft 4000 posterior simulaties van de parameters intercept, temp en sd. Deze simulaties drukken de onzekerheid uit van de modeloutput die je hierboven vindt\n\nhead(sims)\n\n          parameters\niterations (Intercept)      temp    sigma\n      [1,]    16172.03 34.679624 6116.416\n      [2,]    16380.96 22.963727 6173.921\n      [3,]    16054.89 29.856779 6223.247\n      [4,]    16107.86 28.293847 6290.878\n      [5,]    16487.89 18.927277 6172.173\n      [6,]    16692.51  6.882619 6199.710\n\n\nDe gemiddelde waarden van deze verdelingen van simulaties worden weergegeven in de hierboven afgebeelde tabel met regressiesamenvattingsuitvoer.\n\n# gemiddelde en intercept - matchen met de tabel \nmean(sims[,1])\n\n[1] 16218.87\n\nsd(sims[,1])\n\n[1] 275.9854\n\n# gemiddelde en sd temp - matchen met de tabel \nmean(sims[,2])\n\n[1] 26.62242\n\nmedian(sims[,2])\n\n[1] 26.44791\n\nsd(sims[,2])\n\n[1] 21.03643\n\n# gemiddelde en sd sigma - matchen met de tabel \nmean(sims[,3])\n\n[1] 6199.79\n\nsd(sims[,3])\n\n[1] 96.39904"
  },
  {
    "objectID": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#visualiseren-van-de-posterior-distributies-met-bayesplot",
    "href": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#visualiseren-van-de-posterior-distributies-met-bayesplot",
    "title": "Regressie en nog zo iets",
    "section": "Visualiseren van de posterior distributies met bayesplot",
    "text": "Visualiseren van de posterior distributies met bayesplot\nHet uitstekende bayesplot-pakket bevat een aantal handige functies om de posterior distributies van onze coëfficiënten te visualiseren. Laten we de mcmc_areas-functie gebruiken om de 90% geloofwaardige intervallen voor de modelcoëfficiënten weer te geven. die de volgende plot oplevert.\n\ncolor_scheme_set(\"red\")\nplot_title &lt;- ggtitle(\"Posterior distributies\",\n                      \"met medianen en 90% intervallen\")\nmcmc_areas(sims, prob = 0.90) + plot_title\n\n\n\n\nDeze grafiek is zeer interessant en toont ons de posterior distributie van de simulaties van het model dat we hierboven hebben getoond. De plot geeft ons een idee van de variatie van alle parameters, maar de coëfficiënten liggen op zulke verschillende schalen dat de details verloren gaan door ze allemaal samen weer te geven.\nLaten we ons concentreren op de temperatuurcoëfficiënt en een gebiedsplot maken met alleen deze parameter:\n\n# area plot van temperatuur parameter\nmcmc_areas(sims,\n          pars = c(\"temp\"),\n          prob = 0.90) + plot_title\n\n\n\n\nDeze grafiek toont de mediaan van de verdeling (26,69, die zeer dicht bij ons gemiddelde van 26,83 ligt). We kunnen de grenzen van het hierboven getoonde gearceerde gebied bepalen met de posterior_interval functie, of rechtstreeks uit de simulaties zelf:\n\nposterior_interval(fit_1, pars = \"temp\", prob=.9)\n\n           5%      95%\ntemp -8.21517 60.75168\n\n# of rechtstreeks van de posterior distribution\nquantile(sims[,2], probs = c(.05,.95))  \n\n      5%      95% \n-8.21517 60.75168 \n\n\nBeide methoden geven hetzelfde resultaat."
  },
  {
    "objectID": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#visualiseren-van-slopes-van-de-posterior-distributie",
    "href": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#visualiseren-van-slopes-van-de-posterior-distributie",
    "title": "Regressie en nog zo iets",
    "section": "Visualiseren van slopes van de posterior distributie",
    "text": "Visualiseren van slopes van de posterior distributie\nEen andere interessante manier om de verschillende coëfficiënten uit de posterior verdeling te visualiseren is door de regressielijnen van vele simulaties uit de posterior distributie gelijktijdig uit te zetten tegen de ruwe data. Deze visualisatietechniek wordt veel gebruikt in zowel Richard McElreath’s Statistical Rethinking als in Gelmans Regression and Other Stories. Beide boeken maken dit soort tekeningen met behulp van plotfuncties in basis-R. Ik was erg blij deze blog post te vinden met een voorbeeld van hoe je deze plots kan maken met behulp van ggplot2! Ik heb de code lichtjes aangepast om de onderstaande figuur te maken.\nDe eerste stap is het extraheren van de basisinformatie om elke regressielijn te tekenen. We doen dit met de volgende code, waarbij we in essentie ons model-object doorgeven aan een dataframe, en dan enkel het intercept en de temperatuur-hellingen voor elk van onze 4.000 simulaties uit de posterior distributie houden.\nDit geeft het volgende dataframe terug:\n\n# Dit is een data-frame van posterior samples \n# Een rij per sample.\nfits &lt;- fit_1 %&gt;% \n  as_tibble() %&gt;% \n  rename(intercept = `(Intercept)`) %&gt;% \n  select(-sigma)\n# hoe ziet dat dataframe eruit?\nhead(fits)\n\n# A tibble: 6 x 2\n  intercept  temp\n      &lt;dbl&gt; &lt;dbl&gt;\n1    16172. 34.7 \n2    16381. 23.0 \n3    16055. 29.9 \n4    16108. 28.3 \n5    16488. 18.9 \n6    16693.  6.88\n\n\nDit dataframe heeft 4.000 rijen, één voor elke simulatie uit de posterior verdeling in onze originele sims matrix.\nWe stellen dan enkele “esthetische regelaars” in, die specificeren hoeveel lijnen van de posterior verdeling we willen plotten, hun transparantie (de alpha parameter), en de kleuren voor de individuele posterior lijnen en het algemene gemiddelde van de posterior schattingen. De ggplot2 code stelt dan de assen in met het originele data frame (steps_weather), plot een steekproef van regressielijnen uit de posterior verdeling in licht grijs, en plot dan de gemiddelde helling van alle posterior simulaties in blauw.\nDat levert de volgende plot op:\n\n# Eerst de opmaak instellen\nn_draws &lt;- 500\nalpha_level &lt;- .15\ncolor_draw &lt;- \"grey60\"\ncolor_mean &lt;-  \"#3366FF\"\n\n# plot maken\nggplot(steps_weather) + \n  # eerst - eerst de assen van de originele data bepalen\n  aes(x = temp, y = daily_total ) + \n  # restrictie opleggen aan y-as om de focus te leggen op de verschillende slopes in het\n  # centrum van de data\n  coord_cartesian(ylim = c(15000, 18000)) +\n  # Plot een random sample van rijen van  de simulatie\n  # als grijze semi-transparante lijnen\n  geom_abline(\n    aes(intercept = intercept, slope = temp), \n    data = sample_n(fits, n_draws), \n    color = color_draw, \n    alpha = alpha_level\n  ) + \n  # Plot de gemiddelde waarden van onze parameters in blauw\n  # dit correspondeert met de coefficienten die we terugkregen van onze \n  # modelsamenvatting\n  geom_abline(\n    intercept = mean(fits$intercept), \n    slope = mean(fits$temp), \n    size = 1, \n    color = color_mean\n  ) +\n  geom_point() + \n  # definieer de aslabels en de titel van de plot\n  labs(x = 'Gemiddelde dagelijkse temperatuur (Graden Celsius)', \n       y = 'Dagelijkse totale aantal stappen' , \n       title = 'Visualisatie of 500 regressie lijnnen van de posterior eistributie')\n\n\n\n\nDe gemiddelde helling (weergegeven in de grafiek en ook terug te vinden in de modelsamenvatting hierboven) van de temperatuur is 26,8. Maar het plotten van samples uit de posterior verdeling maakt duidelijk dat er nogal wat onzekerheid is over de grootte van deze relatie! Sommige van de hellingen uit de verdeling zijn negatief - zoals we zagen in onze berekening van de onzekerheidsintervallen hierboven. In essentie is er een “gemiddelde” coëfficiëntschatting, maar wat het Bayesiaanse raamwerk heel goed doet (via de posterior verdelingen) is extra informatie verschaffen over de onzekerheid van onze schattingen."
  },
  {
    "objectID": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#posterior-voorspellingscontroles",
    "href": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#posterior-voorspellingscontroles",
    "title": "Regressie en nog zo iets",
    "section": "Posterior voorspellingscontroles",
    "text": "Posterior voorspellingscontroles\nEen laatste manier om grafieken te gebruiken om ons model te begrijpen is door gebruik te maken van posterior predictive checks. Ik hou van deze intuïtieve manier om de logica achter deze reeks technieken uit te leggen: “Het idee achter posterior predictive checking is simpel: als een model een goede fit is, moeten we het kunnen gebruiken om gegevens te genereren die veel lijken op de gegevens die we hebben waargenomen. De gegenereerde gegevens worden de posterior predictive distributie genoemd, dat is de verdeling van de uitkomstvariabele (dagelijks totaal aantal stappen in ons geval) die wordt geïmpliceerd door een model (het regressiemodel dat hierboven is gedefinieerd). Het gemiddelde van deze verdeling wordt weergegeven in de bovenstaande uitvoer van het regressieoverzicht, met de naam mean_PPD.\nEr zijn vele soorten visualisaties die men kan maken om posterieure voorspellende controles uit te voeren. Wij zullen één zo’n analyse uitvoeren (voor meer informatie over dit onderwerp), die het gemiddelde van onze uitkomstvariabele (dagelijks totaal aantal stappen) in onze oorspronkelijke dataset en de posterior predictive distributie van ons regressie model.\nDe code is rechttoe rechtaan.\n\n# posterior predictive checks\n# https://mc-stan.org/bayesplot/reference/pp_check.html\n# http://mc-stan.org/rstanarm/reference/pp_check.stanreg.html\n\n# Het idee achter posterior predictive checking is eenvoudig: als een model een goede  \n# fit heeft dan moeten we data kunnen genereren die erg lijken op de data die we hebben geobserveerd.\n#  Om die data te genereren voor posterior predictive checks (PPCs), simuleren we die van de posterior predictive distributie. \n\n# posterior predictive check - voor meer informatie zie:\n# https://mc-stan.org/bayesplot/reference/pp_check.html\n# http://mc-stan.org/rstanarm/reference/pp_check.stanreg.html\npp_check(fit_1, \"stat\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nWe kunnen zien dat het gemiddelde van onze dagelijkse stappen variabele in de originele dataset in principe in het midden van de posterior voorspellende distributie valt. Volgens deze analyse “genereert ons regressiemodel gegevens die veel lijken op de gegevens die we hebben geobserveerd!”"
  },
  {
    "objectID": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#het-model-gebruiken-om-voorspellingen-te-doen-met-nieuwe-gegevens",
    "href": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#het-model-gebruiken-om-voorspellingen-te-doen-met-nieuwe-gegevens",
    "title": "Regressie en nog zo iets",
    "section": "Het model gebruiken om voorspellingen te doen met nieuwe gegevens",
    "text": "Het model gebruiken om voorspellingen te doen met nieuwe gegevens\nTenslotte zullen we het model gebruiken om voorspellingen te doen over het aantal stappen per dag, gebaseerd op een specifieke waarde van de gemiddelde dagtemperatuur. In Regression and Other Stories bespreken de auteurs in hoofdstuk 9 hoe een Bayesiaans regressiemodel kan worden gebruikt om voorspellingen te doen op een aantal verschillende manieren, waarbij telkens verschillende niveaus van onzekerheid in de voorspellingen worden opgenomen. Wij zullen elk van deze methoden achtereenvolgens toepassen.\nVoor elk van de onderstaande methoden zullen we het gemiddelde aantal stappen per dag voorspellen wanneer de temperatuur 10 graden Celsius bedraagt. We kunnen een nieuw dataframe opzetten dat we zullen gebruiken om de modelvoorspellingen te verkrijgen:\n\n# Gebruik het model om voorspellingen te doen\n# defineer nieuwe data van waaruit we deze voorspellingen maken \n# we doen voorspellingen voor het geval de gemiddelde dagtemperatuur 10 graden Celsius is for when the average daily temperature \nnew &lt;- data.frame(temp = 10)"
  },
  {
    "objectID": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#puntvoorspellingen-met-behulp-van-de-samenvattingen-van-de-afzonderlijke-waardecoëfficiënten-van-de-posterieure-verdelingen",
    "href": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#puntvoorspellingen-met-behulp-van-de-samenvattingen-van-de-afzonderlijke-waardecoëfficiënten-van-de-posterieure-verdelingen",
    "title": "Regressie en nog zo iets",
    "section": "Puntvoorspellingen met behulp van de samenvattingen van de afzonderlijke waardecoëfficiënten van de posterieure verdelingen",
    "text": "Puntvoorspellingen met behulp van de samenvattingen van de afzonderlijke waardecoëfficiënten van de posterieure verdelingen\nDe eerste benadering komt overeen met die welke we zouden gebruiken bij een klassieke regressieanalyse. We gebruiken gewoon de puntschattingen uit de modelsamenvatting, voegen de nieuwe temperatuur in waarvoor we een voorspelling willen, en produceren onze voorspelling in de vorm van een enkel getal. We kunnen dit doen met de predict functie in R, of door de coëfficiënten uit onze modelsamenvatting te vermenigvuldigen. Beide methoden leveren dezelfde voorspelling op:\n\n# gebruik simpel de puntsamenvatting van de posterio distributie \n# voor de modelcoefficienten (van de modelsamenvatting van hierboven)\ny_point_est &lt;- predict(fit_1, newdata = new)\n# zelfde predictie \"met de hand\"\ny_point_est_2 &lt;- mean(sims[,1]) + mean(sims[,2])*new\n\nBeide leveren een puntvoorspelling van 16483.71.\n\n# ze zijn hetzelfde \npredict(fit_1, newdata = new)\n\n      1 \n16485.1 \n\nmean(sims[,1]) + mean(sims[,2])*new\n\n     temp\n1 16485.1"
  },
  {
    "objectID": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#lineaire-voorspellingen-met-onzekerheid-in-de-interceptie-temperatuurcoëfficiënten",
    "href": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#lineaire-voorspellingen-met-onzekerheid-in-de-interceptie-temperatuurcoëfficiënten",
    "title": "Regressie en nog zo iets",
    "section": "Lineaire voorspellingen met onzekerheid (in de interceptie + temperatuurcoëfficiënten)",
    "text": "Lineaire voorspellingen met onzekerheid (in de interceptie + temperatuurcoëfficiënten)\nWe kunnen echter genuanceerder zijn in onze voorspelling van het dagelijkse totale aantal stappen. Het hierboven berekende regressiemodel geeft 4.000 simulaties voor drie parameters - de intercept, de temperatuurcoëfficiënt, en sigma (de standaardafwijking van de residuen).\nDe volgende methode is geïmplementeerd in rstanarm met de posterior_linpred functie, en we kunnen deze gebruiken om de voorspellingen direct te berekenen. We kunnen hetzelfde resultaat ook “met de hand” berekenen met behulp van de matrix van simulaties uit de posterior verdeling van onze coëfficiëntschattingen. Bij deze aanpak wordt gewoon de temperatuur ingevoerd waarvoor wij voorspellingen willen (10 graden Celsius) en wordt voor elk van de simulaties het intercept opgeteld bij de temperatuurcoëfficiënt maal 10. Beide methoden leveren dezelfde vector van 4.000 voorspellingen op:\n\n# lineaire predictor met onzekerheid met gebruikmaking van posterior_linpred\n\ny_linpred &lt;- posterior_linpred(fit_1, newdata = new)\n# uitrekenen \"met de hand\" \n# we gebruiken de sims matrix die we hierboven definieerden \n# sims &lt;- as.matrix(fit_1)\ny_linpred_2 &lt;- sims[,1] + (sims[,2]*10)  \n\nDat geeft dezelfde resultaten.\n\n# check - ze zijn hetzelfde!\nplot(y_linpred,y_linpred_2)\n\n\n\ncor.test(y_linpred, y_linpred_2)\n\n\n    Pearson's product-moment correlation\n\ndata:  y_linpred and y_linpred_2\nt = Inf, df = 3998, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 1 1\nsample estimates:\ncor \n  1"
  },
  {
    "objectID": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#posterior-predictive-distributies-met-de-onzekerheid-in-de-coëfficiëntschattingen-en-in-sigma",
    "href": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#posterior-predictive-distributies-met-de-onzekerheid-in-de-coëfficiëntschattingen-en-in-sigma",
    "title": "Regressie en nog zo iets",
    "section": "Posterior Predictive Distributies met de onzekerheid in de coëfficiëntschattingen en in sigma",
    "text": "Posterior Predictive Distributies met de onzekerheid in de coëfficiëntschattingen en in sigma\nDe laatste voorspellingsmethode voegt nog een extra laag van onzekerheid toe aan onze voorspellingen, door de posterior verdelingen voor sigma mee te nemen in de berekeningen. Deze methode is beschikbaar via de functie posterior_predict, en we gebruiken opnieuw onze matrix van 4.000 simulaties om een vector van 4.000 voorspellingen te berekenen.\nDe posterior predict methode volgt de aanpak van de posterior_linpred functie hierboven, maar voegt een extra foutterm toe gebaseerd op onze schattingen van sigma, de standaardafwijking van de residuen. De berekening zoals getoond in het “met de hand” gedeelte van de code hieronder maakt. Het maakt duidelijk waar de willekeurigheid in het spel komt, en vanwege deze willekeurigheid zullen de resultaten van de posterior_predict functie en de “met de hand” berekening niet overeenkomen tenzij we dezelfde seed instellen voordat we elke berekening uitvoeren. Beide methoden leveren een vector van 4.000 voorspellingen op.\n\n# predictive distributie voor een nieuwe observatie met gebruik van posterior_predict\n\nset.seed(1)\ny_post_pred &lt;- posterior_predict(fit_1, newdata = new)\n\nOf\n\n# uitrekenen \"met de hand\"\nn_sims &lt;- nrow(sims)\nsigma &lt;- sims[,3]\nset.seed(1)\ny_post_pred_2 &lt;- as.numeric(sims[,1] + sims[,2]*10) + rnorm(n_sims, 0, sigma)\n\nDan zien we dezelfde resultaten:\n\n# check - ze zijn hetzelfde!\nplot(y_post_pred, y_post_pred_2)\n\n\n\ncor.test(y_post_pred, y_post_pred_2)\n\n\n    Pearson's product-moment correlation\n\ndata:  y_post_pred and y_post_pred_2\nt = Inf, df = 3998, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 1 1\nsample estimates:\ncor \n  1"
  },
  {
    "objectID": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#visualisatie-van-de-drie-soorten-voorspellingen",
    "href": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#visualisatie-van-de-drie-soorten-voorspellingen",
    "title": "Regressie en nog zo iets",
    "section": "Visualisatie van de drie soorten voorspellingen",
    "text": "Visualisatie van de drie soorten voorspellingen\nLaten we een visualisatie maken die de resultaten weergeeft van de voorspellingen die we hierboven deden. We kunnen een enkele cirkel gebruiken om de puntvoorspelling van de regressiecoëfficiënten weer te geven in de modelsamenvatting, en histogrammen om de posterior verdelingen weer te geven die geproduceerd zijn door de lineaire voorspelling met onzekerheid (posterior_linpred) en posterior predictive distribution (posterior_predict) methoden die hierboven beschreven zijn.\nWe zetten eerst de vectoren van posterior verdelingen die we hierboven hebben gemaakt in een dataframe. We maken ook een dataframe met de enkele puntvoorspelling van onze lineaire voorspelling. Vervolgens stellen we ons kleurenpalet in (afkomstig uit het NineteenEightyR pakket) en maken dan de plot:\n\n# creeer een dataframe die de waarden van de posterior distributies omvat \n# van de voorspellingen van de totaal aantal dagelijkse stappen bij 10 grade Celcius\npost_dists &lt;- as.data.frame(rbind(y_linpred, y_post_pred)) %&gt;% \n      setNames(c('prediction'))\npost_dists$pred_type &lt;- c(rep('posterior_linpred', 4000),\n                          rep('posterior_predict', 4000))\ny_point_est_df = as.data.frame(y_point_est)\n\nDat geeft de volgende plot:\n\n# 70 en meer kleuren - via NineteenEightyR pakket\n# https://github.com/m-clark/NineteenEightyR\npal &lt;- c('#FEDF37', '#FCA811', '#D25117', '#8A4C19', '#573420')\n\nggplot(data = post_dists, aes(x = prediction, fill = pred_type)) + \n  geom_histogram(alpha = .75, position=\"identity\") + \n  geom_point(data = y_point_est_df,\n             aes(x = y_point_est,\n                 y = 100,\n                 fill = 'Linear Point Estimate'),\n             color =  pal[2],\n             size = 4,\n             # alpha = .75,\n             show.legend = F) +\n  scale_fill_manual(name = \"Prediction Method\",\n                    values = c(pal[c(2,3,5)]),\n                    labels = c(bquote(paste(\"Lineaire Punt Schatting \", italic(\"(predict)\"))),\n                               bquote(paste(\"Lineaire Voorspelling met Onzekerheid \" , italic(\"(posterior_linpred)\"))),\n                               bquote(paste(\"Posterior Predictive Distributie \",  italic(\"(posterior_predict)\"))))) +\n  # set the plot labels and title\n  labs(x = \"Predicted Daily Total Step Count\", \n       y = \"Aantal\", \n       title = 'Onzekerheid in Posterior Predictie Methode')   +\n  theme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nDeze grafiek is zeer informatief en maakt duidelijk hoe groot de onzekerheid is die we voor elk van onze voorspellingsmethoden krijgen. Hoewel alle drie voorspellingsmethoden op dezelfde plaats op de x-as gecentreerd zijn, verschillen zij sterk wat betreft de onzekerheid rond de voorspellingsramingen.\nDe puntvoorspelling is een enkele waarde en bevat als zodanig geen onzekerheid. De lineaire voorspelling met onzekerheid, die rekening houdt met de posterior verdeling van onze interceptie- en temperatuurcoëfficiënten, heeft een zeer scherpe piek, waarbij de modelschattingen binnen een relatief klein bereik variëren. De posterior predictive distributie varieert veel meer, met het laagste bereik van de verdeling onder nul, en het hoogste bereik van de verdeling boven 40.000!"
  },
  {
    "objectID": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#samenvatting-en-conclusie",
    "href": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#samenvatting-en-conclusie",
    "title": "Regressie en nog zo iets",
    "section": "Samenvatting en conclusie",
    "text": "Samenvatting en conclusie\nIn dit artikel hebben we een eenvoudig model gemaakt met behulp van het rstanarm pakket in R, om te leren over Bayesiaanse regressie analyse. We gebruikten een dataset bestaande uit mijn geschiedenis van dagelijkse totale stappen, en bouwden een regressie model om het dagelijkse aantal stappen te voorspellen uit de dagelijkse gemiddelde temperatuur in graden Celsius. In tegenstelling tot de gewone kleinste kwadraten benadering die puntschattingen van modelcoëfficiënten oplevert, geeft de Bayesiaanse regressie posterior verdelingen van de coëfficiëntschattingen. Wij hebben een aantal verschillende samenvattingen en visualisaties van deze posterior verdelingen gemaakt om de coëfficiënten en de Bayesiaanse benadering in het algemeen te begrijpen - A) het gebruik van het bayesplot pakket om de posterior verdelingen van onze coëfficiënten te visualiseren\nB) het plotten van 500 hellingen van de posterior verdeling, en\nC) het uitvoeren van een controle van de posterior predictive distributie."
  },
  {
    "objectID": "posts/2022-03-21-hierarchische-logistische-regressie-met-bayesiaanse-technieken/hirarchische-logistische-regressie-met-bayesiaanse-technieken.html",
    "href": "posts/2022-03-21-hierarchische-logistische-regressie-met-bayesiaanse-technieken/hirarchische-logistische-regressie-met-bayesiaanse-technieken.html",
    "title": "Hiërarchische logistische regressie met Bayesiaanse technieken",
    "section": "",
    "text": "Onlangs verscheen een prachtig boek van Alicia A. Johnson, Miles Q. Ot en Mine Dogucu onder de titel Bayes Rules! An Introduction to Applied Bayesian Modeling en het verscheen bij CRC Press (2022). Eerdere versies stonden kon je al via bookdown bekijken (https://www.bayesrulesbook.com/) en vanaf de eerste keer dat ik het zag, was ik hier heel enthousiast over. Het boek heb ik direct besteld en vorige week kon ik het ophalen.\nHet boek bestaat uit vier duidelijke delen. Het eerste deel gaat in op de fundamenten van het Bayesiaanse perspectief. Het leert je denken als een Bayesiaan en het gaat in op die belangrijke Bayesiaanse regel \\(posterior=\\frac{prior.likelihood}{normaliserende constante}\\). Aan de hand van enkele voorbeelden gaan Johnson e.a. in op hoe het in de praktijk werkt. Daarna gaat het in hoe kennis en data op elkaar inwerken en het laat enkele basisanalyses zien en hoe dat in deze vorm van statistiek werkt (normaal, binair en poisson). Het tweede deel is een meer technisch hoofdstuk en laat je ook onder de moterkap van deze techniek kijken. Het gaat ook op de wetenschappelijke principes van de benadering, waar je hier op moet letten, hoe je hiermee hypothesen kunt testen (niet alleen tov van een nulhypothese, maar hoeveel beter de ene hypothese is ten opzichte van de andere hyposthese) en hoe je hiermee ook kunt voorspellen. De twee volgende delen (Deel drie en vier) zijn praktische delen. Deel drie gaat in op regressieanalyses voor continue variabelen en classificatieanalyses voor binaire variabelen. Het vierde deel ten slotte gaat in op geclusterde datasets en hoe je hierarchische Bayesiaanse regressie en classificatieanalyses uitvoert.\nNatuurlijk, er zijn onderhand al verschillende interessante boeken te krijgen die je laten zien hoe Bayesiaanse denken in de praktijk kan werken. De boeken van Gelman, McElreath, Spiegelhalter en Kruschke verschenen de afgelopen tien/vijftine jaar en leren je dit. Maar Bayes Rules! vind ik op dit moment als introductieboek mogelijk wel het beste.\nNu het boek bij mij op het bureau ligt, kan ik er binnenkort een keer een korte recensie over schrijven. Voor nu heb ik uit elk deel een hoofdstuk genomen en het vertaald en bewerkt. Hieronder zie je een bewerking van een deel van het achttiende hoofdstuk van het vierde deel (Non-Normal Hierarchical Regression & Classification).Hoofdstukken zo overzetten is voor mij niet alleen een goede manier om het mij eigen te maken, maar ook een manier om het boek anderen aan te raden. Dus lezen en gebruiken deze Bayes Rules! An Introduction to Applied Bayesian Modeling"
  },
  {
    "objectID": "posts/2022-03-21-hierarchische-logistische-regressie-met-bayesiaanse-technieken/hirarchische-logistische-regressie-met-bayesiaanse-technieken.html#hierarchical-logistic-regression",
    "href": "posts/2022-03-21-hierarchische-logistische-regressie-met-bayesiaanse-technieken/hirarchische-logistische-regressie-met-bayesiaanse-technieken.html#hierarchical-logistic-regression",
    "title": "Hiërarchische logistische regressie met Bayesiaanse technieken",
    "section": "Hierarchical logistic regression",
    "text": "Hierarchical logistic regression\nEerst maar een enkele pakketten laden:\n\n# Laden van pakketten\nlibrary(bayesrules)\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv ggplot2 3.3.6     v purrr   0.3.4\nv tibble  3.1.7     v dplyr   1.0.9\nv tidyr   1.2.0     v stringr 1.4.1\nv readr   2.1.2     v forcats 0.5.1\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'stringr' was built under R version 4.1.3\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(bayesplot)\n\nWarning: package 'bayesplot' was built under R version 4.1.3\n\n\nThis is bayesplot version 1.9.0\n\n\n- Online documentation and vignettes at mc-stan.org/bayesplot\n\n\n- bayesplot theme set to bayesplot::theme_default()\n\n\n   * Does _not_ affect other ggplot2 plots\n\n\n   * See ?bayesplot_theme_set for details on theme setting\n\nlibrary(rstanarm)\n\nWarning: package 'rstanarm' was built under R version 4.1.3\n\n\nLoading required package: Rcpp\n\n\nWarning: package 'Rcpp' was built under R version 4.1.3\n\n\nThis is rstanarm version 2.21.3\n\n\n- See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n\n\n- Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n\n\n- For execution on a local, multicore CPU with excess RAM we recommend calling\n\n\n  options(mc.cores = parallel::detectCores())\n\nlibrary(tidybayes)\n\nWarning: package 'tidybayes' was built under R version 4.1.3\n\nlibrary(broom.mixed)\n\nWarning: package 'broom.mixed' was built under R version 4.1.3\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\n\nBergbeklimmers proberen grote hoogten te beklimmen in de majestueuze Nepalese Himalaya. Dit doen ze vanwege de sensatie van ijle lucht, de uitdaging of het buitenleven. Succes is niet gegarandeerd; slecht weer, defecte uitrusting, verwondingen of gewoon pech zorgen ervoor dat niet alle klimmers hun bestemming bereiken. Dit roept enkele vragen op. Hoe groot is de kans dat een bergbeklimmer de top haalt? Welke factoren kunnen bijdragen aan een hoger succespercentage? Naast het vage gevoel dat een gemiddelde klimmer 50% kans op succes heeft, wegen we dit zwak informatief inzicht af tegen data van klimmers die in het bayesrules pakket zitten. Dit deel van de data is beschikbaar gesteld door ‘The Himalayan Database’ (2020) en verspreid via het #tidytuesday project (R for Data Science 2020b):\n\n# Binnenhalen, herbenoemen & opschonen van data\ndata(climbers_sub)\nclimbers <- climbers_sub %>% \n  select(expedition_id, member_id, success, year, season,\n         age, expedition_role, oxygen_used)\n\nDeze dataset bevat de resultaten van 2076 klimmers vanaf 1978. Slechts 38,87% van hen slaagde erin de top te bereiken:\n\nnrow(climbers)\n\n[1] 2076\n\nclimbers %>% \n  tabyl(success)\n\n success    n   percent\n   FALSE 1269 0.6112717\n    TRUE  807 0.3887283\n\n\nOmdat member_id in essentie een rij van klimmersid is en we maar één observatie per klimmer hebben, is dit geen groepsvariabele. Verder, hoewel het seizon (seison), rol bij de expeditie (expedition_role) en het gebruik van zuurstof (oxygen_used) categorische variabelen zijn meerdere malen geobserveerd, zijn dit potentiële voorspellers van van succes (succes), ook geen groepsvariabele. Dan blijft expeditie_id (expedition_id) over - dit is wel een groepsvariabele. De dataset beslaat 200 verschillende expedities:\n\n# Omvang per expeditie\nclimbers_per_expedition <- climbers %>% \n  group_by(expedition_id) %>% \n  summarize(count = n())\n\n# Aantal expedities\nnrow(climbers_per_expedition)\n\n[1] 200\n\n\nElke expeditie bestaat uit meerdere klimmers. Zo vertrokken onze eerste drie expedities met respectievelijk 5, 6 en 12 klimmers:\n\nclimbers_per_expedition %>% \n  head(3)\n\n# A tibble: 3 x 2\n  expedition_id count\n  <chr>         <int>\n1 AMAD03107         5\n2 AMAD03327         6\n3 AMAD05338        12\n\n\nHet zou fout zijn om deze groepsstructuur te negeren en er anders van uit te gaan dat de individuele klimmers onafhankelijke resultaten boeken. Aangezien elke expeditie als een team werkt, hangt het succes of falen van de ene klimmer in díe expeditie gedeeltelijk af van het succes of falen van anderen in de groep. Bovendien vertrekken alle leden van een expeditie met dezelfde bestemming, met dezelfde leiders en onder dezelfde weersomstandigheden, en zijn dus onderhevig aan dezelfde externe succesfactoren. Het is dus niet alleen juist om rekening te houden met de groepering van de gegevens, maar het kan ook duidelijk maken in welke mate deze factoren variabiliteit veroorzaken in de succespercentages tussen expedities. Meer dan 75 van onze 200 expedities hadden een 0% succesratio - m.a.w. geen enkele klimmer in deze expedities slaagde erin de top te bereiken. Daarentegen hadden bijna 20 expedities een 100% succespercentage. Tussen deze extremen in, is er heel wat variatie in het succespercentage van de expedities.\n\n# Bereken de slagingskans voor elke expeditie\nexpedition_success <- climbers %>% \n  group_by(expedition_id) %>% \n  summarize(success_rate = mean(success))\n\n\n# Plot de slagingskansen over de expedities\nggplot(expedition_success, aes(x = success_rate)) + \n  geom_histogram(color = \"white\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nModel bouw en simulatie\nOm de ‘gegroepeerde’ aard van onze gegevens te weerspiegelen, laat \\(Y_ij\\) aangeven of klimmer \\(i\\) in expeditie\n\\(j\\) succesvol de top van hun piek bereikt:\n\\[\\\nY_ij = \\begin{cases}\n1 Ja \\\\\n0 Nee\\\\\n\\end{cases}\n\\]\\] Er zijn verschillende potentiële voorspellers voor het succes van klimmers in onze dataset. We kijken hier naar slechts twee voorspellers: de leeftijd van de klimmer en of hij extra zuurstof heeft gekregen om gemakkelijker te kunnen ademen op grote hoogte. Als zodanig, definiëren we: \\[ X_ij1=leeftijd van klimmer *i* in expeditie *j*\\]\n\\[X_ij2=of de klimmer in *i* in expeditie *j* zuurstof (oxygen) heeft gekregen\\] Door het aandeel van succes te berekenen bij elke combinatie van leeftijd en zuurstofgebruik, krijgen we een idee van hoe deze factoren gerelateerd zijn aan het klimmerssucces (zij het een wankel idee gezien de kleine steekproefgroottes van sommige combinaties). Kort samengevat lijkt het erop dat het succes van klimmers afneemt met de leeftijd en sterk toeneemt met het gebruik van zuurstof:\n\n# Bereken het slagingspercentage per leeftijd en zuurstofgebruik\ndata_by_age_oxygen <- climbers %>% \n  group_by(age, oxygen_used) %>% \n  summarize(success_rate = mean(success))\n\n`summarise()` has grouped output by 'age'. You can override using the `.groups`\nargument.\n\n# Plot deze relatie\nggplot(data_by_age_oxygen, aes(x = age, y = success_rate, \n                               color = oxygen_used)) + \n  geom_point()\n\n\n\n\nOm een Bayesiaans model van deze relatie op te stellen, erkennen we eerst dat het Bernoulli model redelijk is voor onze binaire responsvariabele \\(Y_ij\\). Stel \\(\\pi_ij\\) de waarschijnlijkheid is dat klimmer\\(i\\) in expeditie\\(j\\) zijn piek succesvol beklimt, d.w.z. dat \\(Y_ij=1\\),\n\\[Y_ij|\\pi_ij \\sim \\Bern{\\pi_ij}\\]\nDit is een complete pooling benadering waarbij een simpel model wordt omgezet in een logistisch regressie model van \\(Y\\) met enkele voorspellers \\(X\\)\n\\[Y_ij|\\beta_0,\\beta_1,beta_2 \\sim^{ind} \\Bernoulli(\\pi_ij) with log(\\frac{\\pi_ij}{1-\\pi_ij})=\\beta_0+\\beta_1X_ij1+\\beta_2X_ij2) \\\\\n\\beta_0c \\sim N(m_0,s_0^2) \\\\\n\\beta_1 \\sim N(m_1, s_1^2) \\\\\n\\beta_2 \\sim N(m_2, s_1^2)\\]\nDit is een goed begin, MAAR het houdt geen rekening met de groepsstructuur van onze data. Overweeg in plaats daarvan het volgende hiërarchische alternatief met onafhankelijke, zwak informatieve priors hieronder afgestemd via stan_glmer() en met een prior model voor \\(beta_0\\) uitgedrukt via het gecentreerde intercept \\(beta_0c\\). Het is immers zinvoller om na te denken over de baseline succesratio bij de typische/gemiddelde klimmer, \\(\\beta_0c\\), dan bij 0-jarige klimmers die geen zuurstof gebruiken, \\(\\beta_0\\)$. Daarom begonnen we onze analyse met de zwakke veronderstelling dat de typische klimmer een kans op succes heeft van 0,5, of met log(kans op succes)=0.\nxxx\nNet zo goed kunnen we dit logistische regressiemodel met willekeurige intercepts omvormen door de expeditiespecifieke intercepties uit te drukken als aanpassingen op het algemene intercept,\n\\[log(\\frac{\\pi_ij}{1-\\pi_ij})=(\\beta_0+b_0j) +\\beta_1X_ij1 + \\beta_2X_ij2\\]\nmet \\(\\beta_0j|\\sigma_0 \\sim^{ind} N(0,\\sigma_0^2)\\) Laten we eens naar de betekenis van en de veronderstellingen achter de modelparameters kijken:\n\nDe expeditie-specifieke intercepten \\(\\beta_0j\\) beschrijven de onderliggende succespercentages, zoals gemeten door de log(kans op succes), voor elke expeditie\\(j\\). Hiermee wordt erkend dat sommige expedities inherent succesvoller zijn dan andere.\nDe expeditiespecifieke intervallen \\(\\beta_0j\\) worden verondersteld normaal verdeeld te zijn rond een gemiddeld intercept \\(\\beta_0\\) met standaardafwijking \\(\\sigma_0\\). Daarmee beschrijft \\(\\beta_0\\) het typische basissucces over alle expedities, en \\(\\sigma_0\\) de tussen-groep variabiliteit in succespercentages van expeditie tot expeditie.\nBeta_1$ beschrijft het gemiddelde verband tussen succes en leeftijd wanneer gecontroleerd wordt voor zuurstofgebruik. Op dezelfde manier beschrijft \\(beta_2\\) de gemiddelde relatie tussen succes en zuurstofverbruik wanneer gecontroleerd wordt voor leeftijd.\n\nSamengevat maakt ons logistisch regressiemodel met willekeurige intercepten de vereenvoudigende (maar volgens ons redelijke) veronderstelling dat expedities unieke intercepten \\(\\beta_0j\\) kunnen hebben, maar delen de gemeenschappelijke regressieparameters \\(\\beta_1\\) en \\(\\beta_2\\). Anders gezegd, hoewel de onderliggende succespercentages kunnen verschillen van expeditie tot expeditie, zijn jonger zijn of zuurstof gebruiken niet voordeliger in de ene expeditie dan in de andere.\nOm de posterior van het model te simuleren, combineert de stan_glmer() code hieronder het beste van twee werelden: family = binomial geeft aan dat het om een logistisch regressiemodel gaat (à la Hoofdstuk 13) en de (1 | expeditie_id) term in de modelformule incorporeert onze hiërarchische groeperingsstructuur (à la Hoofdstuk 17): Consider the meaning of, and assumptions behind, the model parameters:\n\nDe expeditie-specifieke intercepts \\(\\beta_0j\\) beschrijven de onderliggende succespercentages, zoals gemeten door de log(kans op succes), voor elke expeditie\\(j\\). Hiermee wordt erkend dat sommige expedities inherent succesvoller zijn dan andere.\nDe expeditiespecifieke intervallen \\(\\beta_0j\\) worden verondersteld normaal verdeeld te zijn rond een globaal intercept \\(\\beta_0\\) met standaardafwijking \\(\\sigma_0\\). Daarmee beschrijft \\(\\beta_0\\) het typische basissucces over alle expedities, en \\(\\sigma_0\\) de tussen-groep variabiliteit in succespercentages van expeditie tot expeditie.\nBeta_1$ beschrijft het globale verband tussen succes en leeftijd wanneer gecontroleerd wordt voor zuurstofgebruik. Op dezelfde manier beschrijft \\(beta_2\\) de globale relatie tussen succes en zuurstofverbruik wanneer gecontroleerd wordt voor leeftijd.\n\nSamengevat maakt ons logistisch regressiemodel met willekeurige intercepten de vereenvoudigende (maar volgens ons redelijke) veronderstelling dat expedities unieke intercepten \\(\\beta_0j\\) kunnen hebben, maar gemeenschappelijke regressieparameters \\(\\beta_1\\) en \\(\\beta_2\\) delen. In gewone taal, hoewel de onderliggende succespercentages kunnen verschillen van expeditie tot expeditie, zijn jonger zijn of zuurstof gebruiken niet voordeliger in de ene expeditie dan in de andere.\nOm de posterior van het model te simuleren, combineert de stan_glmer() code, zie hieronder, het beste van twee werelden: family = binomial geeft aan dat het om een logistisch regressiemodel gaat en de (1 | expeditie_id) term in de modelformule incorporeert onze hiërarchische groepstructuur:\n\nclimb_model <- stan_glmer(\n  success ~ age + oxygen_used + (1 | expedition_id), \n  data = climbers, family = binomial,\n  prior_intercept = normal(0, 2.5, autoscale = TRUE),\n  prior = normal(0, 2.5, autoscale = TRUE), \n  prior_covariance = decov(reg = 1, conc = 1, shape = 1, scale = 1),\n  chains = 4, iter = 5000*2, seed = 84735\n)\n\n\nSAMPLING FOR MODEL 'bernoulli' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.001 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 10 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 79.289 seconds (Warm-up)\nChain 1:                72.661 seconds (Sampling)\nChain 1:                151.95 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'bernoulli' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 78.588 seconds (Warm-up)\nChain 2:                72.631 seconds (Sampling)\nChain 2:                151.219 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'bernoulli' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0.001 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 10 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 80.316 seconds (Warm-up)\nChain 3:                74.563 seconds (Sampling)\nChain 3:                154.879 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'bernoulli' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 81.826 seconds (Warm-up)\nChain 4:                75.631 seconds (Sampling)\nChain 4:                157.457 seconds (Total)\nChain 4: \n\n\nJe wordt aangemoedigd deze simulatie te volgen met de uitvoering van de code hierboven en te kijken naar enkele MCMC-diagnoses die hieronder staan. De r:\n\n# Bevestig prior specificaties\nprior_summary(climb_model)\n\n# MCMC diagnostiek\nmcmc_trace(climb_model, size = 0.1)\n\n\n\nmcmc_dens_overlay(climb_model)\n\n\n\nmcmc_acf(climb_model)\n\n\n\nneff_ratio(climb_model)\nrhat(climb_model)\n\nTerwijl deze diagnostiek bevestigt dat onze MCMC simulatie op het juiste spoor zit, geeft een posterior predictive check hieronder aan dat ons model op het juiste spoor zit. Van elk van de 100 posterior gesimuleerde datasets, stellen we de proportie klimmers vast die succesvol waren met de success_rate() functie. Deze succespercentages variëren van ruwweg 37% tot 41%, in een klein venster rond het werkelijk waargenomen succespercentage van 38.9% in de klimmers data.\n\n# Defineer slagingspercentage functie\nsuccess_rate <- function(x){mean(x == 1)}\n\n# Posterior predictive check\npp_check(climb_model, nreps = 100,\n         plotfun = \"stat\", stat = \"success_rate\") + \n  xlab(\"succes score\")\n\nWarning: 'nreps' is ignored for this PPC\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nPosterior analyse\nIn onze posterior analyse van het succes van bergbeklimmers, concentreren we ons op het geheel. Behalve dat we gerustgesteld zijn door het feit dat we correct rekening houden met de groepsstructuur van onze gegevens, zijn we niet geïnteresseerd in een specifieke expeditie. Hieronder volgen enkele posterior samenvattingen voor onze regressieparameters \\(\\beta_0\\), \\(\\beta_1\\) en \\(\\beta_2\\).\n\ntidy(climb_model, effects = \"fixed\", conf.int = TRUE, conf.level = 0.80)\n\n# A tibble: 3 x 5\n  term            estimate std.error conf.low conf.high\n  <chr>              <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)      -1.41     0.476    -2.04     -0.802 \n2 age              -0.0475   0.00940  -0.0596   -0.0358\n3 oxygen_usedTRUE   5.80     0.485     5.21      6.46  \n\n\nOm te beginnen zien we dat het 80% posterior ‘çredible’ (geloofwaardigheids) interval (CI) voor de age coëfficiënt \\(\\beta_1\\) ruim onder 0 ligt. We hebben dus significant posterior bewijs dat, wanneer we controleren of een klimmer al dan niet zuurstof gebruikt, de kans op succes afneemt met de leeftijd. Meer specifiek, als we de informatie in \\(\\beta_1\\) vertalen van de log(kansen) naar de kans schaal, is er 80% kans dat de kans op een succesvolle beklimming daalt tussen 3,5% en 5,8% voor elk jaar extra leeftijd: \\(e^{-0,0594}, e^{-0,0358}=(0,942, 0,965)\\).\nOp dezelfde manier levert het 80% posterior geloofwaardig interval voor de oxygen_usedTRUE coëfficiënt \\(beta_2\\) significant posterior bewijs dat, wanneer gecontroleerd wordt voor leeftijd, het gebruik van zuurstof de kans op het beklimmen van de top drastisch verhoogt. Er is een kans van 80% dat het gebruik van zuurstof kan overeenkomen met een 182- tot 617-voudige toename van de kans op succes: \\(e^{5.2}}, e^{6.43}=(182,617)\\), Zuurstof alstublieft!\nDoor onze waarnemingen voor \\(\\beta_1\\) en \\(\\beta_2\\) te combineren, wordt het posterior mediaan model voor de relatie tussen de log(kans op succes) van de klimmers en hun leeftijd (\\(X_1\\))en zuurstofgebruik (\\(X_2\\))\n\\[log(\\frac{\\pi}{1-\\pi})=-1.42-0.0474X_1+5.79X_2\\]\nOf, op de schaal van waarschijnlijkheid: \\[\\pi=\\frac{e^{-1.42-0.0474X_1+5.79X_2}}{1+e^{-1.42-0.0474X_1+5.79X_2}}\\]\nDit posterior mediaan model vertegenwoordigt slechts het midden van een bereik van posterior plausibele relaties tussen succes, leeftijd en zuurstofgebruik. Om een idee te krijgen van dit bereik, toont figuur hieronder 100 posterior plausibele alternatieve modellen. Zowel met als zonder zuurstof neemt de kans op succes af met de leeftijd. Bovendien, op elke leeftijd, is de kans op succes dramatisch hoger wanneer klimmers zuurstof gebruiken. Echter, onze zekerheid over deze trends varieert nogal per leeftijd. We hebben veel minder zekerheid over de slaagkans voor oudere klimmers met zuurstof dan voor jongere klimmers met zuurstof, voor wie de slaagkans over het geheel hoog is. Op dezelfde manier, maar minder drastisch, hebben we minder zekerheid over de slaagkans voor jongere klimmers die geen zuurstof gebruiken dan voor oudere klimmers die geen zuurstof gebruiken, voor wie de slaagkans uniform laag is.\n\nclimbers %>%\n  add_fitted_draws(climb_model, n = 100, re_formula = NA) %>%\n  ggplot(aes(x = age, y = success, color = oxygen_used)) +\n    geom_line(aes(y = .value, group = paste(oxygen_used, .draw)), \n              alpha = 0.1) + \n    labs(y = \"waarschijnlijkheid van succes\")\n\nWarning: `fitted_draws` and `add_fitted_draws` are deprecated as their names were confusing.\nUse [add_]epred_draws() to get the expectation of the posterior predictive.\nUse [add_]linpred_draws() to get the distribution of the linear predictor.\nFor example, you used [add_]fitted_draws(..., scale = \"response\"), which\nmeans you most likely want [add_]epred_draws(...).\n\n\n\n\n\n\n\nPosterior classificatie\nStel dat vier klimmers op een nieuwe expeditie gaan. Twee van hen zijn 20 jaar oud en twee zijn 60 jaar. Van beide leeftijdsgroepen is één klimmer van plan zuurstof te gebruiken en de andere niet:\n\n# Nieuwe expeditie\nnew_expedition <- data.frame(\n  age = c(20, 20, 60, 60), oxygen_used = c(FALSE, TRUE, FALSE, TRUE), \n  expedition_id = rep(\"new\", 4))\nnew_expedition\n\n  age oxygen_used expedition_id\n1  20       FALSE           new\n2  20        TRUE           new\n3  60       FALSE           new\n4  60        TRUE           new\n\n\nNatuurlijk willen ze allemaal weten hoe groot de kans is dat ze de top zullen bereiken. Om dit vast te stellen werken we hier met de posterior_predict() snelkoppelingsfunctie om 20.000 posterior voorspellingen (0 of 1) te simuleren voor elk van onze 4 nieuwe klimmers:\n\n# Posterior voorspellingen van binaire uitkomst\nset.seed(84735)\nbinary_prediction <- posterior_predict(climb_model, newdata = new_expedition)\n\n# Eerste drie voorspellingen\nhead(binary_prediction, 3)\n\n     1 2 3 4\n[1,] 0 1 0 0\n[2,] 1 1 0 1\n[3,] 1 1 0 1\n\n\nVoor elke klimmer wordt de kans op succes benaderd door het geobserveerde aandeel van succes onder hun 20.000 posterieure voorspellingen. Aangezien deze kansen de onzekerheid in het basissuccespercentage van de nieuwe expeditie omvatten, zijn ze gematigder dan de algemene trends die we eerder zichtbaar maakten.\n\n# Vat de posterior voorspellingen van Y samen:\ncolMeans(binary_prediction)\n\n      1       2       3       4 \n0.27815 0.80110 0.14630 0.64710 \n\n\nDeze voorspellingen geven meer inzicht in de verbanden tussen leeftijd, zuurstof, en succes. Bijvoorbeeld, onze posterior voorspelling is dat klimmer 1, die 20 jaar oud is en niet van plan is om zuurstof te gebruiken, 27.88% kans heeft om de top te halen. Deze kans is natuurlijk lager dan voor klimmer 2, die ook 20 is maar wel van plan is om zuurstof te gebruiken. Het is hoger dan de posterior voorspelling van succes voor klimmer 3, die ook niet van plan is zuurstof te gebruiken maar wel 60 jaar oud is. Over het algemeen is de voorspelling van succes het hoogst voor klimmer 2, die jonger is en van plan is zuurstof te gebruiken, en het laagst voor klimmer 3, die ouder is en niet van plan is zuurstof te gebruiken.\nPosterior kans voorspellingen kunnen omgezet worden in posterior classificaties van binaire uitkomsten: ja of nee, verwachtingen of de klimmer zal slagen of niet? Als we een eenvoudige cut-off van 0,5 zouden gebruiken om dit te bepalen, dan zouden we klimmers 1 en 3 aanraden niet aan de expeditie deel te nemen (tenminste, niet zonder zuurstof) en klimmers 2 en 4 het groene licht geven. Maar in deze specifieke context moeten we het waarschijnlijk aan de individuele klimmers overlaten om hun eigen resultaten te interpreteren en hun eigen ja-of-nee beslissingen te nemen over het al dan niet voortzetten van hun expeditie. Zo kan een kans op succes van 65,16% voor sommigen de moeite en het risico waard zijn, maar voor anderen niet.\n\n\nModel evaluatie\nOm onze klimanalyse af te ronden, vragen we ons af: Is ons hiërarchisch-logistisch model een goed model? Lang verhaal kort, het antwoord is ja. - Ten eerste, ons model is eerlijk. De gegevens die we hebben gebruikt zijn openbaar en we verwachten niet dat onze analyse een negatief effect zal hebben op individuen of de samenleving. (Nogmaals, saaie antwoorden op de vraag naar eerlijkheid zijn de beste soort.)\n- Ten tweede Posterior Predictive Checque controle toonde aan dat ons model niet al te verkeerd lijkt - onze posterior gesimuleerde succespercentages schommelen rond de waargenomen succespercentages in onze gegevens.\n- Tenslotte, voor de vraag naar posterior classificatie nauwkeurigheid, kunnen we onze posterior classificaties van succes vergelijken met de werkelijke uitkomsten voor de 2076 klimmers in onze dataset. Standaard beginnen we met een kans cut-off van 0.5 - als de kans op succes van een klimmer groter is dan 0.5, voorspellen we dat hij zal slagen. We implementeren en evalueren deze classificatieregel met classification_summary() hieronder.\n\nset.seed(84735)\nclassification_summary(data = climbers, model = climb_model, cutoff = 0.5)\n\n$confusion_matrix\n     y    0   1\n FALSE 1172  97\n  TRUE   77 730\n\n$accuracy_rates\n                          \nsensitivity      0.9045849\nspecificity      0.9235619\noverall_accuracy 0.9161850\n\n\nIn het algemeen voorspelt ons model met deze classificatieregel de resultaten goed voor 91,61% van onze klimmers. Dit ziet er behoorlijk fantastisch uit gezien het feit dat we enkel informatie gebruiken over de leeftijd en het zuurstofverbruik van de klimmers (terwijl er nog andere voorspellers te bedenken zijn (bv. bestemming, seizoen, enz.). Maar gezien de gevolgen van een foute classificatie in deze specifieke context (bv. risico op verwondingen), moeten we voorrang geven aan specificiteit, ons vermogen om te anticiperen wanneer een klimmer niet zou slagen. Om dit te bereiken voorspelde ons model slechts 92.51% van de mislukte beklimmingen correct. Om dit percentage te verhogen, kunnen we de waarschijnlijkheidsgrens in onze classificatieregel aanpassen.\nIn het algemeen kunnen we, om de specificiteit te verhogen, de waarschijnlijkheidsdrempel verhogen, waardoor het moeilijker wordt om “succes” te voorspellen. Na wat trial and error lijkt het erop dat cut-offs van ruwweg 0.65 of hoger een gewenst specificiteitsniveau van 95% zullen bereiken. Deze overschakeling naar 0.65 verlaagt natuurlijk de gevoeligheid van onze posterior classificaties, van 90.46% naar 81.54%, en dus ons vermogen om te detecteren wanneer een klimmer succesvol zal zijn. Wij denken dat de extra voorzichtigheid hier van belang is.\n\nset.seed(84735)\nclassification_summary(data = climbers, model = climb_model, cutoff = 0.65)\n\n$confusion_matrix\n     y    0   1\n FALSE 1213  56\n  TRUE  149 658\n\n$accuracy_rates\n                          \nsensitivity      0.8153656\nspecificity      0.9558708\noverall_accuracy 0.9012524"
  },
  {
    "objectID": "posts/2022-03-21-hierarchische-logistische-regressie-met-bayesiaanse-technieken/hirarchische-logistische-regressie-met-bayesiaanse-technieken.html#literatuur",
    "href": "posts/2022-03-21-hierarchische-logistische-regressie-met-bayesiaanse-technieken/hirarchische-logistische-regressie-met-bayesiaanse-technieken.html#literatuur",
    "title": "Hiërarchische logistische regressie met Bayesiaanse technieken",
    "section": "Literatuur",
    "text": "Literatuur\nFast, Shannon, and Thomas Hegland. 2011. “Book Challenges: A Statistical Examination.” Project for Statistics 316-Advanced Statistical Modeling, St. Olaf College.\nLegler, Julie, and Paul Roback. 2021. Beyond Multiple Linear Regression: Applied Generalized Linear Models and Multilevel Models in R. Chapman; Hall/CRC. https://bookdown.org/roback/bookdown-BeyondMLR/. ———. 2020b. “Himalayan Climbing Expeditions.” TidyTuesday Github Repostitory. https://github.com/rfordatascience/tidytuesday/tree/master/data/2020/2020-09-22.\nThe Himalayan Database. 2020. https://www.himalayandatabase.com/. Trinh, Ly, and Pony Ameri. 2016. “AirBnB Price Determinants: A Multilevel Modeling Approach.” Project for Statistics 316-Advanced Statistical Modeling, St. Olaf College."
  },
  {
    "objectID": "posts/2022-03-22-bayesiaanse-principes/bayes-principes.html",
    "href": "posts/2022-03-22-bayesiaanse-principes/bayes-principes.html",
    "title": "Bayes’ principes",
    "section": "",
    "text": "Bayes Rules!"
  },
  {
    "objectID": "posts/2022-03-22-bayesiaanse-principes/bayes-principes.html#inleiding",
    "href": "posts/2022-03-22-bayesiaanse-principes/bayes-principes.html#inleiding",
    "title": "Bayes’ principes",
    "section": "Inleiding",
    "text": "Inleiding\nOnlangs verscheen een prachtig boek van Alicia A. Johnson, Miles Q. Ot en Mine Dogucu onder de titel Bayes Rules! An Introduction to Applied Bayesian Modeling en het verscheen bij CRC Press (2022). Eerdere versies kon je al via bookdown bekijken (https://www.bayesrulesbook.com/) en vanaf de eerste keer dat ik het zag, was ik hier heel enthousiast over. Het boek heb ik direct besteld en vorige week kon ik het ophalen.\nHet boek bestaat uit vier duidelijke delen. Het eerste deel gaat in op de fundamenten van het Bayesiaanse perspectief. Het leert je denken als een Bayesiaan en het gaat in op die belangrijke Bayesiaanse regel \\(posterior=\\frac{prior.likelihood}{normaliserende constante}\\). Aan de hand van enkele voorbeelden gaan Johnson e.a. in op hoe het in de praktijk werkt. Daarna gaat het in op hoe kennis en data op elkaar inwerken en het laat enkele basisanalyses zien en hoe dat in deze vorm van statistiek werkt (normaal, binair en poisson). Het tweede deel is een meer technisch hoofdstuk en laat je ook onder de moterkap van deze techniek kijken. Het gaat ook op de wetenschappelijke principes van de benadering, waar je hier op moet letten, hoe je hiermee hypothesen kunt testen (niet alleen tov van een nulhypothese, maar hoeveel beter de ene hypothese is ten opzichte van de andere hyposthese) en hoe je hiermee ook kunt voorspellen. De twee volgende delen (Deel drie en vier) zijn praktische delen. Deel drie gaat in op regressieanalyses voor continue variabelen en classificatieanalyses voor binaire variabelen. Het vierde deel, ten slotte, gaat in op geclusterde datasets en hoe je hierarchische Bayesiaanse regressie en classificatieanalyses uitvoert.\nNatuurlijk, er zijn onderhand al verschillende interessante boeken te krijgen die je laten zien hoe Bayesiaanse denken in de praktijk kan werken. De boeken van Gelman, McElreath, Spiegelhalter en Kruschke verschenen de afgelopen tien/vijftien jaar en leren je dit. Maar Bayes Rules! vind ik op dit moment als introductieboek mogelijk wel het beste.\nNu het boek bij mij op het bureau ligt, kan ik er binnenkort een keer een korte recensie over schrijven. Voor nu heb ik uit elk deel een hoofdstuk genomen en het vertaald en bewerkt. Hieronder zie je een bewerking van het het vierde hoofdstuk van het eerste deel (Balance and Sequentiallity in Bayesian Analysis).Deze hoofdstukken zo overzetten is voor mij niet alleen een goede manier om het mij eigen te maken, maar ook een manier om het boek anderen aan te raden. Dus lezen en gebruiken deze Bayes Rules! An Introduction to Applied Bayesian Modeling"
  },
  {
    "objectID": "posts/2022-03-22-bayesiaanse-principes/bayes-principes.html#balans-en-opeenvolging-in-bayesiaanse-analyses",
    "href": "posts/2022-03-22-bayesiaanse-principes/bayes-principes.html#balans-en-opeenvolging-in-bayesiaanse-analyses",
    "title": "Bayes’ principes",
    "section": "Balans en opeenvolging in Bayesiaanse Analyses",
    "text": "Balans en opeenvolging in Bayesiaanse Analyses\nIn Alison Bechdels stripverhaal The Rule uit 1985 zegt een personage dat ze alleen naar een film gaan als die aan de volgende drie regels voldoet (Bechdel 1986):\n\ner moeten minstens twee vrouwen in de film voorkomen;\n\ndeze twee vrouwen praten met elkaar; en\n\nze praten over iets anders dan een man.\nDeze criteria vormen de Bechdel-test voor de vertegenwoordiging van vrouwen in films. Als je aan films denkt die je hebt gezien, welk percentage van alle recente films slaagt dan volgens jou voor de Bechdel-test? Ligt dat dichter bij 10%, 50%, 80%, of 100%?\n\nLaat \\(\\pi\\), een willekeurige waarde tussen 0 en 1, het onbekende aandeel van recente films zijn die de Bechdel test doorstaan. Drie vrienden - de feminist, de onwetende, en de optimist - hebben enkele vooroordelen over \\(\\pi\\). Nadenkend over films die zij in het verleden heeft gezien, begrijpt de feminist dat het in de meeste films ontbreekt aan sterke vrouwelijke personages. De onwetende herinnert zich niet echt de films die hij gezien heeft, en weet dus niet zeker of het halen van de Bechdel test gebruikelijk of ongewoon is. De optimist ten slotte denkt dat de Bechdel-test een erg lage lat is voor de vertegenwoordiging van vrouwen in films, en gaat er dus van uit dat bijna alle films de test doorstaan. Dit alles om te zeggen dat drie vrienden drie verschillende voorafgaande modellen hebben van \\(\\pi\\). Geen probleem! Een Beta kan voorafgaand aan het opstellen van een model worden afgestemd op iemands voorkennis (zie de figuur hieronder).\nDoor de grootste aannemelijkheid vooraf te geven aan de waarden van \\(\\pi\\) die lager zijn dan 0,5, weerspiegelt de Beta(5,11) prior het inzicht van de feminist: de meerderheid van de films doorstaat de Bechdel test niet. De Beta(14,1) daarentegen plaatst een grotere aannemelijkheid vooraf op waarden van \\(\\pi\\) in de buurt van 1, en komt dus overeen met de vooronderstelling van de optimist. Dan blijft de Beta(1,1) of Unif(0,1) over die, door een gelijke aannemelijkheid te geven aan alle waarden van \\(\\pi\\) tussen 0 en 1, dat overeenkomt met het figuurlijke schouderophalen van de onwetende - het enige dat zij weet is dat \\(\\pi\\) een proportie is, en dus ergens tussen 0 en 1 ligt.\nDe drie analisten komen overeen een steekproef te nemen van \\(n\\) recente films en noteren \\(Y\\), het aantal dat de Bechdel test doorstaat. Herkenning van \\(Y\\) als het aantal “successen” in een vast aantal onafhankelijke proeven, specificeren zij de afhankelijkheid van \\(Y\\) van \\(\\pi\\) met behulp van een binomiaal model. Elke analist heeft dus een uniek Beta-Binomiaal model van \\(\\pi\\) met verschillende voorafgaande hyperparameters\n\\(\\alpha\\) en \\(\\beta\\):\n\\[Y|\\pi \\sim (Bin(n, \\pi) \\\\\n  \\pi \\sim Beta(\\alpha, \\beta)\\]\nWe weten dat elke analist een uniek posterior model heeft van \\(\\pi\\) dat afhangt van zijn of haar unieke prior (via \\(\\alpha\\) en \\(\\beta\\)) en de gemeenschappelijke waargenomen gegevens (via \\(Y\\) en \\(n\\))\n\\[\\pi|(Y=y) \\sim Beta (\\alpha + y, \\beta +n - Y)\\]\nAls je denkt “Kan iedereen zijn eigen voorkeuren hebben?! Zal dit altijd zo subjectief zijn?!” dan stel je de juiste vragen! En de vragen houden daar niet op. In hoeverre zouden hun verschillende priors van de analisten kunnen leiden tot drie verschillende posterior conclusies over de Bechdel test? Hoe zou dit kunnen afhangen van de steekproefgrootte en de uitkomsten van de filmgegevens die ze verzamelen? In hoeverre zullen de posterior inzichten van de analisten evolueren naarmate ze meer en meer gegevens verzamelen? Zullen ze het ooit eens worden over de vertegenwoordiging van vrouwen in film? We zullen deze fundamentele vragen hier onderzoeken en wij kijken naar ons vermogen om het Bayesiaanse denken verder te ontwikkelen.\nDus:\n- We onderzoek de evenwichtige invloed van de prior en data op de posterior.\n- We voeren verschillende achtereenvolgende Bayesiaanse analyses uit.\n\n# Laten we eerst de pakketten openen die we in dit hoofstuk zullen gebruiken (wel eerst binnenhalen)\nlibrary(bayesrules)\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv ggplot2 3.3.6     v purrr   0.3.4\nv tibble  3.1.7     v dplyr   1.0.9\nv tidyr   1.2.0     v stringr 1.4.1\nv readr   2.1.2     v forcats 0.5.1\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'stringr' was built under R version 4.1.3\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\n\n\nVerschillende priors, verschillende posteriors\nDe voorafgaande modellen van \\(\\pi\\) (het percentage recente films dat de Bechdel test doorstaat) zijn afgestemd op de onwetende, de feministe, en de optimist. Niet alleen weerspiegelen de verschillende prior gemiddelden onenigheid over de vraag of \\(\\pi\\) dichter bij 0 of 1 ligt, maar ook weerspiegelen hun verschillende niveaus van variabiliteit het feit dat de drie analisten niet allemaal even zeker zijn van hun kennis en informatie. Hoe zekerder ze daarover zijn, hoe kleiner de voorafgaande variabiliteit. Maar ook, hoe vager de prior-informatie, hoe groter de prior-variabiliteit. De priors van de optimist en de onwetende vertegenwoordigen hier twee uitersten. Een Beta(14,1) prior vertoont de kleinste variabiliteit en daarmee is de optimist het meest zeker in zijn prioriteitsbegrip van \\(\\pi\\) (of om duidelijker te zijn, dat bijna alle films de Bechdel test zullen doorstaan). Dergelijke priors noemen we informatief.\n\nEen informatieve prior reflecteert specifieke informatie over de onbekende variabele met grote zekerheid, bv. lage variabiliteit.\n\nMet de grootste prioriteitsvariabiliteit is de onwetende het minst zeker over \\(\\pi\\). In feite kent deze Beta(1,1) prior evenveel prior plausibiliteit toe aan elke waarde van \\(\\pi\\) tussen 0 en 1. Dit type prior model van “schouderophalen” (van “ik weet het echt niet”) heeft een officiële naam: het is een vage prior.\n\nEen vage of diffuse prior reflecteert weinig specifieke informatie over de onbekende variabele. Een vlakke prior die gelijke plausibiliteit toekent aan alle mogelijke waarden van de variabele, is een speciaal geval.\n\nDe volgende voor de hand liggende vraag is dan: hoe zullen hun verschillende priors de posterior conclusies van de feminist, de onwetende en de optimist beïnvloeden? Om deze vraag te beantwoorden, hebben we gegevens nodig. Onze analisten besluiten een willekeurige steekproef te nemen van \\(n=20\\) recente films te bekijken, gebruikmakend van gegevens die verzameld zijn voor het FiveThirtyEight-artikel over de Bechdel-test Het bayesrules-pakket bevat een gedeeltelijke versie van deze dataset, genaamd bechdel. Een volledige versie wordt geleverd door het fivethirtyeight R-pakket (Kim, Ismay, and Chunn 2020). Samen met de titel en het jaar van elke film in deze dataset, geeft de binaire variabele aan of de film slaagde of niet voor de Bechdel test:\n\n# Importeer data\ndata(bechdel, package = \"bayesrules\")\n\n# Neem een sample van 20 films\nset.seed(84735)\nbechdel_20 &lt;- bechdel %&gt;% \n  sample_n(20)\n\nbechdel_20 %&gt;% \n  head(3)\n\n# A tibble: 3 x 3\n   year title      binary\n  &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt; \n1  2005 King Kong  FAIL  \n2  1983 Flashdance PASS  \n3  2013 The Purge  FAIL  \n\n\nVan de 20 films in deze steekproef slaagden er slechts 9 (45%) voor de test:\n\nbechdel_20 %&gt;% \n  tabyl(binary) %&gt;% \n  adorn_totals(\"row\")\n\n binary  n percent\n   FAIL 11    0.55\n   PASS  9    0.45\n  Total 20    1.00\n\n\nDe posterior modellen van de drie analisten voor\\(\\pi\\) die volgen uit de formule \\(\\pi|(Y=y) \\sim Beta(\\alpha+y, \\beta+n-y)\\) op hun unieke voorgaande modellen en gemeenschappelijke filmgegevens, zijn samengevat in tabel hieronder en de figuur hierboven. Bijvoorbeeld, de posterior parameters van de feminist worden berekend door \\(\\alpha+y=5+9=14\\) en \\(\\beta+n-y=11+20-9=22\\)\n\n\n\nAnalyst\nPrior\nPosterior\n\n\n\n\nfeminist\nBeta(5,11)\nBeta(14,22)\n\n\nonwetende\nBeta(1,1)\nBeta(10,12)\n\n\noptimist\nBeta(14,1)\nBeta(23,12)\n\n\n\nHad je instinct gelijk? Herinner je dat de optimist begon met het vasthoudend optimistisme vooraf over \\(\\pi\\) - zijn prior model had een hoog gemiddelde met lage variabiliteit. Het is dan ook niet verwonderlijk dat zijn posterior model niet zo synchroon loopt met de gegevens als de posteriors van de andere analisten. De trieste gegevens waarbij slechts 45% van de 20 films de test doorstonden was niet genoeg om hem ervan te overtuigen dat er een probleem is in Hollywood - hij denkt nog steeds dat de waarden van \\(\\pi\\) boven 0,5 het meest plausibel is. Aan het andere uiterste staat de onwetende die begon met een vlak, vaag model van \\(\\pi\\). Zonder enige voorinformatie weerspiegelt zijn posterior model direct de inzichten die zijn verkregen uit de waargenomen filmgegevens. In feite is zijn posterior niet te onderscheiden van de geschaalde likelihood functie.\n\n\n\nPosterior models"
  },
  {
    "objectID": "posts/2022-03-22-bayesiaanse-principes/bayes-principes.html#verschillende-data-verschillende-posteriors",
    "href": "posts/2022-03-22-bayesiaanse-principes/bayes-principes.html#verschillende-data-verschillende-posteriors",
    "title": "Bayes’ principes",
    "section": "Verschillende data, verschillende posteriors",
    "text": "Verschillende data, verschillende posteriors\nAls jij je zorgen maakt over het feit dat onze drie analisten achteraf verschillende opvattingen hebben over \\(\\pi\\), de proportie van recente films die de Bechdel halen, wanhoop dan nog niet. Vergeet niet dat data ook een rol spelen in een Bayesiaanse analyse. Om deze dynamiek te onderzoeken, beschouwen we drie nieuwe analisten - Morteza, Nadide, en Ursula - die allen de optimistische Beta(14,1) prior voor \\(\\pi\\) delen maar elk toegang hebben tot andere gegevens. Morteza beoordeelt \\(n=13\\) films uit het jaar 1991, waarvan \\(Y=6\\) (ongeveer 46%) door de Bechdel komen:\n\nbechdel %&gt;% \n  filter(year == 1991) %&gt;% \n  tabyl(binary) %&gt;% \n  adorn_totals(\"row\")\n\n binary  n   percent\n   FAIL  7 0.5384615\n   PASS  6 0.4615385\n  Total 13 1.0000000\n\n\nNadide beoordeelt \\(n=63\\) films uit 2000, waaronder \\(Y=29\\) (ongeveer 46%) door de Bechdel komen:\n\nbechdel %&gt;% \n  filter(year == 2000) %&gt;% \n  tabyl(binary) %&gt;% \n  adorn_totals(\"row\")\n\n binary  n   percent\n   FAIL 34 0.5396825\n   PASS 29 0.4603175\n  Total 63 1.0000000\n\n\nTot slot, beoordeelt Ursula \\(n=99\\) films uit 2013, waarvan er \\(Y=46\\) (ongeveer 46%) door de Bechdel komen:\n\nbechdel %&gt;% \n  filter(year == 2013) %&gt;% \n  tabyl(binary) %&gt;% \n  adorn_totals(\"row\")\n\n binary  n   percent\n   FAIL 53 0.5353535\n   PASS 46 0.4646465\n  Total 99 1.0000000\n\n\nWat een toeval! Hoewel Morteza, Nadide, en Ursula verschillende gegevens hebben verzameld, constateren ze elk een Bechdel slaagpercentage van ongeveer 46%. Toch is hun steekproefomvang \\(n\\) verschillend - Morteza bekeek slechts 13 films terwijl Ursula er 99 bekeek.\nDe posterior modellen van de drie analisten voor \\(\\pi\\) die volgen uit de toepassing van hun gegevens op hun gemeenschappelijke Beta(14,1) voormodel en unieke filmgegevens, zijn samengevat in de figuur en tabel hieronder. Merk op dat hoe groter de steekproefgrootte \\(n\\) hoe “indringender” de likelihoodfunctie. Bijvoorbeeld, de waarschijnlijkheidsfunctie die het slagingspercentage van 46% in Morteza’s kleine steekproef van 13 films weergeeft is vrij breed - deze gegevens zijn relatief plausibel voor elke \\(\\pi\\) tussen 15% en 75%. Ursula’s waarschijnlijkheidsfunctie daarentegen, met het slagingspercentage van 46% in een veel grotere steekproef van 99 films weergeeft, is smal - haar gegevens zijn onwaarschijnlijk voor \\(\\pi\\) waarden buiten het bereik van 35% tot 55%. We zien dat hoe groter de waarschijnlijkheid, hoe meer invloed de gegevens hebben op de posterior. Morteza blijft het minst overtuigd door het lage Bechdel-slaagpercentage dat in zijn kleine steekproef wordt waargenomen, terwijl Ursula het meest overtuigd is. Haar vroege optimisme evolueerde naar een posterior begrip dat \\(\\p\\) waarschijnlijk tussen 40% en 55% ligt.\n\n\n\nPosterior models\n\n\nTabel: De prior en posterior modellen voor \\(\\pi\\) zijn geconstrueerd in het licht van een gemeenschappelijke Beta(14,1)-prior en verschillende gegevens.\n\n\n\nAnalyst\nData\nPosterior\n\n\n\n\nMorteza\n\\(Y=6;n=13\\)\nBeta(20,8)\n\n\nNadide\n\\(Y=26;n=63\\)\nBeta(43,35)\n\n\nUrsula\n\\(Y=46;n=99\\)\nBeta(60,54)"
  },
  {
    "objectID": "posts/2022-03-22-bayesiaanse-principes/bayes-principes.html#het-vinden-van-de-balans-tussen-de-prior-en-de-data",
    "href": "posts/2022-03-22-bayesiaanse-principes/bayes-principes.html#het-vinden-van-de-balans-tussen-de-prior-en-de-data",
    "title": "Bayes’ principes",
    "section": "Het vinden van de balans tussen de prior en de data",
    "text": "Het vinden van de balans tussen de prior en de data\n\nObservaties aan concepten verbinden\nWe hebben gekeken naar de invloed die verschillende priors en verschillende gegevens kunnen hebben op ons posterior begrip van een onbekende variabele. De posterior is echter een meer genuanceerd touwtrekken tussen deze twee kanten. De figuren hieronder illustreren het evenwicht dat het posterior model vindt tussen de prior en de gegevens. Elke rij komt overeen met een uniek prior model en elke kolom met een unieke reeks gegevens.\nVan links naar rechts neemt de steekproefgrootte toe van \\(n=13\\) tot \\(n=99\\) films met behoud van het aandeel films dat de Bechdel-test doorstaat: \\(Y/n\\sim0.46\\) . De waarschijnlijkheid en, dienovereenkomstig, de invloed van de gegevens op de posterior nemen toe met de steekproefgrootte \\(n\\). Dit betekent ook dat de invloed van onze voorkennis afneemt naarmate we nieuwe gegevens vergaren. Verder hangt de snelheid waarmee de posterior balans doorslaat in het voordeel van de gegevens af van de prior. Van boven naar beneden over het rooster gaan de priors van informatief (Beta(14,1)) naar vaag (Beta(1,1)). Het spreekt vanzelf dat hoe informatiever de prior is, hoe groter zijn invloed op de posterior is.\nDoor deze waarnemingen te combineren levert de laatste kolom in het rooster een zeer belangrijke Bayesiaanse clou op: ongeacht de sterkte van en discrepanties tussen hun prioriteitsbegrip van \\(\\pi\\) zullen drie analisten tot een gemeenschappelijke posterior interpretatie komen in het licht van sterke gegevens. Deze vaststelling is een opluchting. Als Bayesiaanse modellen niets zouden betekenen in het licht van steeds meer gegevens, zouden we een probleem hebben.\nMet dit soort gegevens kun je spelen en kijken naar de rol die de prior en de data spelen in een posterior analyse, gebruik je de plot_beta_binomial() en summarize_beta_binomial() functies in het bayesrules pakket om het Beta-Binomial posterior model van \\(\\pi\\) onder verschillende combinaties van voorafgaande Beta(\\(\\alpha, \\beta\\)) modellen en waargenomen gegevens, \\(Y\\) successen in \\(n\\) proeven:\n# Plot the Beta-Binomial model\nplot_beta_binomial(alpha = ___, beta = ___, y = ___, n = ___)\n\n# Obtain numerical summaries of the Beta-Binomial model\nsummarize_beta_binomial(alpha = ___, beta = ___, y = ___, n = ___)\n\n\nConcepten met theorie verbinden\nDe patronen die we hebben waargenomen in het posterior evenwicht tussen de prior en de data zijn intuïtief. Ze worden ook ondersteund door een elegant wiskundig resultaat. Daar moet je het boek maar voor lezen."
  },
  {
    "objectID": "posts/2022-03-22-bayesiaanse-principes/bayes-principes.html#sequentiële-analyse-evolueren-met-dat",
    "href": "posts/2022-03-22-bayesiaanse-principes/bayes-principes.html#sequentiële-analyse-evolueren-met-dat",
    "title": "Bayes’ principes",
    "section": "Sequentiële analyse: Evolueren met dat",
    "text": "Sequentiële analyse: Evolueren met dat\nWe hebben de toenemende invloed van de gegevens en de afnemende invloed van de prior op de posterior onderzocht naarmate er meer en meer gegevens binnenkomen. Overweeg de nuances van dit concept. De uitdrukking “naarmate er meer gegevens binnenkomen” roept het idee op dat het verzamelen van gegevens, en daarmee de evolutie in ons begrip van de posterior, stapsgewijs gebeurt. Zo is het inzicht van wetenschappers in de klimaatverandering in de loop van tientallen jaren geëvolueerd naarmate zij nieuwe informatie kregen. Het inzicht van presidentskandidaten in hun kansen om een verkiezing te winnen evolueert in de loop van maanden naarmate nieuwe opiniepeilingen beschikbaar komen. Het bieden van een formeel kader voor deze evolutie is een van de meest krachtige eigenschappen van Bayesiaanse statistiek!\nIn een eerder hoofdstuk gingen ze in op Milgram’s gedragsstudie over gehoorzaamheid. Daar gaat \\(\\pi\\) om het aandeel van de mensen die het gezag zullen gehoorzamen zelfs als dit betekent dat ze anderen schade toebrengen. In de studie van Milgram betekende gehoorzamen aan het gezag het toedienen van een zware elektrische schok aan een andere deelnemer (wat in feite een list was). Voorafgaand aan de experimenten van Milgram verwachtte een fictieve psycholoog dat weinig mensen gezag zouden gehoorzamen als ze een ander zouden schaden:\n\\(\\pi\\sim Beta(1,10)\\). In het onderzoek stelden we vast dat 26 van de 40 deelnemers aan het onderzoek een schok toebrachten die zij opvatten als een zware schok.\nStel nu dat de psycholoog deze gegevens stapsgewijs, dag na dag, over een periode van drie dagen verzamelde. Elke dag, evalueerde ze \\(n\\) proefpersonen en registreerde ze \\(Y\\), het aantal dat de zwaarste schok kreeg (dus \\(Y|\\pi \\sim Bin(n,\\pi)\\)Y. Van de \\(n=10\\) dag-één deelnemers, alleen \\(Y=1\\) de zwaarste schok. Dus aan het eind van de eerste dag is het begrip van de psycholoog van \\(\\pi\\) al geëvolueerd en blijkt\n\\[\\pi|Y=1) \\sim Beta(2,19)\\]\nDag twee was veel drukker en de resultaten grimmiger: onder \\(n=20\\) deelnemers, gaf \\(Y=17\\) de zwaarste schok. Aan het eind van dag twee was het begrip van de psycholoog van \\(\\pi\\) opnieuw geëvolueerd - \\(\\pi\\) was waarschijnlijk groter dan zij hadden verwacht. Dus, het posterior model van \\(\\pi\\) aan het eind van dag twee is \\(Beta(19,22)\\). Op dag drie heeft \\(Y=8\\) van \\(n=10\\) deelnemers de zwaarste schok toegediend, en dus is het model geëvolueerd van een \\(Beta(19,22)\\) prior naar een \\(Beta(27,24)\\) posterior. De volledige evolutie van de oorspronkelijke \\(Beta(1,10)\\) prior van de psycholoog naar een \\(Beta(27,24)\\) posterior aan het einde van de driedaagse studie is samengevat in de tabel hieronder.\n\n\n\nDag\nData\nModel\n\n\n\n\n0\nNA\nBeta(1,10)\n\n\n1\n\\(Y=1;n=10\\)\nBeta(2,19)\n\n\n2\n\\(Y=17;n=20\\)\nBeta(19,22)\n\n\n3\n\\(Y=8;n=10\\)\nBeta(27,24)\n\n\n\nHet proces dat we zojuist hebben doorlopen, het incrementeel bijwerken van het posterior model van de psycholoog wordt meer algemeen een sequentiële Bayesiaanse analyse of Bayesiaans leren genoemd.\nDe mogelijkheid om te evolueren naarmate nieuwe gegevens binnenkomen, is een van de krachtigste kenmerken van het Bayesiaanse raamwerk. Dit soort sequentiële analyses heeft ook twee fundamentele en gemeenschappelijke sensitieve eigenschappen. Ten eerste is het uiteindelijke posterior model invariant t.a.v. data volgorde: het wordt niet beïnvloed door de volgorde waarin we de data observeren. Bijvoorbeeld, stel dat de psycholoog de studiegegevens van Milgram in omgekeerde volgorde had geobserveerd: \\(Y=8\\) van\n\\(n=10\\) op dag één, \\(Y=17\\) van \\(n=20\\) op dag twee, en \\(Y=1\\) van \\(n=10\\) op dag drie. De resulterende evolutie in hun begrip van \\(\\pi\\) is samengevat in de tabel hieronder. Het evoluerende begrip van de psycholoog van \\(\\pi\\) verloopt een ander pad. Het eindigt echter nog steeds op dezelfde plaats - de Beta(27,24) posterior.\n\n\n\nDag\nData\nModel\n\n\n\n\n0\nNA\nBeta(1,10)\n\n\n1\n\\(Y=8;n=10\\)\nBeta(9,12)\n\n\n2\n\\(Y=17;n=20\\)\nBeta(26,15)\n\n\n3\n\\(Y=1;n=10\\)\nBeta(27,24)\n\n\n\nHet tweede fundamentele kenmerk van een sequentiële analyse is dat de uiteindelijke posterior alleen afhangt van de cumulatieve gegevens. Bijvoorbeeld, in de gecombineerde drie dagen van Milgram’s experiment, waren er \\(n=10+20+20=40\\) deelnemers die \\(Y=1+17+8=26\\) de zwaarste schok opleverde. In paragraaf 3.6 evalueerden wij deze gegevens in één keer, niet stapsgewijs. Daarbij sprongen wij rechtstreeks van het oorspronkelijke Beta(1,10) model van de psycholoog naar het Beta(27,24) posterior model van \\(\\pi\\). Dat wil zeggen, of we de gegevens nu incrementeel of in één keer evalueren, we komen op dezelfde plaats uit."
  },
  {
    "objectID": "posts/2022-03-22-bayesiaanse-principes/bayes-principes.html#bewijs-van-invariantie-van-gegevensvolgorde",
    "href": "posts/2022-03-22-bayesiaanse-principes/bayes-principes.html#bewijs-van-invariantie-van-gegevensvolgorde",
    "title": "Bayes’ principes",
    "section": "Bewijs van invariantie van gegevensvolgorde",
    "text": "Bewijs van invariantie van gegevensvolgorde\nIn de vorige sectie zag je het bewijs van datavolgorde invariantie in actie. Hier zullen we bewijzen dat alle Bayesiaanse modellen van deze eigenschap genieten. Dit deel is leuk, maar hoef je niet echt te weten voor je toekomstige werk.Hier gaan we daar niet verder op in (ook voor dit meer technische deel moet je het boek lezen)."
  },
  {
    "objectID": "posts/2022-03-22-bayesiaanse-principes/bayes-principes.html#een-opmerking-over-subjectiviteit",
    "href": "posts/2022-03-22-bayesiaanse-principes/bayes-principes.html#een-opmerking-over-subjectiviteit",
    "title": "Bayes’ principes",
    "section": "Een opmerking over subjectiviteit",
    "text": "Een opmerking over subjectiviteit\nWe zinspeelden eerder op een veelgehoorde kritiek op Bayesiaanse statistiek - het is te subjectief. In het bijzonder zijn sommigen bezorgd dat het “subjectief” afstemmen van een prior model een Bayesiaanse analist in staat stelt om tot elke conclusie te komen die hij wil. In het licht van wat we hier hebben geleerd, kunnen we deze kritiek meer rigoureus bestrijden. Voordat we dat doen, willen we eerst een aantal concepten nog eens nader bekijken en uitbreiden.\nBevestigd is dat een Bayesiaan inderdaad een prior kan bouwen op basis van “subjectieve” ervaring. Heel zelden is dit een slechte zaak, en heel vaak is het een goede zaak! In het beste geval kan een subjectieve prioriteit een schat aan ervaringen uit het verleden weerspiegelen die in onze analyse moeten worden opgenomen - het zou jammer zijn dat niet te doen. Zelfs als een subjectieve prior ingaat tegen het feitelijk waargenomen bewijs, verdwijnt zijn invloed op de posterior naarmate dit bewijs zich opstapelt. We hebben één uitzondering gezien in het ergste geval. En het was te voorkomen. Als een subjectieve prioriteit koppig genoeg is om de waarschijnlijkheid van 0 toe te kennen aan een mogelijke parameterwaarde, zal geen hoeveelheid tegenbewijs genoeg zijn om het te veranderen.\nTenslotte, hoewel we je aanmoedigen om kritisch te zijn in je toepassing van Bayesiaanse methoden, maak je alsjeblieft geen zorgen dat ze subjectiever zijn dan frequentistische methoden. Geen mens is in staat om alle subjectiviteit uit een analyse te halen. De levenservaringen en kennis die we met ons meedragen bepalen alles, van de onderzoeksvragen die we stellen tot de data die we verzamelen. Het is belangrijk om zowel bij Bayesiaanse als bij frequentistische analyses rekening te houden met de mogelijke implicaties van deze subjectiviteit."
  },
  {
    "objectID": "posts/2022-03-22-bayesiaanse-principes/bayes-principes.html#samenvatting",
    "href": "posts/2022-03-22-bayesiaanse-principes/bayes-principes.html#samenvatting",
    "title": "Bayes’ principes",
    "section": "Samenvatting",
    "text": "Samenvatting\nWij hebben het evenwicht onderzocht dat een posterior model aanbrengt tussen een prior model en de gegevens. In het algemeen zagen wij de volgende tendensen:\n\nInvloed van de prior\nHoe minder vaag en hoe informatiever de prior, d.w.z. hoe groter onze priorzekerheid, hoe meer invloed de prior op de posterior heeft.\nInvloed van de gegevens\nHoe meer gegevens we hebben, hoe meer invloed de gegevens hebben op de posterior. Dus, als twee onderzoekers voldoende gegevens hebben, zullen ze met verschillende priors vergelijkbare posteriors hebben.\n\nVerder hebben we gezien dat we in een sequentiële Bayesiaanse analyse ons posterior model incrementeel bijwerken naarmate er meer en meer gegevens binnenkomen. De uiteindelijke bestemming van deze posterior wordt niet beïnvloed door de volgorde waarin we deze gegevens observeren (d.w.z. de posterior is datavolgorde invariant) of door de vraag of we de gegevens in één keer of opbouwend observeren."
  },
  {
    "objectID": "posts/2022-03-26-naieve-bayesiaanse-classificatie/naeve-bayesiaanse-classificatie.html",
    "href": "posts/2022-03-26-naieve-bayesiaanse-classificatie/naeve-bayesiaanse-classificatie.html",
    "title": "Naïeve Bayesiaanse classificatie",
    "section": "",
    "text": "Bayes Rules!"
  },
  {
    "objectID": "posts/2022-03-26-naieve-bayesiaanse-classificatie/naeve-bayesiaanse-classificatie.html#inleiding",
    "href": "posts/2022-03-26-naieve-bayesiaanse-classificatie/naeve-bayesiaanse-classificatie.html#inleiding",
    "title": "Naïeve Bayesiaanse classificatie",
    "section": "Inleiding",
    "text": "Inleiding\nOnlangs verscheen een prachtig boek van Alicia A. Johnson, Miles Q. Ot en Mine Dogucu onder de titel Bayes Rules! An Introduction to Applied Bayesian Modeling en het verscheen bij CRC Press (2022). Eerdere versies stonden kon je al via bookdown bekijken (https://www.bayesrulesbook.com/) en vanaf de eerste keer dat ik het zag, was ik hier heel enthousiast over. Het boek heb ik direct besteld en vorige week kon ik het ophalen.\nHet boek bestaat uit vier duidelijke delen. Het eerste deel gaat in op de fundamenten van het Bayesiaanse perspectief. Het leert je denken als een Bayesiaan en het gaat in op die belangrijke Bayesiaanse regel \\(posterior=\\frac{prior.likelihood}{normaliserende constante}\\). Aan de hand van enkele voorbeelden gaan Johnson e.a. in op hoe het in de praktijk werkt. Daarna gaat het in op hoe kennis en data op elkaar inwerken en laat het enkele basisanalyses zien en hoe dat in deze vorm van statistiek werkt (normaal, binair en poisson). Het tweede deel is een meer technisch hoofdstuk en laat je ook onder de moterkap van deze techniek kijken. Het gaat ook in op de wetenschappelijke principes van de benadering, waar je hier op moet letten, hoe je hiermee hypothesen kunt testen (niet alleen tov van een nulhypothese, maar hoeveel beter de ene hypothese is ten opzichte van de andere hyposthese) en hoe je hiermee ook kunt voorspellen. De twee volgende delen (Deel drie en vier) zijn praktische delen. Deel drie gaat in op regressieanalyses voor continue variabelen en classificatieanalyses voor binaire variabelen. Het vierde deel ten slotte gaat in op geclusterde datasets en hoe je hierarchische Bayesiaanse regressie en classificatieanalyses uitvoert.\nNatuurlijk, er zijn onderhand al verschillende interessante boeken te krijgen die je laten zien hoe Bayesiaanse denken in de praktijk kan werken. De boeken van Gelman, McElreath, Spiegelhalter en Kruschke verschenen de afgelopen tien/vijftien jaar en leren je dit. Maar Bayes Rules! vind ik op dit moment als introductieboek mogelijk wel het beste.\nNu het boek bij mij op het bureau ligt, kan ik er binnenkort een keer een korte recensie over schrijven. Voor nu heb ik uit elk deel een hoofdstuk genomen en het vertaald en bewerkt. Hieronder zie je een bewerking van het veertiende hoofdstuk van het derde deel (Naïve Bayes Classification).Hoofdstukken zo overzetten is voor mij niet alleen een goede manier om het mij eigen te maken, maar ook een manier om het boek anderen aan te raden. Dus lezen en gebruiken deze Bayes Rules! An Introduction to Applied Bayesian Modeling"
  },
  {
    "objectID": "posts/2022-03-26-naieve-bayesiaanse-classificatie/naeve-bayesiaanse-classificatie.html#naïeve-bayesiaanse-classificatie",
    "href": "posts/2022-03-26-naieve-bayesiaanse-classificatie/naeve-bayesiaanse-classificatie.html#naïeve-bayesiaanse-classificatie",
    "title": "Naïeve Bayesiaanse classificatie",
    "section": "Naïeve Bayesiaanse classificatie",
    "text": "Naïeve Bayesiaanse classificatie\nOp Antartica zijn er verschillende penguinsoorten te vinden waaronder de *Adelie**, Chinstrap en Gentoo-soorten.\n\\[\\\nY = \\begin{cases}\nA=Adelie \\\\\nC=Chinstrap \\\\\nG=Gentoo\\\\\n\\end{cases}\n\\]\\]\nWe zullen deze drie soorten classificeren op basis van het gewicht\n$\\(\\\nX_1 = \\begin{cases} 1=bovenhetgemiddeldegewicht \\\\ 0=onderhetgemiddeldegewicht\\\\ \\end{cases} \\]\\)$\n(\\(x_1=1\\) als het boven het gemiddelde gewicht van 4200 g ligt en een \\(X_1=0\\) als dat gemiddelde lager is dan 4300 gram).\nVerder is er \\[x_2=snavellengte (in mm)\\]\n\\[x_3=flipperlengte\\]\nDe penguins_bayes data, oorspronkelijk ter beschikking gesteld door Gorman, Williams, en Fraser (2014) en vervolgens verspreid door Horst, Hill, en Gorman (2020), bevat de bovenstaande soort- en kenmerkinformatie voor een steekproef van 344 Antarctische pinguïns:\n\n# Laden van pakketten \nlibrary(bayesrules)\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv ggplot2 3.3.6     v purrr   0.3.4\nv tibble  3.1.7     v dplyr   1.0.9\nv tidyr   1.2.0     v stringr 1.4.1\nv readr   2.1.2     v forcats 0.5.1\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'stringr' was built under R version 4.1.3\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(e1071)\n\nWarning: package 'e1071' was built under R version 4.1.3\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\n# Laden van data\ndata(penguins_bayes)\npenguins &lt;- penguins_bayes\n\nOnder deze 344 pinguïns zijn er 152 Adelies, 68 Chinstrap/Kinband pinguïns, en 124 Gentoos. We gaan er steeds van uit dat de proportionele verdeling van deze soorten in onze dataset de verdeling van de soorten in het wild weerspiegelt. Dat wil zeggen dat we bij elke nieuwe pinguïn aannemen dat het hoogst waarschijnlijk een Adélie is (44,2%) en het minst waarschijnlijk een Chinstrap(19,8%). Zo ziet de geschatte verdeling er dan uit:\n\npenguins %&gt;% \n  tabyl(species)\n\n   species   n   percent\n    Adelie 152 0.4418605\n Chinstrap  68 0.1976744\n    Gentoo 124 0.3604651\n\n\nEr zijn drie mogelijkheden of categorieën voor de \\(Y\\). Gelukkig zijn er allerlei hulpmiddel om te schatten over welke soort we het hebben. Logistische regressie werkt hier niet zo goed voor, naïeve Bayes classificatie wel. Ten opzichte van Bayesiaanse logistische regressie heeft naïeve Bayes classificatie een paar voordelen:\n\nhet kan categorische respons variabelen classificeren, dus Y met twee of meer categorieën;\n\ner is niet veel theorie nodig buiten de regel van Bayes;\n\nen zij is rekenkundig efficiënt, d.w.z. dat er geen MCMC-simulatie voor nodig is."
  },
  {
    "objectID": "posts/2022-03-26-naieve-bayesiaanse-classificatie/naeve-bayesiaanse-classificatie.html#classificeren-van-één-pinquin",
    "href": "posts/2022-03-26-naieve-bayesiaanse-classificatie/naeve-bayesiaanse-classificatie.html#classificeren-van-één-pinquin",
    "title": "Naïeve Bayesiaanse classificatie",
    "section": "Classificeren van één pinquin",
    "text": "Classificeren van één pinquin\nLaten we met het Bayesiaans classificeren van één penguin beginnen. Stel dat we een penguin vinden die minder dan 4200 gram weegt, die een snavel heeft van 50 mm en een flipper van 195mm. We willen iets ontwikkelen dat ons helpt om vast te stellen met welk soort we hier te maken hebben.Laten we de drie soorten en hun gewicht eens afbeelden.\n\nggplot(penguins %&gt;% drop_na(above_average_weight), \n       aes(fill = above_average_weight, x = species)) + \n  geom_bar(position = \"fill\")\n\n\n\n\nFig 1. Proporties van elk soort met bovengemiddeld gewicht\n\n\n\n\nChinstraps is relatief het lichtste soort. Maar we moeten tegelijkertijd in ons achterhoofd houden dat dit de minst voorkomende soort is. Dat wil zeggen dat we moeten denken als Bayesianen door de informatie uit onze gegevens over het te combineren met onze informatie vooraf (prior) over de proportionele verdeling van het soort om een posterior model te construeren voor de soort van onze pinguïn. De naïeve Bayes classificatie benadering van deze taak is niets meer dan een direct beroep op de beproefde Bayes’ Regel. In het algemeen, om de posterior waarschijnlijkheid te berekenen dat onze pinguïn van de soort\n\\[f(y|x_1) = \\frac{\\text{prior}\\cdot\\text{likelihood}}{\\text{normaliserende constante}}=\\frac{f(y)L(y|X_1)}{f(x_1)}\\] waarvoor geldt de wet van de totale waarschijnlijkheid\n\\[f(x_1)=\\sum_{ally^'}\\]\nEen tabel waarin de bovengemiddelde gewichtsstatus per soort (above_average_weight) wordt uitgesplitst, verschaft de nodige informatie om deze Bayesiaanse berekening te voltooien:\n\npenguins %&gt;% \n  select(species, above_average_weight) %&gt;% \n  na.omit() %&gt;% \n  tabyl(species, above_average_weight) %&gt;% \n  adorn_totals(c(\"row\", \"col\"))\n\n   species   0   1 Total\n    Adelie 126  25   151\n Chinstrap  61   7    68\n    Gentoo   6 117   123\n     Total 193 149   342\n\n\nIn feite kunnen we het posterior model van de soort van onze pinguïn rechtstreeks uit deze tabel berekenen. Bijvoorbeeld, merk op dat van de 193 pinguïns die onder het gemiddelde gewicht zitten, 126 Adelies zijn. Er is dus ongeveer 65% posterior kans dat deze pinguïn een Adelie is:\n\\[f(y=A|x_1=0)=\\frac{126}{93}\\approx{0.6528}\\] Laten we dit resultaat bevestigen door de informatie uit onze tabel hierboven in de regel van Bayes in te voeren. Deze vervelende stap is niet om te ergeren, maar om te oefenen voor generalisaties die we zullen moeten maken in meer ingewikkelde omgevingen. Ten eerste, onze informatie over het soort geeft aan dat Adelies het meest voorkomen en Chinstraps het minst:\n\\[f(y=A)=\\frac{151}{342}, f(y=C)=\\frac{68}{342}, f(y=G)=\\frac{123}{342}\\]\nVerder tonen de waarschijnlijkheden aan dat een lager dan gemiddeld gewicht het meest voorkomt bij Chinstrapspinguïns. Bijvoorbeeld, 89.71% van de Chinstrapspinguïns maar slechts 4.88% van de Gentoos hebben een lager dan gemiddeld gewicht:\n\\[L=(y=A|x_1=0)=\\frac{126}{151}\\approx{0.8344} \\\\\n  L=(y=A|x_1=0)=\\frac{61}{68}\\approx{0.8971} \\\\\n  L=(y=A|x_1=0)=\\frac{6}{123}\\approx{0.0488}\\]\nDoor deze priors en waarschijnlijkheden te gebruiken, wordt de totale waarschijnlijkheid van een pinguïn met een lager dan gemiddeld gewicht voor alle soorten\n\\[f(x_1=0)=\\frac{151}{342}\\times\\frac{126}{151}+\\frac{68}{342}\\times\\frac{61}{68}+\\frac{123}{342}\\times\\frac{6}{123}=\\frac{193}{342}\\]\nTenslotte, door de regel van Bayes kunnen we bevestigen dat er een 65% posterior kans is dat deze pinguïn een Adelie is:\n\\[f(y=A|x_1=0)=\\frac{f(y=A)L(y=A|x_1=0)}{f(x_1=0)} \\\\\n              =\\frac{151/342.(126/151)}{193/342}\\approx{0.6528}\\]\nTegelijk zien we dat\n\\[f(y=C|x_1=0)\\approx{0.3161}\\] en \\[f(y=G|x_1=0)\\approx{0.0311}\\]\nAlles bij elkaar is de posterior waarschijnlijkheid dat deze pinguïn een Adelie is meer dan dubbel zo groot als die van de andere twee soorten. Dus, onze naïeve Bayes classificatie, gebaseerd op onze voorinformatie en het onder-gemiddelde gewicht van de pinguïn alleen, is dat deze pinguïn een Adelie is. Hoewel een lager dan gemiddeld gewicht relatief minder voorkomt bij Adélie’s dan bij Kinbandpinguïns, werd de uiteindelijke classificatie over de rand geduwd door het feit dat Adélie’s veel algemener zijn.\n\nÉén kwantitatieve voorspeller\nLaten we het gewicht van de pinguïn even buiten beschouwing en het soort indelen aan de hand van het feit dat hij een snavel van 50 mm heeft.\n\nggplot(penguins, aes(x = bill_length_mm, fill = species)) + \n  geom_density(alpha = 0.7) + \n  geom_vline(xintercept = 50, linetype = \"dashed\")\n\nWarning: Removed 2 rows containing non-finite values (stat_density).\n\n\n\n\n\n\n# Bereken het gemiddelde en de sd van het sample voor elke Y-groep \npenguins %&gt;% \n  group_by(species) %&gt;% \n  summarize(mean = mean(bill_length_mm, na.rm = TRUE), \n            sd = sd(bill_length_mm, na.rm = TRUE))\n\n# A tibble: 3 x 3\n  species    mean    sd\n  &lt;fct&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie     38.8  2.66\n2 Chinstrap  48.8  3.34\n3 Gentoo     47.5  3.08\n\n\nHet uitzetten van de afgestemde normale modellen voor elke soort bevestigt dat deze naïeve Bayes-aanname niet perfect is - het is iets idealistischer dan de dichtheidsplots van de ruwe gegevens van het figuur hierboven. Maar het is goed genoeg om verder te gaan.\n\nggplot(penguins, aes(x = bill_length_mm, color = species)) + \n  stat_function(fun = dnorm, args = list(mean = 38.8, sd = 2.66), \n                aes(color = \"Adelie\")) +\n  stat_function(fun = dnorm, args = list(mean = 48.8, sd = 3.34),\n                aes(color = \"Chinstrap\")) +\n  stat_function(fun = dnorm, args = list(mean = 47.5, sd = 3.08),\n                aes(color = \"Gentoo\")) + \n  geom_vline(xintercept = 50, linetype = \"dashed\")\n\n\n\n\nHerinner u dat deze Normaliteitsveronderstelling het mechanisme verschaft dat we nodig hebben om de waarschijnlijkheid van het waarnemen van een 50 mm lange snavel bij elk van de drie soorten te evalueren, \\(L(y|x_2=50)\\) Terugkomend op figuur 14.3, komen deze waarschijnlijkheden overeen met de hoogte van de normale densiteitskromme bij een snavellengte van 50 mm. Dus, een 50 mm lange bek is iets waarschijnlijker bij Kinband- dan bij Gentoo pinguïns, en hoogst onwaarschijnlijk bij Adelie pinguïns. Meer specifiek kunnen we de waarschijnlijkheden berekenen met dnorm()\n\n# L(y = A | x_2 = 50)\ndnorm(50, mean = 38.8, sd = 2.66)\n\n[1] 2.119955e-05\n\n# L(y = C | x_2 = 50)\ndnorm(50, mean = 48.8, sd = 3.34)\n\n[1] 0.1119782\n\n# L(y = G | x_2 = 50)\ndnorm(50, mean = 47.5, sd = 3.08)\n\n[1] 0.09317395\n\n\n\n\nTwee voorspellers\nWe hebben nu twee naïeve Bayes classificaties gemaakt van de soort van onze pinguïn: de ene enkel gebaseerd op het feit dat onze pinguïn onder het gemiddelde gewicht zit en de andere enkel gebaseerd op zijn 50mm lange snavel (naast onze eerdere informatie). En deze classificaties zijn niet identiek: wij classificeerden de pinguïn als Adelie in de eerste analyse en Gentoo in de tweede. Deze discrepantie toont aan dat er ruimte is voor verbetering in onze naïeve Bayes classificatie methode. In het bijzonder, in plaats van enkel te vertrouwen op één enkele voorspeller, kunnen we meerdere voorspellers in ons classificatieproces opnemen.\nBeschouw de informatie dat onze pinguïn een snavellengte heeft van\n\\(X_2=50mm\\) en een flipperlengte van \\(X_3=195mm\\). Elk van deze metingen alleen kan leiden tot een verkeerde classificatie. Net zoals het moeilijk is om een onderscheid te maken tussen de Chinstrap- en de Gentoopinguïn op basis van hun snavellengte, is het moeilijk om een onderscheid te maken tussen de Chinstrap en de Adéliepinguïn op basis van hun flipperlengte alleen (zie hieronder).\n\nggplot(penguins, aes(x = bill_length_mm, fill = species)) + \n  geom_density(alpha = 0.6)\n\nWarning: Removed 2 rows containing non-finite values (stat_density).\n\n\n\n\nggplot(penguins, aes(x = flipper_length_mm, fill = species)) + \n  geom_density(alpha = 0.6)\n\nWarning: Removed 2 rows containing non-finite values (stat_density).\n\n\n\n\n\nMAAR de soorten zijn redelijk te onderscheiden wanneer we de informatie over snavel- en vleugellengte combineren. Onze pinguïn met een 50 mm lange bek en 195 mm lange vleugels, voorgesteld op het snijpunt van de stippellijnen in de figuur hieronder, ligt nu precies tussen de Chinstrap waarnemingen:\n\nggplot(penguins, aes(x = flipper_length_mm, y = bill_length_mm, \n                     color = species)) + \n  geom_point() \n\nWarning: Removed 2 rows containing missing values (geom_point).\n\n\n\n\n\nLaten we naïeve Bayes classificatie gebruiken om deze gegevens in evenwicht te brengen met onze voorafgaande informatie over het soort lidmaatschap. Om de posterior waarschijnlijkheid te berekenen dat de pinguïn van de soort \\(Y=y\\) kunnen we de Bayes aanpassen aan onze twee voorspellers, \\(X_2=x_2\\) en \\(X_3=x_3\\).\nDit geeft weer een nieuwe wending: Hoe kunnen we de likelihood functie berekenen die twee variabelen omvat,\\(L(y|x_2, x_3)\\)? Dit is waar weer een andere “naïeve” veronderstelling binnensluipt. Naïeve Bayes classificatie gaat ervan uit dat voorspellers voorwaardelijk onafhankelijk zijn, dus\n\\[L(y|x_2, x_3)=f(x_2, x_3|y)=f(x_2|y)f(x_3|y)\\] Met andere woorden, binnen elke soort nemen we aan dat de lengte van de snavel van een pinguïn geen verband houdt met de lengte van zijn flipper. Wiskundig en computationeel gezien, maakt deze veronderstelling het naïeve Bayes algoritme efficiënt en beheersbaar. Maar het kan het ook verkeerd maken. Kijk nog eens naar de figuur hiervoven. Binnen elke soort lijken de vleugellengte en de snavellengte positief gecorreleerd te zijn, niet onafhankelijk. Toch gaan we naïef om met de veronderstelling van onvolmaakte onafhankelijkheid, en dus met de mogelijkheid dat onze classificatienauwkeurigheid zou kunnen worden afgezwakt.\nGecombineerd gaat het multivariabele naïeve Bayes-model ervan uit dat onze twee voorspellers Normaal en voorwaardelijk onafhankelijk zijn. We hebben dit normale model al afgestemd op de snalvellengte \\(x_2\\). Op dezelfde manier kunnen we de soortspecifieke normale modellen voor de lengte van de vleugels afstemmen op de overeenkomstige steekproefgemiddelden en standaardafwijkingen:\n\n# Bereken eerst het samplegemiddelde en sd voor elke Y groep\npenguins %&gt;% \n  group_by(species) %&gt;% \n  summarize(mean = mean(flipper_length_mm, na.rm = TRUE), \n            sd = sd(flipper_length_mm, na.rm = TRUE))\n\n# A tibble: 3 x 3\n  species    mean    sd\n  &lt;fct&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie     190.  6.54\n2 Chinstrap  196.  7.13\n3 Gentoo     217.  6.48\n\n\nZo kunnen wij voor elk van de drie soorten nagaan hoe groot de kans is dat een vleugelengte van 195 mm wordt waargenomen, \\(L(y|x_3=195)=f(x_3=195|y)\\)\n\n# L(y = A | x_3 = 195)\ndnorm(195, mean = 190, sd = 6.54)\n\n[1] 0.04554175\n\n# L(y = C | x_3 = 195)\ndnorm(195, mean = 196, sd = 7.13)\n\n[1] 0.05540502\n\n# L(y = G | x_3 = 195)\ndnorm(195, mean = 217, sd = 6.48)\n\n[1] 0.0001933746\n\n\nVoor elke soort hebben we nu de waarschijnlijkheid dat we een snavellengte waarnemen van \\(x_2=50mm\\), de waarschijnlijkheid van het waarnemen van een vleugellengte van \\(x_3=195\\) en de voorafgaande waarschijnlijkheid (prior). Gecombineerd is de kans op het waarnemen van een 50 mm lange snavel en een 195 mm lange vleugel voor elk soort \\(Y=y\\), gewogen met de prior van elk van de soorten als volgt:\n$$f(y^{‘}=A)L(y^{’}=A|x_2=50, x3=195)= \\\nf(y^{‘}=C)L(y^{’}=C|x_2=50, x3=195)= \\ f(y^{‘}=G)L(y^{’}=G|x_2=50, x3=195)= $$ met een som van\n\\[\\sum_{all y^{'}}f(y^{'})L(y^{'}|x_2=50, x_3=195)\\approx0.001241\\]\nAls we dat in de Bayes’regel stoppen dan wordt de waarschijnlijkheid dat de penguin een Adeliesoort is:\n\\[f(y=A|x_2=50,x_3=195)=\\frac{\\frac{151}{342}\\times0.0000212\\times0.04555}{0.001241}\\approx0.0003\\]\nZo kunnen we de kans op de andere twee soorten ook berekeken:\n\\[f(y=C|x_2=50,x_3=195)\\approx0.9944 \\\\\n  f(y=G|x_2=50,x_3=195)\\approx0.0052\\] Conclusie, onze penguin is vrijwel zeker een Chinstrap. Hoewel we niet tot deze conclusie zijn gekomen op basis van een fysiek kenmerk alleen, schetsen deze twee samen een vrij duidelijk beeld."
  },
  {
    "objectID": "posts/2022-03-26-naieve-bayesiaanse-classificatie/naeve-bayesiaanse-classificatie.html#implementeren-en-evalueren-van-naive-bayesisaanse-classificatie",
    "href": "posts/2022-03-26-naieve-bayesiaanse-classificatie/naeve-bayesiaanse-classificatie.html#implementeren-en-evalueren-van-naive-bayesisaanse-classificatie",
    "title": "Naïeve Bayesiaanse classificatie",
    "section": "Implementeren en evalueren van naive Bayesisaanse classificatie",
    "text": "Implementeren en evalueren van naive Bayesisaanse classificatie\nDat was aardig, maar we hoeven niet al dit werk met de hand te doen. Om naïeve Bayes classificatie in R te implementeren, gebruiken we de naïeveBayes() functie in het e1071 pakket (Meyer et al. 2021). Net zoals bij stan_glm(), voeden we naiveBayes() met de data en een formule die aangeeft welke variabelen in de analyse moeten worden gebruikt. Maar aangezien naive Bayes de prioriteitswaarschijnlijkheden rechtstreeks uit de gegevens berekent en de implementatie geen MCMC-simulatie vereist, hoeven we ons geen zorgen te maken over het verstrekken van informatie m.b.t. de priormodellen of Markovketens. Hieronder bouwen we twee naïeve Bayes classificatie-algoritmen, een die alleen bill_length_mm gebruikt en een die ook flipper_length_mm bevat:\n\nnaive_model_1 &lt;- naiveBayes(species ~ bill_length_mm, data = penguins)\nnaive_model_2 &lt;- naiveBayes(species ~ bill_length_mm + flipper_length_mm, \n                            data = penguins)\n\nLaten we deze beide toepassen om onze_penguin te classificeren die we de hele tijd hebben bestudeerd:\n\nour_penguin &lt;- data.frame(bill_length_mm = 50, flipper_length_mm = 195)\n\nWe beginnen met naive_model_1. De predict() functie geeft de posterior waarschijnlijkheden van elke soort terug, samen met een uiteindelijke classificatie. Deze classificatie volgt een eenvoudige regel: classificeer de pinguïn als de soort met de hoogste posterior waarschijnlijkheid. De resultaten van dit proces zijn gelijkaardig aan deze die we hierboven “met de hand” verkregen met een kleine afwijking door een afrondingsfout. In feite, gebaseerd op de snavellengte alleen, is onze beste gok dat deze pinguïn een Gentoo is:\n\npredict(naive_model_1, newdata = our_penguin, type = \"raw\")\n\n           Adelie Chinstrap    Gentoo\n[1,] 0.0001690279 0.3978306 0.6020004\n\npredict(naive_model_1, newdata = our_penguin)\n\n[1] Gentoo\nLevels: Adelie Chinstrap Gentoo\n\n\n\npredict(naive_model_2, newdata = our_penguin, type = \"raw\")\n\n           Adelie Chinstrap      Gentoo\n[1,] 0.0003445688 0.9948681 0.004787365\n\n\nEn net zoals we hierboven concludeerden, als we rekening houden met zowel de snavellengte als de lengte van de vleugels, is onze beste gok dat deze pinguïn een Chinstrap is.\nWe kunnen op dezelfde manier onze naïeve Bayes modellen toepassen om een willekeurig aantal pinguïns te classificeren. Zoals met logistische regressie, zullen we twee gebruikelijke benaderingen volgen om de nauwkeurigheid van deze classificaties te evalueren:\n\nconfusion matrixen construeren die de waargenomen soorten van onze steekproef pinguïns vergelijken met hun naïeve Bayes soortclassificaties;\n\nom een beter idee te krijgen van hoe goed onze naïeve Bayes modellen nieuwe pinguïns classificeren, berekenen we kruisgevalideerde schattingen van de nauwkeurigheid van de classificatie.\n\nOm met de eerste benadering te beginnen, classificeren we elk van de pinguïns met zowel naive_model_1 als naive_model_2 en slaan deze op in penguins als class_1 en class_2:\n\npenguins &lt;- penguins %&gt;% \n  mutate(class_1 = predict(naive_model_1, newdata = .),\n         class_2 = predict(naive_model_2, newdata = .))\n\nDe classificatieresultaten worden hieronder getoond voor vier willekeurig gekozen pinguïns, afgezet tegen de werkelijke species van deze pinguïns. Voor de laatste twee pinguïns, geven de twee modellen dezelfde classificaties (Adelie) en deze classificaties zijn correct. Voor de eerste twee pinguïns, leiden de twee modellen tot verschillende classificaties. In beide gevallen is naive_model_2 correct.\n\nset.seed(84735)\npenguins %&gt;% \n  sample_n(4) %&gt;% \n  select(bill_length_mm, flipper_length_mm, species, class_1, class_2) %&gt;% \n  rename(bill = bill_length_mm, flipper = flipper_length_mm)\n\n# A tibble: 4 x 5\n   bill flipper species   class_1 class_2  \n  &lt;dbl&gt;   &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;   &lt;fct&gt;    \n1  47.5     199 Chinstrap Gentoo  Chinstrap\n2  40.9     214 Gentoo    Adelie  Gentoo   \n3  41.3     194 Adelie    Adelie  Adelie   \n4  38.5     190 Adelie    Adelie  Adelie   \n\n\nHet valt natuurlijk nog te bezien of naive_model_2 beter presteert dan naive_model_1 in het algemeen. Daartoe geven de onderstaande confusion matrixen een overzicht van de classificatienauwkeurigheid van de modellen over alle pinguïns in onze steekproef\n\n# Confusion matrix voor naive_model_1\npenguins %&gt;% \n  tabyl(species, class_1) %&gt;% \n  adorn_percentages(\"row\") %&gt;% \n  adorn_pct_formatting(digits = 2) %&gt;%\n  adorn_ns()\n\n   species       Adelie Chinstrap       Gentoo\n    Adelie 95.39% (145) 0.00% (0)  4.61%   (7)\n Chinstrap  5.88%   (4) 8.82% (6) 85.29%  (58)\n    Gentoo  6.45%   (8) 4.84% (6) 88.71% (110)\n\n# Confusion matrix voor naive_model_2\npenguins %&gt;% \n  tabyl(species, class_2) %&gt;% \n  adorn_percentages(\"row\") %&gt;% \n  adorn_pct_formatting(digits = 2) %&gt;%\n  adorn_ns()\n\n   species       Adelie   Chinstrap       Gentoo\n    Adelie 96.05% (146)  2.63%  (4)  1.32%   (2)\n Chinstrap  7.35%   (5) 86.76% (59)  5.88%   (4)\n    Gentoo  0.81%   (1)  0.81%  (1) 98.39% (122)\n\n\nLaten we, met deze aanwijzingen in gedachten, de twee confusion matrixen onderzoeken. Een snelle blik leert dat naive_model_2 het over de hele linie beter doet. Niet alleen zijn de classificatiepercentages voor elk van de Adelie, Chinstrap en Gentoo soorten hoger dan in naive_model_1, maar ook de totale nauwkeurigheid is hoger.\nHet naive_model_2 classificeert 327 (146+59+122) van de 344 pinguïns correct (95%). Terwijl het naive_model_1 slechts 261 pinguïns correct classificeert (76%). Waar naive_model_2 de grootste verbetering vertoont t.o.v. naive_model_1 is in de classificatie van de Chinstrappinguïns. In naive_model_1 wordt slechts 9% van de Chinstrapspinguïns juist geklasseerd, met maar liefst 85% die verkeerd geklasseerd wordt als Gentoo. Met 87% is de classificatienauwkeurigheid voor Chinstraps veel hoger in naive_model_2.\nTenslotte kunnen we, voor de nodige zorgvuldigheid, 10-voudige kruisvalidatie gebruiken om te evalueren en te vergelijken hoe goed onze naïeve Bayes classificatiemodellen nieuwe pinguïns classificeren, niet enkel deze uit onze steekproef. We doen dit met behulp van de naive_classification_summary_cv() functie in het bayesrules pakket:\n\nset.seed(84735)\ncv_model_2 &lt;- naive_classification_summary_cv(\n  model = naive_model_2, data = penguins, y = \"species\", k = 10)\n\nHet cv_model_2$folds object bevat de classificatienauwkeurigheid voor elk van de 10 vouwen (k=10) terwijl cv_model_2$cv het gemiddelde neemt van de resultaten over alle 10 vouwen:\n\ncv_model_2$cv\n\n   species       Adelie   Chinstrap       Gentoo\n    Adelie 96.05% (146)  2.63%  (4)  1.32%   (2)\n Chinstrap  7.35%   (5) 86.76% (59)  5.88%   (4)\n    Gentoo  0.81%   (1)  0.81%  (1) 98.39% (122)\n\n\nDe nauwkeurigheidspercentages in deze kruisgevalideerde confusion matrixen zijn vergelijkbaar met die in de niet-kruisgevalideerde verwarringsmatrix hierboven. Dit impliceert dat ons naïef Bayes model bijna even goed lijkt te presteren op nieuwe pinguïns als op het originele pinguïn monster dat we gebruikten om dit model te bouwen."
  },
  {
    "objectID": "posts/2022-03-26-naieve-bayesiaanse-classificatie/naeve-bayesiaanse-classificatie.html#naëve-bayes-vs-logistische-regressie",
    "href": "posts/2022-03-26-naieve-bayesiaanse-classificatie/naeve-bayesiaanse-classificatie.html#naëve-bayes-vs-logistische-regressie",
    "title": "Naïeve Bayesiaanse classificatie",
    "section": "Naëve Bayes vs logistische regressie",
    "text": "Naëve Bayes vs logistische regressie\nGezien de drie pinguïnsoorten, vereiste onze classificatie-analyse hierboven een naïeve Bayes classificatie - logistische regressie toepassen was zelfs geen optie. Echter, in scenario’s met een binaire categorische respons variabele \\(Y\\) zijn zowel logistische regressie als naïeve Bayes haalbare classificatiebenaderingen. Zowel naïeve Bayes als logistische regressie hebben hun voor- en nadelen. Hoewel naïeve Bayes zeker rekenkundig efficiënt is, maakt het ook een aantal zeer starre en vaak ongeschikte veronderstellingen over datastructuren. Je hebt misschien ook opgemerkt dat we wat nuance verliezen met naïeve Bayes. In tegenstelling tot het logistische regressiemodel met\n\\[log(\\frac{\\pi}{1-\\pi})=\\beta_0+\\beta_1X_1+...\\beta_kX_p,\\]\nnaïeve Bayes mist regressiecoëfficiënten \\(\\beta_1\\). Dus, hoewel naïeve Bayes informatie over voorspellers \\(X\\) kan omzetten in classificaties van \\(Y\\) maar doet dit zonder veel opheldering over de relaties tussen deze variabelen.\nOf naïeve Bayes of logistische regressie het juiste instrument is voor een binaire classificatie hangt af van de situatie. In het algemeen, als de starre naïeve Bayes-aannamen ongepast zijn of als u belang hecht aan de specifieke verbanden tussen \\(Y\\) en \\(X\\) (d.w.z. je wilt niet gewoon een reeks classificaties), dan moet je logistische regressie gebruiken. Anders is naïeve Bayes misschien precies wat je nodig hebt. Beter nog, kies niet! Probeer beide tools uit en leer ervan."
  },
  {
    "objectID": "posts/2022-03-26-naieve-bayesiaanse-classificatie/naeve-bayesiaanse-classificatie.html#samengevat",
    "href": "posts/2022-03-26-naieve-bayesiaanse-classificatie/naeve-bayesiaanse-classificatie.html#samengevat",
    "title": "Naïeve Bayesiaanse classificatie",
    "section": "Samengevat",
    "text": "Samengevat\nNaive Bayes classificatie is een handig hulpmiddel voor het classificeren van categorische responsvariabelen\n\\(Y\\) met twee of meer categorieën. Stel (\\(X_1, X_2, ...,X_p)\\) zijn een set van \\(p\\) mogelijke voorspellers van \\(Y\\), naïef Bayes berekent de posterior waarschijnlijkheid van elke categorie via de Bayes regel:\n\\[f(y|x_1, x_2,...,x_p)=\\frac{f(y)L(y|x_1, x_2, ...,x_p)}{\\sum_{ally^{'}f(y^{'}L(y^{'}|x_1, x_2, ...,x_p)}}\\]\nDaarbij worden enkele zeer naïeve veronderstellingen gemaakt over het gegevensmodel op basis waarvan wij de waarschijnlijkheid \\(L(y|x_1,x_2, ...,x_p\\) bepalen. De voorspellers \\(X_1\\) zijn voorwaardelijk onafhankelijk en de waarden van de kwantitatieve voorspellers \\(X_i\\)X variëren normaal binnen elke categorie \\(Y=y\\). Deze vereenvoudigende veronderstellingen maken het naïeve Bayes-model rekenkundig efficiënt en eenvoudig toe te passen. Maar als deze vereenvoudigende veronderstellingen niet worden nageleefd (wat vaak voorkomt), kan het naïeve Bayes-model misleidende classificaties opleveren."
  },
  {
    "objectID": "posts/2022-03-26-naieve-bayesiaanse-classificatie/naeve-bayesiaanse-classificatie.html#literatuur",
    "href": "posts/2022-03-26-naieve-bayesiaanse-classificatie/naeve-bayesiaanse-classificatie.html#literatuur",
    "title": "Naïeve Bayesiaanse classificatie",
    "section": "Literatuur",
    "text": "Literatuur\nGorman, Kristen B., Tony D. Williams, and William R. Fraser. 2014. “Ecological Sexual Dimorphism and Environmental Variability Within a Community of Antarctic Penguins (Genus Pygoscelis).” PLoS ONE 9(3) (e90081). hier.\nJohnson, A.A., Ott, M.Q. & Dogucu, M. (2022). *Bayes Rules! An introduction to applied Bayesian Modeling. CRC Press. hier\nHorst, Allison, Alison Hill, and Kristen Gorman. 2020. Palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data. hier."
  },
  {
    "objectID": "posts/2022-03-29-wat-kun-je-met-bayes/wat-kun-je-met-bayes.html",
    "href": "posts/2022-03-29-wat-kun-je-met-bayes/wat-kun-je-met-bayes.html",
    "title": "Wat kun je met Bayes?",
    "section": "",
    "text": "Bayes Rules"
  },
  {
    "objectID": "posts/2022-03-29-wat-kun-je-met-bayes/wat-kun-je-met-bayes.html#inleiding",
    "href": "posts/2022-03-29-wat-kun-je-met-bayes/wat-kun-je-met-bayes.html#inleiding",
    "title": "Wat kun je met Bayes?",
    "section": "Inleiding",
    "text": "Inleiding\nOnlangs verscheen een prachtig boek van Alicia A. Johnson, Miles Q. Ot en Mine Dogucu onder de titel Bayes Rules! An Introduction to Applied Bayesian Modeling en het verscheen bij CRC Press (2022). Eerdere versies kon je al via bookdown bekijken (https://www.bayesrulesbook.com/) en vanaf de eerste keer dat ik het zag, was ik hier heel enthousiast over. Het boek heb ik direct besteld en vorige week kon ik het ophalen.\nHet boek bestaat uit vier duidelijke delen. Het eerste deel gaat in op de fundamenten van het Bayesiaanse perspectief. Het leert je denken als een Bayesiaan en het gaat in op die belangrijke Bayesiaanse regel \\(posterior=\\frac{prior.likelihood}{normaliserende constante}\\). Aan de hand van enkele voorbeelden gaan Johnson e.a. in op hoe het in de praktijk werkt. Daarna gaat het in op hoe kennis en data op elkaar inwerken en laat het enkele basisanalyses zien en hoe dat in deze vorm van statistiek werkt (normaal, binair en poisson). Het tweede deel is een meer technisch hoofdstuk en laat je ook onder de moterkap van deze techniek kijken. Het gaat ook in op de wetenschappelijke principes van de benadering, waar je hier op moet letten, hoe je hiermee hypothesen kunt testen (niet alleen tov van een nulhypothese, maar hoeveel beter de ene hypothese is ten opzichte van de andere hyposthese) en hoe je hiermee ook kunt voorspellen. De twee volgende delen (Deel drie en vier) zijn praktische delen. Deel drie gaat in op regressieanalyses voor continue variabelen en classificatieanalyses voor binaire variabelen. Het vierde deel ten slotte gaat in op geclusterde datasets en hoe je hierarchische Bayesiaanse regressie en classificatieanalyses uitvoert.\nNatuurlijk, er zijn onderhand al verschillende interessante boeken te krijgen die je laten zien hoe Bayesiaanse denken in de praktijk kan werken. De boeken van Gelman, McElreath, Spiegelhalter en Kruschke verschenen de afgelopen tien/vijftien jaar en leren je dit. Maar Bayes Rules! vind ik op dit moment als introductieboek mogelijk wel het beste.\nNu het boek bij mij op het bureau ligt, kan ik er binnenkort een keer een korte recensie over schrijven. Voor nu heb ik uit elk deel een hoofdstuk genomen en het vertaald en bewerkt. Hieronder zie je een bewerking van het achtste hoofdstuk van het tweede deel (Posterior Inference & Prediction).Hoofdstukken zo overzetten is voor mij niet alleen een goede manier om het mij eigen te maken, maar ook een manier om het boek anderen aan te raden. Dus lezen en gebruiken deze Bayes Rules! An Introduction to Applied Bayesian Modeling"
  },
  {
    "objectID": "posts/2022-03-29-wat-kun-je-met-bayes/wat-kun-je-met-bayes.html#posterior-inferentie-en-voorspelling",
    "href": "posts/2022-03-29-wat-kun-je-met-bayes/wat-kun-je-met-bayes.html#posterior-inferentie-en-voorspelling",
    "title": "Wat kun je met Bayes?",
    "section": "Posterior inferentie en voorspelling",
    "text": "Posterior inferentie en voorspelling\nStel je voor dat je in het Museum of Modern Art (MoMA) in New York City staat, gefascineerd door het kunstwerk voor je. Hoewel je begrijpt dat “moderne” kunst niet noodzakelijk “nieuwe” kunst betekent, komt er toch een vraag bij je op: wat is de kans dat deze moderne kunstenaar Gen X is of zelfs jonger, d.w.z. geboren in 1965 of later? Hier voeren we een Bayesiaanse analyse uit met als doel deze vraag te beantwoorden. Laat daartoe\n\\(\\pi\\) het aandeel zijn van de kunstenaars vertegenwoordigd in de grote Amerikaanse musea voor moderne kunst die Gen X of jonger zijn. Het Beta(4,6) prior model voor \\(\\pi\\) dat onze eigen zeer vage aanname weerspiegelt dat grote moderne kunstmusea onevenredig veel kunstenaars tonen die geboren zijn voor 1965, d.w.z, \\(\\pi\\) valt hoogstwaarschijnlijk onder 0,5. Moderne kunst” dateert immers van de jaren 1880 en het kan een tijdje duren voor men zo’n hoge erkenning in de kunstwereld bereikt.\nOm meer te weten te komen over \\(\\pi\\), zullen we \\(n\\)=100 kunstenaars uit de collectie van het MoMA nemen. Deze moma_sample dataset in het bayesrules pakket is een subset van gegevens die door het MoMA zelf beschikbaar zijn gesteld (zie MuseumofModernArt 2020-“MoMA – Collection.” GitHub Repository).\n\n# Op de pakketten (wel eerst installeren uiteraard) \nlibrary(bayesrules)\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv ggplot2 3.3.6     v purrr   0.3.4\nv tibble  3.1.7     v dplyr   1.0.9\nv tidyr   1.2.0     v stringr 1.4.1\nv readr   2.1.2     v forcats 0.5.1\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'stringr' was built under R version 4.1.3\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(rstan)\n\nWarning: package 'rstan' was built under R version 4.1.3\n\n\nLoading required package: StanHeaders\n\n\nrstan (Version 2.21.5, GitRev: 2e1f913d3ca3)\n\n\nFor execution on a local, multicore CPU with excess RAM we recommend calling\noptions(mc.cores = parallel::detectCores()).\nTo avoid recompilation of unchanged Stan programs, we recommend calling\nrstan_options(auto_write = TRUE)\n\n\nDo not specify '-march=native' in 'LOCAL_CPPFLAGS' or a Makevars file\n\n\n\nAttaching package: 'rstan'\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\nlibrary(bayesplot)\n\nWarning: package 'bayesplot' was built under R version 4.1.3\n\n\nThis is bayesplot version 1.9.0\n\n\n- Online documentation and vignettes at mc-stan.org/bayesplot\n\n\n- bayesplot theme set to bayesplot::theme_default()\n\n\n   * Does _not_ affect other ggplot2 plots\n\n\n   * See ?bayesplot_theme_set for details on theme setting\n\nlibrary(broom.mixed)\n\nWarning: package 'broom.mixed' was built under R version 4.1.3\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\n#  data openen\ndata(\"moma_sample\")\n\nOnder deze groep artiesten zitten 14 Gen X of jongere artiesten (\\(Y=14\\)).\n\nmoma_sample %&gt;% \n  group_by(genx) %&gt;% \n  tally()\n\n# A tibble: 2 x 2\n  genx      n\n  &lt;lgl&gt; &lt;int&gt;\n1 FALSE    86\n2 TRUE     14\n\n\n\\(Y|\\pi\\) kun je het beste opvatten en dat betekent dat onze analyse het bèta-binomiale kader volgt. On aangepast posterior model van \\(\\p\\) in het licht van de waargenomen kunstgegevens ziet er als volgt uit:\n\\[Y|\\pi \\sim Bin(100,\\pi) \\\\\n  \\pi\\sim Beta(4,6)\\] wordt \\[\\pi|Y=14) \\sim Beta(18,92)\\]\nmet de corresponderende posterior pdf \\[f(\\pi|y=14)=\\frac{\\Gamma(18+92)}{\\Gamma(18)\\Gamma(92)}\\pi^{18-1}(1-\\pi)^{92-1} for \\pi \\epsilon[0,1].\\]\nDe evolutie in ons begrip van \\(\\pi\\) is hieronder te zien. Terwijl we begonnen met een vaag begrip dat minder dan de helft van de tentoongestelde kunstenaars Gen X zijn, hebben de gegevens ons met enige zekerheid doen stellen dat dit cijfer waarschijnlijk onder 25% ligt.\n\nplot_beta_binomial(alpha = 4, beta = 6, y = 14, n = 100)\n\n\n\n\nNadat we succesvol de posterior hebben geconstrueerd, besefen we ons dat er nog veel werk voor ons ligt. We moeten deze posterior kunnen gebruiken om een rigoureuze posterior analyse uit te voeren. Er zijn drie algemene taken in posterior analyse: schatting, hypothesetest, en voorspelling. Bijvoorbeeld, wat is onze preciese schatting van \\(\\pi\\)? Ondersteunt ons model de bewering dat minder dan 20% van de museumkunstenaars Gen X of jonger zijn? Als we 20 extra museumkunstenaars zouden nemen, hoeveel voorspellen we dan dat Gen X of jonger zullen zijn? Laat je gevoel eens spreken, wat denk je:\na. Ongeveer 16% van de museum kunstenaars zijn Gen X of jonger.\nb. Het is zeer waarschijnlijk dat ongeveer 16% van de museumkunstenaars Gen X of jonger is, maar dat cijfer zou ook tussen 9% en 26% kunnen liggen.\nAls je antwoordde met antwoord b, is je denkwijze Bayesiaans van geest.\n[Figuur: Ons Beta(18, 92) posterior model voor \\(π\\) (links) naast een alternatief Beta(4, 16) posterior model (rechts). De gekleurde gebieden geven de overeenkomstige 95% posterior geloofwaardigheids intervallen weer voor ]\nDe clou hier is dat posterieure schattingen zowel de centrale tendens als de variabiliteit in \\(\\pi\\). Het posterior gemiddelde en de modus van \\(\\pi\\) geven een snel overzicht van enkel de centrale tendens. Deze kenmerken voor onze Beta(18, 92) posterior volgen uit de algemene Beta-eigenschappen komen overeen met onze bovenstaande observatie dat de Gen X vertegenwoordiging hoogstwaarschijnlijk rond 16% ligt:\n\\[E(\\pi|Y=14)=\\frac{18}{18+92} \\approx 0.164 \\\\\nMode(\\pi|Y=14)=\\frac{18-1}{18+92-2} \\approx 0.157\\]"
  },
  {
    "objectID": "posts/2022-03-29-wat-kun-je-met-bayes/wat-kun-je-met-bayes.html#posterior-schatting",
    "href": "posts/2022-03-29-wat-kun-je-met-bayes/wat-kun-je-met-bayes.html#posterior-schatting",
    "title": "Wat kun je met Bayes?",
    "section": "Posterior schatting",
    "text": "Posterior schatting\nLaten we eens opnieuw kijken naar het Beta(18, 92) posterior model voor \\(\\pi\\), het percentage moderne kunst museumkunstenaars dat Gen X of jonger is (zie figuur hierboven). In een Bayesiaanse analyse kunnen we dit hele posterior model zien als een schatting van \\(\\pi\\). Immers, dit model van posterior plausibele waarden geeft een compleet beeld van de centrale tendens en onzekerheid in \\(\\pi\\). Maar bij het specificeren en communiceren van ons posterior begrip is het ook nuttig om eenvoudige posterior samenvattingen te berekenen van \\(\\pi\\).\nBeter is het nog, om zowel de centrale tendens als de variabiliteit in \\(\\pi\\) te kunnen rapporteren als een reeks van aannemelijke posterior aannemelijke \\(\\pi\\) waarden. Dit bereik wordt een posterior geloofwaardigheids interval (CI, Credible Interval) genoemd voor \\(\\pi\\). We hebben bijvoorbeeld eerder opgemerkt dat het aandeel museumkunstenaars dat Gen X of jonger is, hoogstwaarschijnlijk tussen 10% en 24% ligt. Dit bereik vangt de meer plausibele waarden van \\(\\pi\\) terwijl de meer extreme en onwaarschijnlijke scenario’s worden geëlimineerd. In feite zijn 0,1 en 0,24 de 2,5e en 97,5e posterior percentielen (d.w.z. 0,025ste en 0,975ste posterior kwantielen). Dit zijn is het middelste deel, 95% CI van de posterior geloofwaardige \\(\\pi\\)waarden. We kunnen deze Beta(18,92) posterior quantiel berekeningen bevestigen met qbeta():\n\n# 0.025-0.975 interval van de Beta(18,92) posterior\nqbeta(c(0.025, 0.975), 18, 92)\n\n[1] 0.1009084 0.2379286\n\n\nHet resulterende 95% geloofwaardigheidsinterval voor \\(\\pi\\), (0,1, 0,24), wordt weergegeven door het gekleurde gebied in de figuur hierboven (links). Terwijl het gebied onder de gehele posterior pdf 1 is, is het gebied van dit gekleurde gebied 0,95, dus de fractie van \\(\\pi\\)waarden die in dit gebied vallen. Dit onthult een intuïtieve interpretatie van de CI. Er is een 95% posterior waarschijnlijkheid dat ergens tussen 10% en 24% van de museumkunstenaars Gen X of jonger zijn:\n\\[P(\\pi\\epsilon(0.1,0.24)|Y=14)=\\int_{0.1}^{0.24}f(\\pi|y=14)d\\pi=0.95\\]\nSta hier alstublieft even stil. Voelt deze interpretatie natuurlijk en intuïtief aan? Dus, een beetje anticlimactisch? Als dat zo is, zijn we blij dat je er zo over denkt - het betekent dat je denkt als een Bayesiaan.\nBij de constructie van de CI hierboven hebben we een “middelste 95%” benadering gebruikt. Dit is niet onze enige optie. De eerste aanpassing die we kunnen doen is het geloofwaardige niveau van 95% (zie figuur hieronder). Bijvoorbeeld, een middelste 50% CI, van het 25ste tot het 75ste percentiel, zou onze aandacht vestigen op een kleiner bereik van enkele van de meer plausibele \\(\\pi\\) waarden. Er is een 50% posterior waarschijnlijkheid dat ergens ligt tussen 14% en 19% van de museumkunstenaars Gen X of jonger zijn:\n\n# 0.25-0.75 interval van de Beta(18,92) posterior\nqbeta(c(0.25, 0.75), 18, 92)\n\n[1] 0.1388414 0.1862197\n\n\nIn de andere richting zou een bredere 99%-controlegrens van 0,5 tot 99,5 percentiel lopen. In dat geval wordt alleen de extreme 1% uitgeslopten. Als zodanig zou een 99% CI ons een vollediger beeld geven van plausibele en in sommige gevallen zeer onwaarschijnlijke \\(\\pi\\)waarden:\n\n# 0.005-0.995 interval van de Beta(18,92) posterior\nqbeta(c(0.005, 0.995), 18, 92)\n\n[1] 0.08530422 0.26468037\n\n\nHoewel een 95%-niveau een gebruikelijke keuze is, is het enigszins arbitrair en gewoon ingebakken door decennia van traditie. Er is niet één “juist” geloofwaardigheidsniveau. Je kunt net zo makkelijk 50%, 80% of 95% niveaus gebruiken, afhankelijk van de context van de analyse. Elk geeft een ander posterior begrip."
  },
  {
    "objectID": "posts/2022-03-29-wat-kun-je-met-bayes/wat-kun-je-met-bayes.html#posterior-hypothese-testing",
    "href": "posts/2022-03-29-wat-kun-je-met-bayes/wat-kun-je-met-bayes.html#posterior-hypothese-testing",
    "title": "Wat kun je met Bayes?",
    "section": "Posterior hypothese testing",
    "text": "Posterior hypothese testing\n\nEenzijdige tests\nHet testen van hypothesen is een andere veel voorkomende taak bij posterior analyse. Bijvoorbeeld, stel dat we een artikel lezen waarin wordt beweerd dat minder dan 20% van de museumkunstenaars Gen X of jonger zijn. Twee aanwijzingen die we hebben waargenomen uit ons posterior model van \\(\\pi\\) wijzen erop dat deze bewering op zijn minst voor een deel plausibel is:\n\nHet grootste deel van de posterior pdf in figuur hieronder valt onder 0.2.\n\nHet 95% CI voor \\(\\pi\\) (0.1, 0.24) zit vooral onder 0.2.\n\nDeze waarnemingen zijn een goed begin. Maar we kunnen nog preciezer zijn. Om precies te evalueren hoe aannemelijk het is dat \\(\\pi&lt;0.2\\), kunnen we de posterior waarschijnlijkheid van dit scenario berekenen, $P(&lt;0.2|Y=14). Deze posterior waarschijnlijkheid wordt weergegeven door het gearceerde gebied onder de posterior pdf in xxxxx. Het wordt wiskundig berekend door de posterior pdf te integreren in het gebied van 0 tot 0,2:\n\\[P(\\pi&lt;0.2|Y=14)=\\int_{0}^{0.2}f(\\pi|y=14)d\\pi\\] We zullen de integratie omzeilen en deze Beta(18,92) posterior waarschijnlijkheid verkrijgen met pbeta() hieronder. Het resultaat toont een sterk bewijs ten gunste van onze bewering: er is ongeveer 84,9% posterior kans dat Gen X-ers minder dan 20% van de moderne kunst museumkunstenaars uitmaken.\n\n# Posterior waarschijnlijkheid dat pi &lt; 0.20\npost_prob &lt;- pbeta(0.20, 18, 92)\npost_prob\n\n[1] 0.8489856\n\n\nDe analyse van onze bewering is verfrissend eenvoudig. Wij hebben eenvoudigweg de posterior waarschijnlijkheid van het scenario van belang berekend. Hoewel dit niet altijd nodig is, formaliseren mensen uit de praktijk deze procedure vaak in een raamwerk voor het testen van hypothesen. Wij kunnen onze analyse bijvoorbeeld omkaderen met twee concurrerende hypothesen: de nulhypothese \\(H_0\\) stelt dat minstens 20% van de museumkunstenaars Gen X of jonger zijn (de status quo hier), terwijl de alternatieve hypothese \\(H_{\\alpha}\\) (onze bewering) stelt dat dit cijfer lager is dan 20%. In wiskundige notatie:\n$$H_0: \\\nH_{}:&lt; 0.2$$ Merk op dat \\(H_{\\alpha}\\) beweert dat \\(\\pi\\) aan één kant van 0,2 ligt (\\(\\pi&lt;0.2\\)) in tegenstelling tot gewoon verschillend zijn dan 0.2 (\\(\\pi\\ne0.2\\)). We noemen dit dus een eenzijdige hypothesetoets. We hebben de posterior waarschijnlijkheid van de alternatieve hypothese al berekend als \\(P(H_{\\alpha}|Y=14=0.849\\). De posterieure waarschijnlijkheid van de nulhypothese is dus \\(P(H_0|Y=14)=0.151\\). Samengenomen is de posterior odds dat \\(\\pi&lt;0.2\\) ruwweg 5,62 zijn. Dat wil zeggen, onze posterieure beoordeling is dat \\(\\pi\\) bijna 6 keer meer kans heeft om onder 0,2 te liggen dan om boven 0,2 te liggen:\n\\[posterior odds=\\frac{P(H_{alpha}|Y=14)}{P(H_0|Y=14)}\\approx5.62\\]\n\n# Posterior odds\npost_odds &lt;- post_prob / (1 - post_prob)\npost_odds\n\n[1] 5.621883\n\n\n\n# Prior probability that pi &lt; 0.2\nprior_prob &lt;- pbeta(0.20, 4, 6)\nprior_prob\n\n[1] 0.08564173\n\n\n\n# Prior odds\nprior_odds &lt;- prior_prob / (1 - prior_prob)\nprior_odds\n\n[1] 0.09366321\n\n\n\n# Bayes factor\nBF &lt;- post_odds / prior_odds\nBF\n\n[1] 60.02232\n\n\n\n\nTweezijdige tests"
  },
  {
    "objectID": "posts/2022-03-29-wat-kun-je-met-bayes/wat-kun-je-met-bayes.html#posterior-voorspelling",
    "href": "posts/2022-03-29-wat-kun-je-met-bayes/wat-kun-je-met-bayes.html#posterior-voorspelling",
    "title": "Wat kun je met Bayes?",
    "section": "Posterior voorspelling",
    "text": "Posterior voorspelling"
  },
  {
    "objectID": "posts/2022-03-29-wat-kun-je-met-bayes/wat-kun-je-met-bayes.html#posterior-analyse-met-mcmc",
    "href": "posts/2022-03-29-wat-kun-je-met-bayes/wat-kun-je-met-bayes.html#posterior-analyse-met-mcmc",
    "title": "Wat kun je met Bayes?",
    "section": "Posterior analyse met MCMC",
    "text": "Posterior analyse met MCMC\nHet is goed te weten dat er enige theorie achter Bayesiaanse posterior analyse zit. En wanneer we werken met modellen die zo eenvoudig zijn als de Beta-Binomiaal, kunnen we deze theorie direct implementeren - dat wil zeggen, we kunnen exacte posterior geloofwaardige intervallen, waarschijnlijkheden, en voorspellende modellen berekenen. Maar het is duidelijk dat dit mooie terrein ook betreden kan worden bij scenario’s waarin we geen posterior modellen kunnen specificeren, laat staan exacte samenvattingen van hun eigenschappen kunnen berekenen. In deze scenario’s kunnen we posteriores benaderen met behulp van MCMC methoden. Laten we nu eens onderzoeken hoe we dit soort Markov chain steekproefwaarden ook kunnen worden gebruikt om specifieke posterior kenmerken te benaderen. Laten we eens zien hoe we dit soort methodes kunnen gebruiken om posterior analyses uit te voeren.\nHieronder worden vier parallelle Markovketens uitgevoerd van \\(\\pi\\) voor 10.000 iteraties elk. Na het weggooien van de eerste 5.000 iteraties van elke keten, houden we nog vier afzonderlijke Markovketens van 5.000 over, {\\({\\pi^{1}, \\pi^{(2)}, ..., \\pi^{(5000)}}\\)}, of een gecombineerde Markov-keten steekproefgrootte van 20.000.\n\nPosterior simulatie\n\n# STAP 1: DEFINEER HET model\nart_model &lt;- \"\n  data {\n    int&lt;lower = 0, upper = 100&gt; Y;\n  }\n  parameters {\n    real&lt;lower = 0, upper = 1&gt; pi;\n  }\n  model {\n    Y ~ binomial(100, pi);\n    pi ~ beta(4, 6);\n  }\n\"\n\n# STAP 2: SIMULEER DE POSTERIOR\nart_sim &lt;- stan(model_code = art_model, data = list(Y = 14), \n                chains = 4, iter = 5000*2, seed = 84735)\n\n\nSAMPLING FOR MODEL '55ec3099e16ca06fa44f4c9f6d6ebcf4' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.035 seconds (Warm-up)\nChain 1:                0.043 seconds (Sampling)\nChain 1:                0.078 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL '55ec3099e16ca06fa44f4c9f6d6ebcf4' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.035 seconds (Warm-up)\nChain 2:                0.045 seconds (Sampling)\nChain 2:                0.08 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL '55ec3099e16ca06fa44f4c9f6d6ebcf4' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.035 seconds (Warm-up)\nChain 3:                0.041 seconds (Sampling)\nChain 3:                0.076 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL '55ec3099e16ca06fa44f4c9f6d6ebcf4' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.042 seconds (Warm-up)\nChain 4:                0.028 seconds (Sampling)\nChain 4:                0.07 seconds (Total)\nChain 4: \n\n\nBekijk de numerieke en visuele diagnostiek in de figuur hieronder eens. Ten eerste duiden de willekeurigheid in de sporenplots (links), de overeenstemming in de dichtheidplots van de vier parallelle ketens (midden) en een Rhat-waarde van effectief 1 erop dat onze simulatie uiterst stabiel is. Verder gedragen onze afhankelijke ketens zich elk “genoeg” als een onafhankelijk sample. De autocorrelatie, rechts weergegeven voor slechts één keten, neemt snel af en de verhouding van de effectieve steekproefgrootte is bevredigend hoog - onze 20.000 Markov-ketenwaarden zijn even effectief als 7600 onafhankelijke steekproeven (0,38 ⋅ 20000).\n\n# Parallelle spoor(trace) plotten & density plotten\nmcmc_trace(art_sim, pars = \"pi\", size = 0.5) + \n  xlab(\"iteration\")\n\n\n\nmcmc_dens_overlay(art_sim, pars = \"pi\")\n\n\n\n# Autocorrelatie plot\nmcmc_acf(art_sim, pars = \"pi\")\n\n\n\n\n\n# MC diagnostiek\nrhat(art_sim, pars = \"pi\")\n\n[1] 1.000508\n\nneff_ratio(art_sim, pars = \"pi\")\n\n[1] 0.4032414\n\n\n\n\nPosterior schatting en hypothese testen\nWe kunnen nu de gecombineerde 20.000 Markovketenwaarden gebruiken, met vertrouwen, om het Beta(18, 92) posterior model te benaderen van \\(\\pi\\). Het figuur hieronder bevestigt inderdaad dat de volledige MCMC-benadering (rechts) de werkelijke posterior (links) dicht benadert.\n\n# De actuele Beta(18, 92) posterior\nplot_beta(alpha = 18, beta = 92) + \n  lims(x = c(0, 0.35))\n\n\n\n# MCMC posterior benadering\nmcmc_dens(art_sim, pars = \"pi\") + \n  lims(x = c(0,0.35))\n\nScale for 'x' is already present. Adding another scale for 'x', which will\nreplace the existing scale.\n\n\n\n\n\nAls zodanig kunnen wij elk kenmerk van het Beta(18, 92) posterior model benaderen door het overeenkomstige kenmerk van de Markov-keten. Wij kunnen bijvoorbeeld het posterieure gemiddelde benaderen door het gemiddelde van de MCMC-steekproefwaarden, of het 2,5-percentiel posterior benaderen door het 2,5-percentiel van de MCMC-steekproefwaarden. Hiertoe levert de tidy() functie in het broom.mixed pakket (Bolker en Robinson 2021. Broom.mixed: Tidying Methods for Mixed Models)[https://github.com/bbolker/broom.mixed] een aantal handige statistieken voor de gecombineerde 20.000 Markov chain waarden die zijn opgeslagen in art_sim:\n\ntidy(art_sim, conf.int = TRUE, conf.level = 0.95)\n\n# A tibble: 1 x 5\n  term  estimate std.error conf.low conf.high\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 pi       0.161    0.0352    0.100     0.239\n\n\nEn de mcmc_areas()-functie in het bayesplot-pakket biedt een visuele aanvulling (zie de figuur hieronder)\n\n# Kleur in het midden het 95% interval\nmcmc_areas(art_sim, pars = \"pi\", prob = 0.95)\n\n\n\n\nIn de tidy() samenvatting geven conf.low en conf.high de 2,5e en 97,5e percentielen van de Markov-ketenwaarden aan, respectievelijk 0,101 en 0,239. Deze vormen bij benadering het midden 95%- geloofwaardigheidsinterval voor \\(\\pi\\) dat wordt weergegeven door het gearceerde gebied in de mcmc_areas()-plot. Verder meldt de schatting dat de mediaan van onze 20.000 Markov chain-waarden, en dus onze benadering van de werkelijke posterieure mediaan, 0,162 is. Deze mediaan wordt weergegeven door de verticale lijn in de mcmc_areas()-plot. Net als het gemiddelde en de modus geeft de mediaan een andere maat voor een “typische” posterior \\(\\pi\\) waarde. Hij komt overeen met het 50ste posterior percentiel - 50% van posterior \\(\\pi\\) waarden liggen boven de mediaan en 50% liggen eronder. Maar in tegenstelling tot het gemiddelde en de modus, bestaat er geen eenduidige formule voor een Beta(\\(\\alpha, \\beta\\))-mediaan. Dit legt nog meer moois bloot van MCMC simulatie: zelfs als een formule ongrijpbaar is, kunnen we een posterior eenheid schatten door de overeenkomstige eigenschap van onze waargenomen Markov chain steekproefwaarden.\nHoewel het een mooie eerste stop is, geeft de tidy() functie niet altijd elke samenvattende statistiek die van belang is. Hij rapporteert bijvoorbeeld niet het gemiddelde of de modus van onze Markov chain steekproefwaarden. Geen probleem. We kunnen samenvattende statistieken rechtstreeks uit de Markov chain waarden berekenen. De eerste stap is het omzetten van een matrix van de vier parallelle ketens in een enkel dataframe van de gecombineerde ketens:\n\n# Sla de 4 kettingen op in 1 data frame\nart_chains_df &lt;- as.data.frame(art_sim, pars = \"lp__\", include = FALSE)\ndim(art_chains_df)\n\n[1] 20000     1\n\n\nMet de ketens in dataframe-vorm kunnen wij op de gebruikelijke manier te werk gaan en onze dplyr gereedschappen gebruiken om een en ander te transformeren en samen te vatten. Wij kunnen bijvoorbeeld direct het steekproefgemiddelde, de mediaan, de modus en de kwantielen van de gecombineerde Markov-ketenwaarden berekenen. De mediaan- en kwantielwaarden zijn precies die welke door tidy() hierboven worden gerapporteerd, en elimineren dus elk mysterie over die functie!\n\n# Bereken de posterior samenvattingen van pi\nart_chains_df %&gt;% \n  summarize(post_mean = mean(pi), \n            post_median = median(pi),\n            post_mode = sample_mode(pi),\n            lower_95 = quantile(pi, 0.025),\n            upper_95 = quantile(pi, 0.975))\n\n  post_mean post_median post_mode  lower_95  upper_95\n1 0.1635785   0.1614223 0.1549176 0.1004316 0.2389237\n\n\nWij kunnen de ruwe kettingwaarden ook gebruiken om de volgende taak in onze posterior analyse aan te pakken - het testen van de bewering dat minder dan 20% van de grote museumkunstenaars Gen X zijn. Daartoe kunnen wij de posterior waarschijnlijkheid van dit scenario benaderen, \\(P(\\pi&lt;0.20|Y=14)\\), door het aandeel Markovketen \\(\\pi\\)πwaarden die onder 0,20 vallen. Volgens deze benadering is er een kans van 84,6% dat de vertegenwoordiging van Gen X-artiesten onder 0,20 ligt:\n\n# Zet de pi waarden in een tabel die onder de 0.20 zitten\nart_chains_df %&gt;% \n  mutate(exceeds = pi &lt; 0.20) %&gt;% \n  tabyl(exceeds)\n\n exceeds     n percent\n   FALSE  3043 0.15215\n    TRUE 16957 0.84785\n\n\nLaat het op je inwerken en onthoud het punt. We hebben onze MCMC simulatie gebruikt om het posterior model van \\(\\pi\\) te benaderen samen met de eigenschappen van belang. Ter vergelijking, de tabel hieronder toont de Beta(18,92) posterior kenmerken die we eerder hebben berekend naast hun overeenkomstige MCMC-benaderingen. De clou is dit: MCMC werkte. De benaderingen zijn vrij nauwkeurig. Laat dit u geruststellen - ook als modellen te ingewikkeld zijn om te specificeren, kunnen we vertrouwen hebben in onze MCMC-benaderingen van deze modellen (zolang de diagnostiek maar klopt!).\n\n\n\n\ngemiddelde\nmodus\n2.5\n97.5\n\n\n\n\nposterior\n0.16\n0.16\n0.1\n0.24\n\n\nMCMC\n0.1642\n0.1598\n0.1011\n0.2388\n\n\n\n\n\nPosterior voorspelling\nTenslotte kunnen wij onze Markov-ketenwaarden gebruiken om het posterior voorspellingsmodel van \\(Y^{'}\\) het aantal van de volgende 20 artiesten uit de steekproef dat Gen X of jonger zal zijn. Bonus: het simuleren van dit model helpt ons ook intuïtie op te bouwen voor de theorie die ten grondslag ligt aan de posterior voorspelling. Herinner je dat het posterior voorspellingsmodel twee bronnen van variabiliteit weerspiegelt:\n\nSteekproefvariabiliteit in de gegevens\n\\(Y^{'}\\) kan een willekeurig aantal kunstenaars in {0,1,…,20} en hangt af van het onderliggende aandeel van kunstenaars die Gen X zijn, \\(\\pi:Y^{'}|\\pi\\sim Bin(20,\\pi)\\).\n\nPosterieure variabiliteit in \\(\\pi\\)π De verzameling van 20.000 Markovketens \\(\\pi\\) waarden geeft bij benadering een idee van de variabiliteit en het bereik in plausibele \\(\\pi\\)waarden.\n\nOm beide bronnen van variabiliteit in posterieure voorspellingen te vatten \\(Y^{'}\\) kunnen we rbinom() gebruiken om één \\(Bin(20,\\pi\\) uitkomst \\(Y^{'}\\) van elk van de 20.000\\(\\pi\\) ketenwaarden. De eerste drie resultaten weerspiegelen een algemene trend: kleinere waarden van \\(\\pi\\) zullen meestal kleinere waarden van \\(Y^{'}\\) geven. Dit is logisch. Hoe lager de onderliggende vertegenwoordiging van Gen X-kunstenaars in het museum, hoe minder Gen X-kunstenaars we mogen verwachten in onze volgende steekproef van 20 kunstwerken.\n\n# Vastzetten zodat het dezelfde waarden blijft geven \nset.seed(1)\n\n# Voorspel een waarde van Y' voor elke pi waarde in de ketting\nart_chains_df &lt;- art_chains_df %&gt;% \n  mutate(y_predict = rbinom(length(pi), size = 20, prob = pi))\n\n# Check het vervolgens\nart_chains_df %&gt;% \n  head(3)\n\n         pi y_predict\n1 0.1300704         2\n2 0.1755149         3\n3 0.2214144         5\n\n\n\n# Plot de 20,000 voorspellingen\nggplot(art_chains_df, aes(x = y_predict)) + \n  stat_count()\n\n\n\n\n\nart_chains_df %&gt;% \n  summarize(mean = mean(y_predict),\n            lower_80 = quantile(y_predict, 0.1),\n            upper_80 = quantile(y_predict, 0.9))\n\n   mean lower_80 upper_80\n1 3.274        1        6"
  },
  {
    "objectID": "posts/2022-03-29-wat-kun-je-met-bayes/wat-kun-je-met-bayes.html#bayesiaanse-voordelen",
    "href": "posts/2022-03-29-wat-kun-je-met-bayes/wat-kun-je-met-bayes.html#bayesiaanse-voordelen",
    "title": "Wat kun je met Bayes?",
    "section": "Bayesiaanse voordelen",
    "text": "Bayesiaanse voordelen\nZoals je waarschijnlijk merkte, is het moeilijkste deel van een Bayesiaanse analyse vaak het bouwen of simuleren van het posterior model. Zodra je dat stuk op zijn plaats hebt, is het vrij eenvoudig om deze posterior te gebruiken voor schatting, hypothese toetsing en voorspelling. Daarentegen is het opbouwen van de formules om de analoge frequentistische berekeningen uit te voeren vaak minder intuïtief.\nWe kunnen ons ook koesteren in het gemak waarmee Bayesiaanse resultaten kunnen worden geïnterpreteerd. In het algemeen beoordeelt een Bayesiaanse analyse de onzekerheid over een onbekende parameter \\(\\pi\\) in het licht van de waargenomen gegevens \\(Y\\). Neem bijvoorbeeld de studie van de kunstenaars zoals hierboven gepresenteerd. In het licht van de waarneming dat \\(Y=14\\) van de 100 kunstenaars in de steekproef Gen X of jonger waren, stelden we vast dat er een 84,9% posterior kans was dat Gen X vertegenwoordigd was in het hele museum,\n\\(π\\) lager is dan 0,20:\n\\[P(\\pi&lt;0.20|Y=14)=0.849\\]\nDeze berekening heeft geen zin in een frequentistische analyse. Omgekeerd beoordeelt een frequentistische analyse de onzekerheid van de waargenomen gegevens \\(Y\\) in het licht van veronderstelde waarden van \\(π\\). De frequentistische tegenhanger van de Bayesiaanse posterior waarschijnlijkheid hierboven is bijvoorbeeld de p-waarde, waarvan we de formule hier niet zullen behandelen:\n\\[PY(\\leq14|\\pi=0.20)=0.08\\] De omgekeerde volgorde van de conditionering in deze waarschijnlijkheid, \\(Y\\) gegeven \\(π\\) (\\(Y|\\pi\\)) in plaats van \\(π\\) gegeven \\(Y\\) (\\(\\pi|Y)\\)) leidt tot een andere berekening en interpretatie dan de Bayesiaanse waarschijnlijkheid: als \\(π\\) slechts 0,20 zou zijn, dan is er slechts een kans van 8% dat we een steekproef zouden hebben waargenomen waarin ten hoogste \\(Y=14\\) van de 100 kunstenaars Gen X waren. Het is niet onze manier van schrijven die onhandig is, het is de p-waarde. Hoewel het ons interessante informatie verschaft, is de vraag die het beantwoordt een beetje minder natuurlijk voor het menselijk brein: aangezien we de gegevens eigenlijk hebben geobserveerd, maar \\(\\pi\\) niet weten, kan het een breinbreker zijn om een berekening te interpreteren die het tegendeel veronderstelt. Voornamelijk bij het testen van hypothesen is het natuurlijker om te vragen “hoe waarschijnlijk is mijn hypothese?” (wat de Bayesiaanse waarschijnlijkheid antwoordt) dan “hoe waarschijnlijk zijn mijn data als mijn hypothese niet waar zou zijn?” (wat de frequentistische waarschijnlijkheid antwoordt). Aangezien p-waarden zo vaak verkeerd worden geïnterpreteerd, en dus verkeerd worden gebruikt, worden zij in het hele frequentistische en Bayesiaanse spectrum steeds minder benadrukt.\nDeze blog leert je hoe je een posterior model in antwoorden kunt omzetten. Dat wil zeggen dat je gebruik maakte van posterior modellen, exact of bij benadering, om drie posterior analyse taken uit te voeren voor een onbekende parameter \\(\\pi\\):\n\nPosterior schatting\nEen posterior geloofwaardigheidsinterval (CI) geeft een reeks van posterior aannemelijke waarden van \\(\\pi\\) en dus een idee van zowel de posterior typische waarden als de onzekerheid in \\(\\pi\\).\n\nPosterior toetsing van hypothesen Posterior waarschijnlijkheden geven inzicht in overeenkomstige hypothesen betreffende \\(\\pi\\).\n\nPosterior voorspelling\nHet posterior voorspellingsmodel voor een nieuw gegevenspunt \\(Y\\) houdt rekening met zowel de steekproefvariabiliteit in \\(Y\\) en de posterior variabiliteit in \\(\\pi\\)."
  },
  {
    "objectID": "posts/2022-05-05-kaarten-maken-met-r/kaarten-maken-met-r.html#inleiding",
    "href": "posts/2022-05-05-kaarten-maken-met-r/kaarten-maken-met-r.html#inleiding",
    "title": "Kaarten maken met R",
    "section": "Inleiding",
    "text": "Inleiding\nHieronder een korte handleidingen die je leert om met R kaarten te maken. Euginio Petrovich schreef: Drawing maps with R. A basic tutorial in 2020. Hij laat je zien hoe je een kaart van Europa maakt met vooral de pakketten sfen `ggplot, hoe je geografische data combineert met een eenvoudige dataset en hoe je vervolgens deze kaart met deze gegevens verfijnt.Inderdaad een basishandleiding. Dank je Euginio."
  },
  {
    "objectID": "posts/2022-05-05-kaarten-maken-met-r/kaarten-maken-met-r.html#met-r-kaarten-tekenen.-een-basishandleiding",
    "href": "posts/2022-05-05-kaarten-maken-met-r/kaarten-maken-met-r.html#met-r-kaarten-tekenen.-een-basishandleiding",
    "title": "Kaarten maken met R",
    "section": "Met R kaarten tekenen. Een basishandleiding",
    "text": "Met R kaarten tekenen. Een basishandleiding\nMet kaarten kun je op een krachtige wijze informatie visualiseren. Het plotten van gegevens op een kaart kan trends en patronen aan het licht brengen die moeilijk te zien zijn door alleen een spreadsheet te onderzoeken. Kaarten zijn ook zeer nuttig om informatie op een aantrekkelijke en makkelijkere manier over te brengen aan het publiek.\nIn deze korte handleiding leren we hoe we eenvoudige geografische kaarten kunnen genereren met R. In het bijzonder zullen we leren hoe we de volgende kaart van de DR2 leden in Europa kunnen maken:\n\nAan de slag\nR is een gratis en open-source software die vele oplossingen biedt voor het berekenen van gegevens en het produceren van visualisaties. Een groot voordeel van R is dat de basisfunctionaliteiten kunnen worden uitgebreid met andere pakketten die vrij beschikbaar zijn op CRAN, het Comprehensive R Archive Network. Bovendien is er een actieve R-gemeenschap over de hele wereld die de meeste codeervragen beantwoordt die je kunt hebben.\nDe pakketten die nodig zijn voor deze handleiding kunnen worden geïnstalleerd met:\n\n# install.packages(c(\"sf\", \"rnaturalearth\" , \"rnaturalearthdata\", \"rgeos\", \"ggspatial\", \"ggrepel\", \"tidyverse\"))\n\n# Ik heb een hekje geplaatst omdat ik deze pakketten al had binnengehaald. Heb je dat nog niet gedaan, dan moet je het hekje weghalen.\n\nDe eerste vijf pakketten zijn specifiek ontwikkeld voor kaarten: sf wordt gebruikt om ruimtelijke gegevens te beheren, rnaturalearth en rnaturalearthdata bevatten informatie over alle landen van de wereld, alsook informatie die nodig is om die landen op een kaart uit te zetten, en ggspatial verbetert de visualisatie van ruimtelijke gegevens. ggrepel helpt ons bij het beheer van de labels op de kaart, terwijl tidyverse een set R-bibliotheken omvat die de standaard zijn geworden voor gegevensmanipulatie en -visualisatie.\nNa de installatie van de pakketten, moeten we ze laden:\n\nlibrary(\"sf\")\n\nWarning: package 'sf' was built under R version 4.1.3\n\n\nLinking to GEOS 3.9.1, GDAL 3.2.1, PROJ 7.2.1; sf_use_s2() is TRUE\n\nlibrary(\"rnaturalearth\")\n\nWarning: package 'rnaturalearth' was built under R version 4.1.3\n\nlibrary(\"rnaturalearthdata\")\n\nWarning: package 'rnaturalearthdata' was built under R version 4.1.3\n\nlibrary(\"rgeos\")\n\nWarning: package 'rgeos' was built under R version 4.1.3\n\n\nLoading required package: sp\n\n\nWarning: package 'sp' was built under R version 4.1.3\n\n\nrgeos version: 0.5-9, (SVN revision 684)\n GEOS runtime version: 3.9.1-CAPI-1.14.2 \n Please note that rgeos will be retired by the end of 2023,\nplan transition to sf functions using GEOS at your earliest convenience.\n GEOS using OverlayNG\n Linking to sp version: 1.4-6 \n Polygon checking: TRUE \n\nlibrary(\"ggspatial\")\n\nWarning: package 'ggspatial' was built under R version 4.1.3\n\nlibrary(\"ggrepel\")\n\nLoading required package: ggplot2\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\nlibrary(\"tidyverse\")\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv tibble  3.1.7     v dplyr   1.0.9\nv tidyr   1.2.0     v stringr 1.4.1\nv readr   2.1.2     v forcats 0.5.1\nv purrr   0.3.4     \n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'stringr' was built under R version 4.1.3\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\n\nAlvorens de kaarten te maken, moeten we de geografische gegevens in R importeren. Wij hebben ze opgeslagen in DR2_data, het volgende dataframe.\nZoals je kunt zien, zijn steden de basiseenheid van dit dataframe. Voor elk van hen hebben we het land opgegeven, het aantal leden (Members), het label dat we op de kaart zullen weergeven (het bestaat uit de naam van de stad plus het aantal leden tussen haakjes), en de breedtegraad en lengtegraad.\nWe importeren het dataframe, dat is opgeslagen in een CSV bestand, in R met de functie read.csv. Aangezien we een header met de namen van de kolommen hebben gebruikt, zetten we het argument header op TRUE. We moeten ook specificeren dat het scheidingsteken tussen de kolommen de puntkomma is en dat het decimaal scheidingsteken de komma is (en niet de punt, omdat er een Italiaanse versie van Excel is gebruikt om het bestand te produceren).\n\nDR2_data &lt;- read.csv(file=\"Dr2.CSV\",\n    header=TRUE, \n    sep=\";\", \n    dec = \",\")\n\nJe kunt de eerste records van het dataframe controleren met het commando head(DR2_data).\n\nhead(DR2_data)\n\n  ï..ID      City     Country Members         Label      Lat      Lng\n1     1     Turin       Italy      14    Turin (14) 45.07049  7.68682\n2     2     Siena       Italy       2     Siena (2) 43.31822 11.33064\n3     3      Pisa       Italy       1      Pisa (1) 43.70853 10.40360\n4     4  Florence       Italy       1  Florence (1) 43.77925 11.24626\n5     5 Barcelona       Spain       1 Barcelona (1) 41.38879  2.15899\n6     6 Amsterdam Netherlands       1 Amsterdam (1) 52.37403  4.88969\n\n\nWe zijn nu klaar om onze kaart te maken.\n\n\nDe wereldkaart maken\nDe eerste stap van onze kaartoefening is het creëren van een wereldkaart. Om dit te doen, gebruiken we de functie ne_countries om landengegevens op te halen uit rnaturalearth. We specificeren medium als scale en sf als returnclass van het dataframe, zodat de gegevens al in het juiste formaat zijn voor geografische kaarten maken.\n\nworld &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\n\nWij plotten deze gegevens met ggplot2, het tidyversepakket voor visualisatie, en sf:\n\nggplot(data = world) +\n       geom_sf()\n\n\n\n\nWij zullen de wereldkaart gebruiken als basiskaart waarop wij de landen waar DR2-leden gevestigd zijn, zullen markeren.\nOm de DR2-landen op de kaart te markeren, moeten wij nu onze DR2-gegevens “toevoegen” aan het dataframe van de wereld. Wij doen dit met de functie left.join.\n\nworld_joined &lt;- left_join(world, DR2_data, by = c(\"name\" = \"Country\"))\n\nDeze functie vertelt R dat het de DR2-data moet verbinden met de worlddata door te zoeken naar een overeenkomst op de naam van het land (we specificeren de overeenkomstige sleutel tussen de twee gegevenssets in het by argument). Wanneer een overeenkomst wordt gevonden, worden de records uit de twee tabellen gecombineerd. Wanneer geen overeenkomst wordt gevonden, zoals in het geval van Brazilië, wordt de waarde van de DR2-kolommen (bv. “Leden”) van de niet-overeenkomende records op NA gezet, de standaardcode die door R wordt gebruikt voor ontbrekende waarden. Zo zal de record Brazilië NA krijgen als waarde van de kolom “Members”. Het is belangrijk om alle landen in de wereld te behouden en niet alleen die met DR2-leden. Anders zullen wij, wanneer wij onze gegevens op de kaart uitzetten, alle landen zonder DR2 verliezen! Daarom hebben we de left.join gebruikt in plaats van de simple join: we willen dat R alle records in de “linker” dataset behoudt (d.w.z. degene die het eerste argument in de functie bevat).\nWij willen nu de landen met DR2 leden op de wereldkaart markeren. Om dit te doen gebruiken we een if...else in het fill argument. Indien de waarde van de kolom “Members” null is (d.w.z. gelijk aan NA), stellen wij de kleur van het land in op grijs. Indien de waarde niet nul is, d.w.z. indien er DR2 leden zijn in dat land, stellen we de kleur in op rood. Merk op dat we in het eerste geval de kleurnaam hebben gebruikt, terwijl we in het tweede geval de hexadecimale kleurcode hebben gebruikt die overeenkomt met de kleur van het DR2-logo. Het argument kleur specificeert de kleur van de grenzen van de landen.\n\n DR2_countries_map &lt;- ggplot(data =  world_joined)+\n    geom_sf(fill = ifelse(is.na(world_joined$Members), \"lightgrey\", \"#c8242b\"), \n    color = \"black\")\n\nDe Europese landen waar DR2-leden gevestigd zijn, zijn te klein om op een wereldkaart op te vallen. De wereldschaal is dus niet erg effectief om de geografische spreiding van DR2 weer te geven. We moeten inzoomen op het niveau van Europa. Een zeer nuttig kenmerk van het sf-pakket is dat dit zeer gemakkelijk kan worden gedaan door een reeks coördinaten op te geven van het gebied waarin wij geïnteresseerd zijn:\n\nDR2_countries_map +\n    coord_sf(xlim = c(-16.1, 32.88), \n        ylim = c(35, 60), \n        expand = TRUE)\n\n\n\n\n\n\nSteden markeren (puntdata)\nWij weten dat DR2-leden niet alleen in bepaalde landen gevestigd zijn, maar ook in specifieke steden binnen die landen. In ons DR2-dataset hadden we de DR2-steden samen met hun geografische coördinaten. We willen deze steden nu als punten op onze kaart plaatsen.\nWe moeten eerst ons dataframe converteren naar een sf object:\n\nsf_DR2_cities &lt;- st_as_sf(DR2_data, \n    coords = c(\"Lng\", \"Lat\"), \n    remove = FALSE, \n    crs = 4326, \n    agr = \"constant\")\n\nMerk op dat we de kolommen moesten aanduiden waarin de geografische coördinaten van onze steden zijn opgeslagen, evenals andere parameters zoals de gebruikte geografische projectie (hier WGS84, wat de CRS-code #4326 is).\nWe kunnen nu de punten van de steden op de kaart plotten:\n\nDR2_countries_map +\n    geom_sf(data = sf_DR2_cities)\n\n\n\n\nOm ze duidelijk te zien, laten we inzoomen op Europa, zoals we eerder hebben geleerd:\n\nDR2_countries_map +\n    geom_sf(data = sf_DR2_cities) +\n    coord_sf(xlim = c(-16.1, 32.88), \n        ylim = c(35, 60), \n        expand = TRUE)\n\n\n\n\n\n\nLabels toevoegen\nOm de interpretatie van onze kaart te vergemakkelijken, is het zeer nuttig om enkele labels toe te voegen. Wij willen bijvoorbeeld weten hoeveel DR2-leden gevestigd zijn in de steden die wij eerder hebben aangegeven. We hebben de tekst van de labels al in de kolom “Label” van het DR2-datasetje. Nu moeten we deze tekst op de kaart visualiseren. We doen dit door gebruik te maken van de functie geom_label_repel. Deze functie, die is opgenomen in het pakket ggrepel, verbetert de positionering van labels op een plot: ze stoot labels van elkaar af, weg van datapunten, en weg van de randen van het plotgebied.\nIn de parameter esthetica van de functie specificeren we dat we de etiketten op de kaart willen plaatsen op basis van de breedte- en lengtegraad van de steden, en dat hun tekst wordt aangegeven in de kolom “Label”. De andere parameters specificeren de kleur van de labels, de grootte van de tekst, en de hoeveelheid “afstotingskracht” van het positioneringsalgoritme.\n\nDR2_countries_map +\n    geom_sf(data = sf_DR2_cities) +\n    geom_label_repel(data = sf_DR2_cities, \n        aes(x = Lng, y = Lat, label = Label), \n        color = \"black\", \n        fontface = \"bold\", \n        size = 3, \n        force = 5)\n\nWarning: ggrepel: 9 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\nEr is echter een probleem. Als we inzoomen op Europa, vinden we een “indringer”: het label “Montreal (1)” zou niet mogen verschijnen op de Europese kaart!\nOm dit probleempje op te lossen, moeten we de steden in landen buiten Europa uitfilteren. We maken dus een deelverzameling van het DR2-datasetje en specificeren dat we alle records willen behouden waarvan het land niet ( != ) Canada is:\n\nDR2_european_cities &lt;- subset(DR2_data, \n            Country != \"Canada\", \n            select = City:Lng)\n## Convert to the sf format\nsf_DR2_european_cities &lt;- st_as_sf(DR2_european_cities, \n    coords = c(\"Lng\", \"Lat\"), \n    remove = FALSE, \n    crs = 4326, \n    agr = \"constant\")\n\nAls we de nieuwe dataset op de Europese kaart uitzetten, ontdekken we dat de indringer is verwijderd:\n\nDR2_countries_map +\n    geom_sf(data = sf_DR2_european_cities) +\n    geom_label_repel(data = sf_DR2_european_cities, \n        aes(x = Lng, y = Lat, label = Label), \n        color = \"black\", \n        fontface = \"bold\",  \n        size = 3, \n        force = 5)+\n    coord_sf(xlim = c(-16.1, 32.88), \n        ylim = c(35, 60), \n        expand = TRUE)\n\n\n\n\nHet is duidelijk dat er redenen kunnen zijn om het label Montreal te behouden: bijvoorbeeld om aan te tonen dat DR2 ook overzeese leden heeft.\n\n\nVerbetering van de kaart\nIn de tot nu toe gegenereerde versies van de kaart wordt de informatie over het aantal leden van DR2 in de labels weergegeven, als een getal tussen haakjes. Is het mogelijk om dit als een visueel kenmerk weer te geven, zodat het meteen in het oog springt? Een eerste idee zou kunnen zijn om de grootte van de labels evenredig met het aantal leden te veranderen:\n\nDR2_countries_map +\n    geom_sf(data = sf_DR2_european_cities) +\n    geom_label_repel(data = sf_DR2_european_cities, \n        aes(x = Lng, y = Lat, label = Label, size = Members), \n        color = \"black\", \n        fontface = \"bold\",  \n        force = 5)+\n    coord_sf(xlim = c(-16.1, 32.88), \n        ylim = c(35, 60), \n        expand = TRUE)\n\n\n\n\nHet resultaat is echter vrij slecht, vanwege het grote verschil in grootte tussen Turijn en de andere steden. Aangezien de meeste steden slechts één lid hebben, zijn hun labels te klein om leesbaar te zijn. Merk op dat R automatisch een legende toevoegt om de grootte van de labels te interpreteren.\nEen betere oplossing is om de grootte van de stadspunten evenredig te laten zijn met het aantal leden:\n\nDR2_countries_map +\n    geom_sf(data = sf_DR2_european_cities, \n        aes(size = Members))+\n    geom_label_repel(data = sf_DR2_european_cities, \n        aes(x = Lng, y = Lat, label = Label), \n        color = \"black\", \n        fontface = \"bold\",\n        size = 3,   \n        force = 9)+\n    coord_sf(xlim = c(-16.1, 32.88), \n        ylim = c(35, 60), \n        expand = TRUE)\n\n\n\n\nMerk op dat R automatisch een legende creëert op basis van de grootte van de punten:\nOp dezelfde manier kunnen we ook de kleur van de punten gebruiken om het aantal leden weer te geven. We passen de kleurenschaal aan door het uiterste in te stellen op blauw en groen, zodat de grote steden in blauw en de kleine steden in groen worden gekleurd:\n\nDR2_countries_map +\n    geom_sf(data = sf_DR2_european_cities, \n        aes(color = Members, size = Members))+\n    scale_color_gradient(low = \"blue\", high = \"green\")+\n    geom_label_repel(data = sf_DR2_european_cities, \n        aes(x = Lng, y = Lat, label = Label), \n        color = \"black\", \n        fontface = \"bold\",\n        size = 3,   \n        force = 9)+\n    coord_sf(xlim = c(-16.1, 32.88), \n        ylim = c(35, 60), \n        expand = TRUE)\n\n\n\n\nMerk op dat R een tweede legende toevoegt om de kleur van de punten te interpreteren:\nDe laatste kaart lijkt me echter “overbelast”. Dezelfde informatie (de DR2 leden) wordt op drie verschillende manieren gevisualiseerd: met een getal in het label, met de grootte van de punten, en met de kleur van de punten. Persoonlijk vind ik deze oplossing overbodig. Ik denk dat de tweede kaart het meest evenwichtig (en esthetisch het meest aangenaam) is.\n\n\nLaatste accenten\nEen groot voordeel van ggplot2 is dat het toelaat om bijna alle grafische aspecten van de visualisaties te controleren. Door de parameters in de thema-functie te wijzigen, kunnen we onze kaart verfijnen tot ze aan onze smaak beantwoordt. Om de uiteindelijke versie van de kaart te realiseren, veranderen we de kleur van de achtergrond van de kaart (dat is de oceaan) in lichtblauw, we verwijderen de titels van de assen, de teksten en de vinkjes en de legenda. Tenslotte voegen we een titel toe aan onze kaart.\n\nEuropean_DR2_map2 &lt;- DR2_countries_map +\n    geom_sf(data = sf_DR2_european_cities, \n        aes(size = Members))+\n    geom_label_repel(data = sf_DR2_european_cities, \n        aes(x = Lng, y = Lat, label = Label), \n        color = \"black\", \n        fontface = \"bold\",\n        size = 3,   \n        force = 12)+\n    coord_sf(xlim = c(-16.1, 32.88), \n        ylim = c(35, 60), \n        expand = TRUE)+\n    theme_minimal() +\n    theme(panel.background = element_rect(fill = \"aliceblue\"), \n        axis.title.x = element_blank(), \n        axis.title.y = element_blank(), \n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks = element_blank(),\n        legend.position = \"none\") +\n    ggtitle(\"Map of DR2 Members in Europe\")\n\nDe laatste stap is het opslaan van de kaart in een geschikt formaat. We slaan zowel een PDF-versie van de kaart op, die de hoogste kwaliteit behoudt, als een lichtere PNG-versie:\n\nggsave(\"DR2_map_Europe.pdf\")\n\nSaving 7 x 5 in image\n\nggsave(\"DR2_map_Europe.png\", dpi = \"screen\")\n\nSaving 7 x 5 in image\n\n\n\n\n\n\nZo ziet het eruit\n\n\n\n\n\nVerder lezen\nDeze korte handleiding is grotendeels geïnspireerd door de tutorial die Euginio gebruikte om de basis van mapping met R te leren. Hij legt heel duidelijk verschillende andere onderwerpen uit die met maps te maken hebben en hij raadt deze zeker aan. Een andere nuttige tutorial is voor hem deze, die uitlegt hoe je een ander R-pakket voor kaarten, ggmap, gebruikt en een aantal veelvoorkomende data wrangling operaties."
  },
  {
    "objectID": "posts/2022-05-08-missende-waarden/missende-waarden.html#data-imputeren-met-r-het-mice-pakket",
    "href": "posts/2022-05-08-missende-waarden/missende-waarden.html#data-imputeren-met-r-het-mice-pakket",
    "title": "Missende waarden",
    "section": "Data imputeren met R; het MICE pakket",
    "text": "Data imputeren met R; het MICE pakket"
  },
  {
    "objectID": "posts/2022-05-08-missende-waarden/missende-waarden.html#inleiding",
    "href": "posts/2022-05-08-missende-waarden/missende-waarden.html#inleiding",
    "title": "Missende waarden",
    "section": "Inleiding",
    "text": "Inleiding\nOnlangs gaf Stef van Buuren een verhelderende introductie op het pakket mice dat hij de laatste jaren ontwikkeld heeft en dat een standaardpakket geworden is om missende data om te gaan. Zijn presentatie is hier te vinden slides. Hij verwees tijdens de presentatie naar het artikel dat hij hierover eerder schreef (Van Buuren, and Groothuis-Oudshoorn, 2011)[hier](mice: Multivariate Imputation by Chained Equations in R by Stef van Buuren, het boek dat hij hierover maakte hier en ook verwees hij naar een korte Nederlandse talige introductie hier. Na zijn presentatie las ik via Rbloggers de korte post van Michy Allice hier.Voor deze blog heb ik dat artile van Allice bewerkt. Tot slot las ik ook nog het boek van Heymans en Eekhout over dit onderwerp hier dat ook goed is."
  },
  {
    "objectID": "posts/2022-05-08-missende-waarden/missende-waarden.html#het-mice-pakket",
    "href": "posts/2022-05-08-missende-waarden/missende-waarden.html#het-mice-pakket",
    "title": "Missende waarden",
    "section": "Het mice-pakket",
    "text": "Het mice-pakket\nOntbrekende gegevens zijn niet zo’n triviaal probleem bij de analyse van een dataset. Het is meestal ook niet zo eenvoudig om er rekening mee te houden.\nAls de hoeveelheid ontbrekende gegevens zeer klein is in verhouding tot de grootte van de dataset, dan kan het weglaten van de weinige data met ontbrekende kenmerken de beste strategie zijn om de analyse niet te vertekenen,. Met hety weglaten van beschikbare datapunten verdwijnt een bepaalde hoeveelheid informatie. Afhankelijk van de situatie waarmee u te maken hebt, kunt u op zoek gaan naar andere oplossingen voordat u potentieel nuttige datapunten uit uw dataset verwijdert.\nHoewel sommige snelle oplossingen zoals het vervangen door het gemiddelde in sommige gevallen goed kunnen zijn, heb je met zulke eenvoudige benaderingen gewoonlijk bias in de data. Het toepassen van gemiddelde-substitutie laat het gemiddelde onveranderd (wat wenselijk is), maar vermindert de variantie, wat onwenselijk kan zijn.\n\nHet micepakket in R helpt bij het imputeren van ontbrekende waarden met plausibele gegevenswaarden. Deze plausibele waarden worden getrokken uit een distributie die speciaal ontworpen is voor elk ontbrekend datapunt.\n\nHieronder gaan we ontbrekende waarden imputeren met behulp van de dataset airquality (standaard beschikbaar in R). Voor dit blog verwijder ik een aantal datapunten uit de dataset.\n\nlibrary(mice)\n\nWarning: package 'mice' was built under R version 4.1.3\n\n\n\nAttaching package: 'mice'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following objects are masked from 'package:base':\n\n    cbind, rbind\n\ndata &lt;- airquality\ndata[4:10,3] &lt;- rep(NA,7)\ndata[1:5,4] &lt;- NA\nsummary(data)\n\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :57.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:73.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.806   Mean   :78.28  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7       NA's   :7        NA's   :5      \n     Month            Day      \n Min.   :5.000   Min.   : 1.0  \n 1st Qu.:6.000   1st Qu.: 8.0  \n Median :7.000   Median :16.0  \n Mean   :6.993   Mean   :15.8  \n 3rd Qu.:8.000   3rd Qu.:23.0  \n Max.   :9.000   Max.   :31.0  \n                               \n\n\nWat categorische variabelen betreft, is het vervangen van categorische variabelen gewoonlijk niet aan te bevelen. Het is wel gebruikelijk om ontbrekende categorische variabelen te vervangen door de modus van de waargenomen variabelen, maar het is de vraag of dat een goede keuze is. Ook al ontbreken er in dit geval geen datapunten van de categorische variabelen (Month, Day), we verwijderen ze uit onze dataset (we kunnen ze later weer toevoegen als dat nodig is) en bekijken de gegevens met summary().\n\ndata &lt;- data[-c(5,6)]\nsummary(data)\n\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :57.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:73.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.806   Mean   :78.28  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7       NA's   :7        NA's   :5      \n\n\nOzon is blijkbaar de variabele met de meeste ontbrekende datapunten. Hieronder gaan we dieper in op de ontbrekende datapatronen."
  },
  {
    "objectID": "posts/2022-05-08-missende-waarden/missende-waarden.html#snelle-classificatie-van-ontbrekende-gegevens",
    "href": "posts/2022-05-08-missende-waarden/missende-waarden.html#snelle-classificatie-van-ontbrekende-gegevens",
    "title": "Missende waarden",
    "section": "Snelle classificatie van ontbrekende gegevens",
    "text": "Snelle classificatie van ontbrekende gegevens\nEr zijn twee soorten ontbrekende gegevens:\n\nMCAR: volledig willekeurig ontbrekend. Dit is het wenselijke scenario in geval van ontbrekende data.\n\nMNAR: missing not at random. Niet-willekeurig ontbrekende gegevens zijn een ernstiger probleem en in dit geval kan het verstandig zijn het proces van gegevensverzameling verder te controleren en te proberen te begrijpen waarom de informatie ontbreekt. Als bijvoorbeeld de meeste mensen in een enquête een bepaalde vraag niet hebben beantwoord, waarom hebben zij dat dan gedaan? Was de vraag onduidelijk?\n\nErvan uitgaande dat de gegevens MCAR zijn, kan een teveel aan ontbrekende gegevens ook een probleem zijn. Gewoonlijk is een veilige maximumdrempel 5% van het totaal voor grote datasets. Als de ontbrekende gegevens voor een bepaald kenmerk of een bepaalde steekproef meer dan 5% bedragen, moet u dat kenmerk of die steekproef waarschijnlijk weglaten. Daarom controleren we op kenmerken (kolommen) en steekproeven (rijen) waar meer dan 5% van de data ontbreekt met een eenvoudige functie\n\npMiss &lt;- function(x){sum(is.na(x))/length(x)*100}\napply(data,2,pMiss)\n\n    Ozone   Solar.R      Wind      Temp \n24.183007  4.575163  4.575163  3.267974 \n\napply(data,1,pMiss)\n\n  [1]  25  25  25  50 100  50  25  25  25  50  25   0   0   0   0   0   0   0\n [19]   0   0   0   0   0   0  25  25  50   0   0   0   0  25  25  25  25  25\n [37]  25   0  25   0   0  25  25   0  25  25   0   0   0   0   0  25  25  25\n [55]  25  25  25  25  25  25  25   0   0   0  25   0   0   0   0   0   0  25\n [73]   0   0  25   0   0   0   0   0   0   0  25  25   0   0   0   0   0   0\n [91]   0   0   0   0   0  25  25  25   0   0   0  25  25   0   0   0  25   0\n[109]   0   0   0   0   0   0  25   0   0   0  25   0   0   0   0   0   0   0\n[127]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n[145]   0   0   0   0   0  25   0   0   0\n\n\nWe zien dat Ozon bijna 25% van de datapunten mist, dus we kunnen overwegen om het uit de analyse te laten of meer metingen te verzamelen. De andere variabelen blijven onder de drempel van 5%, zodat we ze kunnen behouden. Wat de data betreft, leidt het ontbreken van slechts één kenmerk tot 25% ontbrekende gegevens per dataset."
  },
  {
    "objectID": "posts/2022-05-08-missende-waarden/missende-waarden.html#gebruik-mice-voor-het-bekijken-van-ontbrekende-data-patronen",
    "href": "posts/2022-05-08-missende-waarden/missende-waarden.html#gebruik-mice-voor-het-bekijken-van-ontbrekende-data-patronen",
    "title": "Missende waarden",
    "section": "Gebruik `mice voor het bekijken van ontbrekende data patronen",
    "text": "Gebruik `mice voor het bekijken van ontbrekende data patronen\nHet mice pakket biedt een mooie functie md.pattern() om een beter inzicht te krijgen in het patroon van ontbrekende gegevens.\n\nmd.pattern(data)\n\n\n\n\n    Temp Solar.R Wind Ozone   \n104    1       1    1     1  0\n34     1       1    1     0  1\n3      1       1    0     1  1\n1      1       1    0     0  2\n4      1       0    1     1  1\n1      1       0    1     0  2\n1      1       0    0     1  2\n3      0       1    1     1  1\n1      0       1    0     1  2\n1      0       0    0     0  4\n       5       7    7    37 56\n\n\nDe output vertelt ons dat 104 gegevens compleet zijn, 34 data missen alleen de Ozonmeting, 3 data missen alleen de Solar.R waarde enzovoort.\nEen wellicht meer behulpzame visuele weergave kan worden verkregen met behulp van het VIM pakket als volgt\n\nlibrary(VIM)\n\nWarning: package 'VIM' was built under R version 4.1.3\n\n\nLoading required package: colorspace\n\n\nLoading required package: grid\n\n\nVIM is ready to use.\n\n\nSuggestions and bug-reports can be submitted at: https://github.com/statistikat/VIM/issues\n\n\n\nAttaching package: 'VIM'\n\n\nThe following object is masked from 'package:datasets':\n\n    sleep\n\naggr_plot &lt;- aggr(data, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(data), cex.axis=.7, gap=3, ylab=c(\"Histogram van missende data\",\"Patroon\"))\n\n\n\n\n\n Variables sorted by number of missings: \n Variable      Count\n    Ozone 0.24183007\n  Solar.R 0.04575163\n     Wind 0.04575163\n     Temp 0.03267974\n\n\nUit de grafiek kunnen we opmaken dat bij bijna 70% van de data geen informatie ontbreekt, bij 22% ontbreekt de ozonwaarde en bij de overige variabelen ontbreekt zo’n 5%. Door deze aanpak ziet de situatie er naar mijn mening een stuk duidelijker uit.\nEen andere (hopelijk) behulpzame visuele benadering is een speciale boxplot\n\nmarginplot(data[c(1,2)])\n\n\n\n\nUiteraard zijn we hier beperkt tot het plotten van slechts 2 variabelen tegelijk, maar desalniettemin kunnen we hier een aantal interessante inzichten uit verkrijgen. De rode boxplot links toont de verdeling van Solar.R waarbij Ozone ontbreekt, terwijl de blauwe boxplot de verdeling van de resterende datapunten toont. Hetzelfde geldt voor de boxplots van Ozone onderaan de grafiek. Als onze aanname van MCAR-gegevens juist is, dan verwachten we dat de rode en blauwe boxplots sterk op elkaar lijken."
  },
  {
    "objectID": "posts/2022-05-08-missende-waarden/missende-waarden.html#imputeren-van-missende-data",
    "href": "posts/2022-05-08-missende-waarden/missende-waarden.html#imputeren-van-missende-data",
    "title": "Missende waarden",
    "section": "Imputeren van missende data",
    "text": "Imputeren van missende data\nDe mice() functie zorgt voor het imputatieprocess\n\ntempData &lt;- mice(data,m=5,maxit=50,meth='pmm',seed=500)\n\n\n iter imp variable\n  1   1  Ozone  Solar.R  Wind  Temp\n  1   2  Ozone  Solar.R  Wind  Temp\n  1   3  Ozone  Solar.R  Wind  Temp\n  1   4  Ozone  Solar.R  Wind  Temp\n  1   5  Ozone  Solar.R  Wind  Temp\n  2   1  Ozone  Solar.R  Wind  Temp\n  2   2  Ozone  Solar.R  Wind  Temp\n  2   3  Ozone  Solar.R  Wind  Temp\n  2   4  Ozone  Solar.R  Wind  Temp\n  2   5  Ozone  Solar.R  Wind  Temp\n  3   1  Ozone  Solar.R  Wind  Temp\n  3   2  Ozone  Solar.R  Wind  Temp\n  3   3  Ozone  Solar.R  Wind  Temp\n  3   4  Ozone  Solar.R  Wind  Temp\n  3   5  Ozone  Solar.R  Wind  Temp\n  4   1  Ozone  Solar.R  Wind  Temp\n  4   2  Ozone  Solar.R  Wind  Temp\n  4   3  Ozone  Solar.R  Wind  Temp\n  4   4  Ozone  Solar.R  Wind  Temp\n  4   5  Ozone  Solar.R  Wind  Temp\n  5   1  Ozone  Solar.R  Wind  Temp\n  5   2  Ozone  Solar.R  Wind  Temp\n  5   3  Ozone  Solar.R  Wind  Temp\n  5   4  Ozone  Solar.R  Wind  Temp\n  5   5  Ozone  Solar.R  Wind  Temp\n  6   1  Ozone  Solar.R  Wind  Temp\n  6   2  Ozone  Solar.R  Wind  Temp\n  6   3  Ozone  Solar.R  Wind  Temp\n  6   4  Ozone  Solar.R  Wind  Temp\n  6   5  Ozone  Solar.R  Wind  Temp\n  7   1  Ozone  Solar.R  Wind  Temp\n  7   2  Ozone  Solar.R  Wind  Temp\n  7   3  Ozone  Solar.R  Wind  Temp\n  7   4  Ozone  Solar.R  Wind  Temp\n  7   5  Ozone  Solar.R  Wind  Temp\n  8   1  Ozone  Solar.R  Wind  Temp\n  8   2  Ozone  Solar.R  Wind  Temp\n  8   3  Ozone  Solar.R  Wind  Temp\n  8   4  Ozone  Solar.R  Wind  Temp\n  8   5  Ozone  Solar.R  Wind  Temp\n  9   1  Ozone  Solar.R  Wind  Temp\n  9   2  Ozone  Solar.R  Wind  Temp\n  9   3  Ozone  Solar.R  Wind  Temp\n  9   4  Ozone  Solar.R  Wind  Temp\n  9   5  Ozone  Solar.R  Wind  Temp\n  10   1  Ozone  Solar.R  Wind  Temp\n  10   2  Ozone  Solar.R  Wind  Temp\n  10   3  Ozone  Solar.R  Wind  Temp\n  10   4  Ozone  Solar.R  Wind  Temp\n  10   5  Ozone  Solar.R  Wind  Temp\n  11   1  Ozone  Solar.R  Wind  Temp\n  11   2  Ozone  Solar.R  Wind  Temp\n  11   3  Ozone  Solar.R  Wind  Temp\n  11   4  Ozone  Solar.R  Wind  Temp\n  11   5  Ozone  Solar.R  Wind  Temp\n  12   1  Ozone  Solar.R  Wind  Temp\n  12   2  Ozone  Solar.R  Wind  Temp\n  12   3  Ozone  Solar.R  Wind  Temp\n  12   4  Ozone  Solar.R  Wind  Temp\n  12   5  Ozone  Solar.R  Wind  Temp\n  13   1  Ozone  Solar.R  Wind  Temp\n  13   2  Ozone  Solar.R  Wind  Temp\n  13   3  Ozone  Solar.R  Wind  Temp\n  13   4  Ozone  Solar.R  Wind  Temp\n  13   5  Ozone  Solar.R  Wind  Temp\n  14   1  Ozone  Solar.R  Wind  Temp\n  14   2  Ozone  Solar.R  Wind  Temp\n  14   3  Ozone  Solar.R  Wind  Temp\n  14   4  Ozone  Solar.R  Wind  Temp\n  14   5  Ozone  Solar.R  Wind  Temp\n  15   1  Ozone  Solar.R  Wind  Temp\n  15   2  Ozone  Solar.R  Wind  Temp\n  15   3  Ozone  Solar.R  Wind  Temp\n  15   4  Ozone  Solar.R  Wind  Temp\n  15   5  Ozone  Solar.R  Wind  Temp\n  16   1  Ozone  Solar.R  Wind  Temp\n  16   2  Ozone  Solar.R  Wind  Temp\n  16   3  Ozone  Solar.R  Wind  Temp\n  16   4  Ozone  Solar.R  Wind  Temp\n  16   5  Ozone  Solar.R  Wind  Temp\n  17   1  Ozone  Solar.R  Wind  Temp\n  17   2  Ozone  Solar.R  Wind  Temp\n  17   3  Ozone  Solar.R  Wind  Temp\n  17   4  Ozone  Solar.R  Wind  Temp\n  17   5  Ozone  Solar.R  Wind  Temp\n  18   1  Ozone  Solar.R  Wind  Temp\n  18   2  Ozone  Solar.R  Wind  Temp\n  18   3  Ozone  Solar.R  Wind  Temp\n  18   4  Ozone  Solar.R  Wind  Temp\n  18   5  Ozone  Solar.R  Wind  Temp\n  19   1  Ozone  Solar.R  Wind  Temp\n  19   2  Ozone  Solar.R  Wind  Temp\n  19   3  Ozone  Solar.R  Wind  Temp\n  19   4  Ozone  Solar.R  Wind  Temp\n  19   5  Ozone  Solar.R  Wind  Temp\n  20   1  Ozone  Solar.R  Wind  Temp\n  20   2  Ozone  Solar.R  Wind  Temp\n  20   3  Ozone  Solar.R  Wind  Temp\n  20   4  Ozone  Solar.R  Wind  Temp\n  20   5  Ozone  Solar.R  Wind  Temp\n  21   1  Ozone  Solar.R  Wind  Temp\n  21   2  Ozone  Solar.R  Wind  Temp\n  21   3  Ozone  Solar.R  Wind  Temp\n  21   4  Ozone  Solar.R  Wind  Temp\n  21   5  Ozone  Solar.R  Wind  Temp\n  22   1  Ozone  Solar.R  Wind  Temp\n  22   2  Ozone  Solar.R  Wind  Temp\n  22   3  Ozone  Solar.R  Wind  Temp\n  22   4  Ozone  Solar.R  Wind  Temp\n  22   5  Ozone  Solar.R  Wind  Temp\n  23   1  Ozone  Solar.R  Wind  Temp\n  23   2  Ozone  Solar.R  Wind  Temp\n  23   3  Ozone  Solar.R  Wind  Temp\n  23   4  Ozone  Solar.R  Wind  Temp\n  23   5  Ozone  Solar.R  Wind  Temp\n  24   1  Ozone  Solar.R  Wind  Temp\n  24   2  Ozone  Solar.R  Wind  Temp\n  24   3  Ozone  Solar.R  Wind  Temp\n  24   4  Ozone  Solar.R  Wind  Temp\n  24   5  Ozone  Solar.R  Wind  Temp\n  25   1  Ozone  Solar.R  Wind  Temp\n  25   2  Ozone  Solar.R  Wind  Temp\n  25   3  Ozone  Solar.R  Wind  Temp\n  25   4  Ozone  Solar.R  Wind  Temp\n  25   5  Ozone  Solar.R  Wind  Temp\n  26   1  Ozone  Solar.R  Wind  Temp\n  26   2  Ozone  Solar.R  Wind  Temp\n  26   3  Ozone  Solar.R  Wind  Temp\n  26   4  Ozone  Solar.R  Wind  Temp\n  26   5  Ozone  Solar.R  Wind  Temp\n  27   1  Ozone  Solar.R  Wind  Temp\n  27   2  Ozone  Solar.R  Wind  Temp\n  27   3  Ozone  Solar.R  Wind  Temp\n  27   4  Ozone  Solar.R  Wind  Temp\n  27   5  Ozone  Solar.R  Wind  Temp\n  28   1  Ozone  Solar.R  Wind  Temp\n  28   2  Ozone  Solar.R  Wind  Temp\n  28   3  Ozone  Solar.R  Wind  Temp\n  28   4  Ozone  Solar.R  Wind  Temp\n  28   5  Ozone  Solar.R  Wind  Temp\n  29   1  Ozone  Solar.R  Wind  Temp\n  29   2  Ozone  Solar.R  Wind  Temp\n  29   3  Ozone  Solar.R  Wind  Temp\n  29   4  Ozone  Solar.R  Wind  Temp\n  29   5  Ozone  Solar.R  Wind  Temp\n  30   1  Ozone  Solar.R  Wind  Temp\n  30   2  Ozone  Solar.R  Wind  Temp\n  30   3  Ozone  Solar.R  Wind  Temp\n  30   4  Ozone  Solar.R  Wind  Temp\n  30   5  Ozone  Solar.R  Wind  Temp\n  31   1  Ozone  Solar.R  Wind  Temp\n  31   2  Ozone  Solar.R  Wind  Temp\n  31   3  Ozone  Solar.R  Wind  Temp\n  31   4  Ozone  Solar.R  Wind  Temp\n  31   5  Ozone  Solar.R  Wind  Temp\n  32   1  Ozone  Solar.R  Wind  Temp\n  32   2  Ozone  Solar.R  Wind  Temp\n  32   3  Ozone  Solar.R  Wind  Temp\n  32   4  Ozone  Solar.R  Wind  Temp\n  32   5  Ozone  Solar.R  Wind  Temp\n  33   1  Ozone  Solar.R  Wind  Temp\n  33   2  Ozone  Solar.R  Wind  Temp\n  33   3  Ozone  Solar.R  Wind  Temp\n  33   4  Ozone  Solar.R  Wind  Temp\n  33   5  Ozone  Solar.R  Wind  Temp\n  34   1  Ozone  Solar.R  Wind  Temp\n  34   2  Ozone  Solar.R  Wind  Temp\n  34   3  Ozone  Solar.R  Wind  Temp\n  34   4  Ozone  Solar.R  Wind  Temp\n  34   5  Ozone  Solar.R  Wind  Temp\n  35   1  Ozone  Solar.R  Wind  Temp\n  35   2  Ozone  Solar.R  Wind  Temp\n  35   3  Ozone  Solar.R  Wind  Temp\n  35   4  Ozone  Solar.R  Wind  Temp\n  35   5  Ozone  Solar.R  Wind  Temp\n  36   1  Ozone  Solar.R  Wind  Temp\n  36   2  Ozone  Solar.R  Wind  Temp\n  36   3  Ozone  Solar.R  Wind  Temp\n  36   4  Ozone  Solar.R  Wind  Temp\n  36   5  Ozone  Solar.R  Wind  Temp\n  37   1  Ozone  Solar.R  Wind  Temp\n  37   2  Ozone  Solar.R  Wind  Temp\n  37   3  Ozone  Solar.R  Wind  Temp\n  37   4  Ozone  Solar.R  Wind  Temp\n  37   5  Ozone  Solar.R  Wind  Temp\n  38   1  Ozone  Solar.R  Wind  Temp\n  38   2  Ozone  Solar.R  Wind  Temp\n  38   3  Ozone  Solar.R  Wind  Temp\n  38   4  Ozone  Solar.R  Wind  Temp\n  38   5  Ozone  Solar.R  Wind  Temp\n  39   1  Ozone  Solar.R  Wind  Temp\n  39   2  Ozone  Solar.R  Wind  Temp\n  39   3  Ozone  Solar.R  Wind  Temp\n  39   4  Ozone  Solar.R  Wind  Temp\n  39   5  Ozone  Solar.R  Wind  Temp\n  40   1  Ozone  Solar.R  Wind  Temp\n  40   2  Ozone  Solar.R  Wind  Temp\n  40   3  Ozone  Solar.R  Wind  Temp\n  40   4  Ozone  Solar.R  Wind  Temp\n  40   5  Ozone  Solar.R  Wind  Temp\n  41   1  Ozone  Solar.R  Wind  Temp\n  41   2  Ozone  Solar.R  Wind  Temp\n  41   3  Ozone  Solar.R  Wind  Temp\n  41   4  Ozone  Solar.R  Wind  Temp\n  41   5  Ozone  Solar.R  Wind  Temp\n  42   1  Ozone  Solar.R  Wind  Temp\n  42   2  Ozone  Solar.R  Wind  Temp\n  42   3  Ozone  Solar.R  Wind  Temp\n  42   4  Ozone  Solar.R  Wind  Temp\n  42   5  Ozone  Solar.R  Wind  Temp\n  43   1  Ozone  Solar.R  Wind  Temp\n  43   2  Ozone  Solar.R  Wind  Temp\n  43   3  Ozone  Solar.R  Wind  Temp\n  43   4  Ozone  Solar.R  Wind  Temp\n  43   5  Ozone  Solar.R  Wind  Temp\n  44   1  Ozone  Solar.R  Wind  Temp\n  44   2  Ozone  Solar.R  Wind  Temp\n  44   3  Ozone  Solar.R  Wind  Temp\n  44   4  Ozone  Solar.R  Wind  Temp\n  44   5  Ozone  Solar.R  Wind  Temp\n  45   1  Ozone  Solar.R  Wind  Temp\n  45   2  Ozone  Solar.R  Wind  Temp\n  45   3  Ozone  Solar.R  Wind  Temp\n  45   4  Ozone  Solar.R  Wind  Temp\n  45   5  Ozone  Solar.R  Wind  Temp\n  46   1  Ozone  Solar.R  Wind  Temp\n  46   2  Ozone  Solar.R  Wind  Temp\n  46   3  Ozone  Solar.R  Wind  Temp\n  46   4  Ozone  Solar.R  Wind  Temp\n  46   5  Ozone  Solar.R  Wind  Temp\n  47   1  Ozone  Solar.R  Wind  Temp\n  47   2  Ozone  Solar.R  Wind  Temp\n  47   3  Ozone  Solar.R  Wind  Temp\n  47   4  Ozone  Solar.R  Wind  Temp\n  47   5  Ozone  Solar.R  Wind  Temp\n  48   1  Ozone  Solar.R  Wind  Temp\n  48   2  Ozone  Solar.R  Wind  Temp\n  48   3  Ozone  Solar.R  Wind  Temp\n  48   4  Ozone  Solar.R  Wind  Temp\n  48   5  Ozone  Solar.R  Wind  Temp\n  49   1  Ozone  Solar.R  Wind  Temp\n  49   2  Ozone  Solar.R  Wind  Temp\n  49   3  Ozone  Solar.R  Wind  Temp\n  49   4  Ozone  Solar.R  Wind  Temp\n  49   5  Ozone  Solar.R  Wind  Temp\n  50   1  Ozone  Solar.R  Wind  Temp\n  50   2  Ozone  Solar.R  Wind  Temp\n  50   3  Ozone  Solar.R  Wind  Temp\n  50   4  Ozone  Solar.R  Wind  Temp\n  50   5  Ozone  Solar.R  Wind  Temp\n\nsummary(tempData)\n\nClass: mids\nNumber of multiple imputations:  5 \nImputation methods:\n  Ozone Solar.R    Wind    Temp \n  \"pmm\"   \"pmm\"   \"pmm\"   \"pmm\" \nPredictorMatrix:\n        Ozone Solar.R Wind Temp\nOzone       0       1    1    1\nSolar.R     1       0    1    1\nWind        1       1    0    1\nTemp        1       1    1    0\n\n\nEen paar opmerkingen over de parameters:\n\nm=5 verwijst naar het aantal geïmputeerde datasets. Vijf is de standaard waarde.\n\nmeth='pmm' verwijst naar de imputatie methode. In dit geval gebruiken we pmm (predictive mean matching) als imputatiemethode. Er kunnen ook andere imputatiemethoden worden gebruikt, type methods(mice) voor een lijst van de beschikbare imputatiemethoden.\n\nAls u de geïmputeerde gegevens wilt controleren, bijvoorbeeld voor de variabele Ozon, moet u de volgende regel code invoeren\n\ntempData$imp$Ozone\n\n     1   2   3   4   5\n5   13  19  12 115  63\n10  30  12  13  21   7\n25   8  28   6  18  28\n26   9  32   4  18  37\n27  37  21   4  32  32\n32  40  39  35  32  47\n33  44  28  36  52  20\n34  20  23  37  37  19\n35  32  28  16  32  35\n36  89  80  48  49 115\n37  18   7  16  30  22\n39  96  77 135  76  85\n42  50 168  64  50  41\n43  96  78  96  96  78\n45  63  20  18  24  31\n46  71  37  20  20  28\n52  20  35  37  63  63\n53  16  78  73  48 115\n54  59  35  46  44  23\n55  16  39  28  40  49\n56  24  36  52  21  44\n57  36  20  20  18  23\n58  11  11  24   7  23\n59  44  13  23  23  27\n60  23   4  19   4  32\n61  44  16  46  37  35\n65  30  23  65  30  30\n72  45  37  63  63  44\n75  39  46  32  39  28\n83  37  40  59  32  35\n84  40  59  28  28  35\n102 61  85  96  79  78\n103 31  59  20  31  36\n107 32  24  11  21  21\n115 52  16  11  14  13\n119 78  96 168  76  50\n150 14  12  13  23  11\n\n\nDe uitvoer toont de geïmputeerde data voor elke observatie (eerste kolom links) binnen elke geïmputeerde dataset (eerste rij bovenaan). Als u de gebruikte imputatiemethode voor elke variabele wilt controleren, kunt u dat met mice heel eenvoudig doen\n\ntempData$meth\n\n  Ozone Solar.R    Wind    Temp \n  \"pmm\"   \"pmm\"   \"pmm\"   \"pmm\" \n\n\nNu kunnen we de voltooide dataset terugkrijgen met de complete()functie. Het is bijna gewoon Engels:\n\ncompletedData &lt;- complete(tempData,1)\n\nDe ontbrekende waarden zijn vervangen door de geïmputeerde waarden in de eerste van de vijf datasets. Als je een andere wilt gebruiken, verander dan de tweede parameter in de complete() functie."
  },
  {
    "objectID": "posts/2022-05-08-missende-waarden/missende-waarden.html#de-verdeling-van-de-originele-en-geïmputeerde-data-bekijken",
    "href": "posts/2022-05-08-missende-waarden/missende-waarden.html#de-verdeling-van-de-originele-en-geïmputeerde-data-bekijken",
    "title": "Missende waarden",
    "section": "De verdeling van de originele en geïmputeerde data bekijken",
    "text": "De verdeling van de originele en geïmputeerde data bekijken\nLaten we de verdelingen van de originele en geïmputeerde data vergelijken met behulp van een aantal handige plots. Allereerst kunnen we een scatterplot gebruiken en Ozon uitzetten tegen alle andere variabelen.\nHier is dat het geval:\n\nxyplot(tempData,Ozone ~ Wind+Temp+Solar.R,pch=18,cex=1)\n\n\n\n\nWat wij willen zien is dat de vorm van de magenta punten (geïmputeerd) overeenkomt met de vorm van de blauwe punten (waargenomen). De overeenkomstige vorm zegt ons dat de geïmputeerde waarden inderdaad “plausibele waarden” zijn. Een andere nuttige grafiek is de densitygrafiek:\n\ndensityplot(tempData)\n\n\n\n\nDe dichtheid van de geïmputeerde gegevens voor elke geïmputeerde dataset wordt getoond in magenta, terwijl de dichtheid van de waargenomen gegevens in blauw wordt getoond. Nogmaals, onder onze eerdere aannames verwachten we dat de verdelingen vergelijkbaar zijn.\nEen andere nuttige visuele kijk op de verdelingen kan worden verkregen met de stripplot() functie die de verdelingen van de variabelen als afzonderlijke punten toont\n\nstripplot(tempData, pch = 20, cex = 1.2)"
  },
  {
    "objectID": "posts/2022-05-08-missende-waarden/missende-waarden.html#pooling",
    "href": "posts/2022-05-08-missende-waarden/missende-waarden.html#pooling",
    "title": "Missende waarden",
    "section": "Pooling",
    "text": "Pooling\nVeronderstel dat de volgende stap in onze analyse erin bestaat een lineair model op de gegevens toe te passen. Dan kun je je afvragen welke geïmputeerde dataset je moet kiezen. Het mice pakket maakt het weer heel gemakkelijk om een model te passen op elk van de geïmputeerde datasets en dan deze resultaten samen te voegen\n\nmodelFit1 &lt;- with(tempData,lm(Temp~ Ozone+Solar.R+Wind))\nsummary(pool(modelFit1))\n\n         term    estimate   std.error statistic        df      p.value\n1 (Intercept) 72.70719792 2.761360433 26.330209 117.27936 0.000000e+00\n2       Ozone  0.15924872 0.025914423  6.145177  49.30693 1.367056e-07\n3     Solar.R  0.01252384 0.008678358  1.443112  18.19046 1.659893e-01\n4        Wind -0.34547006 0.207866970 -1.661977 123.65905 9.905045e-02\n\n\nDe variabele modelFit1 bevat de resultaten van de aanpassing die is uitgevoerd over de geïmputeerde datasets, terwijl de pool() functie ze allemaal samenvoegt. Blijkbaar is alleen de Ozon variabele statistisch significant.\nVergeet niet dat we de mice-functie hebben geïnitialiseerd met een specifieke ‘seed’-instelling. Daarom zijn de resultaten enigszins afhankelijk van onze initiële keuze. Om dit effect te verminderen, kunnen we een hoger aantal datasets toerekenen, door de standaard m=5 parameter in de mice() functie als volgt te wijzigen\n\ntempData2 &lt;- mice(data,m=50,seed=245435)\n\n\n iter imp variable\n  1   1  Ozone  Solar.R  Wind  Temp\n  1   2  Ozone  Solar.R  Wind  Temp\n  1   3  Ozone  Solar.R  Wind  Temp\n  1   4  Ozone  Solar.R  Wind  Temp\n  1   5  Ozone  Solar.R  Wind  Temp\n  1   6  Ozone  Solar.R  Wind  Temp\n  1   7  Ozone  Solar.R  Wind  Temp\n  1   8  Ozone  Solar.R  Wind  Temp\n  1   9  Ozone  Solar.R  Wind  Temp\n  1   10  Ozone  Solar.R  Wind  Temp\n  1   11  Ozone  Solar.R  Wind  Temp\n  1   12  Ozone  Solar.R  Wind  Temp\n  1   13  Ozone  Solar.R  Wind  Temp\n  1   14  Ozone  Solar.R  Wind  Temp\n  1   15  Ozone  Solar.R  Wind  Temp\n  1   16  Ozone  Solar.R  Wind  Temp\n  1   17  Ozone  Solar.R  Wind  Temp\n  1   18  Ozone  Solar.R  Wind  Temp\n  1   19  Ozone  Solar.R  Wind  Temp\n  1   20  Ozone  Solar.R  Wind  Temp\n  1   21  Ozone  Solar.R  Wind  Temp\n  1   22  Ozone  Solar.R  Wind  Temp\n  1   23  Ozone  Solar.R  Wind  Temp\n  1   24  Ozone  Solar.R  Wind  Temp\n  1   25  Ozone  Solar.R  Wind  Temp\n  1   26  Ozone  Solar.R  Wind  Temp\n  1   27  Ozone  Solar.R  Wind  Temp\n  1   28  Ozone  Solar.R  Wind  Temp\n  1   29  Ozone  Solar.R  Wind  Temp\n  1   30  Ozone  Solar.R  Wind  Temp\n  1   31  Ozone  Solar.R  Wind  Temp\n  1   32  Ozone  Solar.R  Wind  Temp\n  1   33  Ozone  Solar.R  Wind  Temp\n  1   34  Ozone  Solar.R  Wind  Temp\n  1   35  Ozone  Solar.R  Wind  Temp\n  1   36  Ozone  Solar.R  Wind  Temp\n  1   37  Ozone  Solar.R  Wind  Temp\n  1   38  Ozone  Solar.R  Wind  Temp\n  1   39  Ozone  Solar.R  Wind  Temp\n  1   40  Ozone  Solar.R  Wind  Temp\n  1   41  Ozone  Solar.R  Wind  Temp\n  1   42  Ozone  Solar.R  Wind  Temp\n  1   43  Ozone  Solar.R  Wind  Temp\n  1   44  Ozone  Solar.R  Wind  Temp\n  1   45  Ozone  Solar.R  Wind  Temp\n  1   46  Ozone  Solar.R  Wind  Temp\n  1   47  Ozone  Solar.R  Wind  Temp\n  1   48  Ozone  Solar.R  Wind  Temp\n  1   49  Ozone  Solar.R  Wind  Temp\n  1   50  Ozone  Solar.R  Wind  Temp\n  2   1  Ozone  Solar.R  Wind  Temp\n  2   2  Ozone  Solar.R  Wind  Temp\n  2   3  Ozone  Solar.R  Wind  Temp\n  2   4  Ozone  Solar.R  Wind  Temp\n  2   5  Ozone  Solar.R  Wind  Temp\n  2   6  Ozone  Solar.R  Wind  Temp\n  2   7  Ozone  Solar.R  Wind  Temp\n  2   8  Ozone  Solar.R  Wind  Temp\n  2   9  Ozone  Solar.R  Wind  Temp\n  2   10  Ozone  Solar.R  Wind  Temp\n  2   11  Ozone  Solar.R  Wind  Temp\n  2   12  Ozone  Solar.R  Wind  Temp\n  2   13  Ozone  Solar.R  Wind  Temp\n  2   14  Ozone  Solar.R  Wind  Temp\n  2   15  Ozone  Solar.R  Wind  Temp\n  2   16  Ozone  Solar.R  Wind  Temp\n  2   17  Ozone  Solar.R  Wind  Temp\n  2   18  Ozone  Solar.R  Wind  Temp\n  2   19  Ozone  Solar.R  Wind  Temp\n  2   20  Ozone  Solar.R  Wind  Temp\n  2   21  Ozone  Solar.R  Wind  Temp\n  2   22  Ozone  Solar.R  Wind  Temp\n  2   23  Ozone  Solar.R  Wind  Temp\n  2   24  Ozone  Solar.R  Wind  Temp\n  2   25  Ozone  Solar.R  Wind  Temp\n  2   26  Ozone  Solar.R  Wind  Temp\n  2   27  Ozone  Solar.R  Wind  Temp\n  2   28  Ozone  Solar.R  Wind  Temp\n  2   29  Ozone  Solar.R  Wind  Temp\n  2   30  Ozone  Solar.R  Wind  Temp\n  2   31  Ozone  Solar.R  Wind  Temp\n  2   32  Ozone  Solar.R  Wind  Temp\n  2   33  Ozone  Solar.R  Wind  Temp\n  2   34  Ozone  Solar.R  Wind  Temp\n  2   35  Ozone  Solar.R  Wind  Temp\n  2   36  Ozone  Solar.R  Wind  Temp\n  2   37  Ozone  Solar.R  Wind  Temp\n  2   38  Ozone  Solar.R  Wind  Temp\n  2   39  Ozone  Solar.R  Wind  Temp\n  2   40  Ozone  Solar.R  Wind  Temp\n  2   41  Ozone  Solar.R  Wind  Temp\n  2   42  Ozone  Solar.R  Wind  Temp\n  2   43  Ozone  Solar.R  Wind  Temp\n  2   44  Ozone  Solar.R  Wind  Temp\n  2   45  Ozone  Solar.R  Wind  Temp\n  2   46  Ozone  Solar.R  Wind  Temp\n  2   47  Ozone  Solar.R  Wind  Temp\n  2   48  Ozone  Solar.R  Wind  Temp\n  2   49  Ozone  Solar.R  Wind  Temp\n  2   50  Ozone  Solar.R  Wind  Temp\n  3   1  Ozone  Solar.R  Wind  Temp\n  3   2  Ozone  Solar.R  Wind  Temp\n  3   3  Ozone  Solar.R  Wind  Temp\n  3   4  Ozone  Solar.R  Wind  Temp\n  3   5  Ozone  Solar.R  Wind  Temp\n  3   6  Ozone  Solar.R  Wind  Temp\n  3   7  Ozone  Solar.R  Wind  Temp\n  3   8  Ozone  Solar.R  Wind  Temp\n  3   9  Ozone  Solar.R  Wind  Temp\n  3   10  Ozone  Solar.R  Wind  Temp\n  3   11  Ozone  Solar.R  Wind  Temp\n  3   12  Ozone  Solar.R  Wind  Temp\n  3   13  Ozone  Solar.R  Wind  Temp\n  3   14  Ozone  Solar.R  Wind  Temp\n  3   15  Ozone  Solar.R  Wind  Temp\n  3   16  Ozone  Solar.R  Wind  Temp\n  3   17  Ozone  Solar.R  Wind  Temp\n  3   18  Ozone  Solar.R  Wind  Temp\n  3   19  Ozone  Solar.R  Wind  Temp\n  3   20  Ozone  Solar.R  Wind  Temp\n  3   21  Ozone  Solar.R  Wind  Temp\n  3   22  Ozone  Solar.R  Wind  Temp\n  3   23  Ozone  Solar.R  Wind  Temp\n  3   24  Ozone  Solar.R  Wind  Temp\n  3   25  Ozone  Solar.R  Wind  Temp\n  3   26  Ozone  Solar.R  Wind  Temp\n  3   27  Ozone  Solar.R  Wind  Temp\n  3   28  Ozone  Solar.R  Wind  Temp\n  3   29  Ozone  Solar.R  Wind  Temp\n  3   30  Ozone  Solar.R  Wind  Temp\n  3   31  Ozone  Solar.R  Wind  Temp\n  3   32  Ozone  Solar.R  Wind  Temp\n  3   33  Ozone  Solar.R  Wind  Temp\n  3   34  Ozone  Solar.R  Wind  Temp\n  3   35  Ozone  Solar.R  Wind  Temp\n  3   36  Ozone  Solar.R  Wind  Temp\n  3   37  Ozone  Solar.R  Wind  Temp\n  3   38  Ozone  Solar.R  Wind  Temp\n  3   39  Ozone  Solar.R  Wind  Temp\n  3   40  Ozone  Solar.R  Wind  Temp\n  3   41  Ozone  Solar.R  Wind  Temp\n  3   42  Ozone  Solar.R  Wind  Temp\n  3   43  Ozone  Solar.R  Wind  Temp\n  3   44  Ozone  Solar.R  Wind  Temp\n  3   45  Ozone  Solar.R  Wind  Temp\n  3   46  Ozone  Solar.R  Wind  Temp\n  3   47  Ozone  Solar.R  Wind  Temp\n  3   48  Ozone  Solar.R  Wind  Temp\n  3   49  Ozone  Solar.R  Wind  Temp\n  3   50  Ozone  Solar.R  Wind  Temp\n  4   1  Ozone  Solar.R  Wind  Temp\n  4   2  Ozone  Solar.R  Wind  Temp\n  4   3  Ozone  Solar.R  Wind  Temp\n  4   4  Ozone  Solar.R  Wind  Temp\n  4   5  Ozone  Solar.R  Wind  Temp\n  4   6  Ozone  Solar.R  Wind  Temp\n  4   7  Ozone  Solar.R  Wind  Temp\n  4   8  Ozone  Solar.R  Wind  Temp\n  4   9  Ozone  Solar.R  Wind  Temp\n  4   10  Ozone  Solar.R  Wind  Temp\n  4   11  Ozone  Solar.R  Wind  Temp\n  4   12  Ozone  Solar.R  Wind  Temp\n  4   13  Ozone  Solar.R  Wind  Temp\n  4   14  Ozone  Solar.R  Wind  Temp\n  4   15  Ozone  Solar.R  Wind  Temp\n  4   16  Ozone  Solar.R  Wind  Temp\n  4   17  Ozone  Solar.R  Wind  Temp\n  4   18  Ozone  Solar.R  Wind  Temp\n  4   19  Ozone  Solar.R  Wind  Temp\n  4   20  Ozone  Solar.R  Wind  Temp\n  4   21  Ozone  Solar.R  Wind  Temp\n  4   22  Ozone  Solar.R  Wind  Temp\n  4   23  Ozone  Solar.R  Wind  Temp\n  4   24  Ozone  Solar.R  Wind  Temp\n  4   25  Ozone  Solar.R  Wind  Temp\n  4   26  Ozone  Solar.R  Wind  Temp\n  4   27  Ozone  Solar.R  Wind  Temp\n  4   28  Ozone  Solar.R  Wind  Temp\n  4   29  Ozone  Solar.R  Wind  Temp\n  4   30  Ozone  Solar.R  Wind  Temp\n  4   31  Ozone  Solar.R  Wind  Temp\n  4   32  Ozone  Solar.R  Wind  Temp\n  4   33  Ozone  Solar.R  Wind  Temp\n  4   34  Ozone  Solar.R  Wind  Temp\n  4   35  Ozone  Solar.R  Wind  Temp\n  4   36  Ozone  Solar.R  Wind  Temp\n  4   37  Ozone  Solar.R  Wind  Temp\n  4   38  Ozone  Solar.R  Wind  Temp\n  4   39  Ozone  Solar.R  Wind  Temp\n  4   40  Ozone  Solar.R  Wind  Temp\n  4   41  Ozone  Solar.R  Wind  Temp\n  4   42  Ozone  Solar.R  Wind  Temp\n  4   43  Ozone  Solar.R  Wind  Temp\n  4   44  Ozone  Solar.R  Wind  Temp\n  4   45  Ozone  Solar.R  Wind  Temp\n  4   46  Ozone  Solar.R  Wind  Temp\n  4   47  Ozone  Solar.R  Wind  Temp\n  4   48  Ozone  Solar.R  Wind  Temp\n  4   49  Ozone  Solar.R  Wind  Temp\n  4   50  Ozone  Solar.R  Wind  Temp\n  5   1  Ozone  Solar.R  Wind  Temp\n  5   2  Ozone  Solar.R  Wind  Temp\n  5   3  Ozone  Solar.R  Wind  Temp\n  5   4  Ozone  Solar.R  Wind  Temp\n  5   5  Ozone  Solar.R  Wind  Temp\n  5   6  Ozone  Solar.R  Wind  Temp\n  5   7  Ozone  Solar.R  Wind  Temp\n  5   8  Ozone  Solar.R  Wind  Temp\n  5   9  Ozone  Solar.R  Wind  Temp\n  5   10  Ozone  Solar.R  Wind  Temp\n  5   11  Ozone  Solar.R  Wind  Temp\n  5   12  Ozone  Solar.R  Wind  Temp\n  5   13  Ozone  Solar.R  Wind  Temp\n  5   14  Ozone  Solar.R  Wind  Temp\n  5   15  Ozone  Solar.R  Wind  Temp\n  5   16  Ozone  Solar.R  Wind  Temp\n  5   17  Ozone  Solar.R  Wind  Temp\n  5   18  Ozone  Solar.R  Wind  Temp\n  5   19  Ozone  Solar.R  Wind  Temp\n  5   20  Ozone  Solar.R  Wind  Temp\n  5   21  Ozone  Solar.R  Wind  Temp\n  5   22  Ozone  Solar.R  Wind  Temp\n  5   23  Ozone  Solar.R  Wind  Temp\n  5   24  Ozone  Solar.R  Wind  Temp\n  5   25  Ozone  Solar.R  Wind  Temp\n  5   26  Ozone  Solar.R  Wind  Temp\n  5   27  Ozone  Solar.R  Wind  Temp\n  5   28  Ozone  Solar.R  Wind  Temp\n  5   29  Ozone  Solar.R  Wind  Temp\n  5   30  Ozone  Solar.R  Wind  Temp\n  5   31  Ozone  Solar.R  Wind  Temp\n  5   32  Ozone  Solar.R  Wind  Temp\n  5   33  Ozone  Solar.R  Wind  Temp\n  5   34  Ozone  Solar.R  Wind  Temp\n  5   35  Ozone  Solar.R  Wind  Temp\n  5   36  Ozone  Solar.R  Wind  Temp\n  5   37  Ozone  Solar.R  Wind  Temp\n  5   38  Ozone  Solar.R  Wind  Temp\n  5   39  Ozone  Solar.R  Wind  Temp\n  5   40  Ozone  Solar.R  Wind  Temp\n  5   41  Ozone  Solar.R  Wind  Temp\n  5   42  Ozone  Solar.R  Wind  Temp\n  5   43  Ozone  Solar.R  Wind  Temp\n  5   44  Ozone  Solar.R  Wind  Temp\n  5   45  Ozone  Solar.R  Wind  Temp\n  5   46  Ozone  Solar.R  Wind  Temp\n  5   47  Ozone  Solar.R  Wind  Temp\n  5   48  Ozone  Solar.R  Wind  Temp\n  5   49  Ozone  Solar.R  Wind  Temp\n  5   50  Ozone  Solar.R  Wind  Temp\n\nmodelFit2 &lt;- with(tempData2,lm(Temp~ Ozone+Solar.R+Wind))\nsummary(pool(modelFit2))\n\n         term    estimate   std.error statistic        df      p.value\n1 (Intercept) 72.60178955 2.915916315 24.898448 105.59368 0.000000e+00\n2       Ozone  0.16345639 0.026054628  6.273603  99.86352 9.122568e-09\n3     Solar.R  0.01193645 0.007134344  1.673097 120.41496 9.690433e-02\n4        Wind -0.33592048 0.222350762 -1.510768 107.41496 1.337838e-01\n\n\nNa aanpassing, krijgen we (in dit geval) min of meer dezelfde resultaten als voorheen, waarbij alleen Ozone statistische significantie vertoont.\nDe code kun je hier vinden."
  },
  {
    "objectID": "posts/2022-05-08-missende-waarden/missende-waarden.html#literatuur",
    "href": "posts/2022-05-08-missende-waarden/missende-waarden.html#literatuur",
    "title": "Missende waarden",
    "section": "Literatuur",
    "text": "Literatuur\nAlice, M. (2015). Imputing missing data with R; micepackage. R-bloggers, 4-10-2015. https://www.r-bloggers.com/2015/10/imputing-missing-data-with-r-mice-package/\nHeymans, M. en Eekhout, I. (2019). Applied Missing Data Analysis With SPSS and (R)Studio. Amsterdam. https://bookdown.org/mwheymans/bookmi/\nVan Buuren, S. and Groothuis-Oudshoorn, C.G.M. (2011).mice: Multivariate Imputation by Chained Equations in R. Journal of Statistical Software, 45(3), 1–67.\nVan Buuren (2012). Multiple imputatie in vogelvlucht. https://stefvanbuuren.name/publications/2012%20Vogelvlucht%20-%20STAtOR.pdf\nVan Buuren, S. (2018). Flexible Imputation of Missing Data. Second Edition. Chapman & Hall/CRC, Boca Raton, FL"
  },
  {
    "objectID": "posts/2022-05-31-van-distill-naar-quarto/van-distill-naar-quarto.html",
    "href": "posts/2022-05-31-van-distill-naar-quarto/van-distill-naar-quarto.html",
    "title": "Van distill naar quarto?",
    "section": "",
    "text": "Een jaar geleden besloot Danielle Navarro om weer te gaan bloggen en deze blog zette ze op: blog van Danielle Navarro. Ze koos voor distill, haar keuze. Destijds heeft zij een bewuste keuze gemaakt om distill te gebruiken als mijn blogging platform in plaats van een statische site generator zoals hugo of jekyll en ze heeft geen spijt van die keuze. Gaandeweg vond zij echter een paar dingen die haar dwars zaten bij het gebruik van distill. Het is echter nooit de moeite waard geweest om te overwegen over te stappen op iets nieuws omdat distill zoveel dingen heeft die zij waardeert. Tot nu toe dan.\nNu komt quarto binnen. Ook ik werk al enige tijd met distill. De ervaringen van Danielle herken ik sterk en daarom schrijf ik vanaf nu vanuit Danielle, maar kun je ook mijzelf lezen.\nQuarto, volop in de belangstelling nu, biedt de belofte van een cross-platform, overall format, open source publicatietool gebaseerd op pandoc. Geïntrigeerd besloot ik er een tijdje mee te spelen, en uiteindelijk nam ik de beslissing om mijn blog over te zetten van distill naar quarto. Deze post schetst mijn proces.\n(Ik ben een beetje nerveus: een blog overzetten betekent vaak dingen opnieuw programmeren. Zal het werken? Zal alles reproduceerbaar blijken te zijn? Ik hoop het…)"
  },
  {
    "objectID": "posts/2022-05-31-van-distill-naar-quarto/van-distill-naar-quarto.html#aan-de-slag",
    "href": "posts/2022-05-31-van-distill-naar-quarto/van-distill-naar-quarto.html#aan-de-slag",
    "title": "Van distill naar quarto?",
    "section": "Aan de slag",
    "text": "Aan de slag\nHet allereerste wat ik doe is Alison Hill’s prachtige We don’t talk about quarto lezen. Als je een R markdown gebruiker bent die overweegt de sprong naar quarto te maken en haar samenvatting nog niet gelezen hebt, zul je er geen spijt van krijgen dat nu wel te doen. Het is een mooi overzicht op hoog niveau. Ik raad ook Nick Tierney’s notities aan over het maken van de overstap, die zijn ook erg behulpzaam. (Zelf dus ook Danielle Navarro’s blog goed gelezen en bewerkt en de presentatie van Mine Cetinkaya-Rundel gaf interessant.\nNa het eigen maken van deze achtergrondinformatie, ga ik naar de get started pagina op de quarto website om het installatiebestand te downloaden.\nNu ik quarto geïnstalleerd heb, ben ik in staat om het te gebruiken om een blog te maken. Mijn oude distill blog bestaat in een project map die ik Harrie's Hoekje heb genoemd, dus ik besluit de quarto versie te maken en de map HHquarto te maken.\nEr is een pagina op de quarto website die je door het proces leidt voor creating a blog blog, die ik plichtsgetrouw volg. Vanaf de terminal (Power-shell voor Windows) gebruik ik het quarto create-project commando, en er worden verschillende bestanden aangemaakt:\nquarto create-project quarto-blog --type website:blog\nCreating project at /home/danielle/GitHub/sites/quarto-blog:\n  - Created _quarto.yml\n  - Created index.qmd\n  - Created posts/welcome/index.qmd\n  - Created posts/post-with-code/index.qmd\n  - Created about.qmd\n  - Created styles.css\n  - Created posts/_metadata.yml\nKomende van een R markdown achtergrond, is dit erg vertrouwd:\n\nDe bestanden met een .qmd extensie zijn de quarto markdown documenten. Deze bevatten broncode voor de blog posts (de twee bestanden in de posts map), de home page (het index.qmd bestand in de project root map) en een standalone “over mij” pagina voor de blog (het about.qmd bestand).\nDe bestanden met een .yml extensie zijn de YAML bestanden die gebruikt worden om het blog te configureren. Dit valt in eerste instantie niet op, maar het feit dat het er twee zijn is wel belangrijk. Het _quarto.yml bestand wordt gebruikt voor instellingen die voor de hele site gelden, maar je zult vaak instellingen willen configureren die alleen voor je blog posts gelden. Deze kunnen worden ingesteld door het posts/_metadata.yml bestand te bewerken.\nHet styles.css bestand kan gebruikt worden om CSS regels op te geven die voor de hele site gelden. Ik zal later meer vertellen over stijlen."
  },
  {
    "objectID": "posts/2022-05-31-van-distill-naar-quarto/van-distill-naar-quarto.html#blog-posts-renderen",
    "href": "posts/2022-05-31-van-distill-naar-quarto/van-distill-naar-quarto.html#blog-posts-renderen",
    "title": "Van distill naar quarto?",
    "section": "Blog posts renderen",
    "text": "Blog posts renderen\nEr zijn verschillende manieren om met quarto te werken. Bijvoorbeeld, later in de post zal ik het hebben over de quarto commando-regel interface die je toestaat om met quarto te werken zonder door R of RStudio te gaan. Echter, als ik begin probeer ik de dingen eenvoudig te houden en ga ik voor de optie die mij het meest vertrouwd is: Ik gebruik RStudio.\nOm dit te doen, is het handig om een RStudio project te hebben voor mijn blog. Met behulp van het RStudio bestandsmenu, maak ik een nieuw project vanuit een bestaande directory (d.w.z. mijn HHquarto folder), die het HHquarto.Rproj bestand en andere infrastructuur levert die nodig is om met mijn nieuwe quarto blog te werken als een RStudio project. Als dat eenmaal gedaan is, kan ik een quarto bestand openen in de RStudio editor en zie ik een vertrouwd ogende interface:\n\n\n\n\n\nEen blog post geschreven in quarto markdown open in de RStudio editor. Merk op dat op de plaats waar je normaal de ‘Knit’ knop zou verwachten voor een R markdown document, er een ‘Render’ knop is. Die heeft dezelfde functie en is toegewezen aan dezelfde sneltoetsen als de knop ’Knit\n\n\n\n\nVan hieruit kan ik op de “Render” knop klikken om een enkele pagina te renderen, of ik kan naar het RStudio menu gaan en de “Render Project” optie selecteren om de hele site te bouwen. Standaard wordt de blog gebouwd in de _site map."
  },
  {
    "objectID": "posts/2022-05-31-van-distill-naar-quarto/van-distill-naar-quarto.html#aan-het-spelen",
    "href": "posts/2022-05-31-van-distill-naar-quarto/van-distill-naar-quarto.html#aan-het-spelen",
    "title": "Van distill naar quarto?",
    "section": "Aan het spelen",
    "text": "Aan het spelen\nVooruitgang! Ik maak vooruitgang. Maar voordat ik andere praktische dingen ga doen, heb ik eerst nog wat belangrijke zaken te regelen: wat rondspelen. Doelloos de functionaliteit van een nieuw gereedschap verkennen is altijd leuk en ik vind het een goede manier om mezelf met iets vertrouwd te maken. Ik ben al redelijk vertrouwd met R markdown en ik veronderstel dat de meeste lezers van deze post dat ook zullen zijn, dus voor het grootste deel zijn er geen verrassingen. Toch is het de moeite waard om mezelf de gebruikelijke vragen te stellen:\n\nKan ik voetnoten toevoegen? 1\nKunnen ze genest worden? 2\nKan ik commentaar in de kantlijn toevoegen?\n\n\n\nA comment in the margin\nAls je kijkt naar de quarto article layout documentation, ontdek ik enkele aardige kenmerken. Je kunt de :::{.class} notatie om een deel van de ‘CSS class’ toe te passen op de output, zoals hier:\n:::{.column-margin}\nA comment in the margin\n:::\nDe .column-margin code voor tekst in de kantlijn, maar er zijn verschillende andere commando’s die handig zijn als je plaatjes wilt afbeelden in de blog posts:\n\n.column-body overspant de gebruikelijke breedte van de post\n.column-body-outset strekt zich iets uit buiten de gebruikelijke breedte\n.column-page overspant de hele pagina (inclusief beide kantlijnen)\n.column-screen overspant de breedte van het scherm\n.column-screen-inset code stopt net voor de volledige schermbreedte\n\nJe kunt deze instellen binnen een chunk-optie. Bijvoorbeeld, als je column: margin als chunk-optie instelt, krijgt de uitvoer een .column-margin code en de resulterende figuur verschijnt in de marge in plaats van onder de code. Op dezelfde manier zal het instellen van column: screen als chunk optie de uitvoer een .column-screen klasse geven en de uitvoer zal de volledige breedte beslaan. Hier is een eenvoudig voorbeeld, gebaseerd op het voorbeeld in de quarto documentatie:\n\nlibrary(leaflet)\n\nWarning: package 'leaflet' was built under R version 4.1.3\n\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  addMarkers(\n    lng=151.22251, \n    lat=-33.85943, \n    label=\"Mrs Macquarie's Chair\"\n  ) %&gt;% \n  addProviderTiles(providers$CartoDB.Positron)\n\n\n\n\n\n\nIk moet toegeven, ik ben al een beetje verliefd."
  },
  {
    "objectID": "posts/2022-05-31-van-distill-naar-quarto/van-distill-naar-quarto.html#opmerkingen-over-de-yaml-koppen",
    "href": "posts/2022-05-31-van-distill-naar-quarto/van-distill-naar-quarto.html#opmerkingen-over-de-yaml-koppen",
    "title": "Van distill naar quarto?",
    "section": "Opmerkingen over de YAML koppen",
    "text": "Opmerkingen over de YAML koppen\nDe YAML koppen die gebruikt worden voor blogposts zijn een beetje anders in quarto dan hun equivalenten in distill waren, en het kost me even om uit te zoeken hoe ik de YAML headers van mijn oude R markdown posts moet aanpassen voor de nieuwe quarto blog. Hier is een kort overzicht. Ten eerste, sommige velden vereisen bijna geen veranderingen:\n\nHet title veld is ongewijzigd. Dat was een makkelijke!\nHet date veld is in essentie ongewijzigd, behalve het feit dat er een kleine bug lijkt te zijn in datum parsing voor blogs waarvan ik zeker weet dat die snel zal verdwijnen. Als je ISO-8601 datumformaten gebruikt zoals date: \"2022-04-20\" zal het goed gaan. 3\nHet categorieën veld neemt een lijst van waarden, die (denk ik?) niet anders is dan hoe het er eerder uitzag. Om eerlijk te zijn weet ik het niet meer omdat mijn oude blog ze niet gebruikte. Ik ben er nu mee begonnen.\n\nAndere veranderingen zijn kunstmatig: - Het description veld dat ik gebruikte op mijn oude blog doet nog steeds wat het deed: het geeft een preview tekst op de listing pagina en een samenvatting bovenaan het bestand. Er is echter ook een subtitle veld dat je voor dit doel kunt gebruiken, en de uitvoer heeft hetzelfde uiterlijk en veld als mijn oude beschrijvingen, dus ik heb besloten om al mijn oude beschrijvingsvelden om te zetten naar subtitle vermeldingen. - Om een voorbeeldafbeelding te specificeren die bij een blog post hoort, gebruik je het image veld (bijv. iets als image: thumbnail.jpg) in plaats van het preview veld uit distill. - Er is een nieuw licence veld dat het creative_commons veld uit distill vervangt. Onderaan deze post zie je een “Reuse” appendix die linkt naar een licentie bestand. Om dit te genereren, heb ik een license: \"CC BY\" regel opgenomen in de YAML.\nAndere veranderingen gaan dieper:\n\nIn distill is het mogelijk om het author veld in detail te specificeren, wat de academische conventie weerspiegelt om een auteurs affiliatie te vermelden naast hun werkgever, digitale identificeerder (‘orcid record’) en contactdetails. Quarto ondersteunt dit ook, hoewel de tags iets veranderd zijn: orcid_id is nu orcid, bijvoorbeeld. Een voorbeeld hiervan wordt verderop in dit artikel getoond.\n\nHet specificeren van de inhoudsopgave is iets anders. Net als in distill, kun je de inhoudsopgave aanzetten door toc: true als regel in de YAML header op te nemen, en het toc-depth veld in te stellen om te bepalen hoe gedetailleerd de inhoudsopgave moet zijn. Maar er zijn nieuwe opties. U kunt de tekst aanpassen die boven de inhoudsopgave verschijnt en de plaats waar deze verschijnt. Ik besluit om saai te zijn en met een aantal standaard opties te gaan: toc-title: Inhoudsopgave en toc-location: left.\nEen functie in distill die ik leuk vind is dat het een citaat genereert voor elke post. Je kunt dat ook doen in quarto, en je zult onderaan deze post zien dat ik die functie hier heb gebruikt. Maar quarto beheert dit op een andere manier dan distill, en gebruikt een YAML versie van citation style language (CSL) formattering om de citatie te definiëren. Om te zien hoe het werkt, kun je de quarto pages on citations en creating citable articles doorlezen. Het is iets uitgebreider dan de distill versie, maar veel flexibeler. Voor deze blog is het zo simpel als citation: true in de YAML, maar het kan uitgebreider en geschikt voor elk academisch citatiepatroon dat je maar wilt."
  },
  {
    "objectID": "posts/2022-05-31-van-distill-naar-quarto/van-distill-naar-quarto.html#een-nieuwe-blog-maken",
    "href": "posts/2022-05-31-van-distill-naar-quarto/van-distill-naar-quarto.html#een-nieuwe-blog-maken",
    "title": "Van distill naar quarto?",
    "section": "Een nieuwe blog maken",
    "text": "Een nieuwe blog maken\nOké. Tijd om aan de slag te gaan en de standaard-blog omvormen tot een quarto-versie van mijn distill-blog. Mijn eerste stap is het verwijderen van de twee posts die bij de standaard-blog zaten, en dan deze aanmaken.\nEen map met een index.qmd bestand is het absolute minimum dat ik nodig heb om aan de slag te gaan met een nieuwe post. Ik veronderstel dat er andere manieren zijn om dit te doen, maar wat ik eigenlijk doe is het aanmaken van de map en een leeg bestand vanaf de terminal (om redenen die alleen God kent)\nmkdir posts/2022-04-20_porting-to-quarto\ntouch posts/2022-04-20_porting-to-quarto/index.qmd\nOm eerlijk te zijn, het gebruik van de terminal was overkill. Wat ik in plaats daarvan had kunnen doen, als ik RStudio had bekeken in plaats van de terminal, is de optie “New File” gebruiken in het bestandsmenu en dan de optie “Quarto Doc” kiezen. Dat maakt een nieuw titelloos quarto document dat je kunt opslaan op de juiste locatie."
  },
  {
    "objectID": "posts/2022-05-31-van-distill-naar-quarto/van-distill-naar-quarto.html#yaml-instellingen-overnemen",
    "href": "posts/2022-05-31-van-distill-naar-quarto/van-distill-naar-quarto.html#yaml-instellingen-overnemen",
    "title": "Van distill naar quarto?",
    "section": "YAML instellingen overnemen",
    "text": "YAML instellingen overnemen\nEen handige functie in quarto websites is dat YAML velden worden overgeërfd. Bijvoorbeeld, deze post heeft zijn eigen YAML header die de volgende – en alleen de volgende – velden bevat:\ntitle: \"Een distill blog overzetten naar quarto\"\nsubtitle: | \n  Net als Danielle Navarro recent heb ik mijn blog van distill naar quarto overgezet. Ik heb haar notities gevolgd en her en der aangepast. Ook voor mij lijkt dit op een veelbelovende zet en in ieder geval is het een interessanten test op het gebied van reproduceerbaar. \n  to be an interesting reproducibility test\nauthor: Danielle Navarro. bewerking Harrie Jonkman   \ndate: \"2022-05-31\"\ncategories: [Quarto, Blogging, Reproducibility]\nimage: \"img/preview.jpg\"\nDat is een beetje eigenaardig, want veel van de metadata die nodig zijn om deze post te specificeren ontbreken. De reden dat het ontbreekt is dat ik een aantal velden in het posts/_metadata.yml bestand heb geplaatst. Deze velden worden geërfd door elke blog post. Dit is de volledige inhoud van mijn post metadata bestand:\n# Bevries computer outputs\nfreeze: true\n\n# Schakel banner stijl titelblokken in\ntitle-block-banner: true\n\n# Activeer bijlage CC-licentie\nlicense: \"CC BY\"\n\n# Default voor inhoudsopgave\ntoc: true\ntoc-title: Table of contents\ntoc-location: left\n\n# Default knitr opties\nexecute:\n  echo: true\n  message: true\n  warning: true\n\n# Default author\nauthor:\n  - name: Danielle Navarro\n    url: https://djnavarro.net\n    affiliation: Voltron Data\n    affiliation-url: https://voltrondata.com\n    orcid: 0000-0001-7648-6578\n\n# Default voor velden citeren\ncitation: true\n\n\nDe bevries optie is bijzonder makkelijk in de context van bloggen. Ik adviseer deze documentatiepagina hierover te lezen!\nDat verklaart een hoop, maar als je goed kijkt zul je je realiseren dat er niets in deze velden staat dat het uitvoerformaat specificeert! In Rmarkdown zou ik hiervoor een output veld hebben opgenomen, maar in quarto heet het relevante veld format. Omdat de output voor de hele site geldt, staat dat deel van de YAML header in het _quarto.yml bestand. De relevante regels van dat bestand zijn:\nformat:\n  html:\n    theme: ember.scss\n    css: styles.css\nIk kom hier later op terug. Voor nu is het genoeg om te erkennen dat dit aangeeft dat alle pagina’s op deze site moeten worden gerenderd naar HTML documenten, en met behulp van de ember.scss en styles.css bestanden de blog stijl te specificeren."
  },
  {
    "objectID": "posts/2022-05-31-van-distill-naar-quarto/van-distill-naar-quarto.html#converteren-van-mijn-oude-posts",
    "href": "posts/2022-05-31-van-distill-naar-quarto/van-distill-naar-quarto.html#converteren-van-mijn-oude-posts",
    "title": "Van distill naar quarto?",
    "section": "Converteren van mijn oude posts",
    "text": "Converteren van mijn oude posts\nDe tijd is aangebroken voor een beetje handwerk. Hoewel quarto compatibel is met de meeste bestaande R markdown en ik er waarschijnlijk mee weg kan komen om ze ongemoeid te laten, verwacht ik dat ik op de langere termijn naar andere talen zal overstappen, dus het spreekt me aan om nu van de gelegenheid gebruik te maken om alles over te zetten naar quarto. Het hernoemen van alle index.Rmd bestanden naar index.qmd bestanden is eenvoudig genoeg en kan programmatisch worden gedaan. Maar de meeste van mijn bewerkingen vereisen een kleine hoeveelheid handmatig knutselwerk bij elke post. Niet veel, want het is vooral een kwestie van het hernoemen van een paar YAML velden. Gezien het feit dat er maar een stuk of 20 posts overgezet moeten worden, besluit ik dat het gemakkelijker is om het handmatig te doen dan om te proberen een script te schrijven om de taak te automatiseren. Ik heb het in een middag gedaan (Nou, Danielle dan ben je sneller dan mij. Mij kostte het wel enkele dagen)."
  },
  {
    "objectID": "posts/2022-05-31-van-distill-naar-quarto/van-distill-naar-quarto.html#styleren-van-de-nieuwe-blog",
    "href": "posts/2022-05-31-van-distill-naar-quarto/van-distill-naar-quarto.html#styleren-van-de-nieuwe-blog",
    "title": "Van distill naar quarto?",
    "section": "Styleren van de nieuwe blog",
    "text": "Styleren van de nieuwe blog\nTot nu toe heeft het gebruik van quarto erg “distill-achtig” gevoeld. De structuur van de blog voelt vertrouwd aan, de YAML koppen zijn vergelijkbaar in de geest (hoewel verschillend in de details), enzovoort. Als het aankomt op het aanpassen van het uiterlijk van de blog, lijkt het helemaal niet op distill, en voelt het meer als eenvoudige R markdown-sites. Quarto websites zijn bootstrap gebaseerd, en zoals besproken op de quarto theming page, komen ze met dezelfde thema’s die je misschien kent van R markdown. Als je bijvoorbeeld beslist, zoals ik deed, dat je een heel eenvoudig wit thema wil, dan zou je het “litera” thema kunnen kiezen. Om dit op je blog toe te passen, hoef je er alleen maar voor te zorgen dat je _quarto.yml bestand de volgende regels bevat:\nformat:\n  html:\n    theme: litera\n    css: styles.css\nDit zorgt ervoor dat de uitvoer zal worden weergegeven als HTML objecten, gebruikmakend van het litera bootswatch thema en het toepassen van aangepaste CSS regels die je toevoegt in het styles.css bestand.\nEen erg leuke eigenschap van quarto, als je SASS kunt gebruiken om stijlen te definiëren en iets weet over hoe de bootstrap SASS bestanden zijn georganiseerd,4 is dat het je toestaat je eigen .scss bestand te schrijven om je blog thema preciezer te definiëren, waarbij je toegang hebt tot bootstrap parameters enzovoort. Ik zou je sterk aanraden om eerst meer te lezen over het quarto theming system voordat je zelf met dit aspect aan de slag gaat, maar als je meer kennis (of meer domheid) hebt dan ik, dan lees je hier hoe ik mijn blog heb opgezet. Ten eerste, in plaats van te verwijzen naar het litera thema, verwijst de YAML in mijn _quarto.yml bestand naar mijn eigen aangepaste .scss bestand:\nformat:\n  html:\n    theme: ember.scss\n    css: styles.css\nDe inhoud van de ember.scss file ziet er (bij Danielle) als volgt uit:\n/*-- scss:defaults --*/\n\n// use litera as the base\n$theme: \"litera\" !default;\n\n// import google fonts\n@import 'https://fonts.googleapis.com/css2?family=Atkinson+Hyperlegible:ital,wght@0,400;0,700;1,400;1,700&display=swap';\n@import 'https://fonts.googleapis.com/css2?family=Fira+Code:wght@300;400;500;600;700&display=swap';\n\n// use Atkinson Hyperlegible font if available\n$font-family-sans-serif:  \"Atkinson Hyperlegible\", -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial, \"Noto Sans\", sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\", \"Noto Color Emoji\" !default;\n\n/*-- scss:rules --*/\n\n// litera is serif by default: revert to san-serif\np {\n  font-family: $font-family-sans-serif;\n}\nZoals je kunt zien, doe ik op dit moment niet veel anders dan wat kleine aanpassingen aan het litera thema, maar er is potentieel zo veel meer mee te doen dan ik heb gedaan bij het opzetten van deze blog. Ik ben van plan om hier later meer aan te sleutelen!"
  },
  {
    "objectID": "posts/2022-05-31-van-distill-naar-quarto/van-distill-naar-quarto.html#een-rss-feed-toevoegen",
    "href": "posts/2022-05-31-van-distill-naar-quarto/van-distill-naar-quarto.html#een-rss-feed-toevoegen",
    "title": "Van distill naar quarto?",
    "section": "Een RSS feed toevoegen",
    "text": "Een RSS feed toevoegen\nMijn oude distill blog had een RSS feed, en hoewel ik erken dat het steeds meer een esoterische functie is die de meeste mensen niet gebruiken, heb ik een voorliefde voor RSS. Quarto ondersteunt dit, maar het is niet standaard ingeschakeld. Wat ik moet doen is de YAML aanpassen in het index.qmd bestand dat correspondeert met de homepage, want dat is waar ik mijn primaire lijst van berichten heb. Daarin zie ik een listing veld. Alles wat ik hoef te doen is feed: true eronder te zetten en er is nu een RSS feed voor de site:\ntitle: \"Notes from a data witch\"\nsubtitle: A data science blog by Danielle Navarro\nlisting:\n  feed: true\n  contents: posts\nDe quarto sectie over feeds geeft meer informatie hierover."
  },
  {
    "objectID": "posts/2022-05-31-van-distill-naar-quarto/van-distill-naar-quarto.html#uitzetten-van-de-site",
    "href": "posts/2022-05-31-van-distill-naar-quarto/van-distill-naar-quarto.html#uitzetten-van-de-site",
    "title": "Van distill naar quarto?",
    "section": "Uitzetten van de site",
    "text": "Uitzetten van de site\nHet voorbereiden van de site om deze uit te zetten is relatief pijnloos. Ik vond het nuttig om de quarto website optie pagina te lezen voordat ik dit deed, omdat het een heleboel instellingen noemt om aan te sleutelen, meestal in het _quarto.yml bestand. Ik kies er bijvoorbeeld voor om de navigatiebalk aan te passen, de voorbeeldafbeeldingen van de sociale media, enzovoort. Uiteindelijk bereik ik het punt waar ik tevreden ben en ga ik verder met de implementatie.\nGelukkig valt er over het uitzetproces zelf niet veel te zeggen. De quarto deployment pagina bespreekt verschillende opties voor hoe je dit kunt doen. De meeste van mijn websites worden uitgerold via GitHub Pages of via Netlify. Dit is een Netlify site, dus ik volg de instructies daar en alles gaat soepel. Dit brengt me echter wel bij een ander onderwerp…"
  },
  {
    "objectID": "posts/2022-05-31-van-distill-naar-quarto/van-distill-naar-quarto.html#netlify-herleidt",
    "href": "posts/2022-05-31-van-distill-naar-quarto/van-distill-naar-quarto.html#netlify-herleidt",
    "title": "Van distill naar quarto?",
    "section": "Netlify herleidt",
    "text": "Netlify herleidt\nIk heb mijn blog op een bepaalde manier gestructureerd. Net als de standaard quarto blog, staan alle posts in de posts map, en ze hebben een systematische naam: ze hebben eerst een ISO-8601 geformatteerde datum, en dan een semantische slug. Dus de volledige URL voor deze blog post is:\nblog.djnavarro.net/posts/2022-04-20_porting-to-quarto\nDat is handig voor archiveringsdoeleinden en om alles netjes geordend te houden in mijn projectmap, maar het is ook een beetje onhandig voor het delen van links. In de praktijk is het “posts” gedeelte een beetje overbodig, en ik ga nooit twee keer dezelfde slug gebruiken, dus is het handig om het zo in te stellen dat er ook een kortere URL is voor de post,\nblog.djnavarro.net/porting-to-quarto\nen dat deze korte URL automatisch naar de langere herleidt.\nAangezien ik van plan ben om deze blog uit te rollen naar Netlify, moet ik ervoor zorgen dat wanneer de site gebouwd wordt, er een _redirects bestand wordt aangemaakt in de site map. Dit bestand moet één regel per redirect bevatten, met als eerste het “redirect from” pad, gevolgd door het “redirect to” pad. Hier is hoe die regel eruit ziet voor deze post:\n/porting-to-quarto /posts/2022-04-20_porting-to-quarto\nIk ben niet van plan om deze regels handmatig toe te voegen, dus wat ik in plaats daarvan doe is een R chunk toevoegen aan het index.qmd bestand dat correspondeert met de startpagina van de blog, met de volgende code:\n# lijst namen van de post folders\nposts &lt;- list.dirs(\n  path = here::here(\"posts\"),\n  full.names = FALSE,\n  recursive = FALSE\n)\n\n# extraheer de slugs\nslugs &lt;- gsub(\"^.*_\", \"\", posts)\n\n# regels om een netlify _redirect file toe te voegen\nredirects &lt;- paste0(\"/\", slugs, \" \", \"/posts/\", posts)\n\n# Schrijf de _redirect file\nwriteLines(redirects, here::here(\"_site\", \"_redirects\"))\nElke keer als deze site herbouwd wordt – wat meestal inhoudt dat de home page herbouwd wordt omdat die de lijst met berichten bevat – wordt het _redirects bestand vernieuwd. Er is misschien een schonere manier, maar dit werkt."
  },
  {
    "objectID": "posts/2022-05-31-van-distill-naar-quarto/van-distill-naar-quarto.html#quarto-cli",
    "href": "posts/2022-05-31-van-distill-naar-quarto/van-distill-naar-quarto.html#quarto-cli",
    "title": "Van distill naar quarto?",
    "section": "The quarto CLI",
    "text": "The quarto CLI\nIets wat ik eerder vergat te melden. Ongeveer halverwege het proces van het aanpassen van mijn oude posts om ze geschikt te maken voor de quarto-blog, heb ik besloten om RStudio niet langer te gebruiken voor de rendering, en heb ik wat tijd besteed om mezelf vertrouwd te maken met de quarto-command line interface. Ik heb nog geen specifieke beslissingen genomen over hoe mijn lange termijn workflow met quarto eruit gaat zien, maar ik vond het wel nuttig om een gevoel te krijgen voor het concept van quarto als een standalone installatie. Ik ga hier niet in detail treden, maar even kort: aan de terminal kan ik zien dat ik een aantal help opties heb,\n\nquarto help\n\n  Usage:   quarto \n  Version: 0.9.282\n                  \n\n  Description:\n\n    Quarto CLI\n\n  Options:\n\n    -h, --help     - Show this help.                            \n    -V, --version  - Show the version number for this program.  \n\n  Commands:\n\n    render          [input] [args...]  - Render input file(s) to various document types.                                                \n    serve           [input]            - Serve an interactive document.                                                                 \n    create-project  [dir]              - Create a project for rendering multiple documents                                              \n    preview         [file] [args...]   - Render and preview a Quarto document or website project. Automatically reloads the browser when\n    convert         [input]            - Convert documents to alternate representations.                                                \n    capabilities                       - Query for current capabilities (output formats, engines, kernels etc.)                         \n    check           [target]           - Check and verify correct functioning of Quarto installation and computation engines.           \n    inspect         [path]             - Inspect a Quarto project or input path. Inspecting a project returns its config and engines.   \n    tools           [command] [tool]   - Manage the installation, update, and uninstallation of useful tools.                           \n    help            [command]          - Show this help or the help of a sub-command.\n    \nVan daaruit kan ik de help documentatie voor het quarto render commando bekijken door het volgende in te typen,\n\nquarto render help\n\nenzovoort. Het doorbladeren van deze documentatie naast alle uitstekende inhoud op de quarto-website is een handige manier om extra opties te vinden. Als ik de huidige post zou willen renderen, en mijn terminal bevond zich momenteel in de hoofdmap van het project (d.w.z. mijn quarto-blog map), dan kan ik het als volgt renderen:\n\nquarto render posts/2022-04-20_porting-to-quarto/index.qmd\n\nDe mogelijkheid om dit netjes vanaf de terminal te doen lijkt een handige eigenschap van quarto, hoewel ik moet toegeven dat ik nog niet zeker weet hoe ik het zal gebruiken."
  },
  {
    "objectID": "posts/2022-05-31-van-distill-naar-quarto/van-distill-naar-quarto.html#epiloog",
    "href": "posts/2022-05-31-van-distill-naar-quarto/van-distill-naar-quarto.html#epiloog",
    "title": "Van distill naar quarto?",
    "section": "Epiloog",
    "text": "Epiloog\nToen ik aan dit proces begon was ik er niet helemaal zeker van of ik door zou zetten en de blog daadwerkelijk zou overschakelen naar quarto. De distill-blog heeft me het afgelopen tijd goed gediend en ik hou er niet van om dingen te repareren als ze niet kapot zijn. Hoe langer ik echter met quarto speelde, hoe meer het me beviel, en het proces was veel minder pijnlijk dan ik vreesde dat het zou zijn. Ik heb het gevoel dat het de dingen heeft behouden die ik leuk vind aan distill, maar deze netjes heeft geïntegreerd met andere functies (bijv. de bootstrap grid!) die ik echt miste in distill. Zo nu en dan kom ik wat kleine eigenaardigheden tegen waar sommige ruwe kantjes van quarto nog zichtbaar zijn – het is nog steeds een nieuwe tool – maar ik geniet er erg van.\nMaar hier is Harrie zelf weer. Ik twijfel nog. Vooralsnog blijf ik met distill mijn blog schrijven, maar zal wel nog wat meer meer quarto oefenen."
  },
  {
    "objectID": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html",
    "href": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html",
    "title": "ggplot stap voor stap",
    "section": "",
    "text": "De afgelopen maand toch nog weer eens naar datavisualisatie gekeken en hoe je dat met R het beste kunt doen. Twee boeken nogeens gelezen hierover. Allereerst het boek van Rob Kabacoff Data Visualization with R. Het is een handige introducte op ggploten vooral een handige tutorial voor het visualiseren van data met R. Daarnaast het boek van Clause Wilke Fundamentals of Data Visualization dat meer een algemene reflectie is op data visualisatie. Het ga je op verschillende manieren van data naar visualisatie, wat zijn de onderliggende principes en waaraan moeten we bij datavisualisatie vooral denken. Toch heb ik de afgelopen maand vooral veel geleerd van Cédric Scherer. Ik zag dat hij op de conferentie van R binnenkort een inleiding geeft op het onderwerp datavisualisatie en zo zag ik via zijn Github site verschillend materiaal over datavisualisatie. Mooie en duidelijke inleidingen en twee ervan heb ik in het Nederlands overgezet. Hier vind je in ieder geval een algemene inleiding op ggplot. zie"
  },
  {
    "objectID": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#cédrics-inleiding",
    "href": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#cédrics-inleiding",
    "title": "ggplot stap voor stap",
    "section": "Cédrics inleiding",
    "text": "Cédrics inleiding\nTerug in 2016, moest ik (Cédric vanaf hier) mijn PhD inleidende lezing voorbereiden en begon ik ggplot2 te gebruiken om mijn gegevens te visualiseren. Ik hield nooit van de syntaxis en stijl van basisplots in R, dus ik was snel verliefd op ggplot. Vooral handig was zijn ‘faceting utility’. Maar omdat ik weinig tijd had, plotte ik deze figuren met vallen en opstaan en met behulp van veel googlen. De bron waar ik altijd op terugkwam was een blog post genaamd Beautiful plotting in R: A ggplot2 cheatsheet by Zev Ross, voor het laatst bijgewerkt in januari 2016.\nNa het geven van de lezing, die een aantal fatsoenlijke plots bevatte dankzij de blog post, besloot ik om deze tutorial stap-voor-stap door te nemen. Ik heb er zoveel van geleerd en ben direct begonnen met het aanpassen van de codes. In de loop van de tijd heb ik extra code snippets, grafiektypes en bronnen toegevoegd.\nOmdat het blogartikel van Zev Ross al enkele jaren niet meer werd bijgewerkt en dit stap voor stap een unieke versie van een tutorial werd, besloot ik om de bijgewerkte versie op mijn GitHub te hosten. Nu vindt het zijn juiste plaats op deze homepage! (Plus ik heb een groot aantal updates toegevoegd-om er maar een paar te noemen: - De fantastische patchwork, ggtext en ggforce pakketten.\n- Hoe om te gaan met aangepaste lettertypen en kleuren.\n- Een verzameling van R pakketten op maat gemaakt om interactieve grafieken te maken.\n- En verschillende andere soorten grafieken, waaronder taartdiagrammen (omdat iedereen van taartdiagrammen houdt.\n\n\n\nGrafieken die we gaan maken\n\n\nGrote veranderingen die ik heb aangebracht:\n\nde R-stijlgids volgen (bv. van Hadley Wickham, Google of de stijlgidsen van de Coding Club),\n\nom de stijl en esthetiek van plots te veranderen (bijv. as-titels, legenda’s en mooie kleuren voor alle plots, niet alleen sommige),\n\nom een bijgewerkte versie te hebben die de veranderingen in ggplot2 bijhoudt (huidige versie: 3.3.2),\n\nom gegevensimport aan te passen (GitHub bron),\n\nom extra tips toe te voegen over een breed scala aan onderwerpen, waaronder bijvoorbeeld grafiekkeuze, kleurenpaletten, aanpassen van titels, toevoegen van lijnen, aanpassen van legenda’s, annotaties met labels, pijlen en boxen, multi-panel plots, interactieve visualisaties, …"
  },
  {
    "objectID": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#voorbereiding",
    "href": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#voorbereiding",
    "title": "ggplot stap voor stap",
    "section": "Voorbereiding",
    "text": "Voorbereiding\n\nU kunt het Rmarkdown-script met de code die in deze blogpost is uitgevoerd hier vinden.\n\nU kunt ook het R script met alleen de code hier downloaden.\n\nU dient de volgende pakketten te installeren om de volledige tutorial te kunnen uitvoeren:\n\n{ggplot2}, onderdeel van de {tidyverse} pakketverzameling\n\n{tidyverse} pakket verzameling, namelijk\n\n{dplyr} voor het ordenen van gegevens\n\n{tibble} voor moderne dataframes\n\n{tidyr} voor het opschonen van gegevens\n\n{forcats} voor het hanteren van factoren\n\n{colorspace} voor het manipuleren van kleuren\n\n{corrr} voor het berekenen van correlatiematrices\n\n{cowplot} voor het samenstellen van ggplots\n\n{ggdark} voor het thematiseren en inverteren van kleuren\n\n{ggforce} voor sina plots en andere coole dingen\n\n{ggrepel} voor mooie tekstlabels\n\n{ggridges} voor ridge plots\n\n{ggsci} voor mooie kleurenpaletten\n\n{ggtext} voor geavanceerde tekst rendering\n\n{ggthemes} voor extra thema’s\n\n{grid} voor het maken van grafische objecten\n\n{gridExtra} voor extra functies voor “raster”-grafieken\n{patchwork} voor multi-paneel plots\n\n{rcartocolor} voor geweldige kleurenpaletten\n\n{scico} voor perceptuele uniforme paletten\n\n{showtext} voor aangepaste lettertypen\n\n{shiny} voor interactieve apps\n\neen aantal pakketten voor interactieve visualisaties\n- {charter}\n- {echarts4r}\n- {ggiraph}\n- {highcharter}\n- {plotly}\n\n# Pakketten wel installeren als je ze niet hebt\n# Dat kan goed met Pacman, dat moet wel geinstalleerd zijn\npacman::p_load(\n  tidyverse, \n  akima,\n  colorspace, \n  corrr,  \n  cowplot,\n  ggdark, \n  ggforce,\n  ggiraph,\n  ggrepel, \n  ggridges, \n  ggsci,\n  ggtext, \n  ggthemes, \n  grid, \n  gridExtra,\n  hrbrthemes,\n  patchwork,\n  rcartocolor, \n  scico, showtext, \n  shiny,\n  plotly, \n  highcharter, \n  echarts4r)\n\n(Om pedagogische redenen en als mensen naar een plot springen, laadt Cédric het benodigde pakket naast {ggplot2} in de betreffende sectie)."
  },
  {
    "objectID": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#de-dataset",
    "href": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#de-dataset",
    "title": "ggplot stap voor stap",
    "section": "De dataset",
    "text": "De dataset\nDe studie die we gebruiken is National Morbidity and Mortality Air Pollution Study (NMMAPS). Om de plots hanteerbaar te maken, beperken we de gegevens tot Chicago en 1997-2000. Voor meer details over deze dataset, raadpleeg Roger Peng’s boek Statistical Methods in Environmental Epidemiology with R. Je kunt de gegevens die we in deze tutorial gebruiken hier downloaden (maar dat hoeft niet).\nWe kunnen de data importeren in onze R sessie, bijvoorbeeld met read_csv() uit het readr pakket. Om later bij de gegevens te kunnen, slaan we ze op in een variabele genaamd chic met behulp van de toewijzingspijl: &lt;-.\n\nchic &lt;- readr::read_csv(\"https://raw.githubusercontent.com/z3tt/ggplot-courses/main/data/chicago-nmmaps-custom.csv\")\n\nRows: 1461 Columns: 11\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr  (3): city, season, month\ndbl  (7): temp, o3, dewpoint, pm10, yday, month_numeric, year\ndate (1): date\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nDe :: wordt naamruimte genoemd en kan worden gebruikt om toegang te krijgen tot een functie zonder het pakket te laden. Hier zou je ook library(readr) eerst kunnen uitvoeren en daarna “chic &lt;- read_csv()”.\n\nLaten we zien hoe de dataset eruit ziet.\n\ntibble::glimpse(chic)\n\nRows: 1,461\nColumns: 11\n$ city          &lt;chr&gt; \"chic\", \"chic\", \"chic\", \"chic\", \"chic\", \"chic\", \"chic\", ~\n$ date          &lt;date&gt; 1997-01-01, 1997-01-02, 1997-01-03, 1997-01-04, 1997-01~\n$ temp          &lt;dbl&gt; 36.0, 45.0, 40.0, 51.5, 27.0, 17.0, 16.0, 19.0, 26.0, 16~\n$ o3            &lt;dbl&gt; 5.659256, 5.525417, 6.288548, 7.537758, 20.760798, 14.94~\n$ dewpoint      &lt;dbl&gt; 37.500, 47.250, 38.000, 45.500, 11.250, 5.750, 7.000, 17~\n$ pm10          &lt;dbl&gt; 13.052268, 41.948600, 27.041751, 25.072573, 15.343121, 9~\n$ season        &lt;chr&gt; \"Winter\", \"Winter\", \"Winter\", \"Winter\", \"Winter\", \"Winte~\n$ yday          &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1~\n$ month         &lt;chr&gt; \"Jan\", \"Jan\", \"Jan\", \"Jan\", \"Jan\", \"Jan\", \"Jan\", \"Jan\", ~\n$ month_numeric &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,~\n$ year          &lt;dbl&gt; 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 19~\n\n\n\nhead(chic, 10)\n\n# A tibble: 10 x 11\n   city  date        temp    o3 dewpoint  pm10 season  yday month month_~1  year\n   &lt;chr&gt; &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1 chic  1997-01-01  36    5.66    37.5  13.1  Winter     1 Jan          1  1997\n 2 chic  1997-01-02  45    5.53    47.2  41.9  Winter     2 Jan          1  1997\n 3 chic  1997-01-03  40    6.29    38    27.0  Winter     3 Jan          1  1997\n 4 chic  1997-01-04  51.5  7.54    45.5  25.1  Winter     4 Jan          1  1997\n 5 chic  1997-01-05  27   20.8     11.2  15.3  Winter     5 Jan          1  1997\n 6 chic  1997-01-06  17   14.9      5.75  9.36 Winter     6 Jan          1  1997\n 7 chic  1997-01-07  16   11.9      7    20.2  Winter     7 Jan          1  1997\n 8 chic  1997-01-08  19    8.68    17.8  33.1  Winter     8 Jan          1  1997\n 9 chic  1997-01-09  26   13.4     24    12.1  Winter     9 Jan          1  1997\n10 chic  1997-01-10  16   10.4      5.38 24.8  Winter    10 Jan          1  1997\n# ... with abbreviated variable name 1: month_numeric"
  },
  {
    "objectID": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#het-ggplot2-pakket",
    "href": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#het-ggplot2-pakket",
    "title": "ggplot stap voor stap",
    "section": "Het ggplot2 pakket",
    "text": "Het ggplot2 pakket\n\nggplot2 is a systeem voor het maken van grafieken gebaseerd op The Grammar of Graphics. Je levert de gegevens aan, vertelt ggplot2 hoe variabelen af te beelden met een bepaalde esthetiek, welke grafische technieken te gebruiken en het zorgt voor de details.\n\nEen ggplot is opgebouwd uit een paar basis elementen:\n\nData: De ruwe data die je wil plotten.\n\n**Geometries* geom_: De geometrische vormen die de gegevens zullen weergeven.\n\nAesthetics aes(): Esthetica van de geometrische en statistische objecten, zoals positie, kleur, grootte, vorm en transparantie.\n\nScales scale_: Kaarten tussen de gegevens en de esthetische dimensies, zoals gegevensbereik naar plotbreedte of factorwaarden naar kleuren.\n\nStatistical transformations stat_: Statistische samenvattingen van de gegevens, zoals kwantielen, passende curven, en sommen.\n\nCoordinate system coord_: De transformatie die wordt gebruikt om gegevenscoördinaten om te zetten in het vlak van de gegevensrechthoek.\n\nFacets facet_: De ordening van de gegevens in een raster van plots.\n\nVisual themes theme(): De algemene visuele standaardinstellingen van een plot, zoals achtergrond, rasters, assen, standaard lettertype, afmetingen en kleuren.\n\n\nHet aantal elementen kan variëren, afhankelijk van hoe u ze groepeert en aan wie u het vraagt."
  },
  {
    "objectID": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#een-standaard-ggplot",
    "href": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#een-standaard-ggplot",
    "title": "ggplot stap voor stap",
    "section": "Een standaard ggplot",
    "text": "Een standaard ggplot\nAllereerst, om de functionaliteit van ggplot2 te kunnen gebruiken, moeten we het pakket laden (dat we ook kunnen laden via tidyverse suite):\n\n#library(ggplot2)\nlibrary(tidyverse)\n\nDe syntax van ggplot2 is anders dan die van basic R. Volgens de basiselementen heeft een standaard ggplot drie dingen nodig die je moet specificeren: de gegevens, de esthetiek en een geometrie.\nWe beginnen altijd met het definiëren van een plot-object door ggplot(data = df) aan te roepen, wat ggplot2 alleen vertelt dat we met die data gaan werken. In de meeste gevallen wil je twee variabelen plotten-één op de x-as en één op de y-as. Dit zijn positie-esthetica en dus voegen we aes(x = var1, y = var2) toe aan de ggplot() aanroep (ja, de aes() staat voor esthetica). Er zijn echter ook gevallen waarin men één of zelfs drie of meer variabelen moet specificeren.\n\nWe specificeren de gegevens buiten aes() en voegen de variabelen toe waarnaar ggplot de esthetica binnen aes() tot uitdrukking brengt.\n\nHier maken we de variabele datum zichtbaar naar de x-positie en de variabele temp naar de y-positie. Later zullen we ook de variabelen toewijzen aan allerlei andere esthetica zoals kleur, grootte en vorm.\n\n(g &lt;- ggplot(chic, aes(x = date, y = temp)))\n\n\n\n\nHm, alleen een paneel wordt aangemaakt als je dit uitvoert. Waarom? Dat komt omdat ggplot2 niet weet hoe we de gegevens willen plotten-we moeten nog een geometrie opgeven!\nMet ggplot2 kun je het huidige ggobject opslaan in een variabele van jouw keuze door het toe te wijzen aan een variabele, in ons geval genaamd g. Je kunt dit ggobject later uitbreiden door andere lagen toe te voegen, hetzij allemaal tegelijk of door het toe te wijzen aan dezelfde of een andere variabele.\n\nDoor haakjes te gebruiken tijdens het toewijzen van een object, zal het object onmiddellijk worden afgedrukt (in plaats van g &lt;- ggplot(...) en dan g te schrijven schrijven we gewoon (g &lt;- ggplot(...))).\n\nEr zijn vele, vele verschillende geometrieën (geoms genoemd omdat elke functie gewoonlijk begint met geom_) die je standaard aan een ggplot kunt toevoegen (zie hier voor een volledige lijst) en nog meer die door uitbreidingspakketten worden aangeboden (zie hier voor een verzameling van uitbreidingspakketten). Laten we ggplot2 vertellen welke stijl we willen gebruiken, bijvoorbeeld door geom_pint() toe te voegen om een scatter plot te maken:\n\ng + geom_point()\n\n\n\n\nNooi! Maar deze data kunnen ook gevisualiseerd worden als een lijnplot (niet optimaal, maar mensen doen dit soort dingen de hele tijd). Dus voegen we simpelweg geom_line() toe en voilá:\n\ng + geom_line()\n\n\n\n\nJe kunt ook verschillende geometrische lagen combineren - en dit is waar de magie en het plezier begint!\n\ng + geom_line() + geom_point()\n\n\n\n\nDat is het voor nu over geometrieën. Geen zorgen, we gaan later nog verschillende plot types leren.\n\nVerander de eigenschappen van geometrieën\nBinnen het geom_* commando, kun je al visuele esthetiek manipuleren, zoals de kleur, vorm, en grootte van je punten. Laten we alle punten in grote vuurrode diamanten veranderen!\n\ng + geom_point(color = \"firebrick\", shape = \"diamond\", size = 2)\n\n\n\n\n\nggplot2 begrijpt zowel color als colour als de korte versie col.\n\n*Je kunt voorgedefinieerde kleuren gebruiken (hier is een volledige lijst) of hex-kleurcodes, en zelfs RGB/RGBA kleuren gebruiken met rgb() functie.\nElke geom komt met z’n eigen eigenschappen (genoemd arguments) en hetzelfde argument kan in iets anders veranderen afhankelijk van de geom die je gebruikt.\n\ng + geom_point(color = \"firebrick\", shape = \"diamond\", size = 2) +\n    geom_line(color = \"firebrick\", linetype = \"dotted\", size = .3)\n\n\n\n\n\n\nVerander het standaard ggplot2-thema\nEn om nog wat meer van ggplot’s veelzijdigheid te illustreren, laten we ons ontdoen van het grijzige standaard ggplot2 uiterlijk door een ander ingebouwd thema in te stellen, b.v. theme_bw()-door theme_set() op te roepen zullen alle volgende plots hetzelfde zwart-wit thema hebben. De rode punten zien er nu veel beter uit!\n\ntheme_set(theme_bw())\n\ng + geom_point(color = \"firebrick\")\n\n\n\n\nMeer informatie over het gebruik van ingebouwde thema’s en het aanpassen van thema’s vind je in “Working with Themes”. Vanaf het volgende hoofdstuk, zullen we ook de theme() functie gebruiken om bepaalde elementen van het thema aan te passen.\n\ntheme() is een essentieel commando om handmatig allerlei thema-elementen (teksten, rechthoeken, en lijnen) aan te passen.\n\nOm te zien welke details van een ggplot thema kunnen worden aangepast, kijk hier - en neem even de tijd, dit is een lange lijst."
  },
  {
    "objectID": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#werken-met-assen",
    "href": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#werken-met-assen",
    "title": "ggplot stap voor stap",
    "section": "Werken met assen",
    "text": "Werken met assen\n\nVerander astitels\nLaten we wat goed geschreven labels toevoegen aan de assen. Hiervoor voegen we labs() toe met een tekenreeks voor elk label dat we willen veranderen (hier x en y):\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\")\n\n\n\n\nJe kunt ook elke astitel toevoegen via xlab() en ylab(). *\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  xlab(\"Jaar\") +\n  ylab(\"Temperatuur (°F)\")\n\n\n\n\nNormaal kan je ook symbolen specificeren door gewoon het symbool zelf toe te voegen, maar onderstaande code laat ook toe om niet alleen symbolen toe te voegen, maar bv. superscripts:\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Jaar\", y = expression(paste(\"Temperatuur (\", degree ~ F, \")\"^\"(Hey, waarom zouden we metrische eenheden gebruiken?!)\")))\n\n\n\n\n\n\nVergroot de ruimte tussen assen en as-titels\ntheme() is een essentieel commando om bepaalde thema-elementen aan te passen (teksten en titels, kaders, symbolen, achtergronden, …). We gaan ze veel gebruiken! Voor nu gaan we tekstelementen wijzigen. We kunnen de eigenschappen van alle of bepaalde tekstelementen (hier as-titels) wijzigen door het overschrijven van het standaard element_text() binnen de theme() aanroep:\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  theme(axis.title.x = element_text(vjust = 0, size = 15),\n        axis.title.y = element_text(vjust = 2, size = 15))\n\n\n\n\nvjust verwijst naar de verticale uitlijning, die gewoonlijk tussen 0 en 1 ligt, maar je kunt ook waarden buiten dat bereik opgeven. Merk op dat, hoewel we de as-titel op de y-as horizontaal verplaatsen, we vjust moeten specificeren (wat correct is vanuit het perspectief van het label). Je kunt de afstand ook veranderen door de marge van beide tekst elementen op te geven:\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  theme(axis.title.x = element_text(margin = margin(t = 10), size = 15),\n        axis.title.y = element_text(margin = margin(r = 10), size = 15))\n\n\n\n\nDe labels t en r in het margin() object verwijzen naar boven en rechts, respectievelijk. Je kunt de vier marges ook opgeven als margin(t, r, b, l). Merk op dat we nu de rechtermarge moeten veranderen om de ruimte op de y-as aan te passen, niet de ondermarge.\n\nEen goede manier om de volgorde van de marges te onthouden is “t-r-ou-b-l-e”.\n\n\n\nWijzig de esthetiek van de as-titels\nWe gebruiken weer de theme() functie en wijzigen het element axis.title en/of de ondergeschikte elementen axis.title.x en axis.title.y. Binnen de element_text() kunnen we bijvoorbeeld de standaardwaarden voor size, color, en face overschrijven:\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  theme(axis.title = element_text(size = 15, color = \"firebrick\",\n                                  face = \"italic\"))\n\n\n\n\nHet face argument kan worden gebruikt om het lettertype vet or schuin of zelfd bold.italic(vet.schuin) te krijgen.\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  theme(axis.title.x = element_text(color = \"sienna\", size = 15),\n        axis.title.y = element_text(color = \"orangered\", size = 15))\n\n\n\n\n\nJe kunt ook een combinatie gebruiken van axis.title en axis.title.y, omdat axis.title.x de waarden erft van axis.title. Zie hieronder.\n\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  theme(axis.title = element_text(color = \"sienna\", size = 15),\n        axis.title.y = element_text(color = \"orangered\", size = 15))\n\n\n\n\nMen kan sommige eigenschappen voor beide assen wijzigen en andere slechts voor één of eigenschappen voor elke as afzonderlijk:\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  theme(axis.title = element_text(color = \"sienna\", size = 15, face = \"bold\"),\n        axis.title.y = element_text(face = \"bold.italic\"))\n\n\n\n\n\n\nVerander de aesthetics van de astekst\nOp dezelfde manier kun je ook het uiterlijk van de as-tekst (hier de getallen) veranderen door axis.text en/of de ondergeschikte elementen axis.text.x en axis.text.y te gebruiken:\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  theme(axis.text = element_text(color = \"dodgerblue\", size = 12),\n        axis.text.x = element_text(face = \"italic\"))\n\n\n\n\n\n\nRoteer astekst\nDoor een angle op te geven kun je alle tekstelementen draaien. Met hjust en vjust kun je de positie van de tekst achteraf horizontaal (0 = left, 1 = right) en verticaal (0 = top, 1 = bottom) aanpassen:\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Year\", y = \"Temperature (°F)\") +\n  theme(axis.text.x = element_text(angle = 50, vjust = 1, hjust = 1, size = 12))\n\n\n\n\n\n\nAstekst en tekens weghalen\nEr zal niet snel een reden voor zijn—maar zo werkt het:\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  theme(axis.ticks.y = element_blank(),\n        axis.text.y = element_blank())\n\n\n\n\nIk heb drie thema-elementen geïntroduceerd - tekst, lijnen en rechthoeken - maar eigenlijk is er nog een: element_blank() dat het element verwijdert (en dus niet als een officieel element wordt beschouwd).\n\nAls je van een theme-element af wilt, is het element altijd element_blank().\n\n\n\nHaal astitels weg\nWe zouden opnieuw theme_blank() kunnen gebruiken, maar het is veel eenvoudiger om gewoon het label te verwijderen in de labs() (of xlab()) aanroep:\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = NULL, y = \"\")\n\n\n\n\n\nMerk op dat NULL het element verwijdert (vergelijkbaar met element_blank()) terwijl lege aanhalingstekens \"\" de spatiëring voor de as-titel behouden en gewoon niets afdrukken.\n\n\n\nAsbereik beperken\nSoms wil je een bepaald bereik van je gegevens nader bekijken. Je kunt dit doen zonder je gegevens te splitsen:\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  ylim(c(0, 50))\n\nWarning: Removed 777 rows containing missing values (geom_point).\n\n\n\n\n\nAls alternatief kun je scale_y_continuous(limits = c(0, 50)) of coord_cartesian(ylim = c(0, 50)) gebruiken. De eerste verwijdert alle datapunten buiten het bereik, terwijl de tweede het zichtbare gebied aanpast en vergelijkbaar is met ylim(c(0, 50)). Je kunt je afvragen: Dus uiteindelijk leveren beide hetzelfde op. Niet echt, er is een belangrijk verschil-vergelijk de twee volgende plots:\nPlaatje2.PNG)\nJe hebt misschien gezien dat er links een lege buffer is rond de y-grens, terwijl er rechts punten tot aan de grens en zelfs verder worden uitgezet. Dit illustreert perfect het subsetting (links) versus het zoomen (rechts). Om te tonen waarom dit belangrijk is, laten we eens kijken naar een ander grafiektype, een box plot:\n\nOmdat scale_x|y_continuous() de data eerst subset, krijgen we totaal verschillende (en verkeerde, tenminste als dit niet je doel was) schattingen voor de boxp-lots! Ik hoop dat je nu niet terug hoeft te gaan naar je oude scripts om te controleren of je misschien je data hebt gemanipuleerd tijdens het plotten en verkeerde samenvattende statistieken hebt gerapporteerd in je rapport, paper of thesis…\n\n\nForceer Plot om te starten bij Origin\nVerwant hiermee, je kan R forceren om de grafiek te plotten startend bij de oorsprong:\n\nlibrary(tidyverse)\n\nchic_high &lt;- dplyr::filter(chic, temp &gt; 25, o3 &gt; 20)\n\nggplot(chic_high, aes(x = temp, y = o3)) +\n  geom_point(color = \"darkcyan\") +\n  labs(x = \"Temperatuur hoger dan 25°F\",\n       y = \"Ozon hoger dan 20 ppb\") +\n  expand_limits(x = 0, y = 0)\n\n\n\n\n\nGebruik vancoord_cartesian(xlim = c(0, NA), ylim = c(0, NA)) levert hetzelfde resultaat op.\n\n\nlibrary(tidyverse)\n\nchic_high &lt;- dplyr::filter(chic, temp &gt; 25, o3 &gt; 20)\n\nggplot(chic_high, aes(x = temp, y = o3)) +\n  geom_point(color = \"darkcyan\") +\n  labs(x = \"Temperatuur hoger dan25°F\",\n       y = \"Ozon hoger dan 20 ppb\") +\n  coord_cartesian(xlim = c(0, NA), ylim = c(0, NA))\n\n\n\n\nMaar we kunnen het ook forceren om letterlijk bij het begin te beginnen.\n\nggplot(chic_high, aes(x = temp, y = o3)) +\n  geom_point(color = \"darkcyan\") +\n  labs(x = \"Temperatuur hoger dan 25°F\",\n       y = \"Ozon hoger dan 20 ppb\") +\n  expand_limits(x = 0, y = 0) +\n  coord_cartesian(expand = FALSE, clip = \"off\")\n\n\n\n\n\nHet argument clip = \"off\" in elk coördinatensysteem, altijd beginnend met coord_*, stelt je in staat om buiten het paneelgebied te tekenen.\n\nHier zorg ik ervoor dat de tikmerken op c(0, 0) niet weggesneden worden. Voor meer details zie ook Twitter thread door Claus Wilke.\n\n\nAssen met dezelfde schalen\nLaten we ter demonstratie de temperatuur uitzetten tegen de temperatuur met wat willekeurige ruis. De coord_equal() is een coördinatensysteem met een gespecificeerde verhouding die het aantal eenheden op de y-as weergeeft dat gelijk is aan één eenheid op de x-as.\nDe standaardwaarde, ratio = 1, zorgt ervoor dat één eenheid op de x-as even lang is als één eenheid op de y-as:\n\nggplot(chic, aes(x = temp, y = temp + rnorm(nrow(chic), sd = 20))) +\n  geom_point(color = \"sienna\") +\n  labs(x = \"Temperatuur (°F)\", y = \"Temperatuur (°F) + random ruis\") +\n  xlim(c(0, 100)) + ylim(c(0, 150)) +\n  coord_fixed()\n\nWarning: Removed 49 rows containing missing values (geom_point).\n\n\n\n\n\nVerhoudingen hoger dan één maken eenheden op de y-as langer dan eenheden op de x-as, en omgekeerd:\n\nggplot(chic, aes(x = temp, y = temp + rnorm(nrow(chic), sd = 20))) +\n  geom_point(color = \"sienna\") +\n  labs(x = \"Temperatuur (°F)\", y = \"Temperatuur (°F) + random ruis\") +\n  xlim(c(0, 100)) + ylim(c(0, 150)) +\n  coord_fixed(ratio = 1/5)\n\nWarning: Removed 62 rows containing missing values (geom_point).\n\n\n\n\n\n\n\nGebruik een functie om labels te veranderen\nSoms is het handig om uw labels een beetje te wijzigen, bijvoorbeeld door eenheden of procenttekens toe te voegen zonder ze aan uw gegevens toe te voegen. In dat geval kun je een functie gebruiken:\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Jaar\", y = NULL) +\n  scale_y_continuous(label = function(x) {return(paste(x, \"Graden Fahrenheit\"))})"
  },
  {
    "objectID": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#met-titels-werken",
    "href": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#met-titels-werken",
    "title": "ggplot stap voor stap",
    "section": "Met titels werken",
    "text": "Met titels werken\n\nEen titel toevoegen\nWe kunnen een titel toevoegen via de ggtitle() functie:\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  ggtitle(\"Temperaturen in Chicago\")\n\n\n\n\nAls alternatief kun je labs() gebruiken. Hier kun je meerdere argumenten toevoegen, bijvoorbeeld een ondertitel, een onderschrift en een tag (en ook as-titels zoals eerder getoond):\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\",\n       title = \"Temperaturen in Chicago\",\n       subtitle = \"Seizoenspatroon van dagelijkse temperaturen van 1997 tot 2001\",\n       caption = \"Data: NMMAPS\",\n       tag = \"Figuur 1\")\n\n\n\n\n\n\nTitel vet maken & spatie toevoegen aan de basislijn\nNogmaals, omdat we de eigenschappen van een themaelement willen wijzigen, gebruiken we de theme() functie en zoals voor de tekstelementen axis.title en axis.text wijzigen we het lettertype en de marge. Alle volgende wijzigingen van thema-elementen werken niet alleen voor de titel, maar voor alle andere labels zoals plot.subtitle, plot.caption, plot.caption, legend.title, legend.text, axis.title en axis.text.\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\",\n       title = \"Temperaturen in Chicago\") +\n  theme(plot.title = element_text(face = \"bold\",\n                                  margin = margin(10, 0, 10, 0),\n                                  size = 14))\n\n\n\n\n\nEen mooie manier om de volgorde van de marge-argumenten te onthouden is “t-r-oub-l-e”.\n\n\n\nPositie van titels aanpassen\nDe algemene uitlijning (links, midden, rechts) wordt geregeld met hjust (dat staat voor horizontale aanpassing):\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Year\", y = NULL,\n       title = \"Temperaturen in Chicago\",\n       caption = \"Data: NMMAPS\") +\n  theme(plot.title = element_text(hjust = 1, size = 16, face = \"bold.italic\"))\n\n\n\n\nNatuurlijk is het daar ook mogelijk om de verticale uitlijning aan te passen, geregeld door vjust.\nDe gebruiker de uitlijning van de titel, ondertitel en bijschrift opgeven op basis van het paneelgebied (de standaard) of de plotmarge via plot.title.position en plot.caption.position. De laatste optie is in de meeste gevallen de betere keuze en veel mensen waren erg blij met deze nieuwe functie, omdat met name bij zeer lange y-as labels de uitlijning er verschrikkelijk uitziet:\n\n(g &lt;- ggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  scale_y_continuous(label = function(x) {return(paste(x, \"Graden Fahrenheit\"))}) +\n  labs(x = \"Jaar\", y = NULL,\n       title = \"Temperaturen in Chicago 1997-2001 \n       in Graden Fahrenheit\",\n       caption = \"Data: NMMAPS\") +\n  theme(plot.title = element_text(size = 14, face = \"bold.italic\"),\n       plot.caption = element_text(hjust = 0)))\n\n\n\n\n\ng + theme(plot.title.position = \"plot\",\n          plot.caption.position = \"plot\")\n\n\n\n\n\n\nGebruik een niet traditioneel lettertype in jouw titel\nJe kunt ook verschillende lettertypes gebruiken, niet alleen het standaard lettertype dat door ggplot wordt geleverd (en dat verschilt van besturingssysteem tot besturingssysteem). Er zijn verschillende pakketten die je helpen om lettertypes te gebruiken die op je machine geïnstalleerd zijn (en die je misschien gebruikt in je office programma). Hier gebruik ik het showtext pakket dat het makkelijk maakt om verschillende types lettertypes (TrueType, OpenType, Type 1, web fonts, etc.) te gebruiken in R plots. Nadat we het pakket hebben geladen, moet je het lettertype importeren dat ook op je apparaat moet zijn geïnstalleerd. Ik gebruik regelmatig Google fonts dat geïmporteerd kan worden met de functie font_add_google() maar je kunt ook andere lettertypen toevoegen met font_add(). (Merk op dat zelfs in het geval van het gebruik van Google fonts je het font moet installeren - en Rstudio opnieuw moet opstarten - om het font te kunnen gebruiken).\n\nlibrary(showtext)\nfont_add_google(\"Playfair Display\", ## name of Google font\n                \"Playfair\")  ## name that will be used in R\nfont_add_google(\"Bangers\", \"Bangers\")\n\nNu kunnen we die lettertype families gebruiken met - ja, je raadt het al - theme():\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\",\n       title = \"Temperaturen in Chicago\",\n       subtitle = \"Dagelijkese temperaturen in °F van 1997 tot 2001\") +\n  theme(plot.title = element_text(family = \"Bangers\", hjust = .5, size = 25),\n        plot.subtitle = element_text(family = \"Playfair\", hjust = .5, size = 15))\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not\nfound in Windows font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not\nfound in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\nJe kunt ook een niet-standaard lettertype instellen voor alle tekstelementen van jouw plots, voor meer details zie “Working with themes”. Ik ga Roboto Condensed gebruiken als het nieuwe lettertype voor alle volgende plots.\n\nfont_add_google(\"Roboto Condensed\", \"Roboto Condensed\")\ntheme_set(theme_bw(base_size = 12, base_family = \"Roboto Condensed\"))\n\n(Voorheen gebruikte deze handleiding het extrafont pakket, dat het tot vorig jaar prima deed. Plotseling kon ik geen nieuwe lettertypen meer toevoegen en nadat ik een nieuwe laptop had, vond het pakket helemaal geen lettertypen meer… Ik stel nu meestal het ragg pakket voor. Het is me echter niet gelukt om het te laten werken voor mijn homepage dus gebruik ik het showtext pakket dat ook geweldig is met het enige verschil dat je het lettertype dat je wilt gebruiken expliciet moet importeren met showtext. Het lijkt er echter op dat er enkele technische details zijn die niet optimaal door showtext worden opgelost, zodat je het pakket misschien als allerlaatste redmiddel wilt gebruiken.).\n\n\nAfstand veranderen in multi-line tekst\nJe kunt het lineheight argument gebruiken om de afstand tussen de regels te veranderen. In dit voorbeeld, heb ik de regels samengeknepen (lineheight &lt; 1).\n\n\nWijzig de spatiëring in meer-regelige tekst\nJe kunt het lineheight argument gebruiken om de afstand tussen de regels te veranderen. In dit voorbeeld, heb ik de regels samengeknepen (lineheight &lt; 1).\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Year\", y = \"Temperature (°F)\") +\n  ggtitle(\"Temperatures in Chicago\\nfrom 1997 to 2001\") +\n  theme(plot.title = element_text(lineheight = .8, size = 16))\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not\nfound in Windows font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not\nfound in Windows font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not\nfound in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database"
  },
  {
    "objectID": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#met-legendas-werken",
    "href": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#met-legendas-werken",
    "title": "ggplot stap voor stap",
    "section": "Met legenda’s werken",
    "text": "Met legenda’s werken\nWe zullen de plot een kleurcode geven op basis van het seizoen. Of om het op een meer ggplot-achtige manier te zeggen: we koppelen de variabele season aan de aesthetic color. Een leuke eigenschap van ggplot2 is dat het standaard een legenda toevoegt wanneer het een variabele aan een esthetiek koppelt. Je kunt zien dat de titel van de legenda standaard is wat we in het kleur argument hebben opgegeven:\n\nggplot(chic,\n       aes(x = date, y = temp, color = season)) +\n  geom_point() +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\nDe legenda uitzetten\nEen van de eerste vragen is vaak: “Hoe kan ik de legenda uitzetten?”.\nHet is vrij eenvoudig en werkt altijd met theme(legend.position = \"none\"):\n\nggplot(chic,\n       aes(x = date, y = temp, color = season)) +\n  geom_point() +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  theme(legend.position = \"none\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nJe kunt ook guides(color = \"none\") of scale_color_discrete(guide = \"none\") gebruiken, afhankelijk van het specifieke geval. Terwijl de verandering van het thema-element alle legenda’s in een keer verwijdert, kunt je met de laatstgenoemde opties bepaalde legenda’s verwijderen terwijl sommige andere behouden blijven:\n\nggplot(chic,\n       aes(x = date, y = temp,\n           color = season, shape = season)) +\n  geom_point() +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  guides(color = \"none\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\nHier, bijvoorbeeld, behouden we de legende voor de vormen terwijl we die voor de kleuren weggooien.\n\n\nVerwijder legenda titels\nZoals we al geleerd hebben, gebruik element_blank() om niets te tekenen:\n\nggplot(chic, aes(x = date, y = temp, color = season)) +\n  geom_point() +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  theme(legend.title = element_blank())\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\nJe kunt hetzelfde bereiken door de legendanaam op NULL te zetten, ofwel via scale_color_discrete(name = NULL) of labs(color = NULL).\n\n\n\nLegenda positie veranderen\nAls men de legenda niet rechts wil plaatsen, gebruikt men legend.position als argument in theme. Mogelijke posities zijn “boven”, “rechts” (wat de standaard is), “onder”, en “links”.You can achieve the same by setting the legend name to NULL, either via scale_color_discrete(name = NULL) or labs(color = NULL).\n\nggplot(chic, aes(x = date, y = temp, color = season)) +\n  geom_point() +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  theme(legend.position = \"top\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\nJe kunt de legenda ook binnen het paneel plaatsen door een vector op te geven met relatieve x en y coördinaten variërend van 0 (links of onder) tot 1 (rechts of boven):\n\nggplot(chic, aes(x = date, y = temp, color = season)) +\n  geom_point() +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\",\n       color = NULL) +\n  theme(legend.position = c(.15, .15),\n        legend.background = element_rect(fill = \"transparent\"))\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\nHier overschrijf ik ook de standaard witte legende achtergrond met een transparante vulling om er zeker van te zijn dat de legende geen data punten verbergt.\n\n\nLegenda richting wijzigen\nZoals je gezien hebt, is de legende-richting standaard verticaal, maar horizontaal als je de “top” of “bottom” positie kiest. Maar u kunt de richting ook veranderen zoals u wilt:\n\nggplot(chic, aes(x = date, y = temp, color = season)) +\n  geom_point() +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  theme(legend.position = c(.5, .97),\n        legend.background = element_rect(fill = \"transparent\")) +\n  guides(color = guide_legend(direction = \"horizontal\"))\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\nStijl van de legende titel veranderen\nU kunt het uiterlijk van de legendatitel veranderen door het thema-element legend.title aan te passen:\n\nggplot(chic, aes(x = date, y = temp, color = season)) +\n  geom_point() +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  theme(legend.title = element_text(family = \"Playfair\",\n                                    color = \"chocolate\",\n                                    size = 14, face = \"bold\"))\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not\nfound in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\nTitel legenda wijzigen\nDe eenvoudigste manier om de titel van de legenda te veranderen is de labs() laag:\n\nggplot(chic, aes(x = date, y = temp, color = season)) +\n  geom_point() +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\",\n       color = \"Seizoenen\\ngeïndiceerd\\ndoor kleuren:\") +\n  theme(legend.title = element_text(family = \"Playfair\",\n                                    color = \"chocolate\",\n                                    size = 14, face = \"bold\"))\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\nDe legenda details kunnen worden veranderd via scale_color_discrete(name = \"title\") of guides(color = guide_legend(\"title\")):\n\nggplot(chic, aes(x = date, y = temp, color = season)) +\n  geom_point() +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  theme(legend.title = element_text(family = \"Playfair\",\n                                    color = \"chocolate\",\n                                    size = 14, face = \"bold\")) +\n  scale_color_discrete(name = \"Seizoenen\\ngeïndiceerd\\ndoor kleuren:\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\nVerander de volgorde van de legenda toetsen\nWe kunnen dit bereiken door de niveaus van season te veranderen:\n\nchic$season &lt;-\n  factor(chic$season,\n         levels = c(\"Winter\", \"Spring\", \"Summer\", \"Autumn\"))\n\nggplot(chic, aes(x = date, y = temp, color = season)) +\n  geom_point() +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\nLegendalabels veranderen\nWe gaan de seizoenen vervangen door de maanden die ze bestrijken door een vector van namen op te geven in de scale_color_discrete() aanroep:\n\nggplot(chic, aes(x = date, y = temp, color = season)) +\n  geom_point() +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  scale_color_discrete(\n    name = \"Seasons:\",\n    labels = c(\"Mar—May\", \"Jun—Aug\", \"Sep—Nov\", \"Dec—Feb\")\n  ) +\n  theme(legend.title = element_text(\n    family = \"Playfair\", color = \"chocolate\", size = 14, face = 2\n  ))\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\nAchtergrondvakken in de legenda wijzigen\nOm de achtergrondkleur (vulling) van de legenda toetsen te veranderen, passen we de instelling voor het thema element legend.key aan:\n\nggplot(chic, aes(x = date, y = temp, color = season)) +\n  geom_point() +\n  labs(x = \"Year\", y = \"Temperature (°F)\") +\n  theme(legend.key = element_rect(fill = \"darkgoldenrod1\"),\n        legend.title = element_text(family = \"Playfair\",\n                                    color = \"chocolate\",\n                                    size = 14, face = 2)) +\n  scale_color_discrete(\"Seasons:\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\nAls je ze helemaal weg wilt hebben, gebruik dan fill = NA of fill = \"transparent\".\n\n\nFormaat van de legenda-symbolen wijzigen\nPunten in de legenda kunnen een beetje verloren gaan met de standaard grootte, vooral zonder de kaders. Om de standaardgrootte op te heffen gebruikt men weer de guides laag zoals hier:\n\nggplot(chic, aes(x = date, y = temp, color = season)) +\n  geom_point() +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  theme(legend.key = element_rect(fill = NA),\n        legend.title = element_text(color = \"chocolate\",\n                                    size = 14, face = 2)) +\n  scale_color_discrete(\"Seasons:\") +\n  guides(color = guide_legend(override.aes = list(size = 6)))\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not\nfound in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\nLaat een laag uit de legende\nLaten we zeggen dat je twee verschillende geoms hebt gemapt op dezelfde variabele. Bijvoorbeeld, kleur als esthetiek voor zowel een puntlaag als een tapijtlaag van dezelfde gegevens. Standaard komen zowel de punten als de “lijn” zo in de legenda terecht:\n\nggplot(chic, aes(x = date, y = temp, color = season)) +\n  geom_point() +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  geom_rug()\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\nJe kunt show.legend = FALSE gebruiken om een laag in de legenda uit te schakelen:\n\nggplot(chic, aes(x = date, y = temp, color = season)) +\n  geom_point() +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  geom_rug(show.legend = FALSE)\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\nHandmatig toevoegen van legenda items\nggplot2 voegt niet automatisch een legenda toe, tenzij je de esthetiek (kleur, grootte etc.) aan een variabele koppelt. Er zijn echter momenten dat ik een legenda wil hebben, zodat het duidelijk is wat je aan het plotten bent.\nHier is de standaard:\n\nggplot(chic, aes(x = date, y = o3)) +\n  geom_line(color = \"gray\") +\n  geom_point(color = \"darkorange2\") +\n  labs(x = \"Jaar\", y = \"Ozon\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nWe kunnen een legenda forceren door een leidraad naar een variabele te mappen. We mappen de lijnen en de punten met aes() en we mappen niet naar een variabele in onze dataset, maar naar een enkele string (zodat we voor elk maar één kleur krijgen).\n\nggplot(chic, aes(x = date, y = o3)) +\n  geom_line(aes(color = \"line\")) +\n  geom_point(aes(color = \"points\")) +\n  labs(x = \"Jaar\", y = \"Ozon\") +\n  scale_color_discrete(\"Type:\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\nWe komen in de buurt, maar dit is niet wat we willen. We willen grijs en rood! Om de kleur te veranderen, gebruiken we scale_color_manual(). Daarnaast veranderen we de legenda met de guide() functie.\nVoila! We hebben nu een plot met grijze lijnen en rode punten, en een enkele grijze lijn en een enkel rood punt als legenda symbolen:\n\nggplot(chic, aes(x = date, y = o3)) +\n  geom_line(aes(color = \"line\")) +\n  geom_point(aes(color = \"points\")) +\n  labs(x = \"Jaar\", y = \"Ozon\") +\n  scale_color_manual(name = NULL,\n                     guide = \"legend\",\n                     values = c(\"points\" = \"darkorange2\",\n                                \"line\" = \"gray\")) +\n  guides(color = guide_legend(override.aes = list(linetype = c(1, 0),\n                                                  shape = c(NA, 16))))\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\nGebruik andere legenda stijlen\nDe standaard-legenda voor categorische variabelen zoals season is een guide_legend() zoals je in verschillende eerdere voorbeelden hebt gezien. Als je een continue variabele mapt naar een esthetiek, zal ggplot2 standaard geen guide_legend() gebruiken maar guide_colorbar() (of guide_colourbar()):\n\nggplot(chic,\n       aes(x = date, y = temp, color = temp)) +\n  geom_point() +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\", color = \"Temperatuur (°F)\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\nEchter, door guide_legend() te gebruiken kun je de legenda dwingen om discrete kleuren te tonen voor een gegeven aantal breuken zoals in het geval van een categorische variabele:\n\nggplot(chic,\n       aes(x = date, y = temp, color = temp)) +\n  geom_point() +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\", color = \"Temperatuur (°F)\") +\n  guides(color = guide_legend())\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\nJe kunt ook binned scales gebruiken:\n\nggplot(chic,\n       aes(x = date, y = temp, color = temp)) +\n  geom_point() +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\", color = \"Temperatuur (°F)\") +\n  guides(color = guide_bins())\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n… of schalen als discrete colorbars:\n\nggplot(chic,\n       aes(x = date, y = temp, color = temp)) +\n  geom_point() +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\", color = \"Temperatuur (°F)\") +\n  guides(color = guide_colorsteps())\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database"
  },
  {
    "objectID": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#werken-met-achtergronden-rasterlijnen",
    "href": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#werken-met-achtergronden-rasterlijnen",
    "title": "ggplot stap voor stap",
    "section": "Werken met Achtergronden & Rasterlijnen",
    "text": "Werken met Achtergronden & Rasterlijnen\nEr zijn manieren om het hele uiterlijk van uw plot te veranderen met één functie (zie “Working with themes” sectie hieronder) maar als je alleen de kleuren van sommige elementen wilt veranderen, kunt je dat ook doen.\n\nDe achtergrondkleur van het paneel veranderen\nOm de achtergrondkleur (vulling) van het paneelgebied (d.w.z. het gebied waar de gegevens worden uitgezet) te veranderen, moet het thema-element panel.background worden aangepast:\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"#1D8565\", size = 2) +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  theme(panel.background = element_rect(\n    fill = \"#64D2AA\", color = \"#64D2AA\", size = 2)\n  )\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nMerk op dat de echte kleur - de omtrek van de achtergrond van het paneel - niet verandert, ook al hebben we die gespecificeerd. Dit komt omdat er een laag bovenop de panel.background zit, namelijk panel.border. Zorg er wel voor dat je hier een transparante vulling gebruikt, anders worden je gegevens verborgen achter deze laag. In het volgende voorbeeld illustreer ik dat door een semitransparante hex kleur te gebruiken voor het fill argument in element_rect:\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"#1D8565\", size = 2) +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  theme(panel.border = element_rect(\n    fill = \"#64D2AA99\", color = \"#64D2AA\", size = 2))\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\nRasterlijnen veranderen\nEr zijn twee soorten rasterlijnen: grote rasterlijnen die de vinkjes aangeven en kleine rasterlijnen tussen de grote rasterlijnen. Je kunt deze allemaal veranderen door de standaardwaarden voor panel.grid te overschrijven of voor elke set rasterlijnen afzonderlijk, panel.grid.major en panel.grid.minor.\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  theme(panel.grid.major = element_line(color = \"gray10\", size = .5),\n        panel.grid.minor = element_line(color = \"gray70\", size = .25))\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nJe kunt zelfs instellingen opgeven voor de vier verschillende niveaus:\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  theme(panel.grid.major = element_line(size = .5, linetype = \"dashed\"),\n        panel.grid.minor = element_line(size = .25, linetype = \"dotted\"),\n        panel.grid.major.x = element_line(color = \"red1\"),\n        panel.grid.major.y = element_line(color = \"blue1\"),\n        panel.grid.minor.x = element_line(color = \"red4\"),\n        panel.grid.minor.y = element_line(color = \"blue4\"))\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nEn natuurlijk kun je sommige of alle rasterlijnen verwijderen als je dat wilt:\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  theme(panel.grid.minor = element_blank())\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  theme(panel.grid = element_blank())\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\nAfstand tussen rasterlijnen wijzigen\nBovendien kun je ook de onderbrekingen tussen de grote en kleine rasterlijnen bepalen:\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  scale_y_continuous(breaks = seq(0, 100, 10),\n                     minor_breaks = seq(0, 100, 2.5))\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\nVerander de achtergrondkleur van de plot\nOp dezelfde manier, om de achtergrondkleur (vulling) van het plot gebied te veranderen, moet men het theme element plot.background aanpassen:\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  theme(plot.background = element_rect(fill = \"gray60\",\n                                       color = \"gray30\", size = 2))\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nJe kunt een unieke achtergrondkleur krijgen door ofwel dezelfde kleuren in panel.background en plot.background te zetten, of door de achtergrondvulling van het paneel op \"transparent\" of NA te zetten:\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  theme(panel.background = element_rect(fill = NA),\n        plot.background = element_rect(fill = \"gray60\",\n                                       color = \"gray30\", size = 2))\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\nWerken met marges\nSoms is het handig om een beetje ruimte toe te voegen aan de plotmarge. Net als bij de vorige voorbeelden kunnen we een argument gebruiken voor de theme() functie. In dit geval is het argument plot.margin. Zoals in het vorige voorbeeld hebben we de standaard marge al geïllustreerd door de achtergrondkleur te veranderen met plot.background.\nLaten we nu extra ruimte toevoegen aan zowel links als rechts. Het argument, plot.margin, kan overweg met verschillende eenheden (cm, inches, etc.) maar vereist het gebruik van de functie unit uit het pakket grid om de eenheden te specificeren. Je kunt ofwel dezelfde waarde opgeven voor alle zijden (het makkelijkst via rep(x, 4)) of bepaalde afstanden voor elk. Hier gebruik ik een marge van 1 cm aan de boven- en onderkant, 3 cm marge aan de rechterkant, en 8 cm marge aan de linkerkant.\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  theme(plot.background = element_rect(fill = \"gray60\"),\n        plot.margin = margin(t = 1, r = 3, b = 1, l = 8, unit = \"cm\"))\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nDe volgorde van de margezijden is boven, rechts, onder, links-een mooie manier om deze volgorde te onthouden is “trouble dat sorteert de eerste letter van de vier zijden.\n\nJe kunt ook unit() gebruiken in plaats van margin().\n\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  theme(plot.background = element_rect(fill = \"gray60\"),\n        plot.margin = unit(c(1, 3, 1, 8), \"cm\"))\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database"
  },
  {
    "objectID": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#werken-met-multi-paneel-plots",
    "href": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#werken-met-multi-paneel-plots",
    "title": "ggplot stap voor stap",
    "section": "Werken met multi-paneel plots",
    "text": "Werken met multi-paneel plots\nHet ggplot2 pakket heeft twee mooie functies voor het maken van multi-panel plots, facets genaamd. Ze zijn verwant maar een beetje verschillend: facet_wrap creëert in wezen een lint van plots gebaseerd op een enkele variabele terwijl facet_grid een raster van twee variabelen overspant.\n\nMaak een raster van kleine veelvouden gebaseerd op twee variabelen\nIn het geval van twee variabelen, doet facet_grid het werk. Hier bepaalt de volgorde van de variabelen het aantal rijen en kolommen:\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"orangered\", alpha = .3) +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  facet_grid(year ~ season)\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nOm van rij- naar kolomindeling te veranderen kun je facet_grid(year ~ season) veranderen in facet_grid(season ~ year).\n\n\nMaak kleine veelvouden gebaseerd op één variabele\nfacet_wrap maakt een facet van een enkele variabele, geschreven met een tilde ervoor: facet_wrap(~ variable). Het uiterlijk van deze subplots wordt geregeld door de argumenten ncol en nrow:\n\ng &lt;-\n  ggplot(chic, aes(x = date, y = temp)) +\n    geom_point(color = \"chartreuse4\", alpha = .3) +\n    labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))\n\ng + facet_wrap(~ year)\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nDienovereenkomstig kunt je de plots rangschikken zoals je wilt, in plaats van als een matrix in één rij…\n\ng + facet_wrap(~ year, nrow = 1)\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n… of zelfs als een asymmetrisch raster van percelen:\n\ng + facet_wrap(~ year, ncol = 3) + theme(axis.title.x = element_text(hjust = .15))\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\nAssen vrij laten lopen\nDe standaard voor multi-panel plots in ggplot2 is om gelijkwaardige schalen te gebruiken in elk paneel. Maar soms wil je toestaan dat de gegevens van een paneel zelf de schaal bepalen. Dit is vaak geen goed idee omdat het de gebruiker een verkeerde indruk kan geven over de data. Maar soms is het wel degelijk nuttig en om dit te doen kunt u scales = \"free\" instellen:\n\ng + facet_wrap(~ year, nrow = 2, scales = \"free\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nMerk op dat zowel de x-as als de y-as verschillen in hun bereik!\n\n\nGebruik facet_wrap met twee variabelen\nDe functie facet_wrap kan ook twee variabelen aannemen:\n\ng + facet_wrap(year ~ season, nrow = 4, scales = \"free_x\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nBij het gebruik van facet_wrap heeft u nog steeds controle over het ontwerp van het grid: u kunt het aantal plots per rij en kolom herschikken en u kunt ook alle assen vrij laten rondlopen. In tegenstelling hiermee neemt facet_grid ook een free argument, maar laat het alleen per kolom of rij vrij rondlopen:\n\ng + facet_grid(year ~ season, scales = \"free_x\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\nStijl van strookteksten wijzigen\nDoor thema te gebruiken, kunt u het uiterlijk van de strook tekst (d.w.z. de titel voor elk facet) en de strook tekstvakken wijzigen:\n\ng + facet_wrap(~ year, nrow = 1, scales = \"free_x\") +\n  theme(strip.text = element_text(face = \"bold\", color = \"chartreuse4\",\n                                  hjust = 0, size = 20),\n        strip.background = element_rect(fill = \"chartreuse3\", linetype = \"dotted\"))\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not\nfound in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nDe volgende two functions adapted from this answer by Claus Wilke, maakt de auteur van het ggtext pakket het mogelijk om specifieke labels te markeren in combinatie met element_textbox() dat wordt geleverd door ggtext.\n\nlibrary(ggtext)\nlibrary(rlang)\n\nWarning: package 'rlang' was built under R version 4.1.3\n\n\n\nAttaching package: 'rlang'\n\n\nThe following objects are masked from 'package:purrr':\n\n    %@%, as_function, flatten, flatten_chr, flatten_dbl, flatten_int,\n    flatten_lgl, flatten_raw, invoke, splice\n\nelement_textbox_highlight &lt;- function(..., hi.labels = NULL, hi.fill = NULL,\n                                      hi.col = NULL, hi.box.col = NULL, hi.family = NULL) {\n  structure(\n    c(element_textbox(...),\n      list(hi.labels = hi.labels, hi.fill = hi.fill, hi.col = hi.col, hi.box.col = hi.box.col, hi.family = hi.family)\n    ),\n    class = c(\"element_textbox_highlight\", \"element_textbox\", \"element_text\", \"element\")\n  )\n}\n\nelement_grob.element_textbox_highlight &lt;- function(element, label = \"\", ...) {\n  if (label %in% element$hi.labels) {\n    element$fill &lt;- element$hi.fill %||% element$fill\n    element$colour &lt;- element$hi.col %||% element$colour\n    element$box.colour &lt;- element$hi.box.col %||% element$box.colour\n    element$family &lt;- element$hi.family %||% element$family\n  }\n  NextMethod()\n}\n\nNu kun je het gebruiken en bijvoorbeeld alle strookteksten opgeven:\n\ng + facet_wrap(year ~ season, nrow = 4, scales = \"free_x\") +\n  theme(\n    strip.background = element_blank(),\n    strip.text = element_textbox_highlight(\n      family = \"Playfair\", size = 12, face = \"bold\",\n      fill = \"white\", box.color = \"chartreuse4\", color = \"chartreuse4\",\n      halign = .5, linetype = 1, r = unit(5, \"pt\"), width = unit(1, \"npc\"),\n      padding = margin(5, 0, 3, 0), margin = margin(0, 1, 3, 1),\n      hi.labels = c(\"1997\", \"1998\", \"1999\", \"2000\"),\n      hi.fill = \"chartreuse4\", hi.box.col = \"black\", hi.col = \"white\"\n    )\n  )\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not\nfound in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(aes(color = season == \"Summer\"), alpha = .3) +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  facet_wrap(~ season, nrow = 1) +\n  scale_color_manual(values = c(\"gray40\", \"firebrick\"), guide = \"none\") +\n  theme(\n    axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),\n    strip.background = element_blank(),\n    strip.text = element_textbox_highlight(\n      size = 12, face = \"bold\",\n      fill = \"white\", box.color = \"white\", color = \"gray40\",\n      halign = .5, linetype = 1, r = unit(0, \"pt\"), width = unit(1, \"npc\"),\n      padding = margin(2, 0, 1, 0), margin = margin(0, 1, 3, 1),\n      hi.labels = \"Summer\", hi.family = \"Bangers\",\n      hi.fill = \"firebrick\", hi.box.col = \"firebrick\", hi.col = \"white\"\n    )\n  )\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\nEen paneel van verschillende plots maken\nEr zijn verschillende manieren om plots te combineren. De makkelijkste aanpak is volgens mij het patchwork package van Thomas Lin Pedersen:\n\np1 &lt;- ggplot(chic, aes(x = date, y = temp,\n                       color = season)) +\n        geom_point() +\n        geom_rug() +\n        labs(x = \"Jaar\", y = \"Temperatuur (°F)\")\n\np2 &lt;- ggplot(chic, aes(x = date, y = o3)) +\n        geom_line(color = \"gray\") +\n        geom_point(color = \"darkorange2\") +\n        labs(x = \"Jaar\", y = \"Ozon\")\n\nlibrary(patchwork)\np1 + p2\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nWe kunnen de volgorde veranderen door beide plots te “verdelen” (en let op de uitlijning, ook al heeft de ene een legende en de andere niet!):\n\np1 / p2\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nEn ook geneste plots zijn mogelijk!\n\n(g + p2) / p1\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n(Let op de uitlijning van de plots, hoewel slechts één plot een legenda heeft).\nAls alternatief biedt het cowplot package van Claus Wilke de functionaliteit om meerdere plots te combineren (en vele andere goede utilities):\n\nlibrary(cowplot)\nplot_grid(plot_grid(g, p1), p2, ncol = 1)\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Roboto Condensed' not found in PostScript font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Roboto Condensed' not found in PostScript font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n… en ook doet gridExtra package het goed:\n\nlibrary(gridExtra)\ngrid.arrange(g, p1, p2,\n             layout_matrix = rbind(c(1, 2), c(3, 3)))\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nHetzelfde idee van het definiëren van een lay-out kan worden gebruikt met patchwork, waarmee complexe composities kunnen worden gemaakt:\n\nlayout &lt;- \"\nAABBBB#\nAACCDDE\n##CCDD#\n##CC###\n\"\n\np2 + p1 + p1 + g + p2 +\n  plot_layout(design = layout)\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database"
  },
  {
    "objectID": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#werken-met-kleuren",
    "href": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#werken-met-kleuren",
    "title": "ggplot stap voor stap",
    "section": "Werken met kleuren",
    "text": "Werken met kleuren\nVoor eenvoudige toepassingen is het werken met kleuren in ggplot2 rechttoe rechtaan. Voor een meer gevorderde behandeling van het onderwerp moet je waarschijnlijk Hadley’s boek aanschaffen, dat een goede dekking heeft. Andere goede bronnen zijn het R Cookbook en de `color sectie in de R Graph Gallery door Yan Holtz.\nEr zijn twee belangrijke verschillen als het gaat om kleuren in ggplot2. Beide argumenten, color en fill, kunnen\n\ngespecificeerd als enkele kleur of\n\ntoegekend aan variabelen.\n\nZoals je in het begin van deze tutorial al hebt gezien, worden variabelen die binnen de aesthetiek vallen gecodeerd door variabelen en die erbuiten vallen zijn eigenschappen die niets met de variabelen te maken hebben. Deze complete nonsens plot met het aantal records per jaar en seizoen illustreert dat feit:\n\nggplot(chic, aes(year)) +\n  geom_bar(aes(fill = season), color = \"grey\", size = 2) +\n  labs(x = \"Jaar\", y = \"Observaties\", fill = \"Seizoen:\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\nAfzonderlijke kleuren specificeren\nStatische, enkele kleuren zijn eenvoudig te gebruiken. We kunnen een enkele kleur specificeren voor een geom:\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"steelblue\", size = 2) +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n… en in het geval dat het beide biedt, een color (omtrekkleur) en een fill (vulkleur):\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(shape = 21, size = 2, stroke = 1,\n             color = \"#3cc08f\", fill = \"#c08f3c\") +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nTian Zheng van Columbia heeft een handige [PDF van R kleuren] gemaakt (http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf). Natuurlijk kun je ook hex kleurcodes opgeven (gewoon als strings zoals in het voorbeeld hierboven), maar ook RGB of RGBA waarden (via de rgb() functie: rgb(red, green, blue, alpha)).\n\n\nKleuren toewijzen aan variabelen\nIn ggplot2 worden kleuren die aan variabelen zijn toegekend, gewijzigd via de functies scale_color_* en scale_fill_*. Om kleuren te gebruiken met uw gegevens, moet u vooral weten of u te maken heeft met een categorische of continue variabele. Het kleurenpalet moet worden gekozen afhankelijk van het type variabele, waarbij sequentiële of divergerende kleurenpaletten worden gebruikt voor continue variabelen en kwalitatieve kleurenpaletten voor categorische variabelen:\n\n\n\nHands on data visualization by Jack Dougherty and Ilya Ilyankou\n\n\n\n\nKwalitatieve variabelen\nKwalitatieve of categorische variabelen staan voor soorten gegevens die in groepen (categorieën) kunnen worden ingedeeld. De variabele kan verder worden gespecificeerd als nominaal, ordinaal, en binair (dichotomisch). Voorbeelden van kwalitatieve/categorische variabelen zijn:\n\n\n\nGemaakt door Allison Horst\n\n\nHet standaard categorisch kleurenpalet ziet er als volgt uit:\n\n(ga &lt;- ggplot(chic, aes(x = date, y = temp, color = season)) +\n  geom_point() +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\", color = NULL))\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\nHandmatig kwalitatieve kleuren kiezen\nJe kunt je eigen set kleuren kiezen en ze toewijzen aan een categorische variabele via de functie scale_*_manual() (de * kan color, colour, of fill zijn). Het aantal gespecificeerde kleuren moet overeenkomen met het aantal categorieën:\n\nga + scale_color_manual(values = c(\"dodgerblue4\",\n                                   \"darkolivegreen4\",\n                                   \"darkorchid3\",\n                                   \"goldenrod1\"))\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\nGebruik Ingebouwde Kwalitatieve Kleurenpaletten\nDe ColorBrewer palettes is een populair online hulpmiddel voor het selecteren van kleurenschema’s voor kaarten. De verschillende kleurensets zijn ontworpen om aantrekkelijke kleurenschema’s te maken met een gelijkaardig uitzicht, variërend van drie tot twaalf. Deze paletten zijn beschikbaar als ingebouwde functies in het ggplot2 pakket en kunnen worden toegepast door scale_*_brewer() aan te roepen:\n\nga + scale_color_brewer(palette = \"Set1\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\nJe kunt alle beschikbare kleurenpaletten bekijken via RColorBrewer::display.brewer.all().\n\nEr zijn veel uitbreidingspakketten die extra kleurpaletten bieden. Het gebruik ervan verschilt afhankelijk van de manier waarop het pakket is ontworpen. Voor een uitgebreid overzicht van kleurenpaletten die in R beschikbaar zijn, zie de collection provided by Emil Hvitfeldt. Men kan ook zijn paletteer package gebruiken, een uitgebreide verzameling van kleurenpaletten in R die een consistente syntax gebruikt.\nVoorbeelden:\nHet ggthemes package bijvoorbeeld geeft R gebruikers toegang tot de Tableau kleuren. Tableau is een beroemde visualisatie software met een bekend kleuren palet.\n\nlibrary(ggthemes)\nga + scale_color_tableau()\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\nHet ggsci package biedt kleurenpaletten voor wetenschappelijke tijdschriften en sci-fi thema’s. Wil je een plot met kleuren die eruit zien alsof ze gepubliceerd zijn in Science of Nature? Hier ga je!\n\nlibrary(ggsci)\ng1 &lt;- ga + scale_color_aaas()\ng2 &lt;- ga + scale_color_npg()\n\nlibrary(patchwork)\n(g1 + g2) * theme(legend.position = \"top\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\nKwantitatieve variabelen\nKwantitatieve variabelen vertegenwoordigen een meetbare grootheid en zijn dus numeriek. Kwantitatieve gegevens kunnen verder worden ingedeeld in continue of discrete gegevens:\n\n\n\nGemaakt door Allison Horst\n\n\nIn ons voorbeeld veranderen we de variabele die we willen kleuren in ozon, een continue variabele die sterk gerelateerd is aan temperatuur (hogere temperatuur = hogere ozon). De functie scale_*_gradient() is een sequentiële gradiënt terwijl scale_*_gradient2() divergerend is.\nHier is het standaard ggplot2 sequentieel kleurenschema voor continue variabelen:\n\ngb &lt;- ggplot(chic, aes(x = date, y = temp, color = temp)) +\n  geom_point() +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\", color = \"Temperatuur (°F):\")\n\ngb + scale_color_continuous()\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\nDeze code laat dezelfde plot zien:\n\ngb + scale_color_gradient()\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\nEn hier is het divergerende standaard kleurschema:\n\nmid &lt;- mean(chic$temp)  ## midpoint\n\ngb + scale_color_gradient2(midpoint = mid)\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\nHandmatig een sequentieel kleurenschema instellen\nJe kunt handmatig geleidelijk veranderende kleurenpaletten instellen voor continue variabelen via scale_*_gradient():\n\ngb + scale_color_gradient(low = \"darkkhaki\",\n                          high = \"darkgreen\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\nTemperatuurgegevens zijn normaal verdeeld, dus wat denk je van een divergerende kleurenschema (in plaats van sequentieel)… Voor divergerende kleuren kun je de scale_*_gradient2() functie gebruiken:\n\ngb + scale_color_gradient2(midpoint = mid, low = \"#dd8a0b\",\n                           mid = \"grey92\", high = \"#32a676\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\nHet mooie Viridis Kleurenpalet\nDe viridis kleurenpaletten maken jouw plots niet alleen mooi en goed waarneembaar, maar ook gemakkelijker te lezen door mensen met kleurenblindheid en goed af te drukken in grijstinten. U kunt testen hoe uw grafieken er onder verschillende vormen van kleurenblindheid uitzien met het dichromate pakket.\nEn ze worden nu ook geleverd met ggplot2! De volgende multi-panel plot illustreert drie van de vier viridis paletten:\n\np1 &lt;- gb + scale_color_viridis_c() + ggtitle(\"'viridis' (default)\")\np2 &lt;- gb + scale_color_viridis_c(option = \"inferno\") + ggtitle(\"'inferno'\")\np3 &lt;- gb + scale_color_viridis_c(option = \"plasma\") + ggtitle(\"'plasma'\")\np4 &lt;- gb + scale_color_viridis_c(option = \"cividis\") + ggtitle(\"'cividis'\")\n\nlibrary(patchwork)\n(p1 + p2 + p3 + p4) * theme(legend.position = \"bottom\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not\nfound in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\nHet is ook mogelijk om de viridis kleurenpaletten te gebruiken voor discrete variabelen:\n\nga + scale_color_viridis_d(guide = \"none\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\nKwantitatieve kleurenpaletten uit uitbreidingspakketten gebruiken\nDe vele uitbreidingspakketten bieden niet alleen bijkomende categorische kleurenpaletten, maar ook sequentiële, divergerende en zelfs cyclische paletten. Nogmaals, ik verwijs je naar de geweldige verzameling geleverd door Emil Hvitfeldt voor een overzicht.\nVoorbeelden:\nHet rcartocolors pakket koppelt het prachtige CARTOcolors aan ggplot2 en bevat verschillende van mijn meest gebruikte paletten:\n\nlibrary(rcartocolor)\ng1 &lt;- gb + scale_color_carto_c(palette = \"BurgYl\")\ng2 &lt;- gb + scale_color_carto_c(palette = \"Earth\")\n\n(g1 + g2) * theme(legend.position = \"bottom\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\nHet scico pakket biedt toegang tot de kleurenpaletten ontwikkeld door Fabio Crameri. Deze kleurenpaletten zijn niet alleen mooi en vaak ongebruikelijk, maar ook een goede keuze omdat ze ontwikkeld zijn om perceptueel uniform en geordend te zijn. Bovendien werken ze ook voor mensen met een kleurentekort en in grijstinten:\n\nlibrary(scico)\ng1 &lt;- gb + scale_color_scico(palette = \"berlin\")\ng2 &lt;- gb + scale_color_scico(palette = \"hawaii\", direction = -1)\n\n(g1 + g2) * theme(legend.position = \"bottom\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\nKleurpaletten naderhand aanpassen\nSinds de laatste versie van ggplot2 3.0.0, kan men de esthetiek van de lagen wijzigen nadat ze zijn toegewezen aan de gegevens. Of zoals ggplot2 het zegt: “Gebruik after_scale() om de evaluatie van de mapping te markeren voor nadat de gegevens zijn geschaald.”\nDus waarom niet meteen de aangepaste kleuren gebruiken? Aangezien ggplot2 slechts één kleur en één vul schaal kan behandelen, is dit een interessante functionaliteit. Kijk eens naar het volgende voorbeeld waar we invert_color() uit het ggdark [package] gebruiken (https://github.com/nsgrantham/ggdark):\n\nggplot(chic, aes(date, temp, color = temp)) +\n  geom_point(size = 5) +\n  geom_point(aes(color = temp,\n                 color = after_scale(invert_color(color))),\n             size = 2) +\n  scale_color_scico(palette = \"hawaii\", guide = \"none\") +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\")\n\nWarning: Duplicated aesthetics after name standardisation: colour\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nHet achteraf veranderen van het kleurenschema is vooral leuk met functies uit de ggdark en colorspace pakketten, namelijk invert_color(), lighten(), darken() en desature(). Je kunt deze functies zelfs combineren. Hier plotten we een boxplot met beide argumenten, color en fill:\n\nlibrary(colorspace)\n\nggplot(chic, aes(date, temp)) +\n  geom_boxplot(aes(color = season,\n                   fill = after_scale(desaturate(lighten(color, .6), .6))),\n               size = 1) +\n  scale_color_brewer(palette = \"Dark2\", guide = \"none\") +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nMerk op dat je de kleur en/of vulling moet opgeven in de aes() van de respectievelijke geom_*() of stat_*() om after_scale() te laten werken.\n\nDit lijkt een beetje ingewikkeld voor nu-je zou gewoon de color en fill schalen voor beide kunnen gebruiken. Ja, dat is waar, maar denk eens aan gevallen waarin je meerdere color en/of fill schalen nodig hebt. In zo’n geval zou het onzinnig zijn om de fill schaal te bezetten met een iets donkerder versie van het palet dat voor color wordt gebruikt."
  },
  {
    "objectID": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#werken-met-themas",
    "href": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#werken-met-themas",
    "title": "ggplot stap voor stap",
    "section": "Werken met Thema’s",
    "text": "Werken met Thema’s\n\nDe algemene uitzetstijl veranderen\nU kunt het uiterlijk van de plot veranderen door thema’s te gebruiken. ggplot2 wordt geleverd met acht ingebouwde thema’s:\n\n\n\nDe acht ingebouwde thema’s\n\n\nEr zijn verschillende pakketten die aanvullende thema’s leveren, sommige zelfs met verschillende standaard kleurenpaletten. Jeffrey Arnold heeft bijvoorbeeld de bibliotheek ggthemes samengesteld met verschillende aangepaste thema’s die populaire ontwerpen imiteren. Voor een lijst kunt u de ggthemes package site bezoeken. Zonder enige codering kun je gewoon verschillende stijlen aanpassen, waarvan sommige bekend staan om hun stijl en esthetiek.\nHier is een voorbeeld van het kopiëren van de plotting style in het The Economist magazine door gebruik te maken van theme_economist() en scale_color_economist():\n\nlibrary(ggthemes)\n\nggplot(chic, aes(x = date, y = temp, color = season)) +\n  geom_point() +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  ggtitle(\"Ups en downs van Chicago's dagelijkse temperaturen\") +\n  theme_economist() +\n  scale_color_economist(name = NULL)\n\n\n\n\nEen ander voorbeeld is de plottenstijl van Tufte, een minimaal inktthema gebaseerd op Edward Tufte’s boek The Visual Display of Quantitative Information. Dit is het boek dat Minard’s grafiek van Napoleon’s opmars naar Rusland populair maakte als een van de beste statistische tekeningen ooit gemaakt. Tufte’s plots werden beroemd door het purisme in hun stijl. Maar kijk zelf maar:\n\nchic_2000 &lt;- filter(chic, year == 2000)\n\nggplot(chic_2000, aes(x = temp, y = o3)) +\n  geom_point() +\n  labs(x = \"Temperatuur (°F)\", y = \"Ozon\") +\n  ggtitle(\"Temperatuur en Ozon niveaus in het jaar 2000 in Chicago\") +\n  theme_tufte()\n\n\n\n\nIk heb het aantal datapunten hier beperkt, gewoon om in Tufte’s minimalistische stijl te passen. Als je van deze manier van plotten houdt, kijk dan eens naar deze blog entry over het maken van verschillende Tufte plots in R.\nEen ander mooi pakket met moderne thema’s en een preset van niet-standaard lettertypen is het hrbrthemes pakket door Bob Rudis met verschillende lichte maar ook donkere thema’s:\n\nlibrary(hrbrthemes)\n\nggplot(chic, aes(x = temp, y = o3)) +\n  geom_point(aes(color = dewpoint), show.legend = FALSE) +\n  labs(x = \"Temperatuur (°F)\", y = \"Ozon\") +\n  ggtitle(\"Temperatuur en Ozonniveaus in Chicago\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\nVerander het lettertype van alle tekstelementen\nHet is ongelofelijk eenvoudig om de instellingen van alle tekst elementen in een keer te veranderen. Alle thema’s komen met een argument genaamd basis_familie:\n\ng &lt;- ggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"firebrick\") +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\",\n       title = \"Temperaturen in Chicago\")\n\ng + theme_bw(base_family = \"Playfair\")\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not\nfound in Windows font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not\nfound in Windows font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not\nfound in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\nVerander de grootte van alle tekst elementen\nDe thema_*() functies hebben ook een aantal andere basis_* argumenten. Als je het standaard thema bekijkt (zie hoofdstuk “Maak en gebruik je eigen thema” hieronder) zul je zien dat de grootte van alle elementen relatief (rel()) is ten opzichte van de base_size. Als gevolg hiervan kunt u eenvoudig de base_size veranderen als u de leesbaarheid van uw plots wilt vergroten:\n\ng + theme_bw(base_size = 20, base_family = \"Roboto Condensed\")\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not\nfound in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\nDe grootte van alle lijn- en rechthoekelementen wijzigen\nOp dezelfde manier kun je de grootte van alle elementen van het type lijn en recht wijzigen:\n\ng + theme_bw(base_line_size = 1, base_rect_size = 1)\n\n\n\n\n\n\nMaak je eigen thema\nAls je het thema voor een hele sessie wilt veranderen, kun je theme_set gebruiken zoals in theme_set(theme_bw()). Het standaard thema heet theme_gray. Als je je eigen aangepaste thema wilt maken, zou je de code direct uit het grijze thema kunnen halen en aanpassen. Merk op dat de rel() functie de maten verandert ten opzichte van de base_size.\n\ntheme_gray\n\nfunction (base_size = 11, base_family = \"\", base_line_size = base_size/22, \n    base_rect_size = base_size/22) \n{\n    half_line &lt;- base_size/2\n    t &lt;- theme(line = element_line(colour = \"black\", size = base_line_size, \n        linetype = 1, lineend = \"butt\"), rect = element_rect(fill = \"white\", \n        colour = \"black\", size = base_rect_size, linetype = 1), \n        text = element_text(family = base_family, face = \"plain\", \n            colour = \"black\", size = base_size, lineheight = 0.9, \n            hjust = 0.5, vjust = 0.5, angle = 0, margin = margin(), \n            debug = FALSE), axis.line = element_blank(), axis.line.x = NULL, \n        axis.line.y = NULL, axis.text = element_text(size = rel(0.8), \n            colour = \"grey30\"), axis.text.x = element_text(margin = margin(t = 0.8 * \n            half_line/2), vjust = 1), axis.text.x.top = element_text(margin = margin(b = 0.8 * \n            half_line/2), vjust = 0), axis.text.y = element_text(margin = margin(r = 0.8 * \n            half_line/2), hjust = 1), axis.text.y.right = element_text(margin = margin(l = 0.8 * \n            half_line/2), hjust = 0), axis.ticks = element_line(colour = \"grey20\"), \n        axis.ticks.length = unit(half_line/2, \"pt\"), axis.ticks.length.x = NULL, \n        axis.ticks.length.x.top = NULL, axis.ticks.length.x.bottom = NULL, \n        axis.ticks.length.y = NULL, axis.ticks.length.y.left = NULL, \n        axis.ticks.length.y.right = NULL, axis.title.x = element_text(margin = margin(t = half_line/2), \n            vjust = 1), axis.title.x.top = element_text(margin = margin(b = half_line/2), \n            vjust = 0), axis.title.y = element_text(angle = 90, \n            margin = margin(r = half_line/2), vjust = 1), axis.title.y.right = element_text(angle = -90, \n            margin = margin(l = half_line/2), vjust = 0), legend.background = element_rect(colour = NA), \n        legend.spacing = unit(2 * half_line, \"pt\"), legend.spacing.x = NULL, \n        legend.spacing.y = NULL, legend.margin = margin(half_line, \n            half_line, half_line, half_line), legend.key = element_rect(fill = \"grey95\", \n            colour = NA), legend.key.size = unit(1.2, \"lines\"), \n        legend.key.height = NULL, legend.key.width = NULL, legend.text = element_text(size = rel(0.8)), \n        legend.text.align = NULL, legend.title = element_text(hjust = 0), \n        legend.title.align = NULL, legend.position = \"right\", \n        legend.direction = NULL, legend.justification = \"center\", \n        legend.box = NULL, legend.box.margin = margin(0, 0, 0, \n            0, \"cm\"), legend.box.background = element_blank(), \n        legend.box.spacing = unit(2 * half_line, \"pt\"), panel.background = element_rect(fill = \"grey92\", \n            colour = NA), panel.border = element_blank(), panel.grid = element_line(colour = \"white\"), \n        panel.grid.minor = element_line(size = rel(0.5)), panel.spacing = unit(half_line, \n            \"pt\"), panel.spacing.x = NULL, panel.spacing.y = NULL, \n        panel.ontop = FALSE, strip.background = element_rect(fill = \"grey85\", \n            colour = NA), strip.text = element_text(colour = \"grey10\", \n            size = rel(0.8), margin = margin(0.8 * half_line, \n                0.8 * half_line, 0.8 * half_line, 0.8 * half_line)), \n        strip.text.x = NULL, strip.text.y = element_text(angle = -90), \n        strip.text.y.left = element_text(angle = 90), strip.placement = \"inside\", \n        strip.placement.x = NULL, strip.placement.y = NULL, strip.switch.pad.grid = unit(half_line/2, \n            \"pt\"), strip.switch.pad.wrap = unit(half_line/2, \n            \"pt\"), plot.background = element_rect(colour = \"white\"), \n        plot.title = element_text(size = rel(1.2), hjust = 0, \n            vjust = 1, margin = margin(b = half_line)), plot.title.position = \"panel\", \n        plot.subtitle = element_text(hjust = 0, vjust = 1, margin = margin(b = half_line)), \n        plot.caption = element_text(size = rel(0.8), hjust = 1, \n            vjust = 1, margin = margin(t = half_line)), plot.caption.position = \"panel\", \n        plot.tag = element_text(size = rel(1.2), hjust = 0.5, \n            vjust = 0.5), plot.tag.position = \"topleft\", plot.margin = margin(half_line, \n            half_line, half_line, half_line), complete = TRUE)\n    ggplot_global$theme_all_null %+replace% t\n}\n&lt;bytecode: 0x0000000023963ba0&gt;\n&lt;environment: namespace:ggplot2&gt;\n\n\nLaten we nu de standaard themafunctie wijzigen en het resultaat bekijken:\n\ntheme_custom &lt;- function (base_size = 12, base_family = \"Roboto Condensed\") {\n  half_line &lt;- base_size/2\n  theme(\n    line = element_line(color = \"black\", size = .5,\n                        linetype = 1, lineend = \"butt\"),\n    rect = element_rect(fill = \"white\", color = \"black\",\n                        size = .5, linetype = 1),\n    text = element_text(family = base_family, face = \"plain\",\n                        color = \"black\", size = base_size,\n                        lineheight = .9, hjust = .5, vjust = .5,\n                        angle = 0, margin = margin(), debug = FALSE),\n    axis.line = element_blank(),\n    axis.line.x = NULL,\n    axis.line.y = NULL,\n    axis.text = element_text(size = base_size * 1.1, color = \"gray30\"),\n    axis.text.x = element_text(margin = margin(t = .8 * half_line/2),\n                               vjust = 1),\n    axis.text.x.top = element_text(margin = margin(b = .8 * half_line/2),\n                                   vjust = 0),\n    axis.text.y = element_text(margin = margin(r = .8 * half_line/2),\n                               hjust = 1),\n    axis.text.y.right = element_text(margin = margin(l = .8 * half_line/2),\n                                     hjust = 0),\n    axis.ticks = element_line(color = \"gray30\", size = .7),\n    axis.ticks.length = unit(half_line / 1.5, \"pt\"),\n    axis.ticks.length.x = NULL,\n    axis.ticks.length.x.top = NULL,\n    axis.ticks.length.x.bottom = NULL,\n    axis.ticks.length.y = NULL,\n    axis.ticks.length.y.left = NULL,\n    axis.ticks.length.y.right = NULL,\n    axis.title.x = element_text(margin = margin(t = half_line),\n                                vjust = 1, size = base_size * 1.3,\n                                face = \"bold\"),\n    axis.title.x.top = element_text(margin = margin(b = half_line),\n                                    vjust = 0),\n    axis.title.y = element_text(angle = 90, vjust = 1,\n                                margin = margin(r = half_line),\n                                size = base_size * 1.3, face = \"bold\"),\n    axis.title.y.right = element_text(angle = -90, vjust = 0,\n                                      margin = margin(l = half_line)),\n    legend.background = element_rect(color = NA),\n    legend.spacing = unit(.4, \"cm\"),\n    legend.spacing.x = NULL,\n    legend.spacing.y = NULL,\n    legend.margin = margin(.2, .2, .2, .2, \"cm\"),\n    legend.key = element_rect(fill = \"gray95\", color = \"white\"),\n    legend.key.size = unit(1.2, \"lines\"),\n    legend.key.height = NULL,\n    legend.key.width = NULL,\n    legend.text = element_text(size = rel(.8)),\n    legend.text.align = NULL,\n    legend.title = element_text(hjust = 0),\n    legend.title.align = NULL,\n    legend.position = \"right\",\n    legend.direction = NULL,\n    legend.justification = \"center\",\n    legend.box = NULL,\n    legend.box.margin = margin(0, 0, 0, 0, \"cm\"),\n    legend.box.background = element_blank(),\n    legend.box.spacing = unit(.4, \"cm\"),\n    panel.background = element_rect(fill = \"white\", color = NA),\n    panel.border = element_rect(color = \"gray30\",\n                                fill = NA, size = .7),\n    panel.grid.major = element_line(color = \"gray90\", size = 1),\n    panel.grid.minor = element_line(color = \"gray90\", size = .5,\n                                    linetype = \"dashed\"),\n    panel.spacing = unit(base_size, \"pt\"),\n    panel.spacing.x = NULL,\n    panel.spacing.y = NULL,\n    panel.ontop = FALSE,\n    strip.background = element_rect(fill = \"white\", color = \"gray30\"),\n    strip.text = element_text(color = \"black\", size = base_size),\n    strip.text.x = element_text(margin = margin(t = half_line,\n                                                b = half_line)),\n    strip.text.y = element_text(angle = -90,\n                                margin = margin(l = half_line,\n                                                r = half_line)),\n    strip.text.y.left = element_text(angle = 90),\n    strip.placement = \"inside\",\n    strip.placement.x = NULL,\n    strip.placement.y = NULL,\n    strip.switch.pad.grid = unit(0.1, \"cm\"),\n    strip.switch.pad.wrap = unit(0.1, \"cm\"),\n    plot.background = element_rect(color = NA),\n    plot.title = element_text(size = base_size * 1.8, hjust = .5,\n                              vjust = 1, face = \"bold\",\n                              margin = margin(b = half_line * 1.2)),\n    plot.title.position = \"panel\",\n    plot.subtitle = element_text(size = base_size * 1.3,\n                                 hjust = .5, vjust = 1,\n                                 margin = margin(b = half_line * .9)),\n    plot.caption = element_text(size = rel(0.9), hjust = 1, vjust = 1,\n                                margin = margin(t = half_line * .9)),\n    plot.caption.position = \"panel\",\n    plot.tag = element_text(size = rel(1.2), hjust = .5, vjust = .5),\n    plot.tag.position = \"topleft\",\n    plot.margin = margin(base_size, base_size, base_size, base_size),\n    complete = TRUE\n  )\n}\n\n\nJe kunt alleen de standaardwaarden overschrijven voor alle elementen die je wilt veranderen. Hier heb ik ze allemaal opgesomd, zodat je kunt zien dat je letterlijk alles kunt veranderen!\n\nKijk eens naar de gewijzigde esthetiek met de nieuwe look van het paneel en de rasterlijnen, alsook de assen, teksten en titels:\n\ntheme_set(theme_custom())\n\nggplot(chic, aes(x = date, y = temp, color = season)) +\n  geom_point() + labs(x = \"Jaar\", y = \"Temperatuur (°F)\") + guides(color = FALSE)\n\nWarning: `guides(&lt;scale&gt; = FALSE)` is deprecated. Please use `guides(&lt;scale&gt; =\n\"none\")` instead.\n\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not\nfound in Windows font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not\nfound in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nDeze manier om het plotontwerp te veranderen wordt sterk aanbevolen! Het stelt je in staat om snel elk element van jouw plots te veranderen door het eenmaal te veranderen. Je kunt binnen enkele seconden al uw resultaten in een congruente stijl plotten en het aan andere behoeften aanpassen (b.v. een presentatie met grotere lettergrootte of tijdschriftvereisten).\n\n\nHet huidige thema bijwerken\nJe kunt ook snelle veranderingen instellen met theme_update():\n\ntheme_custom &lt;- theme_update(panel.background = element_rect(fill = \"gray60\"))\n\nggplot(chic, aes(x = date, y = temp, color = season)) +\n  geom_point() + labs(x = \"Jaar\", y = \"Temperatuur (°F)\") + guides(color = FALSE)\n\nWarning: `guides(&lt;scale&gt; = FALSE)` is deprecated. Please use `guides(&lt;scale&gt; =\n\"none\")` instead.\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nVoor verdere oefeningen gaan we ons eigen thema gebruiken met een witte vulling en zonder de kleine rasterlijnen:\n\ntheme_custom &lt;- theme_update(panel.background = element_rect(fill = \"white\"),\n                             panel.grid.major = element_line(size = .5),\n                             panel.grid.minor = element_blank())"
  },
  {
    "objectID": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#werken-met-lijnen",
    "href": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#werken-met-lijnen",
    "title": "ggplot stap voor stap",
    "section": "Werken met lijnen",
    "text": "Werken met lijnen\n\nHorizontaal of verticaal lijnen toevoegen aan een plot\nHet kan zijn dat je een bepaald bereik of een bepaalde drempel wilt markeren, wat je kunt doen door een lijn te plotten op gedefinieerde coördinaten met geom_hline() (voor “horizontale lijnen”) of geom_vline() (voor “verticale lijnen”):\n\nggplot(chic, aes(x = date, y = temp, color = o3)) +\n  geom_point() +\n  geom_hline(yintercept = c(0, 73)) +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\ng &lt;- ggplot(chic, aes(x = temp, y = dewpoint)) +\n  geom_point(color = \"dodgerblue\", alpha = .5) +\n  labs(x = \"Temperatuur (°F)\", y = \"Dauwpunt\")\n\ng +\n  geom_vline(aes(xintercept = median(temp)), size = 1.5,\n             color = \"firebrick\", linetype = \"dashed\") +\n  geom_hline(aes(yintercept = median(dewpoint)), size = 1.5,\n             color = \"firebrick\", linetype = \"dashed\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nAls je een lijn wilt toevoegen waarvan de helling niet 0 of 1 is, dan moet je geom_abline() gebruiken. Dit is bijvoorbeeld het geval als je een regressielijn wilt toevoegen met de argumenten intercept en helling:\n\nreg &lt;- lm(dewpoint ~ temp, data = chic)\n\ng +\n  geom_abline(intercept = coefficients(reg)[1],\n              slope = coefficients(reg)[2],\n              color = \"darkorange2\", size = 1.5) +\n  labs(title = paste0(\"y = \", round(coefficients(reg)[2], 2),\n                      \" * x + \", round(coefficients(reg)[1], 2)))\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not\nfound in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\nLater zullen we leren hoe we met één commando een lineaire fit kunnen toevoegen met stat_smooth(method = \"lm\"). Er kunnen echter andere redenen zijn om een lijn met een gegeven helling toe te voegen en dit is hoe men dat doet.\n\n\nEen lijn toevoegen binnen een Plot\nDe vorige benaderingen besloegen altijd het hele bereik van het plotpaneel. Soms oms wil je alleen een bepaald gebied markeren of lijnen gebruiken voor annotaties. In dit geval is geom_linerange() er om te helpen:\n\ng +\n  ## vertical line\n  geom_linerange(aes(x = 50, ymin = 20, ymax = 55),\n                 color = \"steelblue\", size = 2) +\n  ## horizontal line\n  geom_linerange(aes(xmin = -Inf, xmax = 25, y = 0),\n                 color = \"red\", size = 1)\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nOf je kunt geom_segment() gebruiken om lijnen te tekenen met een helling verschillend van 0 en 1:\n\ng +\n  geom_segment(aes(x = 50, xend = 75,\n                   y = 20, yend = 45),\n               color = \"purple\", size = 2)\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\nKromme lijnen en pijlen toevoegen aan een Plot\ngeom_curve() voegt krommen toe. Nou ja, en rechte lijnen als je wilt:\n\ng +\n  geom_curve(aes(x = 0, y = 60, xend = 75, yend = 0),\n             size = 2, color = \"tan\") +\n  geom_curve(aes(x = 0, y = 60, xend = 75, yend = 0),\n             curvature = -0.7, angle = 45,\n             color = \"darkgoldenrod1\", size = 1) +\n  geom_curve(aes(x = 0, y = 60, xend = 75, yend = 0),\n             curvature = 0, size = 1.5)\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nDezelfde geom kan worden gebruikt om pijlen te tekenen:\n\ng +\n  geom_curve(aes(x = 0, y = 60, xend = 75, yend = 0),\n             size = 2, color = \"tan\",\n             arrow = arrow(length = unit(0.07, \"npc\"))) +\n  geom_curve(aes(x = 5, y = 55, xend = 70, yend = 5),\n             curvature = -0.7, angle = 45,\n             color = \"darkgoldenrod1\", size = 1,\n             arrow = arrow(length = unit(0.03, \"npc\"),\n                           type = \"closed\",\n                           ends = \"both\"))\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database"
  },
  {
    "objectID": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#werken-met-tekst",
    "href": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#werken-met-tekst",
    "title": "ggplot stap voor stap",
    "section": "Werken met tekst",
    "text": "Werken met tekst\n\nLabels toevoegen aan uw gegevens\nSoms willen wij onze gegevenspunten labelen. Om overlapping en verdringing door tekstlabels te voorkomen, gebruiken we een 1% steekproef van de originele data, die gelijkelijk de vier seizoenen vertegenwoordigt. We gebruiken geom_label() dat een nieuwe esthetiek bevat, genaamd label:\n\nset.seed(2020)\n\nlibrary(dplyr)\nsample &lt;- chic %&gt;%\n  dplyr::group_by(season) %&gt;%\n  dplyr::sample_frac(0.01)\n\n## code without pipes:\n## sample &lt;- sample_frac(group_by(chic, season), .01)\n\nggplot(sample, aes(x = date, y = temp, color = season)) +\n  geom_point() +\n  geom_label(aes(label = season), hjust = .5, vjust = -.5) +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  xlim(as.Date(c('1997-01-01', '2000-12-31'))) +\n  ylim(c(0, 90)) +\n  theme(legend.position = \"none\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nOké, het vermijden van overlappende labels is niet gelukt. Maar maak je geen zorgen, we gaan het in een minuutje oplossen!\n\nJe kunt ook geom_text() gebruiken als je niet van kaders rond je labels houdt.\n\n\nggplot(sample, aes(x = date, y = temp, color = season)) +\n  geom_point() +\n  geom_text(aes(label = season), fontface = \"bold\",\n            hjust = .5, vjust = -.25) +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  xlim(as.Date(c('1997-01-01', '2000-12-31'))) +\n  ylim(c(0, 90)) +\n  theme(legend.position = \"none\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nEen cool ding is het ggrepel pakket dat geoms levert voor ggplot2 om overlappende tekst af te stoten zoals in onze voorbeelden hierboven. We vervangen gewoon geom_text() door geom_text_repel() en geom_label() door geom_label_repel():\n\nlibrary(ggrepel)\n\nggplot(sample, aes(x = date, y = temp, color = season)) +\n  geom_point() +\n  geom_label_repel(aes(label = season), fontface = \"bold\") +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  theme(legend.position = \"none\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nHet ziet er misschien mooier uit met gevulde vakjes, dus we mappen season naar fill in plaats van naar color en stellen een witte kleur in voor de tekst:\n\nggplot(sample, aes(x = date, y = temp)) +\n  geom_point(data = chic, size = .5) +\n  geom_point(aes(color = season), size = 1.5) +\n  geom_label_repel(aes(label = season, fill = season),\n                   color = \"white\", fontface = \"bold\",\n                   segment.color = \"grey30\") +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  theme(legend.position = \"none\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nDit werkt ook voor de pure tekst labels door gebruik te maken van geom_text_repel(). Kijk eens naar alle gebruiksvoorbeelden.\n\n\nTekst annotaties toevoegen\nEr zijn verschillende manieren waarop je annotaties kunt toevoegen aan een ggplot. We kunnen weer geom_text() of geom_label() gebruiken:\n\ng &lt;-\n  ggplot(chic, aes(x = temp, y = dewpoint)) +\n  geom_point(alpha = .5) +\n  labs(x = \"Temperatuur (°F)\", y = \"Dauwpunt\")\n\ng +\n  geom_text(aes(x = 25, y = 60,\n                label = \"Dit is een bruikbare annotatie\"))\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nEchter, nu heeft ggplot één tekstlabel getekend per datapunt - dat zijn 1.461 labels en je ziet er maar één! Je kunt dit oplossen door het stat argument op \"unique\" te zetten:\n\ng +\n  geom_text(aes(x = 25, y = 60,\n                label = \"Dit is een bruikbare annotatie\"),\n            stat = \"unique\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nTussen haakjes, natuurlijk kun je de eigenschappen van de weergegeven tekst veranderen:\n\ng +\n  geom_text(aes(x = 25, y = 60,\n                label = \"Dit is een bruikbare annotatie\"),\n            stat = \"unique\", family = \"Bangers\",\n            size = 7, color = \"darkcyan\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nAls je een van de facetfuncties gebruikt om uw gegevens te visualiseren, kunt je in de problemen komen. Eén ding is dat je de annotatie misschien maar één keer wilt opnemen:\n\nann &lt;- data.frame(\n  o3 = 30,\n  temp = 20,\n  season = factor(\"Summer\", levels = levels(chic$season)),\n  label = \"Hier is genoeg ruimte \\nvoor wat annotaties.\"\n)\n\ng &lt;-\n  ggplot(chic, aes(x = o3, y = temp)) +\n  geom_point() +\n  labs(x = \"Ozon\", y = \"Temperatuur (°F)\")\n\ng +\n  geom_text(data = ann, aes(label = label),\n            size = 7, fontface = \"bold\",\n            family = \"Roboto Condensed\") +\n  facet_wrap(~season)\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nEen andere uitdaging zijn facetten in combinatie met vrije schalen die in uw tekst kunnen snijden:\n\ng +\n  geom_text(aes(x = 23, y = 97,\n                label = \"Dit is geen bruikbare annotatie\"),\n            size = 5, fontface = \"bold\") +\n  scale_y_continuous(limits = c(NA, 100)) +\n  facet_wrap(~season, scales = \"free_x\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nEen oplossing is om vooraf het middelpunt van de as, hier x, te berekenen:\n\nlibrary(tidyverse)\n(ann &lt;-\n  chic %&gt;%\n  group_by(season) %&gt;%\n  summarize(o3 = min(o3, na.rm = TRUE) +\n              (max(o3, na.rm = TRUE) - min(o3, na.rm = TRUE)) / 2))\n\n# A tibble: 4 x 2\n  season    o3\n  &lt;fct&gt;  &lt;dbl&gt;\n1 Winter  21.5\n2 Spring  31.0\n3 Summer  29.2\n4 Autumn  23.3\n\n\n… en gebruik de geaggregeerde gegevens om de plaatsing van de annotatie te specificeren:\n\ng +\n  geom_text(data = ann,\n            aes(x = o3, y = 97,\n                label = \"Dit is een bruikbare annotatie\"),\n            size = 5, fontface = \"bold\") +\n  scale_y_continuous(limits = c(NA, 100)) +\n  facet_wrap(~season, scales = \"free_x\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nEr is echter een eenvoudiger aanpak (wat betreft het vastleggen van de coordinaten)-maar het duurt ook even om de code uit het hoofd te kennen. Het grid pakket in combinatie met ggplot2’s annotation_custom() stelt je in staat om de locatie te specificeren op basis van geschaalde coördinaten waarbij 0 laag is en 1 hoog. grobTree() creëert een raster grafisch object en textGrob creëert het tekst grafisch object. De waarde hiervan is vooral duidelijk wanneer je meerdere plots hebt met verschillende schalen.\n\nlibrary(grid)\nmy_grob &lt;- grobTree(textGrob(\"Deze tekst blijft op de plaats staan!\",\n                             x = .1, y = .9, hjust = 0,\n                             gp = gpar(col = \"black\",\n                                       fontsize = 15,\n                                       fontface = \"bold\")))\n\ng +\n  annotation_custom(my_grob) +\n  facet_wrap(~season, scales = \"free_x\") +\n  scale_y_continuous(limits = c(NA, 100))\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\nMarkdown en HTML rendering gebruiken voor annotaties\nWe gebruiken opnieuw Claus Wilke’s ggtext package dat is ontworpen voor verbeterde tekst renderingondersteuning voor ggplot2. Het ggtext pakket definieert twee nieuwe themaelementen, element_markdown() en element_textbox(). Het pakket biedt ook extra geoms. geom_richtext() is een vervanging voor geom_text() en geom_label() en rendert tekst als markdown…\n\nlibrary(ggtext)\n\nlab_md &lt;- \"De **temperatuur** in *°F* versus **ozonniveau** in *ppm*\"\n\ng +\n  geom_richtext(aes(x = 35, y = 3, label = lab_md),\n                stat = \"unique\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n… of html:\n\nlab_html &lt;- \"&#9733; De &lt;b style='color:red;'&gt;temperatuur&lt;/b&gt; in &lt;i&gt;°F&lt;/i&gt; versus &lt;b style='color:blue;'&gt;ozonniveau&lt;/b&gt;in &lt;i&gt;ppm&lt;/i&gt; &#9733;\"\n\ng +\n  geom_richtext(aes(x = 33, y = 3, label = lab_html),\n                stat = \"unique\")\n\nWarning in text_info(label, fontkey, fontfamily, fontface, fontsize, cache):\nunable to translate '&lt;U+2605&gt;png111.0396692913386' to native encoding\n\n\nWarning in text_info_cache[[key]] &lt;- info: unable to translate\n'&lt;U+2605&gt;png111.0396692913386' to native encoding\n\n\nWarning in text_info(label, fontkey, fontfamily, fontface, fontsize, cache):\nunable to translate '&lt;U+2605&gt;png111.0396692913386' to native encoding\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nDe geom komt met een heleboel details die je kunt aanpassen, zoals de hoek (wat niet mogelijk is in de standaard geom_text() en geom_label()), eigenschappen van het vak en eigenschappen van de tekst.\n\ng +\n  geom_richtext(aes(x = 10, y = 25, label = lab_md),\n                stat = \"unique\", angle = 30,\n                color = \"white\", fill = \"steelblue\",\n                label.color = NA, hjust = 0, vjust = 0,\n                family = \"Playfair Display\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nDe andere geom uit het ggtext pakket is geom_textbox(). Deze geom maakt het mogelijk om strings dynamisch te verpakken, wat erg handig is voor langere annotaties, zoals infoboxen en ondertitels.\n\nlab_long &lt;- \"**Lorem ipsum dolor**&lt;br&gt;&lt;i style='font-size:8pt;color:red;'&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.&lt;br&gt;Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.&lt;/i&gt;\"\n\ng +\n  geom_textbox(aes(x = 40, y = 10, label = lab_long),\n               width = unit(15, \"lines\"), stat = \"unique\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nMerk op dat het niet mogelijk is om het tekstvak te draaien (altijd horizontaal) of om de uitlijning van de tekst te veranderen (altijd links uitgelijnd)."
  },
  {
    "objectID": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#werken-met-coördinaten",
    "href": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#werken-met-coördinaten",
    "title": "ggplot stap voor stap",
    "section": "Werken met Coördinaten",
    "text": "Werken met Coördinaten\n\nEen plot omdraaien\nHet is ongelooflijk eenvoudig om een plot om te draaien. Hier heb ik de coord_flip() toegevoegd, alles is wat je nodig hebt om de plot om te draaien. Dit is vooral zinvol wanneer je geom’s gebruikt om categorische data weer te geven, bijvoorbeeld staafdiagrammen of, zoals in het volgende voorbeeld, box and whiskers plots:\n\nggplot(chic, aes(x = season, y = o3)) +\n  geom_boxplot(fill = \"indianred\") +\n  labs(x = \"Seizoen\", y = \"Ozon\") +\n  coord_flip()\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\nSinds ggplot2 versie 3.0.0 is het ook mogelijk om geom’s horizontaal te tekenen via het argument orientation = \"y\". Uitklappen om voorbeeld te zien.\n\n\n\nFixeer een As\nJe kunt de hoogte-breedte verhouding van het Cartesisch coördinatenstelsel vastzetten en letterlijk een fysieke weergave van de eenheden langs de x- en y-assen afdwingen:\n\nggplot(chic, aes(x = temp, y = o3)) +\n  geom_point() +\n  labs(x = \"Temperatuur (°F)\", y = \"Ozonniveau\") +\n  scale_x_continuous(breaks = seq(0, 80, by = 20)) +\n  coord_fixed(ratio = 1)\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nThis way one can ensure not only a fixed step length on the axes but also that the exported plot looks as expected. However, your saved plot likely contains a lot of white space in case you do not use a suitable aspect ratio:\n\nggplot(chic, aes(x = temp, y = o3)) +\n  geom_point() +\n  labs(x = \"Temperatuur (°F)\", y = \"Ozon\\nniveau\") +\n  scale_x_continuous(breaks = seq(0, 80, by = 20)) +\n  coord_fixed(ratio = 1/3) +\n  theme(plot.background = element_rect(fill = \"grey80\"))\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\nEen as omkeren\nJe kunt een as ook eenvoudig omkeren met scale_x_reverse() of scale_y_reverse(), respectievelijk:\n\nggplot(chic, aes(x = date, y = temp, color = o3)) +\n  geom_point() +\n  labs(x = \"Year\", y = \"Temperature (°F)\") +\n  scale_y_reverse()\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\nMerk op dat dit alleen werkt voor continue data. Als je discrete data wilt omdraaien, gebruik dan de fct_rev() functie uit het forcats pakket.\n\n\n\nTransformeer een As\n… of transformeer de standaard lineaire afbeelding door scale_y_log10() of scale_y_sqrt() te gebruiken. Als voorbeeld, hier is een log10-getransformeerde as (die NA’s introduceert in dit geval, dus wees voorzichtig):\n\nggplot(chic, aes(x = date, y = temp, color = o3)) +\n  geom_point() +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  scale_y_log10(lim = c(0.1, 100))\n\nWarning in self$trans$transform(x): NaNs produced\n\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 3 rows containing missing values (geom_point).\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\nEen Plot Circulariseren\nHet is ook mogelijk om het coördinatenstelsel te circulariseren (polariseren?) door coord_polar() op te roepen.\n\nlibrary(tidyverse)\n\nchic %&gt;%\n  dplyr::group_by(season) %&gt;%\n  dplyr::summarize(o3 = median(o3)) %&gt;%\n  ggplot(aes(x = season, y = o3)) +\n    geom_col(aes(fill = season), color = NA) +\n    labs(x = \"\", y = \"Mediaan Ozonniveau\") +\n    coord_polar() +\n    guides(fill = FALSE)\n\nWarning: `guides(&lt;scale&gt; = FALSE)` is deprecated. Please use `guides(&lt;scale&gt; =\n\"none\")` instead.\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nMet dit coördinatenstelsel kunnen ook cirkeldiagrammen worden getekend:\n\nchic_sum &lt;-\n  chic %&gt;%\n  dplyr::mutate(o3_avg = median(o3)) %&gt;%\n  dplyr::filter(o3 &gt; o3_avg) %&gt;%\n  dplyr::mutate(n_all = n()) %&gt;%\n  dplyr::group_by(season) %&gt;%\n  dplyr::summarize(rel = n() / unique(n_all))\n\nggplot(chic_sum, aes(x = \"\", y = rel)) +\n  geom_col(aes(fill = season), width = 1, color = NA) +\n  labs(x = \"\", y = \"Proportie dagen boven\\nhet mediaan Ozonniveau\") +\n  coord_polar(theta = \"y\") +\n  scale_fill_brewer(palette = \"Set1\", name = \"Season:\") +\n  theme(axis.ticks = element_blank(),\n        panel.grid = element_blank())\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\nIk stel voor om altijd ook te kijken naar het resultaat van dezelfde code in een Cartesisch coördinatensysteem, wat de standaard is, om de logica achter coord_polar() en theta te begrijpen:\n\nggplot(chic_sum, aes(x = \"\", y = rel)) +\n  geom_col(aes(fill = season), width = 1, color = NA) +\n  labs(x = \"\", y = \"Proportie dagen boven \\nhet mediaan Ozonniveau\") +\n  #coord_polar(theta = \"y\") +\n  scale_fill_brewer(palette = \"Set1\", name = \"Season:\") +\n  theme(axis.ticks = element_blank(),\n        panel.grid = element_blank())\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database"
  },
  {
    "objectID": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#werken-met-grafiek-typen",
    "href": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#werken-met-grafiek-typen",
    "title": "ggplot stap voor stap",
    "section": "Werken met Grafiek Typen",
    "text": "Werken met Grafiek Typen\n\nAlternatieven voor een Box Plot\nBox plots zijn geweldig, maar ze kunnen zo ontzettend saai zijn. En zelfs als je gewend bent om naar box plots te kijken, bedenk dan dat er veel mensen naar uw plot kunnen kijken die nog nooit een box and whisker plot hebben gezien. pand for a short recap on box and whiskers plots.\nEr zijn alternatieven, maar eerst plotten we een gewone boxplot:\n\ng &lt;-\n  ggplot(chic, aes(x = season, y = o3,\n                   color = season)) +\n    labs(x = \"Seizoen\", y = \"Ozon\") +\n    scale_color_brewer(palette = \"Dark2\", guide = \"none\")\n\ng + geom_boxplot()\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\n1. Alternatief: Uitzetten van punten\nLaten we gewoon elk gegevenspunt van de ruwe gegevens uitzetten:\n\ng + geom_point()\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nNiet alleen saai, maar ook niet informatief. Om de plot te verbeteren, zou men transparantie kunnen toevoegen om overplotting tegen te gaan:\n\ng + geom_point(alpha = .1)\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nTransparantie instellen is hier echter moeilijk, omdat ofwel de overlapping nog te groot is, ofwel de extreme waarden niet zichtbaar zijn. Slecht, dus laten we iets anders proberen.\n\n\n2. Alternatief: Jitter de punten\nProbeer een beetje jitter toe te voegen aan de data. Ik hou hiervan voor interne visualisatie, maar wees voorzichtig met jittering omdat je opzettelijk ruis toevoegt aan je data en dit kan resulteren in een verkeerde interpretatie van je data.\n\ng + geom_jitter(width = .3, alpha = .5)\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\n3. Alternatief: Vioolplots\nVioolplots, vergelijkbaar met boxplots behalve dat u een kerneldichtheid gebruikt om te tonen waar u de meeste gegevens hebt, zijn een nuttige visualisatie.\n\ng + geom_violin(fill = \"gray80\", size = 1, alpha = .5)\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\n4. Alternatief: Vioolplots combineren met Jitter\nWe kunnen natuurlijk beide combineren, geschatte dichtheden en de ruwe datapunten:\n\ng + geom_violin(fill = \"gray80\", size = 1, alpha = .5) +\n    geom_jitter(alpha = .25, width = .3) +\n    coord_flip()\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nHet ggforce pakket biedt zogenaamde sina functies waarbij de breedte van de jitter wordt geregeld door de dichtheidsverdeling van de gegevens-dat maakt de jittering een beetje visueel aantrekkelijker:\n\nlibrary(ggforce)\n\ng + geom_violin(fill = \"gray80\", size = 1, alpha = .5) +\n    geom_sina(alpha = .25) +\n    coord_flip()\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\n5. Alternatief: Vioolplots combineren met boxplots\nOm de kwantielen gemakkelijk te kunnen schatten, kunnen wij de box van de boxplot ook binnen de violen plaatsen om het 25%-kwartiel, de mediaan en het 75%-kwartiel aan te geven:\n\ng + geom_violin(aes(fill = season), size = 1, alpha = .5) +\n    geom_boxplot(outlier.alpha = 0, coef = 0,\n                 color = \"gray40\", width = .2) +\n    scale_fill_brewer(palette = \"Dark2\", guide = \"none\") +\n    coord_flip()\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\nEen deken maken voor een plot\nEen deken geeft de gegevens van één kwantitatieve variabele weer, weergegeven als markeringen langs een as. In de meeste gevallen wordt het gebruikt als aanvulling op scatter plots of heatmaps om de algemene verdeling van één of beide variabelen te visualiseren:\n\nggplot(chic, aes(x = date, y = temp,\n                 color = season)) +\n  geom_point(show.legend = FALSE) +\n  geom_rug(show.legend = FALSE) +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\nggplot(chic, aes(x = date, y = temp, color = season)) +\n  geom_point(show.legend = FALSE) +\n  geom_rug(sides = \"r\", alpha = .3, show.legend = FALSE) +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\nEen correlatiematrix maken\nEr zijn verschillende pakketten die het mogelijk maken om correlatiematrix-plots te maken, sommige gebruiken ook de ggplot2 infrastructuur en geven dus ggplots terug. Ik ga je tonen hoe dit te doen zonder extensiepakketten.\nDe eerste stap is het maken van de correlatiematrix. Hier gebruiken we het corrr pakket dat goed werkt met pipes, maar er zijn ook vele andere. We gebruiken Pearson omdat alle variabelen redelijk normaal verdeeld zijn (maar je kunt Spearman overwegen als je variabelen een ander patroon volgen). Merk op dat aangezien een correlatiematrix overbodige informatie bevat, we de helft ervan op NA zetten.\n\nlibrary(tidyverse)\n\ncorm &lt;-\n  chic %&gt;%\n  select(temp, dewpoint, pm10, o3) %&gt;%\n  corrr::correlate(diagonal = 1) %&gt;%\n  corrr::shave(upper = FALSE)\n\n\nCorrelation method: 'pearson'\nMissing treated using: 'pairwise.complete.obs'\n\n\nNow we put the resulting matrix in long format using the pivot_longer() function from the tidyr package. We also directly format the labels and place empty quotes for the upper triangle. Note that I use sprintf() to ensure that the label always display two digits.\ncorm &lt;- corm %&gt;%\n  pivot_longer(\n    cols = -term,\n    names_to = \"colname\",\n    values_to = \"corr\"\n  ) %&gt;%\n  mutate(\n    rowname = fct_inorder(term),\n    colname = fct_inorder(colname),\n    label = ifelse(is.na(corr), \"\", sprintf(\"%1.2f\", corr))\nFor the plot we will use geom_tile() for the heatmap and geom_text() for the labels:\nggplot(corm, aes(rowname, fct_rev(colname),\n                 fill = corr)) +\n  geom_tile() +\n  geom_text(aes(label = label)) +\n  coord_fixed() +\n  labs(x = NULL, y = NULL)\nI like to have a diverging color palette—it is important that the scale is centered at zero correlation!—with white indicating missing data. Also I like to have no grid lines and padding around the heatmap as well as labels that are colored depending on the underlying fill:\nggplot(corm, aes(rowname, fct_rev(colname),\n                 fill = corr)) +\n  geom_tile() +\n  geom_text(aes(\n    label = label,\n    color = abs(corr) &lt; .75\n  )) +\n  coord_fixed(expand = FALSE) +\n  scale_color_manual(\n    values = c(\"white\", \"black\"),\n    guide = \"none\"\n  ) +\n  scale_fill_distiller(\n    palette = \"PuOr\", na.value = \"white\",\n    direction = 1, limits = c(-1, 1),\n    name = \"Pearson\\nCorrelation:\"\n  ) +\n  labs(x = NULL, y = NULL) +\n  theme(panel.border = element_rect(color = NA, fill = NA),\n        legend.position = c(.85, .8))\n\n\nEen Contourplot maken\nContourplots zijn een mooie manier om een reeks waarden weer te geven. Men kan ze gebruiken om gegevens te rangschikken en zo de dichtheid van waarnemingen te tonen:\n\nggplot(chic, aes(temp, o3)) +\n  geom_density_2d() +\n  labs(x = \"Temperatuur (°F)\", x = \"Ozonniveau\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\nggplot(chic, aes(temp, o3)) +\n  geom_density_2d_filled(show.legend = FALSE) +\n  coord_cartesian(expand = FALSE) +\n  labs(x = \"Temperatuur (°F)\", x = \"Ozonniveau\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nMaar nu plotten wij driedimensionale gegevens. Wij gaan de drempelwaarden voor het dauwpunt (d.w.z. de temperatuur waarbij waterdamp in de lucht condenseert tot vloeibare dauw) in relatie tot de temperatuur en de ozonconcentratie uitzetten:\n\n## interpolate data\nlibrary(akima)\nfld &lt;- with(chic, interp(x = temp, y = o3, z = dewpoint))\n\n## prepare data in long format\nlibrary(reshape2)\n\n\nAttaching package: 'reshape2'\n\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\ndf &lt;- melt(fld$z, na.rm = TRUE)\nnames(df) &lt;- c(\"x\", \"y\", \"Dewpoint\")\n\ng &lt;- ggplot(data = df, aes(x = x, y = y, z = Dewpoint))  +\n  labs(x = \"Temperatuur (°F)\", y = \"Ozonniveau\",\n       color = \"Dauwpunt\")\n\ng + stat_contour(aes(color = ..level.., fill = Dewpoint))\n\nWarning: Ignoring unknown aesthetics: fill\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\nVerrassing! Zoals het is gedefinieerd, is het getrokken punt in de meeste gevallen gelijk aan de gemeten temperatuur.\nDe lijnen geven verschillende niveaus van dauwpunten aan, maar dit is geen mooie plot en ook moeilijk te lezen door ontbrekende randen. Laten we een tegel-plot proberen met gebruikmaking van het viridis kleurenpalet om het dauwpunt van elke combinatie van ozon-niveau en temperatuur te coderen:\n\ng + geom_tile(aes(fill = Dewpoint)) +\n    scale_fill_viridis_c(option = \"inferno\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\nHoe ziet het eruit als we een contourplot en een tegelplot combineren om het gebied onder de contourlijnen op te vullen?\n\ng + geom_tile(aes(fill = Dewpoint)) +\n    stat_contour(color = \"white\", size = .7, bins = 5) +\n    scale_fill_viridis_c()\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\nMaak een Heatmap\nNet als bij onze eerste isolijnenkaarten, kunnen we eenvoudig de aantallen of dichtheden van punten weergeven, gekwantificeerd naar een hexagonaal raster via geom_hex():\n\nggplot(chic, aes(temp, o3)) +\n  geom_hex() +\n  scale_fill_distiller(palette = \"YlOrRd\", direction = 1) +\n  labs(x = \"Temperatuur (°F)\", y = \"Ozonniveau\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\nVaak verschijnen er witte lijnen in de resulterende plot. Men kan dat verhelpen door ook kleur toe te wijzen aan ofwel ..count.. (de standaard) of ..density…..\n\nggplot(chic, aes(temp, o3)) +\n  geom_hex(aes(color = ..count..)) +\n  scale_fill_distiller(palette = \"YlOrRd\", direction = 1) +\n  scale_color_distiller(palette = \"YlOrRd\", direction = 1) +\n  labs(x = \"Temperatuur (°F)\", y = \"Ozonniveau\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n… of door voor alle zeshoekige cellen dezelfde kleur als omlijning in te stellen:\n\nggplot(chic, aes(temp, o3)) +\n  geom_hex(color = \"grey\") +\n  scale_fill_distiller(palette = \"YlOrRd\", direction = 1) +\n  labs(x = \"Temperatuur (°F)\", y = \"Ozonniveau\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\nMen kan ook de standaard binning wijzigen om het aantal hexagonale cellen in of uit te breiden:\n\nggplot(chic, aes(temp, o3, fill = ..density..)) +\n  geom_hex(bins = 50, color = \"grey\") +\n  scale_fill_distiller(palette = \"YlOrRd\", direction = 1) +\n  labs(x = \"Temperatuur (°F)\", y = \"Ozonniveau\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\nAls je een regelmatig raster wilt hebben, kun je ook geom_bin2d() gebruiken, dat de data samenvat tot rechthoekige raster cellen gebaseerd op bins:\n\nggplot(chic, aes(temp, o3, fill = ..density..)) +\n  geom_bin2d(bins = 15, color = \"grey\") +\n  scale_fill_distiller(palette = \"YlOrRd\", direction = 1) +\n  labs(x = \"Temperatuur (°F)\", y = \"Ozonniveau\")\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\n\n\nMaak een Ridge Plot\nRidge(line) plots zijn een nieuw type plots dat op dit moment erg populair is.\nHoewel je deze plots kunt maken met basic ggplot2 commands heeft de populariteit geleid tot een pakket dat het maken van deze plots makkelijker maakt: ggridges. We gaan dit pakket hier gebruiken.\n\nlibrary(ggridges)\nggplot(chic, aes(x = temp, y = factor(year))) +\n   geom_density_ridges(fill = \"gray90\") +\n   labs(x = \"Temperatuur (°F)\", y = \"Jaar\")\n\nPicking joint bandwidth of 5.23\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nJe kunt de overlap en de trailing tails eenvoudig opgeven met de argumenten rel_min_height en scale, respectievelijk. Het pakket wordt ook geleverd met een eigen thema (maar ik bouw liever mijn eigen thema, zie hoofdstuk [“Maak en gebruik je eigen thema”]](https://www.cedricscherer.com/2019/08/05/a-ggplot2-tutorial-for-beautiful-plotting-in-r/#themes). Bovendien veranderen we de kleuren op basis van jaartal om het aantrekkelijker te maken.\n\nggplot(chic, aes(x = temp, y = factor(year), fill = year)) +\n  geom_density_ridges(alpha = .8, color = \"white\",\n                      scale = 2.5, rel_min_height = .01) +\n  labs(x = \"Temperatuur (°F)\", y = \"Jaar\") +\n  guides(fill = FALSE) +\n  theme_ridges()\n\nWarning: `guides(&lt;scale&gt; = FALSE)` is deprecated. Please use `guides(&lt;scale&gt; =\n\"none\")` instead.\n\n\nPicking joint bandwidth of 5.23\n\n\n\n\n\nJe kan ook de overlapping wegwerken door waarden lager dan 1 te gebruiken voor het schalingsargument (maar dit is op één of andere manier in tegenspraak met het idee van ridge plots…). Hier is een voorbeeld dat bovendien gebruik maakt van het viridis kleurverloop en het in-build thema:\n\nggplot(chic, aes(x = temp, y = season, fill = ..x..)) +\n  geom_density_ridges_gradient(scale = .9, gradient_lwd = .5,\n                               color = \"black\") +\n  scale_fill_viridis_c(option = \"plasma\", name = \"\") +\n  labs(x = \"Temperatuur (°F)\", y = \"Seizoen\") +\n  theme_ridges(font_family = \"Roboto Condensed\", grid = FALSE)\n\nPicking joint bandwidth of 2.99\n\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not\nfound in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\n\n\n\nWe kunnen ook verschillende groepen per richel vergelijken en ze inkleuren volgens hun groep. Dit volgt het idee van Marc Belzunces.\n\nlibrary(tidyverse)\n\n## only plot extreme season using dplyr from the tidyverse\nggplot(data = filter(chic, season %in% c(\"Summer\", \"Winter\")),\n         aes(x = temp, y = year, fill = paste(year, season))) +\n  geom_density_ridges(alpha = .7, rel_min_height = .01,\n                      color = \"white\", from = -5, to = 95) +\n  scale_fill_cyclical(breaks = c(\"1997 Summer\", \"1997 Winter\"),\n                      labels = c(`1997 Summer` = \"Zomer\",\n                                 `1997 Winter` = \"Winter\"),\n                      values = c(\"tomato\", \"dodgerblue\"),\n                      name = \"Seizoen:\", guide = \"legend\") +\n  theme_ridges(grid = FALSE) +\n  labs(x = \"Temperatuur (°F)\", y = \"Jaar\")\n\nPicking joint bandwidth of 3.17\n\n\n\n\n\nHet ggridges pakket is ook nuttig om histogrammen voor verschillende groepen te maken met stat = \"binline\" in het geom_density_ridges() commando:\n\nggplot(chic, aes(x = temp, y = factor(year), fill = year)) +\n  geom_density_ridges(stat = \"binline\", bins = 25, scale = .9,\n                      draw_baseline = FALSE, show.legend = FALSE) +\n  theme_minimal() +\n  labs(x = \"Temperatuur (°F)\", y = \"Seizoen\")"
  },
  {
    "objectID": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#werken-met-linten-auc-ci-enz.",
    "href": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#werken-met-linten-auc-ci-enz.",
    "title": "ggplot stap voor stap",
    "section": "Werken met linten (AUC, CI, enz.)",
    "text": "Werken met linten (AUC, CI, enz.)\nDit is geen perfecte dataset om dit te demonstreren, maar het gebruik van linten kan nuttig zijn. In dit voorbeeld zullen we een 30-daags lopend gemiddelde maken met behulp van de filter() functie, zodat ons lint niet te veel ruis bevat.\n\nchic$o3run &lt;- as.numeric(stats::filter(chic$o3, rep(1/30, 30), sides = 2))\n\nggplot(chic, aes(x = date, y = o3run)) +\n   geom_line(color = \"chocolate\", lwd = .8) +\n   labs(x = \"Jaar\", y = \"Ozon\")\n\nWarning: Removed 29 row(s) containing missing values (geom_path).\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nHoe ziet het eruit als we het gebied onder de kromme invullen met de geom_ribbon() functie?\n\nggplot(chic, aes(x = date, y = o3run)) +\n   geom_ribbon(aes(ymin = 0, ymax = o3run),\n               fill = \"orange\", alpha = .4) +\n   geom_line(color = \"chocolate\", lwd = .8) +\n   labs(x = \"Jaar\", y = \"Ozon\")\n\nWarning: Removed 29 row(s) containing missing values (geom_path).\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\nLeuk om de area under the curve (AUC) aan te geven, maar dit is niet de conventionele manier om geom_ribbon() te gebruiken.\n\nEn eigenlijk een mooiere manier om hetzelfde te bereiken is geom_area().\n\nIn plaats daarvan tekenen we een lint dat ons één standaardafwijking boven en onder onze gegevens geeft:\n\nchic$mino3 &lt;- chic$o3run - sd(chic$o3run, na.rm = TRUE)\nchic$maxo3 &lt;- chic$o3run + sd(chic$o3run, na.rm = TRUE)\n\nggplot(chic, aes(x = date, y = o3run)) +\n   geom_ribbon(aes(ymin = mino3, ymax = maxo3), alpha = .5,\n               fill = \"darkseagreen3\", color = \"transparent\") +\n   geom_line(color = \"aquamarine4\", lwd = .7) +\n   labs(x = \"Jaar\", y = \"Ozon\")\n\nWarning: Removed 29 row(s) containing missing values (geom_path).\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database"
  },
  {
    "objectID": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#werken-met-afvlakkingen",
    "href": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#werken-met-afvlakkingen",
    "title": "ggplot stap voor stap",
    "section": "Werken met afvlakkingen",
    "text": "Werken met afvlakkingen\nHet is verbazend eenvoudig om afvlakkingen toe te voegen aan uw gegevens met ggplot2.\n\nStandaard: Een LOESS of GAM afvlakking toevoegen\nJe kunt eenvoudig stat_smooth() gebruiken - zelfs een formule is niet nodig. Dit voegt een LOESS (locally weighted scatter plot smoothing, method = \"loess\") toe als je minder dan 1000 punten hebt of een GAM (generalized additive model, method = \"gam\") anders. Omdat we meer dan 1000 punten hebben, is de afvlakking gebaseerd op een GAM:\n\nggplot(chic, aes(x = date, y = temp)) +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  stat_smooth() +\n  geom_point(color = \"gray40\", alpha = .5)\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\nIn de meeste gevallen wil men dat de punten boven op het lint liggen, dus zorg ervoor dat u de afvlakking altijd oproept voordat u de punten toevoegt.\n\n\n\nEen lineaire afvlakking toevoegen\nDe standaardinstelling is een LOESS of GAM afvlakking, maar het is ook eenvoudig om een standaard lineaire fit toe te voegen:\n\n# Anders dan bij Cédric zat Deaths als y variabele er niet in, vandaar dewpoint\nggplot(chic, aes(x = temp, y = dewpoint)) +\n   labs(x = \"Temperatuur (°F)\", y = \"Dauw\") +\n   stat_smooth(method = \"lm\", se = FALSE,\n               color = \"firebrick\", size = 1.3) +\n   geom_point(color = \"gray40\", alpha = .5)\n\n`geom_smooth()` using formula 'y ~ x'\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\n\nDe formule voor afvlakken specificeren\nMet ggplot2 kunt u het model opgeven dat u wilt gebruiken. Misschien wilt u een polynomiale regressie?\n\nggplot(chic, aes(x = o3, y = temp))+\n  labs(x = \"Ozonniveau\", y = \"Temperatuur (°F)\") +\n  geom_smooth(\n    method = \"lm\",\n    formula = y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5),\n    color = \"black\",\n    fill = \"firebrick\"\n  ) +\n  geom_point(color = \"gray40\", alpha = .3)\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\n\n\n\n\nHuh, geom_smooth()? Er is een belangrijk verschil tussen geom en stat lagen, maar hier maakt het echt niet uit welke je gebruikt. Vergroot om beide te vergelijken.\n\nLaten we zeggen dat je de GAM-dimensie wilt vergroten (wat extra wiggles toevoegen aan de smooth):\n\ncols &lt;- c(\"darkorange2\", \"firebrick\", \"dodgerblue3\")\n\nggplot(chic, aes(x = date, y = temp)) +\n  geom_point(color = \"gray40\", alpha = .3) +\n  labs(x = \"Jaar\", y = \"Temperatuur (°F)\") +\n  stat_smooth(aes(col = \"1000\"),\n              method = \"gam\",\n              formula = y ~ s(x, k = 1000),\n              se = FALSE, size = 1.3) +\n  stat_smooth(aes(col = \"100\"),\n              method = \"gam\",\n              formula = y ~ s(x, k = 100),\n              se = FALSE, size = 1) +\n  stat_smooth(aes(col = \"10\"),\n              method = \"gam\",\n              formula = y ~ s(x, k = 10),\n              se = FALSE, size = .8) +\n  scale_color_manual(name = \"k\", values = cols)\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database"
  },
  {
    "objectID": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#werken-met-interactieve-diagrammen",
    "href": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#werken-met-interactieve-diagrammen",
    "title": "ggplot stap voor stap",
    "section": "Werken met interactieve diagrammen",
    "text": "Werken met interactieve diagrammen\nDe volgende verzameling bevat bibliotheken die kunnen worden gebruikt in combinatie met ggplot2 of op zichzelf om interactieve visualisaties te maken in R (vaak gebruikmakend van bestaande JavaScript bibliotheken).\n\nCombinatie van {ggplot2} en {shiny}\nshiny is een pakket van RStudio dat het ongelooflijk gemakkelijk maakt om interactieve webapplicaties te bouwen met R. Voor een introductie en live voorbeelden, bezoek de Shiny homepage.\nOm de gebruiksmogelijkheden te bekijken, kun je de Hello Shiny voorbeelden bekijken. Dit is de eerste:\n# Dit moet je zelf even runnen\nlibrary(shiny)\nrunExample(\"01_hello\")\nNatuurlijk kun je ggplots gebruiken in deze apps. Dit voorbeeld demonstreert de mogelijkheid om wat interactieve gebruikerservaring toe te voegen:\n#Deze ook\nrunExample(\"04_mpg\")"
  },
  {
    "objectID": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#plot.ly-via-plotly-en-ggplot2",
    "href": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#plot.ly-via-plotly-en-ggplot2",
    "title": "ggplot stap voor stap",
    "section": "Plot.ly via plotly en ggplot2",
    "text": "Plot.ly via plotly en ggplot2\nPlot.ly is een tool voor het maken van online, interactieve grafieken en web apps. Met het plotly pakket kunt je deze direct vanuit uw ggplot2 plots maken en de workflow is verrassend eenvoudig en kan worden gedaan vanuit R. Het is echter mogelijk dat sommige van uw thema-instellingen worden gewijzigd en achteraf handmatig moeten worden aangepast. Ook is het helaas niet eenvoudig om facets of echte multi-panel plots te maken die mooi schalen.\n\ng &lt;- ggplot(chic, aes(date, temp)) +\n  geom_line(color = \"grey\") +\n  geom_point(aes(color = season)) +\n  scale_color_brewer(palette = \"Dark2\", guide = \"none\") +\n  labs(x = NULL, y = \"Temperatuur (°F)\") +\n  theme_bw()\n\n\nlibrary(plotly)\n\nggplotly(g)\n\n\n\n\n\nHier, bijvoorbeeld, behoudt het de algemene thema-instelling maar voegt de legende opnieuw toe.\n\nggiraph en ggplot2\nggiraph is een R pakket waarmee je dynamische ggplot2 grafieken kunt maken. Hiermee kun je tooltips, animaties en JavaScript acties aan de grafieken toevoegen. Het pakket maakt het ook mogelijk om grafische elementen te selecteren bij gebruik in Shiny applicaties.\n\nlibrary(ggiraph)\n\ng &lt;- ggplot(chic, aes(date, temp)) +\n  geom_line(color = \"grey\") +\n  geom_point_interactive(\n    aes(color = season, tooltip = season, data_id = season)\n  ) +\n  scale_color_brewer(palette = \"Dark2\", guide = \"none\") +\n  labs(x = NULL, y = \"Temperatuur (°F)\") +\n  theme_bw()\n\ngirafe(ggobj = g)\n\n\n\n\n\n\n\nHighcharts via highcharter\nHighcharts, een software bibliotheek voor interactieve grafieken, is een andere visualisatie bibliotheek geschreven in puur JavaScript dat is overgezet naar R. Het pakket highcharter maakt het mogelijk om ze te gebruiken-maar wees ervan bewust dat Highcharts alleen gratis is in geval van niet-commercieel gebruik.\n\nlibrary(highcharter)\n\nhchart(chic, \"scatter\", hcaes(x = date, y = temp, group = season))\n\n\n\n\n\n\n\n\nEcharts via echarts4r\nApache ECharts is een gratis, krachtige bibliotheek voor grafieken en visualisatie die een eenvoudige manier biedt om intuïtieve, interactieve en sterk aanpasbare grafieken te bouwen. Hoewel het is geschreven in JavaScript, kan men het gebruiken in R via de echarts4r library met dank aan John Coene. Bekijk de indrukwekkende voorbeeld galerij of deze twee apps (App 1 en App 2) die gebruik maken van de echarts4r functionaliteit.\n\nlibrary(echarts4r)\n\nchic %&gt;%\n  e_charts(date) %&gt;%\n  e_scatter(temp, symbol_size = 7) %&gt;%\n  e_visual_map(temp) %&gt;%\n  e_y_axis(name = \"Temperatuur (°F)\") %&gt;%\n  e_legend(FALSE)\n\n\n\n\n\n\n\nChart.js via charter\ncharter is een ander pakket ontwikkeld door John Coene dat het gebruik van een JavaScript visualisatie bibliotheek in R mogelijk maakt. Het pakket maakt het mogelijk om interactieve plots te bouwen met behulp van het Charts.js framework.\nlibrary(charter)\n# This example doesn't work in RMarkdown\n\nchic$date_num &lt;- as.numeric(chic$date)\n## doesn't work with class date\n\nchart(data = chic, caes(date_num, temp)) %&gt;%\n  c_scatter(caes(color = season, group = season)) %&gt;%\n  c_colors(RColorBrewer::brewer.pal(4, name = \"Dark2\"))"
  },
  {
    "objectID": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#opmerkingen-tips-hulpbronnen",
    "href": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#opmerkingen-tips-hulpbronnen",
    "title": "ggplot stap voor stap",
    "section": "Opmerkingen, tips & hulpbronnen",
    "text": "Opmerkingen, tips & hulpbronnen\n\nggplot2 gebruiken in lussen en functies\nDe raster-gebaseerde grafische functies in lattice en ggplot2 maken een grafiek object. Wanneer je deze functies interactief gebruikt op de command line, wordt het resultaat automatisch afgedrukt, maar in source() of binnen je eigen functies heb je een expliciet print() statement nodig, d.w.z. print(g) in de meeste van onze voorbeelden. Zie ook de [Q&A pagina van R}(https://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-do-lattice_002ftrellis-graphics-not-work_003f).\n\n\nAanvullende bronnen\n\nggplot2: Elegant Graphics for Data Analysis” van Hadley Wickham, verkrijgen via open-access!\n\n“Fundamentals of Data Visualization” by Claus O. Wilke over datavisualisatie in algemeen maar ook met gebruik van ggplot2. (Je kunt de codes op zijn Github profile vinden.)\n\n“Cookbook for R” by Winston Chang met codes voor het produceren van R plots\n\nGallerie van de Top 50 ggplot2 visualizations\nGallerie van ggplot2 extensie pakketten\n\nHow to extend ggplot2 by Hadley Wickham\n\nDe fantastische R4DS Online Learning Community die hulp en begeleiding biedt voor alles wat te maken heeft met de inhoud van de “R for Data Science” book by Hadley Wickham\n\n#TidyTuesday(https://github.com/rfordatascience/tidytuesday), een wekelijks sociaal data project met de nadruk op ggplots-check ook #TidyTuesday on Twitter and this collection of contributions by Neil Grantham\n\nEen tweedelige, 4.5-urige tutorialseries van Thomas Linn Pedersen ([Deel 1}(https://www.youtube.com/watch?v=h29g21z0a68) | Deel 2)"
  },
  {
    "objectID": "posts/2022-07-06-bayesrules/bayesrules.html",
    "href": "posts/2022-07-06-bayesrules/bayesrules.html",
    "title": "Bayes Rules!",
    "section": "",
    "text": "Bayes Rules!"
  },
  {
    "objectID": "posts/2022-07-06-bayesrules/bayesrules.html#bayes-rules-een-prachtig-bayesiaans-studieboek",
    "href": "posts/2022-07-06-bayesrules/bayesrules.html#bayes-rules-een-prachtig-bayesiaans-studieboek",
    "title": "Bayes Rules!",
    "section": "Bayes Rules! Een prachtig Bayesiaans studieboek",
    "text": "Bayes Rules! Een prachtig Bayesiaans studieboek\nDe afgelopen weken heb ik in een aantal blogs over delen van Bayes Rules! An Introduction to Applied Bayesian Modeling (Johnson, Ott en Dogucu, 2022) hier geschreven. Wat mij betreft op dit moment wel het beste introductieboek op toegepaste Bayesiaanse statistiek. Nog één keer wil ik hier de aandacht op vestigen. Het boek bestaat uit vier delen. Het eerste deel biedt een onderbouwing van het Bayesiaanse perspectief. Johnson et all. leren je hoe je als Bayesiaan denkt vanuit de balans van de kennis die we al hebben (prior kennis) en de kennis die we in een studie opdoen (informatie van de data, likelihood). Hoe meer nieuwe kennis we hebben, hoe beter we de kennis kunnen verfijnen. Mensen die allereerst anders tegen zaken aankijken, kunnen op basis van nieuwe kennis naar elkaar toegroeien. Johnson et all. laten zien hoe we bestaande kennis kunnen uitdrukken en samenvatten, hoe we nieuwe kennis uitdrukken en we vervolgens een posterior model kunnen maken als.\n\\[\\text{posterior} = \\frac{\\text{prior} \\cdot \\text{likelihood}}{\\text{normalizing constant}} \\propto \\text{prior} \\cdot \\text{likelihood}\\]\nOftewel meer technisch\n\\[f(\\pi|y) = \\frac{f(\\pi)L(\\pi|y)}{f(y)} \\propto f(\\pi)L(\\pi|y)\\] Aan de hand van voorbeelden laten ze zien hoe dit alles werkt. Verschillende priors leiden en verschillende data leiden steeds tot verschillende posteriors. Steeds gaat het om balanceren en verfijnen van de uitkomsten. Voor niet al te ingewikkelde problemen zijn er enkele standaardmodellen te gebruiken (bv. Beta-binomial, gamma-poisson en normal-normal) en ze laten zien hoe dat werkt. Maar daar kom je al snel niet mee weg. Omdat het snel te ingewikkeld wordt, moet je dat de uitkomsten schatten. Omdat je met de computers in grote hoeveelheden kunt werken, kun je waarschijnlijkheidsmodellen simuleren. Dat doe je met MCMC-technieken waarmee in R of andere programma’s goed is te werken. Je laat de computer het schattingswerk doen en jij diagnosticeert vervolgens de kwaliteit van de uitkomsten. MCMC kun je met verschillende technieken uitvoeren (Metropolis-Hastings, Gibbs sampling en Hamiltonian Monte Carlo) en tegenwoordig is rstan het state of the art-pakket. In het boek wordt onder de motorkap hiervan gekeken. De hele Bayesiaanse techniek stelt je in staat om posterior schattingen te maken, hypothesen te testen en te vergelijken en om voorspellingen te doen. Hoofdstuk 8, waarmee het tweede deel afsluit en dat op schatten, testen en voorspellen ingaat, is wat mij betreft een kernhoofdstuk van het boek. De rest van het boek worden verschillende analyses gepresenteerd en laten Johnson et all. zien hoe je dat kunt doen. Ze beginnen met een simpele Bayesiaanse normale regressie met een uitkomstmaat \\(y\\) en een predictor \\(x\\), laten zien wat je aan wel weet en wat je nog niet weet van de parameters die van belang zijn, wat het onderzoek aan kennis oplevert en wat dit vervolgens aan posterior kennis oplevert. Maar ook hoe je deze modellen evalueert en of het een eerlijk en goed model is en wat de nauwkeurigheid ervan is. Aan het simpele model kunnen makkelijk variabelen toegevoegd worden. Ook andere voorbeelden van regressie (poisson en negatief binomiale regressie) worden getoond. Met Bayesiaanse technieken kan ook worden geclassificeerd en daarom worden uitgebreide voorbeelden van logistische regressie en naïeve Bayesiaanse classificatie getoond. Het vierde deel gaat, ten slotte, verder op regressie en classificatievoorbeelden in, maar hier worden geclusterde voorbeelden getoond. Hierbij wordt het midden gezocht tussen compleet gepoolde modellen (universeel model waarbij geen rekening wordt gehouden met de groep waartoe je behoort) en niet gepoolde modellen (waarbij alleen maar vanuit de groep wordt gedacht). Normaal hierarchische regressie modellen zonder voorspellers, met voorspellers, geclusterde logistische modellen en verdere uitbreidingen worden getoond. Wat het boek zo mooi maakt, is dat ze met een beperkt aantal pakketten (waaronder hun eigen bayesrules met datasets en duidelijke visualisaties) werken en steeds op een vergelijkbare wijze. Een heerlijk boek waar je heel lang in kunt blijven hangen. &gt; Goodbye, dear Alicia, Miles and Mine. Ik heb me wat door het boek heen gewerkt, voel me hierdoor wat sterker in deze vorm van analyse en hoop nog wat meer Bayesiaanse dingen te doen. Jullie worden bedankt.\nJohnson, A.A., Ott, M.Q. & `Dogucu, M. (2022). Bayes Rules! An Introduction to Applied Bayesian Modeling. Boca Raton: CRC Press."
  },
  {
    "objectID": "posts/2022-08-15-geografische-analyse-van-risicos-met-inla/geografische-analyse-van-risicos-met-inla.html",
    "href": "posts/2022-08-15-geografische-analyse-van-risicos-met-inla/geografische-analyse-van-risicos-met-inla.html",
    "title": "Geografische analyse van risico’s met INLA",
    "section": "",
    "text": "Paula Moraga’s boek"
  },
  {
    "objectID": "posts/2022-08-15-geografische-analyse-van-risicos-met-inla/geografische-analyse-van-risicos-met-inla.html#introductie",
    "href": "posts/2022-08-15-geografische-analyse-van-risicos-met-inla/geografische-analyse-van-risicos-met-inla.html#introductie",
    "title": "Geografische analyse van risico’s met INLA",
    "section": "1. Introductie",
    "text": "1. Introductie\nGeospatial Health Data: Modeling and Visualization with R-INLA and Shiny beschrijft hoe je geografische gezondheidsgegevens in R analyseert en visualiseert. Het is een goede introductie op hoe je ziekten en ziektelast in kaart kunt brengen en hoe nieuwe computertechnieken als de integrated nested Laplace approximation (INLA) werken om plaatselijke en geostatistische gegevens te analyseren en te visualiseren. Deze nieuwe benadering maakt het mogelijk de ziektelast te kwantificeren, geografische patronen en veranderingen in de tijd te begrijpen, risicofactoren te identificeren en ongelijkheden tussen bevolkingsgroepen te meten. Het boek van Paula Moraga laat ook zien hoe interactieve en statische visualisaties kunnen worden gemaakt, zoals ziektekaarten en tijdplots en beschrijft verschillende R-pakketten die kunnen worden gebruikt om analyses eenvoudig om te zetten in informatieve en interactieve rapporten, dashboards, en Shiny webapplicaties. Dit alles vergemakkelijkt de communicatie van inzichten naar medewerkers en beleidsmakers.\nHet boek bevat gedetailleerde voorbeelden van verschillende ziekte- en milieutoepassingen die gebruik maken van gegevens uit de praktijk, zoals malaria in Gambia, kanker in Schotland en de VS en luchtvervuiling in Spanje. De voorbeelden in het boek zijn toegespitst op gezondheidstoepassingen, maar de behandelde benaderingen zijn ook toepasbaar op andere gebieden die gebruik maken van geografische gegevens zoals epidemiologie, ecologie, demografie of criminologie. Het boek behandelt de volgende onderwerpen:\n\nSoorten ruimtelijke gegevens en coördinaatreferentiesystemen,\n\nManipuleren en transformeren van punt-, areaal- en rasterdata,\n\nOphalen van ruimtelijke milieugegevens met een hoge resolutie,\n\nHet passen en interpreteren van Bayesiaanse ruimtelijke en spatio-temporele modellen met het R-INLA pakket,\n\nModelleren van ziekterisico en kwantificeren van risicofactoren in verschillende settings,\n\nCreëren van interactieve en statische visualisaties zoals ziekterisicokaarten en tijdplots,\n\nCreëren van reproduceerbare rapporten met R Markdown,\n\nHet ontwikkelen van dashboards met flexdashboard,\n\nHet bouwen van interactieve Shiny web applicaties.\n\nHet boek maakt gebruik van publiek beschikbare gegevens en beschrijft de R-code voor het importeren, manipuleren, modelleren en visualiseren van de gegevens, evenals de interpretatie van de resultaten. Hierdoor is de inhoud volledig reproduceerbaar en toegankelijk voor studenten, onderzoekers en praktijkmensen (hoewel dat in mijn geval nietg altijd even makkelijk lukte en uiteindelijk is het jammer dat ze niet deze gegevens op Github heeft gezet). In deze tutorial laat ik via Paula Moraga zien hoe INLA werkt. In het tweede deel volg ik een geografische analyse van …"
  },
  {
    "objectID": "posts/2022-08-15-geografische-analyse-van-risicos-met-inla/geografische-analyse-van-risicos-met-inla.html#hoe-werkt-inla",
    "href": "posts/2022-08-15-geografische-analyse-van-risicos-met-inla/geografische-analyse-van-risicos-met-inla.html#hoe-werkt-inla",
    "title": "Geografische analyse van risico’s met INLA",
    "section": "2. Hoe werkt INLA",
    "text": "2. Hoe werkt INLA\nDe “integrated nested Laplace approximation” (INLA) benadering is geïmplementeerd in het R pakket R-INLA (Havard Rue et al. 2021). Instructies hoe het pakket installeert vind je op de INLA website (http://www.r-inla.org). Daar vind je ook documentatie over het pakket, voorbeeld, een discussieforum en andere bronnen over de theorie en toepassingen van INLA. Het pakket R-INLA staat niet op CRAN (the Comprehensive R Archive Network) omdat het externe C-bibliotheken gebruikt en dat zorgt voor bepaalde problemen. Dus als je het pakket wilt installeren gebruik dan install.packages() en voeg de URL van de R-INLA repository toe. Bijvoorbeeld om een stabiele versie van het pakket te installeren, moeten we de volgende instructie toevoegen:\ninstall.packages(\"INLA\",\nrepos = \"https://inla.r-inla-download.org/R/stable\", dep = TRUE)\nOm dan het pakket en twee andere pakketten te laden, is het nodig het volgende te typen:\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv ggplot2 3.3.6     v purrr   0.3.4\nv tibble  3.1.7     v dplyr   1.0.9\nv tidyr   1.2.0     v stringr 1.4.1\nv readr   2.1.2     v forcats 0.5.1\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'stringr' was built under R version 4.1.3\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(sf)\n\nWarning: package 'sf' was built under R version 4.1.3\n\n\nLinking to GEOS 3.9.1, GDAL 3.2.1, PROJ 7.2.1; sf_use_s2() is TRUE\n\nlibrary(INLA)\n\nWarning: package 'INLA' was built under R version 4.1.3\n\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoading required package: foreach\n\n\nWarning: package 'foreach' was built under R version 4.1.3\n\n\n\nAttaching package: 'foreach'\n\n\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n\n\nLoading required package: parallel\n\n\nLoading required package: sp\n\n\nWarning: package 'sp' was built under R version 4.1.3\n\n\nThis is INLA_22.05.03 built 2022-05-03 07:58:22 UTC.\n - See www.r-inla.org/contact-us for how to get help.\n\n\nOm een model met INLA te fitten moeten we twee stappen nemen. - Eerst schrijven we de lineaire voorspeller van het model als een formule-object in R.\n- Dan voeren we het model uit door de inla() functie aan te roepen waarin we de formule, de familie, de data en andere opties specificeren. De uitvoering van inla() geeft een object terug dat de informatie van het gepaste model bevat, met inbegrip van verscheidene samenvattingen en de posterior marginals van de parameters, de lineaire voorspellers, en de gepaste waarden.\n- Deze posteriors kunnen vervolgens worden nabewerkt met behulp van een reeks functies die door R-INLA worden geleverd. - Het pakket biedt ook schattingen van verschillende criteria om Bayesiaanse modellen te beoordelen en te vergelijken. Deze omvatten het informatiecriterium voor modelafwijking (DIC) (Spiegelhalter et al. 2002), het Watanabe-Akaike-informatiecriterium (WAIC) (Watanabe 2010), de marginale waarschijnlijkheid en de conditionele voorspellende ordinaten (CPO) (Held, Schrödle, and Rue 2010). Meer details over het gebruik van R-INLA worden hieronder gegeven.\n\nLineaire voorspeller\nDe syntaxis van de lineaire voorspeller in R-INLA is vergelijkbaar met de syntaxis die wordt gebruikt om lineaire modellen te fitten met de (bekende) functie lm(). We moeten de uitkomstvariabele schrijven, dan het ~ symbool, en tenslotte de vaste en willekeurige effecten gescheiden door + operatoren. Willekeurige effecten worden gespecificeerd door gebruik te maken van de f() functie. Het eerste argument van f() is een indexvector die het element van het willekeurige effect specificeert dat van toepassing is op elke observatie, en het tweede argument is de naam van het model (b.v. “iid”, “ar1”). Bijkomende parameters van f() kunnen bekeken worden door ?f in te typen. Bijvoorbeeld, als we het model\n\\[Y \\sim N(\\eta_i, \\sigma^2),i=1,...,\\eta\\] \\[\\eta_i=\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + u_1,\\]\nwaar \\(Y_i\\) de uitkomstvariable is, \\(\\eta_1\\) is de lineaire voorspeller, met \\(x_1, x_2\\) als twee verklarende variabelen en met \\(u_1 \\sim N(0,\\sigma^2)\\) wordt de formule geschreven als\n\\[y \\sim x_1 + x_2 + f(i, model=\"iid\")\\]\nMerk op dat de formule standaard een intercept bevat. Als we \\(beta_0\\) expliciet in de formule willen opnemen, zouden we het intercept moeten verwijderen (0 toevoegen) en het als een covariate term moeten opnemen (b0 toevoegen).\n\\[y \\sim 0 +\\beta_0 + x_1 + x_2 + f(i, model= \"iid\")\\]\n\n\nDe inla() functie\nDe inla() functie wordt gebruikt om het model te fitten. De belangrijkste argumenten van inla() zijn de volgende:\n\nformula: formuleobject dat de lineaire voorspeller specificeert,\n\ndata: dataframe met de data. Als we de responsvariabele voor enkele observaties willen voorspellen, moeten we de responsvariabele van deze observaties specificeren als NA;\n\nfamily: string of vector van tekenreeksen die de waarschijnlijkheidsfamilie aangeven, zoals gaussian, poisson of binomial. Standaard is de familie gaussian. Een lijst van mogelijke alternatieven kan worden bekeken door names(inla.models()$likelihood) in te typen, en details voor individuele families kunnen worden bekeken met inla.doc(“familyname”);\n\ncontrol.compute: lijst met de specificatie van verschillende rekenvariabelen, zoals dic dat een Booleaanse variabele is die aangeeft of de DIC van het model moet worden berekend;\n\ncontrol.predictor: lijst met de specificatie van verschillende variabelen voor de voorspellers, zoals link, de linkfunctie van het model, en compute, een Booleaanse variabele die aangeeft of de marginale dichtheden voor de lineaire voorspeller moeten worden berekend.\n\n\n\nPrior specificatie\nDe namen van de priors die beschikbaar zijn in R-INLA kunnen worden bekeken door names(inla.models()$prior) in te typen, en een lijst met de opties van elk van de priors kan worden bekeken met inla.models()$prior. De documentatie over een specifieke prior kan worden bekeken met inla.doc(“priorname”).\nStandaard wordt aan het intercept van het model een Gaussische prior toegekend met gemiddelde en precisie gelijk aan 0. Aan de rest van de vaste effecten worden Gaussische priors toegekend met gemiddelde gelijk aan 0 en precisie gelijk aan 0.001. Deze waarden kunnen worden bekeken met inla.set.control.fixed.default()[c(\"mean.intercept\", \"prec.intercept\", \"mean\", \"prec\")]. De waarden van deze priors kunnen worden veranderd in het control.fixed argument van inla() door een lijst toe te wijzen met het gemiddelde en de precisie van de Gaussische verdelingen. Specifiek bevat de lijst mean.intercept en prec.intercept die het prior gemiddelde en de precisie voor het intercept vertegenwoordigen, en mean en prec die het prior gemiddelde en de precisie vertegenwoordigen voor alle vaste effecten behalve het intercept.\nThe names of the priors available in R-INLA can be seen by typing names(inla.models()\\(prior), and a list with the options of each of the priors can be seen with inla.models()\\)prior. The documentation regarding a specific prior can be seen with inla.doc(“priorname”).\nprior.fixed &lt;- list(mean.intercept = &lt;&gt;, prec.intercept = &lt;&gt;,\n                    mean = &lt;&gt;, prec = &lt;&gt;)\nres &lt;- inla(formula,\n  data = d,\n  control.fixed = prior.fixed\n)\nDe priors van de hyperparameters \\(\\theta\\) worden toegewezen in de argument hyper van f().\nformula &lt;- y ~ 1 + f(&lt;&gt;, model = &lt;&gt;, hyper = prior.f)\nDe priors van de parameters van de likelihood worden toegewezen in de parameter control.family van inla().\nres &lt;- inla(formula,\n  data = d,\n  control.fixed = prior.fixed,\n  control.family = list(..., hyper = prior.l)\n)\nhyper accepteert een benoemde lijst met namen gelijk aan elk van de hyperparameters en waarden gelijk aan een lijst met de specificatie van de priors. Specifiek bevat de lijst de volgende waarden:\n\ninitial: initiële waarde van de hyperparameter (goede initiële waarden kunnen het inferentieproces sneller maken),\nprior: naam van de prior verdeling (bijv. “iid”, “bym2”),\nparam: vector met de waarden van de parameters van de prior verdeling,\nvaste: Booleaanse variabele die aangeeft of de hyperparameter een vaste waarde is.\n\nprior.prec &lt;- list(initial = &lt;&gt;, prior = &lt;&gt;,\n                   param = &lt;&gt;, fixed = &lt;&gt;)\nprior &lt;- list(prec = prior.prec)\nPriors moeten worden ingesteld in de interne schaal van de hyperparameters. Bijvoorbeeld, het iid model definieert een vector van onafhankelijke en Gaussisch verdeelde willekeurige variabelen met precisie \\(\\Gamma\\). We kunnen de documentatie van dit model bekijken door inla.doc(“iid”) in te typen en zien dat de precisie \\(\\Gamma\\) in de log-schaal wordt weergegeven als \\(log(\\Gamma)\\). Daarom moet de prioriteit gedefinieerd worden op de log-precisie \\(log(\\Gamma)\\).\nR-INLA biedt ook een nuttig kader voor het bouwen van priors genaamd Penalized Complexity of PC priors (Fuglstad et al. 2019). PC priors worden gedefinieerd op individuele modelcomponenten die kunnen worden beschouwd als een flexibele uitbreiding van een eenvoudig, interpreteerbaar, basismodel. PC priors bestraffen afwijkingen van het basismodel. Zo controleren ze de flexibiliteit, verminderen ze over-fitting, en verbeteren ze de voorspellende prestaties. PC-prioriteiten hebben één parameter die de in het model toegestane mate van flexibiliteit bepaalt. Deze priors worden gespecificeerd door de waarden \\(U,\\alpha\\) zo in te stellen dat\n\\[P(T(\\epsilon)&gt;U)=alpha,\\]\nwaarin \\(T(\\epsilon)\\) een interpreteerbare transformatie is van de flexibiliteitsparameter \\(\\epsilon\\) \\(U\\) een bovengrens is die een staartgebeurtenis specificeert, en \\(alpha\\) de waarschijnlijkheid van deze gebeurtenis is.\n\n\nVoorbeeld\nHier laten we een voorbeeld zien dat demonstreert hoe een model te specificeren en te fitten en de resultaten te inspecteren met behulp van een echte dataset en R-INLA. Meer specifiek modelleren we gegevens over sterftecijfers na operaties in 12 ziekenhuizen. Het doel van deze analyse is om aan de hand van sterftecijfers de prestaties van elk ziekenhuis te beoordelen en te bepalen of een ziekenhuis ongewoon goed of slecht presteert.\n\n\nData\nWe gebruiken de data Surg die het aantal operaties en het aantal sterfgevallen bevat in 12 ziekenhuizen waar hartchirurgie op baby’s wordt uitgevoerd. Surgis een dataframe met drie kolommen, namelijkhospitalvoor het ziekenhuis,nvoor het aantal operaties dat in elk ziekenhuis in een periode van een jaar is uitgevoerd, enr` voor het aantal sterfgevallen binnen 30 dagen na de operatie in elk ziekenhuis.\n\nSurg\n\n     n  r hospital\n1   47  0        A\n2  148 18        B\n3  119  8        C\n4  810 46        D\n5  211  8        E\n6  196 13        F\n7  148  9        G\n8  215 31        H\n9  207 14        I\n10  97  8        J\n11 256 29        K\n12 360 24        L\n\n\n\n\nModel\nWij specificeren een model om de sterftecijfers in elk van de ziekenhuizen te verkrijgen. We gaan uit van een binomiale waarschijnlijkheid voor het aantal sterfgevallen in elk ziekenhuis, \\(Y_i\\), met sterftecijfer \\(p_i\\)\n\\[Y_i \\sim Binomial(n_1,p_1), i=1, ...,12\\]\nWe nemen ook aan dat de sterftecijfers in de verschillende ziekenhuizen op één of andere manier gelijkaardig zijn en specificeren een random effects-model voor de werkelijke sterftecijfers \\(p_i\\)\n\\[logit(p_1)=alpha + u_1, u_1 \\sim N(0,sigma^2)\\].\nStandaard wordt een niet-informatieve prior gespecificeerd voor \\(\\alpha\\), die het populatielogit-sterftecijfer is\n\\[\\alpha \\sim N(0, 1/\\Gamma), \\Gamma=0\\]\nIn R-INLA is de standaard prior voor de precisie van de willekeurige effecten \\(U_i\\) \\(1/sigma^2 \\sim Gamma(1,5x10^-5)\\)$. We kunnen deze prioriteit veranderen door een Penalized Complexity (PC) prioriteit in te stellen op de standaardafwijking \\(\\sigma\\). We kunnen bijvoorbeeld specificeren dat de waarschijnlijkheid dat \\(\\sigma\\) groter is dan 1 klein is, gelijk aan 0,01: \\(P(\\sigma&gt;1)=0,01\\). In R-INLA wordt deze prioriteit gespecificeerd als\n\nprior.prec &lt;- list(prec = list(prior = \"pc.prec\",\n                               param = c(1, 0.01)))\n\nen het model wordt vertaald in R-code met behulp van de volgende formule:\n\nformula &lt;- r ~ f(hospital, model = \"iid\", hyper = prior.prec)\n\nInformatie over het model genaamd “iid” kan worden gevonden door inla.doc(“iid”) in te typen, en documentatie over de PC-voorafgaande “pc.prec” kan worden gezien met inla.doc(“pc.prec”).\nVervolgens roepen we inla() aan met vermelding van de formule, de data, de familie en het aantal trials. We voegen control.predictor = list(compute = TRUE) toe om de posterior marginals van de parameters te berekenen, en control.compute = list(dic = TRUE) om aan te geven dat de DIC berekend moet worden.\n\nres &lt;- inla(formula,\n  data = Surg,\n  family = \"binomial\", Ntrials = n,\n  control.predictor = list(compute = TRUE),\n  control.compute = list(dic = TRUE)\n)\n\nWarning in .recacheSubclasses(def@className, def, env): undefined subclass\n\"packedMatrix\" of class \"replValueSp\"; definition not updated\n\n\nWarning in .recacheSubclasses(def@className, def, env): undefined subclass\n\"packedMatrix\" of class \"mMatrix\"; definition not updated\n\n\n\n\nResultaten\nWanneer inla() wordt uitgevoerd, krijgen we een object van de klasse inla dat de informatie van het gepaste model bevat, inclusief samenvattingen en posterior marginale dichtheden van de vaste effecten, de willekeurige effecten, de hyperparameters, de lineaire voorspellers en de gepaste waarden. Een samenvatting van het geretourneerde object res kan worden bekeken met summary(res).\n\nsummary(res)\n\n\nCall:\n   c(\"inla.core(formula = formula, family = family, contrasts = contrasts, \n   \", \" data = data, quantiles = quantiles, E = E, offset = offset, \", \" \n   scale = scale, weights = weights, Ntrials = Ntrials, strata = strata, \n   \", \" lp.scale = lp.scale, link.covariates = link.covariates, verbose = \n   verbose, \", \" lincomb = lincomb, selection = selection, control.compute \n   = control.compute, \", \" control.predictor = control.predictor, \n   control.family = control.family, \", \" control.inla = control.inla, \n   control.fixed = control.fixed, \", \" control.mode = control.mode, \n   control.expert = control.expert, \", \" control.hazard = control.hazard, \n   control.lincomb = control.lincomb, \", \" control.update = \n   control.update, control.lp.scale = control.lp.scale, \", \" \n   control.pardiso = control.pardiso, only.hyperparam = only.hyperparam, \n   \", \" inla.call = inla.call, inla.arg = inla.arg, num.threads = \n   num.threads, \", \" blas.num.threads = blas.num.threads, keep = keep, \n   working.directory = working.directory, \", \" silent = silent, inla.mode \n   = inla.mode, safe = FALSE, debug = debug, \", \" .parent.frame = \n   .parent.frame)\") \nTime used:\n    Pre = 0.361, Running = 0.178, Post = 0.0156, Total = 0.555 \nFixed effects:\n              mean   sd 0.025quant 0.5quant 0.975quant mode kld\n(Intercept) -2.545 0.14     -2.838    -2.54     -2.281   NA   0\n\nRandom effects:\n  Name    Model\n    hospital IID model\n\nModel hyperparameters:\n                        mean    sd 0.025quant 0.5quant 0.975quant mode\nPrecision for hospital 12.03 18.45       2.39     8.29      41.48   NA\n\nDeviance Information Criterion (DIC) ...............: 74.92\nDeviance Information Criterion (DIC, saturated) ....: -10.09\nEffective number of parameters .....................: 8.25\n\nMarginal log-Likelihood:  -41.17 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\n\nWe kunnen de resultaten plotten met plot(res) of ook met plot(res, plot.prior = TRUE) als we de prior en posterior verdelingen in dezelfde plot willen plotten. Bij het uitvoeren van inla(), stellen we control.compute = list(dic = TRUE) in; het resultaat bevat dus de DIC van het model. De DIC is gebaseerd op een afweging tussen de fit van de data met het model en de complexiteit van het model, waarbij kleinere waarden van de DIC wijzen op een beter model.\n\nres$dic$dic\n\n[1] 74.91639\n\n\nSamenvattingen van de vaste effecten kunnen worden verkregen door res$summary.fixed te typen. Dit geeft een dataframe met het gemiddelde, de standaardafwijking, de 2.5, 50 en 97.5 percentielen, en de modus van de posterior. De kolom kld vertegenwoordigt de symmetrische Kullback-Leibler divergentie (Kullback en Leibler 1951) die het verschil beschrijft tussen de Gaussische en de vereenvoudigde of volledige Laplace benaderingen voor elke posterior.\n\nres$summary.fixed\n\n                 mean        sd 0.025quant  0.5quant 0.975quant mode\n(Intercept) -2.544918 0.1397936  -2.837923 -2.539921   -2.28079   NA\n                     kld\n(Intercept) 8.954842e-07\n\n\nWe kunnen ook de samenvattingen van de willekeurige effecten en de hyperparameters verkrijgen door respectievelijk res$summary.random (dat is een lijst) en res$summary.hyperpar (dat is een dataframe) in te typen.\n\nres$summary.random\n\n$hospital\n   ID        mean        sd  0.025quant    0.5quant 0.975quant mode\n1   A -0.33406820 0.3602496 -1.16244166 -0.29229930  0.2667848   NA\n2   B  0.35104026 0.2505440 -0.10884559  0.33952295  0.8720476   NA\n3   C -0.04135773 0.2607975 -0.57640794 -0.03644290  0.4658754   NA\n4   D -0.21854717 0.1807342 -0.58257453 -0.21594766  0.1335713   NA\n5   E -0.35557089 0.2630880 -0.92502879 -0.33630171  0.1091291   NA\n6   F -0.05946957 0.2351525 -0.53899792 -0.05529945  0.3976316   NA\n7   G -0.09895756 0.2528987 -0.62440372 -0.09094182  0.3839509   NA\n8   H  0.55150426 0.2367636  0.11498312  0.54305052  1.0390541   NA\n9   I -0.04843948 0.2317468 -0.51893437 -0.04509937  0.4039301   NA\n10  J  0.06200418 0.2677363 -0.46881109  0.05949329  0.6007338   NA\n11  K  0.35085676 0.2194395 -0.05555904  0.34210247  0.8051331   NA\n12  L -0.06821881 0.2061014 -0.48258048 -0.06622692  0.3363430   NA\n            kld\n1  2.001880e-05\n2  2.389700e-07\n3  6.118419e-08\n4  5.310581e-07\n5  7.107832e-07\n6  1.358690e-08\n7  9.493184e-08\n8  5.446908e-07\n9  1.059835e-08\n10 7.918263e-09\n11 4.687372e-07\n12 1.350071e-07\n\n\n\nres$summary.hyperpar\n\n                           mean       sd 0.025quant 0.5quant 0.975quant mode\nPrecision for hospital 12.02899 18.44555   2.388014  8.28797   41.47951   NA\n\n\nBij het uitvoeren van inla(), als we in control.predictor compute = TRUE instellen, bevat het geretourneerde object ook de volgende objecten:\n\nsummary.linear.predictor: dataframe met het gemiddelde, de standaardafwijking, en de kwantielen van de lineaire voorspellers,\n\nsummary.fitted.values: dataframe met het gemiddelde, de standaardafwijking, en de kwantielen van de gepaste waarden, verkregen door de lineaire voorspellers te transformeren met de inverse van de link-functie,\n\nmarginals.linear.predictor: lijst met de posterior marginals van de lineaire voorspellers,\n\nmarginals.fitted.values: lijst met de posterior marginals van de gepaste waarden, verkregen door de lineaire voorspellers te transformeren met de inverse van de link functie.\n\nMerk op dat als een observatie NA is, de gebruikte link functie de identiteit is. Als we willen dat summary.fitted.values en marginals.fitted.values de waarden in de getransformeerde schaal bevatten, moeten we de juiste link in control.predictor instellen. Als alternatief kunnen we handmatig de marginale in het inla object transformeren met de inla.tmarginal() functie.\nDe voorspelde sterftecijfers in ons voorbeeld kunnen worden verkregen met res$summary.fitted.values.\n\nres$summary.fitted.values\n\n                          mean          sd 0.025quant   0.5quant 0.975quant\nfitted.Predictor.01 0.05642778 0.018677757 0.02294091 0.05560010 0.09582051\nfitted.Predictor.02 0.10255660 0.021272480 0.06698668 0.10044747 0.14981711\nfitted.Predictor.03 0.07215853 0.017029578 0.04221709 0.07094233 0.10937080\nfitted.Predictor.04 0.05999148 0.007838246 0.04535747 0.05974367 0.07606421\nfitted.Predictor.05 0.05385859 0.012887205 0.03040246 0.05331382 0.08060048\nfitted.Predictor.06 0.07051287 0.014441435 0.04457621 0.06968184 0.10138361\nfitted.Predictor.07 0.06828696 0.015506637 0.04055066 0.06737396 0.10157276\nfitted.Predictor.08 0.12189766 0.021778636 0.08366009 0.12048708 0.16832000\nfitted.Predictor.09 0.07117427 0.014254142 0.04556131 0.07035496 0.10162729\nfitted.Predictor.10 0.07945514 0.019285314 0.04664630 0.07763853 0.12273357\nfitted.Predictor.11 0.10187113 0.017164260 0.07180930 0.10065672 0.13876928\nfitted.Predictor.12 0.06944861 0.011553476 0.04829115 0.06892959 0.09364931\n                    mode\nfitted.Predictor.01   NA\nfitted.Predictor.02   NA\nfitted.Predictor.03   NA\nfitted.Predictor.04   NA\nfitted.Predictor.05   NA\nfitted.Predictor.06   NA\nfitted.Predictor.07   NA\nfitted.Predictor.08   NA\nfitted.Predictor.09   NA\nfitted.Predictor.10   NA\nfitted.Predictor.11   NA\nfitted.Predictor.12   NA\n\n\nDe kolom mean laat zien dat de ziekenhuizen 2, 8 en 11 de hoogste posterior gemiddelden van de sterftecijfers hebben. De kolommen 0.025quant en 0.975quant bevatten de onder- en bovengrenzen van 95% geloofwaardige intervallen van de sterftecijfers en geven maatstaven voor de onzekerheid.\nWe kunnen ook een lijst verkrijgen met de posterior marginals van de vaste effecten door res$marginals.fixed te typen en lijsten met de posterior marginals van de random effecten en de hyperparameters door respectievelijk marginals.random en marginals.hyperpar te typen. Marginals zijn benoemde lijsten die matrices met 2 kolommen bevatten. Kolom x vertegenwoordigt de waarde van de parameter, en kolom y is de dichtheid. R-INLA bevat verschillende functies om de posterior marginals te manipuleren. Bijvoorbeeld, inla.emarginal() en inla.qmarginal() berekenen de verwachting en kwantielen, respectievelijk, van de posterior marginalen. inla.smarginal() kan worden gebruikt om een spline afvlakking te verkrijgen, inla.tmarginal() kan worden gebruikt om de marginalen te transformeren, en inla.zmarginal() geeft samenvattende statistieken.\nIn ons voorbeeld bevat het eerste element van de posterior marginalen van de vaste effecten, res$marginals.fixed[[1]], de posterior elementen van het intercept $ We kunnen inla.smarginal() toepassen om een spline afvlakking van de marginale dichtheid te verkrijgen en deze vervolgens plotten met de ggplot() functie van het ggplot2 pakket.\n\nlibrary(ggplot2)\nalpha &lt;- res$marginals.fixed[[1]]\nggplot(data.frame(inla.smarginal(alpha)), aes(x, y)) +\n  geom_line() +\n  theme_bw()\n\n\n\n\nHet kwantiel en de verdelingsfuncties worden gegeven door inla.qmarginal() en inla.pmarginal(), respectievelijk. We kunnen het kwantiel 0.05 van \\(\\alpha\\) verkrijgen, en de kans dat \\(\\alpha\\) lager is dan dit kwantiel als volgt uitzetten:\n\nquant &lt;- inla.qmarginal(0.05, alpha)\nquant\n\n[1] -2.781812\n\n\n\ninla.pmarginal(quant, alpha)\n\n[1] 0.05\n\n\nEen grafiek van de waarschijnlijkheid dat \\(\\alpha\\) lager is dan het 0,05 kwantiel kan als volgt worden gemaakt\n\nggplot(data.frame(inla.smarginal(alpha)), aes(x, y)) +\n  geom_line() +\n  geom_area(data = subset(data.frame(inla.smarginal(alpha)),\n                                     x &lt; quant),\n            fill = \"black\") +\n  theme_bw()\n\n\n\n\nDe functie inla.dmarginal() berekent de dichtheid bij bepaalde waarden. Bijvoorbeeld, de dichtheid bij waarde -2.5 kan als volgt berekend worden:\n\ninla.dmarginal(-2.5, alpha)\n\n[1] 2.9718\n\n\n\nggplot(data.frame(inla.smarginal(alpha)), aes(x, y)) +\n  geom_line() +\n  geom_vline(xintercept = -2.5, linetype = \"dashed\") +\n  theme_bw()\n\n\n\n\nIndien we een transformatie van de marginale willen verkrijgen, kunnen we inla.tmarginal() gebruiken. Indien we bijvoorbeeld de variantie van het willekeurige effect \\(u_i\\) willen verkrijgen, kunnen we de marginale van de precisie \\(gamma\\) verkrijgen en dan de inverse functie toepassen.\n\nmarg.variance &lt;- inla.tmarginal(function(x) 1/x,\nres$marginals.hyperpar$\"Precision for hospital\")\n\nEen plot van de posterior van de variantie van het willekeurige effect \\(u_i\\) is weergegeven in\n\nggplot(data.frame(inla.smarginal(marg.variance)), aes(x, y)) +\n  geom_line() +\n  theme_bw()\n\n\n\n\nAls we nu de gemiddelde posterior van de variantie willen verkrijgen, kunnen we `inla.emarginal() gebruiken.\n\nm &lt;- inla.emarginal(function(x) x, marg.variance)\nm\n\n[1] 0.1459816\n\n\nDe standaardafwijking kan worden berekend met de uitdrukking\n\\[Var[X]=E[X^2} - E[X]^2\\]\n\nmm &lt;- inla.emarginal(function(x) x^2, marg.variance)\nsqrt(mm - m^2)\n\n[1] 0.1031089\n\n\nKwantielen worden berekend met de inla.qmarginal() functie.\n\ninla.qmarginal(c(0.025, 0.5, 0.975), marg.variance)\n\n[1] 0.02381119 0.12043974 0.41883198"
  },
  {
    "objectID": "posts/2022-08-15-geografische-analyse-van-risicos-met-inla/geografische-analyse-van-risicos-met-inla.html#gezondheid-in-gebieden",
    "href": "posts/2022-08-15-geografische-analyse-van-risicos-met-inla/geografische-analyse-van-risicos-met-inla.html#gezondheid-in-gebieden",
    "title": "Geografische analyse van risico’s met INLA",
    "section": "3. Gezondheid in gebieden",
    "text": "3. Gezondheid in gebieden\nOnderstaande is gebaseerd op Paula Moraga’s Geospatial Health Data bookGeospatial Health Data, 2019, Chapman&Hall/CRC en haar tutorial en specieke het derde hoofdstuk waarin ze ingaat op het analyseren van gezondheid in gebieden.\nIn deze tutorial worden methodes om ziekte in kaarten te zetten gepresenteerd, specifiek om het risico op longkanker in Pennsylvania, Verenigde Staten, in het jaar 2002 in te schatten. We gebruiken gegevens van het R-pakket SpatialEpi dat de bevolking, de gevallen van longkanker en de verhouding rokers in de districten van Pennsylvania bevat. De bevolkingsgegevens zijn afkomstig van de 2000 decennium census. De ziektegevallen en het percentage rokers zijn afkomstig van de website van het Pennsylvania Departement van Gezondheid.\nWij laten zien hoe de waargenomen en verwachte ziektegevallen en de gestandaardiseerde sterfteratio’s (SMR’s) in elk van de districten van Pennsylvania kunnen worden berekend. We verkrijgen ook schattingen van het ziekterisico en kwantificeren risicofactoren met behulp van INLA. Tenslotte laten we zien hoe we interactieve kaarten kunnen maken van de ziekterisicoschattingen met behulp van leaflet.\nEen uitgebreide versie van dit voorbeeld verscheen in Moraga, P. Small Area Disease Risk Estimation and Visualization Using R. The R Journal, 10(1):495-506, 2018, en het boek Moraga, P. Geospatial Health Data: Modeling and Visualization with R-INLA and Shiny. Chapman & Hall/CRC, 2019."
  },
  {
    "objectID": "posts/2022-08-15-geografische-analyse-van-risicos-met-inla/geografische-analyse-van-risicos-met-inla.html#data-en-kaart",
    "href": "posts/2022-08-15-geografische-analyse-van-risicos-met-inla/geografische-analyse-van-risicos-met-inla.html#data-en-kaart",
    "title": "Geografische analyse van risico’s met INLA",
    "section": "Data en kaart",
    "text": "Data en kaart\nWe laden het pakket SpatialEpien halen de data binnen.\n\nlibrary(SpatialEpi)\n\nWarning: package 'SpatialEpi' was built under R version 4.1.3\n\ndata(pennLC)\n\nVervolgens inspecteren we de data:\n\nclass(pennLC)\n\n[1] \"list\"\n\n\n\nnames(pennLC)\n\n[1] \"geo\"             \"data\"            \"smoking\"         \"spatial.polygon\"\n\n\nWe zien dat pennLC een listobject is met de volgende elementen:\n\ngeo: een tabel met provincie-id’s, en de lengte- en breedtegraad van het zwaartepunt van elke provincie,\n\ndata: een tabel met provincie-id’s, aantal gevallen, bevolking en strata-informatie,\n\nsmoking: een tabel met provincie-identiteiten en aandeel rokers,\n\nspatial.polygon: een SpatialPolygons-object.\n\nWe inspecteren de eerste rijen van pennLC\\(data en pennLC\\)smoking. pennLC$data bevat het aantal gevallen van longkanker en de bevolking op districtsniveau, gestratificeerd naar ras (blank en niet-blank), geslacht (vrouw en man) en leeftijd (jonger dan 40, 40-59, 60-69 en 70+).\n\n?pennLC\n\nstarting httpd help server ... done\n\n\n\nhead(pennLC$data)\n\n  county cases population race gender      age\n1  adams     0       1492    o      f Under.40\n2  adams     0        365    o      f    40.59\n3  adams     1         68    o      f    60.69\n4  adams     0         73    o      f      70+\n5  adams     0      23351    w      f Under.40\n6  adams     5      12136    w      f    40.59\n\n\nDe geo-gegevens zien er zo uit:\n\nhead(pennLC$geo)\n\n     county         x        y\n1     adams -77.21550 39.87776\n2 allegheny -79.97980 40.46986\n3 armstrong -79.46558 40.81654\n4    beaver -80.34774 40.68327\n5   bedford -78.48906 40.01033\n6     berks -75.92717 40.41920\n\n\npennLC$smoking bevat county-specifieke proportie rokers.\n\nhead(pennLC$smoking)\n\n     county smoking\n1     adams   0.234\n2 allegheny   0.245\n3 armstrong   0.250\n4    beaver   0.276\n5   bedford   0.228\n6     berks   0.249\n\n\nDe kaart van de graafschappen van Pennsylvania wordt gegeven door het object SpatialPolygons met de naam pennLC$spatial.polygon. Wij kunnen de kaart als volgt plotten:\n\nmap &lt;- pennLC$spatial.polygon\nplot(map)\n\nWarning in wkt(obj): CRS object has no comment\n\n\n\n\n\n\nclass(map)\n\n[1] \"SpatialPolygons\"\nattr(,\"package\")\n[1] \"sp\""
  },
  {
    "objectID": "posts/2022-08-15-geografische-analyse-van-risicos-met-inla/geografische-analyse-van-risicos-met-inla.html#data-voorbereiding",
    "href": "posts/2022-08-15-geografische-analyse-van-risicos-met-inla/geografische-analyse-van-risicos-met-inla.html#data-voorbereiding",
    "title": "Geografische analyse van risico’s met INLA",
    "section": "2 Data voorbereiding",
    "text": "2 Data voorbereiding\nWe maken nu een dataframe met de naam d met kolommen die de id’s van de provincies, het waargenomen en het verwachte aantal gevallen, de rookverhoudingen en de SMR’s bevatten. Specifiek zal d de volgende kolommen bevatten:\n\ncounty: id voor elke county,\n\nY: geobserveerde aantal casussen in elke county,\n\nE: verwachte aantal casussen in elke county,\n\nsmoking: rook proportie in elke county,\n\nSMR: SMR van elke county.\n\n\n2.1 Geobserveerde casussen\npennLC$data bevat de gevallen in elke county gestratificeerd naar ras, geslacht en leeftijd. We kunnen het aantal gevallen in elke county, Y, verkrijgen door de rijen van pennLC$data per county te aggregeren en het waargenomen aantal gevallen op te tellen met het pakket dplyr.\n\nlibrary(dplyr)\nd &lt;- group_by(pennLC$data, county) %&gt;% summarize(Y = sum(cases))\nhead(d)\n\n# A tibble: 6 x 2\n  county        Y\n  &lt;fct&gt;     &lt;int&gt;\n1 adams        55\n2 allegheny  1275\n3 armstrong    49\n4 beaver      172\n5 bedford      37\n6 berks       308\n\n\nEen alternatief voor dplyr om het aantal gevallen in elke provincie te berekenen is om de aggregate() functie te gebruiken. aggregate() accepteert de volgende drie argumenten:\n\nx: dataframe met de gegevens,\n\nby: lijst van groeperingselementen (elk element moet even lang zijn als de variabelen in dataframe x),\n\nFUN: functie om de samenvattingsstatistieken te berekenen die op alle gegevensverzamelingen moeten worden toegepast.\n\nWe kunnen aggregate() gebruiken met x gelijk aan de vector van de gevallen, by gelijk aan een lijst met de districten, en FUN gelijk aan sum. Vervolgens stellen we de namen van de kolommen van het verkregen dataframe gelijk aan county en Y.\n\nd &lt;- aggregate(\n  x = pennLC$data$cases,\n  by = list(county = pennLC$data$county),\n  FUN = sum\n)\nnames(d) &lt;- c(\"county\", \"Y\")\n\n\n\nVerwachte gevallen\nNu berekenen we het verwachte aantal gevallen in elke county, dat het totale aantal ziektegevallen vertegenwoordigt dat men zou verwachten als de bevolking in het county zich gedroeg zoals de bevolking van Pennsylvania zich gedraagt. Het verwachte aantal gevallen \\(E_i\\) in elke county i kan met indirecte standaardisering als volgt worden berekend\nEi=∑j=1mr(s)jn(i)j,\nwaarbij r(s)j het percentage is (aantal gevallen gedeeld door bevolking) in stratum j van de bevolking van Pennsylvania, en \\(n_j^(i)\\) de bevolking in stratum \\(j\\) van county \\(i\\).\nWe kunnen de verwachte tellingen hiervoor berekenen met de expected() functie van het SpatialEpi pakket. Deze functie heeft drie argumenten, te weten,\n\npopulatie: een vector van bevolkingstellingen voor elk strata in elk gebied,\n\ngevallen: een vector met het aantal gevallen voor elk strata in elk gebied,\n\nn.strata: het aantal beschouwde strata.\n\nDe vectoren populatie en gevallen moeten eerst per gebied worden gesorteerd en daarna moeten binnen elk gebied de tellingen voor alle strata in dezelfde volgorde worden opgesomd. Alle strata moeten in de vectoren worden opgenomen, ook strata met 0 gevallen.\nOm de verwachte tellingen te krijgen sorteren we de gegevens eerst met de order() functie waarbij we de volgorde aangeven als county, ras, geslacht en tenslotte leeftijd.\n\npennLC$data &lt;- pennLC$data[order(pennLC$data$county, pennLC$data$race, pennLC$data$gender, pennLC$data$age), ]\n\nVervolgens krijgen we de verwachte tellingen E in elke county door de expected() functie aan te roepen waarbij we population gelijk stellen aan pennLC$data$population en cases gelijk stellen aan pennLC$data$cases. Er zijn 2 rassen, 2 geslachten en 4 leeftijdsgroepen voor elke county, dus het aantal strata is ingesteld op 2 x 2 x 4 = 16.\n\nE &lt;- expected(population = pennLC$data$population, cases = pennLC$data$cases, n.strata = 16)\n\nNu voegen we de vector E toe aan het dataframe d, dat de ids van de provincies (county) en de waargenomen tellingen (Y) bevat.\n\nd$E &lt;- E\nhead(d)\n\n     county    Y          E\n1     adams   55   69.62730\n2 allegheny 1275 1182.42804\n3 armstrong   49   67.61012\n4    beaver  172  172.55806\n5   bedford   37   44.19013\n6     berks  308  300.70598\n\n\n\n\nVerhouding rokers\nWe voegen ook aan d de variabele roken toe, die het percentage rokers in elk county weergeeft. We voegen deze variabele toe met de merge() functie waarbij we county specificeren als de kolom voor het samenvoegen.\n\nd &lt;- merge(d, pennLC$smoking, by = \"county\")\n\n\n\nSMRs\nVoor elke county \\(i\\) wordt de SMR gedefinieerd als de verhouding tussen de waargenomen tellingen en de verwachte tellingen.\n\\[SMR_i=Y_i/E_i\\]\nWij berekenen de vector van SMR’s als de verhouding tussen de waargenomen en de verwachte tellingen, en voegen deze toe aan het gegevenskader d.\n\nd$SMR &lt;- d$Y/d$E\n\n\n\nData\nDe definitieve dataset ziet er zo uit:\n\nhead(d)\n\n     county    Y          E smoking       SMR\n1     adams   55   69.62730   0.234 0.7899200\n2 allegheny 1275 1182.42804   0.245 1.0782897\n3 armstrong   49   67.61012   0.250 0.7247435\n4    beaver  172  172.55806   0.276 0.9967660\n5   bedford   37   44.19013   0.228 0.8372910\n6     berks  308  300.70598   0.249 1.0242563\n\n\n\n\nGegevens aan de kaart toevoegen\nWij hebben een dataframe d geconstrueerd met de waargenomen en verwachte ziektetellingen, de rokerspercentages en de SMR voor elk van de counties. De kaart van de Pennsylvania counties wordt gegeven door het object SpatialPolygons dat kaart heet. Met behulp van dit object en het data frame d kunnen we een SpatialPolygonsDataFrame maken waarmee we kaarten kunnen maken van de variabelen in d. Om dat te doen stellen we eerst de rijnamen van het data frame d gelijk aan d$county. Dan voegen we de SpatialPolygons kaart samen met het data frame d dat overeenkomt met de SpatialPolygons member Polygons ID slot waarden met de data frame rij namen (match.ID = TRUE).\n\nlibrary(sp)\nrownames(d) &lt;- d$county\nmap &lt;- SpatialPolygonsDataFrame(map, d, match.ID = TRUE)\nhead(map@data)\n\n             county    Y          E smoking       SMR\nadams         adams   55   69.62730   0.234 0.7899200\nallegheny allegheny 1275 1182.42804   0.245 1.0782897\narmstrong armstrong   49   67.61012   0.250 0.7247435\nbeaver       beaver  172  172.55806   0.276 0.9967660\nbedford     bedford   37   44.19013   0.228 0.8372910\nberks         berks  308  300.70598   0.249 1.0242563"
  },
  {
    "objectID": "posts/2022-08-15-geografische-analyse-van-risicos-met-inla/geografische-analyse-van-risicos-met-inla.html#smr-in-kaart-brengen",
    "href": "posts/2022-08-15-geografische-analyse-van-risicos-met-inla/geografische-analyse-van-risicos-met-inla.html#smr-in-kaart-brengen",
    "title": "Geografische analyse van risico’s met INLA",
    "section": "SMR in kaart brengen",
    "text": "SMR in kaart brengen\nWe kunnen de waargenomen en verwachte ziektetellingen, de SMR’s, en de rokersverhoudingen visualiseren in een interactieve chroplethkaart. We maken deze kaart met behulp van het leaflet pakket.\nWe maken de kaart door eerst leaflet() aan te roepen en de standaard OpenStreetMap kaarttegels aan de kaart toe te voegen met addTiles(). Daarna voegen we de Pennsylvania counties toe met addPolygons() waarbij we de kleur van de grenzen van de gebieden opgeven (color) en de lijndikte (weight). We vullen de gebieden met de kleuren gegeven door de kleurenpalet-functie gegenereerd met colorNumeric(), en stellen fillOpacity in op een waarde kleiner dan 1 om de achtergrondkaart te kunnen zien. We gebruiken colorNumeric() om een kleurenpalet functie te maken die data waarden in kaart brengt in kleuren volgens een gegeven kleurenpalet. We maken de functie met de volgende twee parameters:\n\npalette: kleurfunctie waarnaar waarden zullen worden gemapt, en\ndomein: mogelijke waarden die kunnen worden toegewezen.\n\nTenslotte voegen we de legenda toe door de kleurenpalette-functie (pal) op te geven en de waarden die gebruikt worden om kleuren te genereren uit de palette-functie (values). We stellen opacity in op dezelfde waarde als de opacity in de gebieden, en specificeren een titel en een positie voor de legenda.\n\nlibrary(leaflet)\n\nWarning: package 'leaflet' was built under R version 4.1.3\n\nl &lt;- leaflet(map) %&gt;% addTiles()\n\npal &lt;- colorNumeric(palette = \"YlOrRd\", domain = map$SMR)\n\nl %&gt;% addPolygons(color = \"grey\", weight = 1, fillColor = ~pal(SMR), fillOpacity = 0.5) %&gt;%\n  addLegend(pal = pal, values = ~SMR, opacity = 0.5, title = \"SMR\", position = \"bottomright\")\n\n\n\n\n\nWe kunnen de kaart verbeteren door de provincies te markeren als de muis er met de muis overheen gaat en informatie te tonen over de waargenomen en verwachte tellingen, SMR’s en rookverhoudingen. We doen dit door de argumenten highlightOptions, label en labelOptions toe te voegen aan addPolygons(). We kiezen ervoor om de gebieden te highlighten met een grotere lijndikte (highlightOptions(weight = 4)). We maken de labels met HTML syntaxis. Eerst maken we de tekst die moet worden weergegeven met de functie sprintf() die een tekenvector teruggeeft die een opgemaakte combinatie van tekst en variabele waarden bevat. Vervolgens passen we htmltools::HTML() toe die de tekst als HTML markeert. In labelOptions specificeren we de labelstijl, de tekstgrootte en de richting. Mogelijke waarden voor richting zijn links, rechts en auto en dit specificeert de richting waarin het label wordt weergegeven ten opzichte van de marker. Wij kiezen auto zodat de optimale richting wordt gekozen afhankelijk van de positie van de marker.\n\nlabels &lt;- sprintf(\"&lt;strong&gt;%s&lt;/strong&gt;&lt;br/&gt;Observed: %s &lt;br/&gt;Expected: %s &lt;br/&gt;Smokers proportion: %s &lt;br/&gt;SMR: %s\",\n  map$county, map$Y,  round(map$E, 2), map$smoking, round(map$SMR, 2)) %&gt;%\n  lapply(htmltools::HTML)\n\nl %&gt;% addPolygons(color = \"grey\", weight = 1, fillColor = ~pal(SMR), fillOpacity = 0.5,\n    highlightOptions = highlightOptions(weight = 4),\n    label = labels,\n    labelOptions = labelOptions(style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n    textsize = \"15px\", direction = \"auto\")) %&gt;%\n    addLegend(pal = pal, values = ~SMR, opacity = 0.5, title = \"SMR\", position = \"bottomright\")\n\n\n\n\n\nNu kunnen we de kaart bekijken en zien welke gebieden\n\nSMR = 1, wat betekent dat de waargenomen gevallen dezelfde zijn als verwacht,\n\nSMR &gt; 1, wat aangeeft dat de waargenomen gevallen hoger zijn dan verwacht,\n\nSMR &lt; 1, wat aangeeft dat de waargenomen gevallen lager zijn dan verwacht.\n\nDeze kaart geeft een idee van het ziekterisico in Pennsylvania. SMR’s kunnen echter misleidend en onvoldoende betrouwbaar zijn in districten met kleine bevolkingsaantallen. Modelmatige benaderingen daarentegen maken het mogelijk covariaten op te nemen en informatie te lenen van naburige counties om de lokale schattingen te verbeteren, wat resulteert in het afvlakken van extreme waarden op basis van kleine steekproefgroottes. In de volgende sectie zullen we laten zien hoe we ziekterisicoschattingen kunnen verkrijgen met behulp van een model dat INLA gebruikt."
  },
  {
    "objectID": "posts/2022-08-15-geografische-analyse-van-risicos-met-inla/geografische-analyse-van-risicos-met-inla.html#modeleren",
    "href": "posts/2022-08-15-geografische-analyse-van-risicos-met-inla/geografische-analyse-van-risicos-met-inla.html#modeleren",
    "title": "Geografische analyse van risico’s met INLA",
    "section": "Modeleren",
    "text": "Modeleren\nIn this Section we specify the model for the observed data, and detail the required steps to fit the model and obtain the disease risk estimates using INLA.\n\nModel\nStel \\(Y_i\\) en \\(E_i\\) respectievelijk het waargenomen en het verwachte aantal ziektegevallen zijn en stel \\(\\theta_i\\) is het relatieve risico voor district \\(i=1,...,n\\). Het model wordt als volgt gespecificeerd:\n\\[Y_i|Poisson(E_i \\times \\theta_i), i=1,...,n,\\] \\[log(\\theta_i)=\\beta_0+\\beta_1 \\times smoking_i+u_i+v_i\\].\n\n\\(\\beta_0\\) is het intercept,\n\n\\(\\beta_1\\) is de coëfficiënt van het rokersaandeel covariaat,\n\n\\(u_i\\) een gestructureerd ruimtelijk effect is, \\(u_i|u-i∼N(u¯δi,1τunδi\\)) (conditioneel autoregressief model (CAR)),\n\n\\(v_i\\) een ongestructureerd ruimtelijk effect is, \\(v_i∼N(0,1/τv)\\).\n\n\n\nBuurtmatrix\nWe maken de buurtmatrix die nodig is om het ruimtelijk willekeurig effect te definiëren met behulp van de poly2nb() en de nb2INLA() functies van het spdeppakket. Eerst gebruiken we poly2nb() om een burenlijst te maken op basis van gebieden met aangrenzende grenzen. Elk element van de lijst nb vertegenwoordigt een gebied en bevat de indices van zijn buren. Bijvoorbeeld, nb[[2]] bevat de buren van gebied 2.\n\nlibrary(spdep)\n\nWarning: package 'spdep' was built under R version 4.1.3\n\n\nLoading required package: spData\n\nlibrary(INLA)\nnb &lt;- poly2nb(map)\nhead(nb)\n\n[[1]]\n[1] 21 28 67\n\n[[2]]\n[1]  3  4 10 63 65\n\n[[3]]\n[1]  2 10 16 32 33 65\n\n[[4]]\n[1]  2 10 37 63\n\n[[5]]\n[1]  7 11 29 31 56\n\n[[6]]\n[1] 15 36 38 39 46 54\n\n\nDaarna gebruiken we de nb2INLA() functie om de nb lijst om te zetten in een bestand genaamd map.adj met de representatie van de buurt matrix zoals vereist door INLA. Het map.adj bestand wordt opgeslagen in de werkdirectory die kan worden verkregen met getwd(). Vervolgens lezen we het map.adj bestand met behulp van de inla.read.graph() functie van INLA, en slaan het op in het object g dat we later zullen gebruiken om het ruimtelijke ziektemodel met INLA te specificeren.\n\nnb2INLA(\"map.adj\", nb)\ng &lt;- inla.read.graph(filename = \"map.adj\")\n\n\n\nInferentie met INLA\nHet model omvat twee willekeurige effecten, namelijk \\(u_i\\) voor het modelleren van ruimtelijke restvariatie, en \\(v_i\\) voor het modelleren van ongestructureerde ruis. Wij moeten twee vectoren in de gegevens opnemen die de indices van deze willekeurige effecten aanduiden. Wij noemen \\(re_u\\) de indexvector voor \\(u_i\\), en \\(re_v\\) de indexvector voor \\(v_i\\). We stellen zowel \\(re_u\\) als \\(re_v\\) gelijk aan 1,…,n, waarbij n het aantal counties is. In ons voorbeeld, n=67 en dit kan worden verkregen met het aantal rijen in de data (nrow(map@data)).\n\nmap$re_u &lt;- 1:nrow(map@data)\nmap$re_v &lt;- 1:nrow(map@data)\n\nWij specificeren de modelformule door de respons in het linkerdeel op te nemen en de vaste en toevallige effecten in het rechterdeel. Willekeurige effecten worden ingesteld met f() met parameters gelijk aan de naam van de variabele en het gekozen model. Voor \\(u_i\\) gebruiken we model = \"besag\" met een buurtmatrix gegeven door g. Voor \\(v_i\\) kiezen we model = \"iid\".\n\nformula &lt;- Y ~ smoking + f(re_u, model = \"besag\", graph = g, scale.model = TRUE) + f(re_v, model = \"iid\")\n\nWe fitten het model door de inla() functie aan te roepen en de standaard priors in INLA te gebruiken. We specificeren de formule, familie, data en de verwachte tellingen en stellen control.predictor gelijk aan list(compute = TRUE) om de posterieure gemiddelden van de voorspellers te berekenen.\n\nres &lt;- inla(formula, family = \"poisson\", data = map@data, E = E, control.predictor = list(compute = TRUE))\n\n\n\nResultaten\nWe kunnen het resultaatobject res inspecteren met summary().\n\nsummary(res)\n\n\nCall:\n   c(\"inla.core(formula = formula, family = family, contrasts = contrasts, \n   \", \" data = data, quantiles = quantiles, E = E, offset = offset, \", \" \n   scale = scale, weights = weights, Ntrials = Ntrials, strata = strata, \n   \", \" lp.scale = lp.scale, link.covariates = link.covariates, verbose = \n   verbose, \", \" lincomb = lincomb, selection = selection, control.compute \n   = control.compute, \", \" control.predictor = control.predictor, \n   control.family = control.family, \", \" control.inla = control.inla, \n   control.fixed = control.fixed, \", \" control.mode = control.mode, \n   control.expert = control.expert, \", \" control.hazard = control.hazard, \n   control.lincomb = control.lincomb, \", \" control.update = \n   control.update, control.lp.scale = control.lp.scale, \", \" \n   control.pardiso = control.pardiso, only.hyperparam = only.hyperparam, \n   \", \" inla.call = inla.call, inla.arg = inla.arg, num.threads = \n   num.threads, \", \" blas.num.threads = blas.num.threads, keep = keep, \n   working.directory = working.directory, \", \" silent = silent, inla.mode \n   = inla.mode, safe = FALSE, debug = debug, \", \" .parent.frame = \n   .parent.frame)\") \nTime used:\n    Pre = 0.408, Running = 0.363, Post = 0.0651, Total = 0.836 \nFixed effects:\n              mean    sd 0.025quant 0.5quant 0.975quant mode kld\n(Intercept) -0.323 0.150     -0.620   -0.323     -0.028   NA   0\nsmoking      1.155 0.624     -0.078    1.156      2.382   NA   0\n\nRandom effects:\n  Name    Model\n    re_u Besags ICAR model\n   re_v IID model\n\nModel hyperparameters:\n                       mean       sd 0.025quant 0.5quant 0.975quant mode\nPrecision for re_u   231.19   124.15      76.99   203.24     550.11   NA\nPrecision for re_v 18051.81 18041.22    1142.95 12555.36   66158.02   NA\n\nMarginal log-Likelihood:  -289.44 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\n\nWe zien dat de intercept \\(\\beta_0\\)= -0.323 met een 95% geloofwaardig interval gelijk aan (-0.621, -0.028) en de rookcoëfficiënt is \\(\\beta_1\\)= 1.156 met een 95% geloofwaardig interval gelijk aan (-0.081, 2.384). We kunnen de posterior verdeling van de rookcoëfficiënt plotten. We doen dit door een afvlakking van de marginale verdeling van de coëfficiënt te berekenen met inla.smarginal() en deze vervolgens te plotten met ggplot() van het ggplot2 pakket.\n\nlibrary(ggplot2)\nmarginal &lt;- inla.smarginal(res$marginals.fixed$smoking)\nmarginal &lt;- data.frame(marginal)\nggplot(marginal, aes(x = x, y = y)) + geom_line() + labs(x = expression(beta[1]), y = \"Density\") +\n  geom_vline(xintercept = 0, col = \"blue\") + theme_bw()\n\n\n\n\n\n\nDe resultaten toevoegen aan de kaart\nDe schattingen van het ziekterisico en de onzekerheid voor elk van de counties worden gegeven door de gemiddelde posterior en de 95% geloofwaardige intervallen van \\(\\theta_i\\), \\(i=1,...,n\\) die in res$summary.fitted.values staan. Kolom gemiddelde is de gemiddelde posterior en 0.025quant en 0.975quant zijn de 2.5 en 97.5 percentielen, respectievelijk. We voegen deze gegevens toe aan map om kaarten van deze variabelen te kunnen maken. We kennen het gemiddelde toe aan de schatting van het relatieve risico, en 0,025quant en 0,975quant aan de onder- en bovengrens van 95% geloofwaardigheidsintervallen van de risico’s.\n\nhead(res$summary.fitted.values)\n\n                         mean         sd 0.025quant  0.5quant 0.975quant mode\nfitted.Predictor.01 0.8789836 0.05834698  0.7637145 0.8791639  0.9939803   NA\nfitted.Predictor.02 1.0599090 0.02761024  1.0070182 1.0594681  1.1152835   NA\nfitted.Predictor.03 0.9637748 0.05138000  0.8580262 0.9652589  1.0616495   NA\nfitted.Predictor.04 1.0272365 0.05119129  0.9274745 1.0268765  1.1292698   NA\nfitted.Predictor.05 0.9074353 0.05474247  0.7985135 0.9077686  1.0152414   NA\nfitted.Predictor.06 0.9949925 0.04020308  0.9182596 0.9941313  1.0767069   NA\n\n\n\nmap$RR &lt;- res$summary.fitted.values[, \"mean\"]\nmap$LL &lt;- res$summary.fitted.values[, \"0.025quant\"]\nmap$UL &lt;- res$summary.fitted.values[, \"0.975quant\"]"
  },
  {
    "objectID": "posts/2022-08-15-geografische-analyse-van-risicos-met-inla/geografische-analyse-van-risicos-met-inla.html#ziekterisico-in-kaart-brengen",
    "href": "posts/2022-08-15-geografische-analyse-van-risicos-met-inla/geografische-analyse-van-risicos-met-inla.html#ziekterisico-in-kaart-brengen",
    "title": "Geografische analyse van risico’s met INLA",
    "section": "Ziekterisico in kaart brengen",
    "text": "Ziekterisico in kaart brengen\nWe tonen het geschatte ziekterisico in een interactieve kaart met leaflet. In de kaart voegen we labels toe die verschijnen als de muis over de counties beweegt, met informatie over waargenomen en verwachte tellingen, SMRs, rokerspercentages, RRs, en onder- en bovengrenzen van 95% geloofwaardige intervallen.\nWe zien dat districten met een hoger ziekterisico in het westen en zuidoosten van Pennsylvania liggen en districten met een lager risico in het centrum. De 95% geloofwaardige intervallen geven de onzekerheid in de risicoschattingen aan.\n\npal &lt;- colorNumeric(palette = \"YlOrRd\", domain = map$RR)\n\nlabels &lt;- sprintf(\"&lt;strong&gt; %s &lt;/strong&gt; &lt;br/&gt; Observed: %s &lt;br/&gt; Expected: %s &lt;br/&gt;\n                  Smokers proportion: %s &lt;br/&gt;SMR: %s &lt;br/&gt;RR: %s (%s, %s)\",\n                  map$county, map$Y,  round(map$E, 2),  map$smoking, round(map$SMR, 2),\n                  round(map$RR, 2), round(map$LL, 2), round(map$UL, 2)) %&gt;%\n  lapply(htmltools::HTML)\n\nleaflet(map) %&gt;% addTiles() %&gt;%\n    addPolygons(color = \"grey\", weight = 1, fillColor = ~pal(RR),  fillOpacity = 0.5,\n    highlightOptions = highlightOptions(weight = 4),\n    label = labels,\n    labelOptions = labelOptions(style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n    textsize = \"15px\", direction = \"auto\")) %&gt;%\n    addLegend(pal = pal, values = ~RR, opacity = 0.5, title = \"RR\", position = \"bottomright\")"
  },
  {
    "objectID": "posts/2022-08-18-bayes-en-stan/bayes-en-stan.html",
    "href": "posts/2022-08-18-bayes-en-stan/bayes-en-stan.html",
    "title": "Bayes en Stan",
    "section": "",
    "text": "Onlangs las ik Probability and Bayesian Modeling van Albert en Hu. Dat boek gaat over waarschijnlijkheid en bayesiaanse statistiek. Om concepten en werkwijzen duidelijk te maken gebruiken Albert en Hu het programma JAGS. Later plaatste Jim Albert ook een blog op internet waarin hij een deel van de analyses overdeed en daarbij het programma Stan en de R-interface Brms. gebruikt. Deze blog van Albert heb ik bewerkt en wat ingekort. Zo kon ik oefenen met Stan en Brms. In deze blog heb ik tegelijk geoefend met het nieuwe programma Quatrodat gemaakt is voor het maken van eigentijdse wetenschappelijke producten; voor het maken van boeken bijvoorbeeld. In deze blog heb ik voor dit keer een klein html-boekje gemaakt van Alberts tutorial dat uit acht hoofdstukken bestaat. Zo was het dit keer niet alleen oefenen met bayesiaanse analyse maar ook met wetenschappelijke opmaak.\nHier vind je het boekje. Alle documenten vind je weer in deze map"
  },
  {
    "objectID": "posts/2022-09-20-wijken-plotten/wijken.html",
    "href": "posts/2022-09-20-wijken-plotten/wijken.html",
    "title": "Wijken plotten",
    "section": "",
    "text": "Inleiding\nEen instructieve handleiding op het maken van kaart is van de hand van Daryn Ramsden, die deze handleiding schreef om te laten zien hoe je dit met de R-pakketten ‘ggplot’(en sf) kunt doen. Hij doet dat van de stad Chicago met z’n communities. Ook hij koppelt deze geografische data aan aanvullende informatie.Ook Daryn bedankt.\nDit deel, schrijft Daryn, gaat over het gebruik van het ggplot2-pakket om eenvoudige kaarten te maken met behulp van R. Alle nodige invoergegevens zijn toegankelijk op hier.\nHij heeft ontdekt dat ggplot2 een toegankelijke ingang is voor het maken van kaarten in R: ggplot2 is een makkelijke datavisualisatieoplossing in het algemeen en het maken van kaarten met ggplot2 vereist relatief weinig nieuwe kennis om aan de slag te gaan met het maken van eenvoudige kaarten.\nHieronder gaat het niet om principes van goede cartografie. De kaarten worden in deze blog in de eerste plaats gemaakt om de functionaliteit van de gebruikte pakketten te demonstreren. Hij raadt iedereen die kaarten publiceert voor publieke consumptie aan om meer na te denken over de technische keuzes die gemaakt worden, bv. de keuze van schalen, dan deze post demonstreert.\n\n\nHet sf pakket\nHet sf pakket is een sleutel tot het eenvoudig maken van kaarten met behulp van ggplot2. De naam van het pakket is afgeleid van simple features, een gestandaardiseerde manier om ruimtelijke vectorgegevens te coderen.\n\n\nread_sf gebruiken om een GeoJSON bestand in te lezen\nDe code hieronder gebruikt sf::read_sf om vector data in te lezen in GeoJSON formaat. Dit GeoJSON bestand is afkomstig van de Chicago Data Portal en bevat de grenzen van 77 geïdentificeerde communities in de stad Chicago, Illinois.\n\nlibrary(sf)\n\nWarning: package 'sf' was built under R version 4.1.3\n\n\nLinking to GEOS 3.9.1, GDAL 3.2.1, PROJ 7.2.1; sf_use_s2() is TRUE\n\nchi_map &lt;- read_sf(\"https://raw.githubusercontent.com/thisisdaryn/data/master/geo/chicago/Comm_Areas.geojson\") \n\nEen blik op het chi_map object laat zien dat het lijkt op een typisch dataframe, misschien met een paar uitzonderingen: er is een begrenzingskader, bbox, wat tekst, proj4string die het gebruikte project aangeeft, en een geometry variabele die lengte- en breedtecoördinaten lijkt te bevatten. (De schermafbeelding hieronder is het resultaat van het uitvoeren van de head functie met chi_map als invoer op mijn eigen machine).\n\nhead(chi_map)\n\nSimple feature collection with 6 features and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -87.7069 ymin: 41.79448 xmax: -87.58001 ymax: 41.99076\nGeodetic CRS:  WGS 84\n# A tibble: 6 x 10\n  community       area  shape_~1 perim~2 area_~3 area_~4 comar~5 comarea shape~6\n  &lt;chr&gt;           &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  \n1 DOUGLAS         0     4600462~ 0       35      35      0       0       31027.~\n2 OAKLAND         0     1691396~ 0       36      36      0       0       19565.~\n3 FULLER PARK     0     1991670~ 0       37      37      0       0       25339.~\n4 GRAND BOULEVARD 0     4849250~ 0       38      38      0       0       28196.~\n5 KENWOOD         0     2907174~ 0       39      39      0       0       23325.~\n6 LINCOLN SQUARE  0     7135232~ 0       4       4       0       0       36624.~\n# ... with 1 more variable: geometry &lt;MULTIPOLYGON [°]&gt;, and abbreviated\n#   variable names 1: shape_area, 2: perimeter, 3: area_num_1, 4: area_numbe,\n#   5: comarea_id, 6: shape_len\n\n\nHier zien we de top van het sf-object ingelezen uit een GeoJSON bestand.\n\n\nEen opmerking over het gebruik van shapefiles\nAls alternatief hadden wij de gegevens kunnen inlezen van een shapefile die de equivalente geospatiale informatie bevat. Shapefiles zijn een industrie-standaard, zeer algemeen bestandsformaat, en het is aannemelijk dat dit het formaat is waarmee de gegevens waarmee je wilt werken het gemakkelijkst gedistribueerd worden.\nDe github repository bevat ook een shapefile directory, Comm_Areas_shp met gelijkwaardige geospatiale informatie die ook verkregen werd via de Chicago Data Portal.\n\n\nGebruik van ggplot2 met geom_sf\nDe sleutel tot het gebruik van ggplot2 om kaarten te maken met sf objecten is dat ze ook dataframes zijn en dus in principe klaar om gebruikt te worden als data voor ggplot2::ggplot.\n\n\nEerste kaart met geom_sf\nWe kunnen een eerste kaart maken door ons kaart-dataframe te gebruiken als data-input voor ggplot2::ggplot en door gebruik te maken van een speciale geometrie, ggplot2::geom_sf:\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\nggplot(data = chi_map) + geom_sf()\n\n\n\n\n\n\nLabels toevoegen met geom_sf_text\nEen andere handige geometrie die ons in staat stelt om vrij eenvoudig informatie aan kaarten toe te voegen is ggplot2::geom_sf_text. In de onderstaande code gebruiken we deze geometrie om het aantal identificatienummers van elk gebied in de Chicago gemeenschap toe te voegen aan de eenvoudige kaart.\n\nggplot(data = chi_map) + \n  geom_sf() + \n  geom_sf_text(aes(label = area_num_1))\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n\n\n\n\n\n\nHet thema van een kaart veranderen\nWe kunnen het thema van een kaart veranderen, net als met elke andere ggplot2 grafiek. Bijvoorbeeld, hier is de vorige kaart met een extra oproep aan ggplot2::theme_bw om een zwart-wit thema te krijgen.\n\nggplot(data = chi_map) + \n  geom_sf() + \n  geom_sf_text(aes(label = area_num_1)) +\n  theme_bw()\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n\n\n\n\n\n\nAndere gegevens verkrijgen\nOm chloropleth-kaarten te maken, moeten we andere informatie verkrijgen die overeenstemt met de geografische gebieden die in de kaart worden afgebakend.\nChicago Open Data heeft een dataset van Publieke Gezondheidsstatistiek voor communities. Dit is een goede databron om te gebruiken om chloropleth kaarten te maken in combinatie met de geospatiale gegevens.\n\nlibrary(readr)\n\nWarning: package 'readr' was built under R version 4.1.3\n\nchi_health &lt;- read_csv(\"https://raw.githubusercontent.com/thisisdaryn/data/master/geo/chicago/Chicago_Health_Statistics.csv\") \n\nRows: 77 Columns: 29\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr  (2): Community Area Name, Gonorrhea in Males\ndbl (27): Community Area, Birth Rate, General Fertility Rate, Low Birth Weig...\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(chi_health)\n\n# A tibble: 6 x 29\n  Community Ar~1 Commu~2 Birth~3 Gener~4 Low B~5 Prena~6 Prete~7 Teen ~8 Assau~9\n           &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1              1 Rogers~    16.4    62      11      73      11.2    40.8     7.7\n2              2 West R~    17.3    83.3     8.1    71.1     8.3    29.9     5.8\n3              3 Uptown     13.1    50.5     8.3    77.7    10.3    35.1     5.4\n4              4 Lincol~    17.1    61       8.1    80.5     9.7    38.4     5  \n5              5 North ~    22.4    76.2     9.1    80.4     9.8     8.4     1  \n6              6 Lake V~    13.5    38.7     6.3    79.1     8.1    15.8     1.4\n# ... with 20 more variables: `Breast cancer in females` &lt;dbl&gt;,\n#   `Cancer (All Sites)` &lt;dbl&gt;, `Colorectal Cancer` &lt;dbl&gt;,\n#   `Diabetes-related` &lt;dbl&gt;, `Firearm-related` &lt;dbl&gt;,\n#   `Infant Mortality Rate` &lt;dbl&gt;, `Lung Cancer` &lt;dbl&gt;,\n#   `Prostate Cancer in Males` &lt;dbl&gt;, `Stroke (Cerebrovascular Disease)` &lt;dbl&gt;,\n#   `Childhood Blood Lead Level Screening` &lt;dbl&gt;,\n#   `Childhood Lead Poisoning` &lt;dbl&gt;, `Gonorrhea in Females` &lt;dbl&gt;, ...\n\n\nIn dit bestand zijn de identificaties van communities numerieke variabelen. Om de twee databestanden (chi_map en Chi_health) te kunnen samenvoegen, moet een nieuwe kolom, area_num_1, worden aangemaakt als tekst/karakterdata. (Door dezelfde naam, area_num_1, te gebruiken die reeds in het chi_map data frame staat, wordt het samenvoegen bijzonder gemakkelijk.).\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nchi_health &lt;- chi_health %&gt;% \n  mutate(area_num_1 = as.character(`Community Area`))\n\n\n\nSamenvoegen van een sf-object met een ander dataframe\nVervolgens, gebruik dplyr::left_join om chi_map te verbinden met chi_health:\n\nchi_health_map &lt;- left_join(chi_map, chi_health, by = \"area_num_1\")\n\nDit creëert een enkel dataframe met de geografische grenzen en de waarden voor de gemeten variabelen.\n\n\nEen chloropleth-kaart maken\nOm een chloropleth-kaart te maken met behulp van een van de statistieken in het samengevoegde dataframe, kunnen we de fill aesthetic gebruiken. Hier gebruiken we de kolom “Unemployment” van de samengevoegde gegevens:\n\nchi_health_map_nl&lt;-chi_health_map %&gt;%\n  rename(Werkloosheid=Unemployment) \n\n\nggplot(data = chi_health_map_nl, aes(fill = Werkloosheid)) + \n  geom_sf() + \n  ggtitle(\"Werkloosheidspercentages in de communities van Chicago\")\n\n\n\n\n\n\nDe schaal veranderen\nDe vorige kaart gebruikt de standaard continue schaal voor de vulling esthetiek. Merk op dat we dezelfde kaart kunnen maken als voorheen door scale_fill_continuous te gebruiken:\n\nggplot(data = chi_health_map_nl, aes(fill = Werkloosheid)) +\n  geom_sf() + \n  scale_fill_continuous() + \n  ggtitle(\"Werkloosheidpercentages in communities van Chicago\")\n\n\n\n\n\n\nGebruik van viridis scale\nWe kunnen viridis scale als alternatief gebruiken die ook beschikbaar is voor scale_fill_continuous:\n\nggplot(data = chi_health_map_nl, aes(fill = Werkloosheid)) +\n  geom_sf() + \n  scale_fill_continuous(type = \"viridis\") + \n  ggtitle(\"Werkloosheidsperscentages in communities van Chicago\")\n\n\n\n\n\n\nContinue schaal met de hand instellen\nEen andere optie is met de hand instellen van laag and hoog argumenten voor scale_fill_continuous.\n\nggplot(data = chi_health_map_nl, aes(fill = Werkloosheid)) +\n  geom_sf() + \n  scale_fill_continuous(low = \"ivory\", high = \"brown\") + \n  ggtitle(\"Werkloosheidspercentages in communities van Chicago\")\n\n\n\n\n\n\nEen divergente schaal gebruiken\nMisschien wil je wel een divergente schaal gebruiken:\nOm een divergerende - maar nog steeds continue schaal te krijgen - kun je ggplot2::scale_fill_gradient2 gebruiken. Om deze schaal te gebruiken, stelt je kleuren in voor de argumenten low, high en mid. Het kan zijn dat u ook het midpoint argument moet instellen (dat anders standaard op 0 zou worden gezet).\n\nggplot(data = chi_health_map_nl, aes(fill = Werkloosheid)) +\n  geom_sf() + \n  scale_fill_gradient2(low = \"blue\", mid = \"white\", high = \"red\", midpoint = 20) +\n  ggtitle(\"Werkloosheidspercentages in communities van Chicago\")\n\n\n\n\n\n\nGebruik van een discrete schaal\nTot nu toe hebben we continue schalen gebruikt om de communities van de chloroplethkaarten in te kleuren. Dit is een gevolg van het gebruik van een numerieke variabele als onze fill aes. Als alternatief kunnen we een discrete schaal gebruiken door de fill toe te wijzen aan een categorische variabele.\n\n\nEen nieuwe categorische variabele creëren\nMaak eerst een nieuwe categorische variabele voor Werkloosheid - of een andere variabele in de gegevens van uw keuze. De code gebruikt dplyr::case_when om de variabele Werkloosheid in bereiken te verdelen:\n\nchi_health_map_nl &lt;- chi_health_map_nl %&gt;% \n  mutate(Werkl_cat = case_when(\n    Werkloosheid &lt;= 10 ~ \"0 - 10\",\n    10 &lt; Werkloosheid & Werkloosheid &lt;= 20 ~ \"10+ - 20\",\n    20 &lt; Werkloosheid & Werkloosheid &lt;= 30 ~ \"20+ - 30\",\n    30 &lt; Werkloosheid  ~ \"30+\"))\n\nNow, plot the new map:\n\nggplot(data = chi_health_map_nl) + \n  geom_sf(aes(fill = Werkl_cat))  +\n  labs(fill = \"Werkloosheid (%)\") + \n  ggtitle(\"Werkloosheidspercentages in communities van Chicago\")\n\n\n\n\n\n\nPunten plotten op de kaart\nOm een voorbeeld te geven van het plotten van punten op een kaart, kunnen we gebruik maken van Chicago Restaurant Inspectie Data voor februari 2020. Lees eerst de gegevens in:\n\ninspections &lt;- read_csv(\"https://raw.githubusercontent.com/thisisdaryn/data/master/geo/chicago/Food_Inspections.csv\")\n\nRows: 950 Columns: 17\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (12): DBA Name, AKA Name, Facility Type, Risk, Address, City, State, Ins...\ndbl  (5): Inspection ID, License #, Zip, Latitude, Longitude\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nDeze dataset heeft de volgende variabelen: - Inspection\n- ID,\n- DBA\n- Name,\n- AKA,\n- Name,\n- License #,\n- Facility\n- Type,\n- Risk,\n- Address,\n- City,\n- State,\n- Zip,\n- Inspection Date,\n- Inspection\n- Type,\n- Results,\n- Violations,\n- Latitude,\n- Longitude, and\n- Location.\nNu kunnen we elk geïnspecteerd restaurant in de tijdsperiode op de kaart plaatsen door de x en y aes te koppelen aan de Longitude en Latitude variabelen in het dataframe. (Bovendien, de code hieronder mapt de kleur aes aan de Results variabele).\n\nggplot() + \n  geom_sf(data = chi_map, fill = \"ivory\", colour = \"ivory\") + \n  geom_point(data = inspections, aes(x = Longitude, y = Latitude, colour = Results)) +\n  scale_color_viridis_d()\n\nWarning: Removed 4 rows containing missing values (geom_point).\n\n\n\n\n\n\n\nInteractieve kaarten met plotly\nHet maken van interactieve kaarten met plotly kan ook relatief eenvoudig zijn. In de onderstaande code voeren we de volgende stappen uit:\n\nMaak een nieuwe variabele, desc met daarin de Restaurantnaam, de inspectiedatum en het resultaat van de inspectie aan de hand van de corresponderende variabelen in de gegevens.\n\nMaak een kaart met een uniforme achtergrondkleur met punten voor elke restaurantinspectie\n\nMaak een interactieve grafiek met plotly::ggplotly met de desc variabele als tooltip\n\n\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\ninspections &lt;- inspections %&gt;% mutate(desc = paste(`AKA Name`, `Inspection Date`, Results, sep = \"\\n\"))\n\ninsp_plt &lt;- ggplot() + \n  geom_sf(data = chi_map, fill = \"ivory\", colour = \"ivory\") + \n  geom_point(data = inspections, \n             aes(x = Longitude, y = Latitude, colour = Results, text = desc)) +\n  scale_color_viridis_d()\n\nWarning: Ignoring unknown aesthetics: text\n\nggplotly(insp_plt, tooltip = \"desc\")"
  },
  {
    "objectID": "posts/2021-02-19-website-met-distill/website-met-distill.html",
    "href": "posts/2021-02-19-website-met-distill/website-met-distill.html",
    "title": "Website met distill",
    "section": "",
    "text": "Mijn eigen Harrie’s Hoekje blog, over een aantal ontwikkelingen in de dataanalyse, maak ik ook met het gebruik van het pakket Distill. Met Distill kun je wetenschappelijke websites maken, een blog en artikelen schrijven. Over hoe je dat doet schreef Lisa Lendway een kort en krachtige blog. Dat kan ik niet beter. Dank je, Lisa, hiervoor. Haar blog staat hier"
  },
  {
    "objectID": "posts/2021-02-19-website-met-distill/website-met-distill.html#waarom-een-website",
    "href": "posts/2021-02-19-website-met-distill/website-met-distill.html#waarom-een-website",
    "title": "Website met distill",
    "section": "Waarom een website?",
    "text": "Waarom een website?\nNou, eindelijk heb ik het gedaan!, zo begint zij haar blog dat ik (Harrie) hier verder volg.\nIk (Lisa) heb een website gemaakt. En om dat te vieren, ga ik met jullie delen hoe ik het gedaan heb. En waarom heb ik dat gedaan? Twee belangrijke redenen zijn er: 1. om materiaal te delen dat nuttig kan zijn voor anderen, 2. om wat dingen voor mezelf te documenteren, allemaal op één plek.\nIk koos voor een {distill} site omdat het me genoeg vrijheid leek te geven om mijn site aan te passen en ook weer niet zo veel vrijheid dat ik zou verzanden in details (bv. kleuren kiezen … oeps, daar heb ik toch nog veel tijd aan besteed)."
  },
  {
    "objectID": "posts/2021-02-19-website-met-distill/website-met-distill.html#bronnen",
    "href": "posts/2021-02-19-website-met-distill/website-met-distill.html#bronnen",
    "title": "Website met distill",
    "section": "Bronnen",
    "text": "Bronnen\nVoordat ik begin, zal ik wat bronnen delen die ik heb gebruikt.\n\nAlison Hill en Desirée De Leon’s webinar over Sharing on Short Notice. KIJK HIERNAAR voordat je verder gaat. Hier werd ik voor het eerst geïntroduceerd op netlify en toen zag ik pas hoe makkelijk het is om html-files tot een website om te vormen website. Je zou daar zelfs eerst mee kunnen beginnen voordat jij je op een website springt. Misschien spreken sommige van de andere opties die ze bespreken jou meer aan dan {distill}.\nDe distill documentatie, ook al in de vorm van een … distill website!\nDistill websites van anderen: Ijeamaka Anyene, Shannon Pileggi(aka Piping Hot Data), Miles McBain, Tom Mock, en meer!\nAlison Hill’s website voor hoog niveau inhoud en design inspiratie. Iedere keer vind ik wel een nieuw bron als ik haar website bezoek. Bijvoorbeeld, bekijk eens haar praatje op ‘Recent updates in the R markdown family’.\nEn meer! Ik zal proberen in dit blog op enkele bronnen terug te komen.\n\nKijk ook naar de video die Lisa maakte en die je hier vindt."
  },
  {
    "objectID": "posts/2021-02-19-website-met-distill/website-met-distill.html#bouwen-van-de-site",
    "href": "posts/2021-02-19-website-met-distill/website-met-distill.html#bouwen-van-de-site",
    "title": "Website met distill",
    "section": "Bouwen van de site",
    "text": "Bouwen van de site\nLaten we nu verder gaan met het maken van de site. Onderweg kom ik terug op de YouTube-video. Ik kom steeds terug op dezelfde YouTube-video, maar ik zal ze daar zetten waar ik het op dat moment over heb. Zo is het makkelijker voor je om delen over te slaan als je dat wilt.\n\nEen GitHub repo opzetten & het project starten\nKijk naar Tom Mock’s post hier. Ik denk dat zijn manier om dit te doen logischer is dan de mijne. Helaas, zag ik het toen ik mijn ding had gedaan :(\nIk probeer er een gewoonte van te maken om al mijn projecten met een GitHub repo te beginnen. Dus, dat is wat ik hier ook heb gedaan. Hier zijn alle stappen:\n\nMaak een repo\n\nCreëer project in R Studio door de repo te klonen * Laad de {distill} bibliotheek\n\nMaak een “starter” site met de create_website() functie. Ik heb dit gebruikt in plaats van create_blog() omdat ik van mijn hoofdpagina een About pagina wilde maken in plaats van een blog. Ik zal het blog gedeelte later toevoegen. Lees de {distill} documentatie om je te helpen bij het beslissingsproces. Omdat ik eerst mijn GitHub repo heb gemaakt, moest ik wat rare dingen doen om de mappenstructuur te fixen. Het werkt, maar het is een beetje lelijk.\n\nVerplaats alle bestanden behalve het .Rproj bestand van de zojuist gemaakte map naar de hoofdmap van het archief.\n\nVerwijder de website map (zou leeg moeten zijn, behalve het .Rproj bestand).\n\nVerwijder het README.md bestand in de hoofdmap van het archief (als ik dat niet deed, bouwde de site later niet).\n\n\nOf kijk naar dit deel van de video (tot minuut 8:04):"
  },
  {
    "objectID": "posts/2021-02-19-website-met-distill/website-met-distill.html#de-site-voor-de-eerste-keer-bouwen",
    "href": "posts/2021-02-19-website-met-distill/website-met-distill.html#de-site-voor-de-eerste-keer-bouwen",
    "title": "Website met distill",
    "section": "De site voor de eerste keer bouwen",
    "text": "De site voor de eerste keer bouwen\nVervolgens willen we de site bouwen. Om dit op een eenvoudige manier te doen, sla je jouw bestanden op, sluit je RStudio en open je het opnieuw, waarbij jij ervoor zorgt dat je je in het project van jouw distill-site bevindt. Wanneer je dit doet, zou er een Build tab moeten verschijnen in jouw paneel aan de rechterbovenhoek (of waar u gewoonlijk jouw Environment, History, enz. hebt). Klik op het Build Website-icoon en je zou je site moeten zien! (8:25 in de video, als je het mij wilt zien doen).\nOp dit punt zijn er veel verschillende richtingen die je op kunt gaan. Ik zal je vertellen wat ik gedaan heb. Als je niet veel meer wilt aanpassen, kun je naar @ref(netlify) gaan om een eenvoudige manier te vinden om je website te publiceren."
  },
  {
    "objectID": "posts/2021-02-19-website-met-distill/website-met-distill.html#aanpassen-van-de-home-page",
    "href": "posts/2021-02-19-website-met-distill/website-met-distill.html#aanpassen-van-de-home-page",
    "title": "Website met distill",
    "section": "Aanpassen van de home page",
    "text": "Aanpassen van de home page\nIk wilde dat mijn “Home” pagina mijn “About” pagina zou worden. Om dit te doen, heb ik eerst wat veranderingen aangebracht in het _site.yml bestand, het “About” gedeelte van de navigatiebalk verwijderd en de tekst voor de homepage hernoemd naar “About”.\nDan, om te beginnen met het aanpassen van mijn “About” pagina, voeg ik een foto van mezelf toe aan het index.Rmd bestand en plaats ik wat plaatshouders voor plaatsen waar ik wat informatie zal schrijven.\nBekijk dit in de video (tot minuut 17:35):"
  },
  {
    "objectID": "posts/2021-02-19-website-met-distill/website-met-distill.html#voeg-het-blog-toe-en-maak-jouw-eerste-post",
    "href": "posts/2021-02-19-website-met-distill/website-met-distill.html#voeg-het-blog-toe-en-maak-jouw-eerste-post",
    "title": "Website met distill",
    "section": "Voeg het blog toe en maak jouw eerste post",
    "text": "Voeg het blog toe en maak jouw eerste post\nAls je de blog route vanaf het begin hebt gevolgd, hoef je dit deel niet te doen. Merk op dat ik in de video de dingen in de verkeerde volgorde deed\n\nVoeg een post toe met create_post(\"mijnpost\"). Dit genereert een R Markdown bestand met de naam mypost.Rmd (tenzij je de slug verandert), een _posts map, en een map die de datum en de naam van het bericht heeft. Door te beginnen met de datum, houdt het je berichten in een mooie volgorde :)\n\nBewerk jouuw blog post-Rmarkdownbestand naar believen. Zorg ervoor dat je dit bestand knit zodat het op de blog verschijnt. Deze bestanden worden niet automatisch gebreid. Dat is met opzet.\n\nMaak een nieuw R Markdown bestand met ALLEEN een yaml kop met een titel en listing. Sla het op in de hoofd repository.\nWijzig het _site.yml bestand om de listing pagina te linken. De tekst kan zijn wat je maar wilt - dit is wat er op de navigatiebalk komt te staan. De href waarde is de .html van het listing .Rmd bestand.\n\nVoeg een aangepaste blog preview afbeelding toe. Zet de afbeelding die je hiervoor wilt gebruiken in de map voor de blog post. In de yaml kop van het R Markdown bestand van uw blog, voeg je preview: image.png toe, waar image.png de naam van jouw afbeelding is. Standaard zal de preview de eerste plot zijn die gegenereerd wordt in uw R code.\n\nBekijk dit in de video (tot minuut 33:27):"
  },
  {
    "objectID": "posts/2021-02-19-website-met-distill/website-met-distill.html#pas_site.yml-aan",
    "href": "posts/2021-02-19-website-met-distill/website-met-distill.html#pas_site.yml-aan",
    "title": "Website met distill",
    "section": "Pas_site.yml aan",
    "text": "Pas_site.yml aan\nIn dit deel voeg ik enkele aangepaste iconen toe aan de bovenste navigatiebalk van de site. Deze bevatten een persoonlijke favicon aan de linkerkant (die ik uiteindelijk toch weer weghaal) en links naar mijn GitHub, LinkedIn en Twitter pagina’s (en later voeg ik er een toe aan mijn YouTube kanaal).\nVoeg het volgende toe aan het _site.yml bestand na de navbar koptekst. Wees voorzichtig met inspringen. Je kunt mijn bestand hier bekijken (ik heb meer bewerkt sinds het maken van de video, dat wel).\n- icon: fa fa-github\n  href: https://github.com/YOUR_USERNAME\n- icon: fa fa-linkedin\n  href: https://www.linkedin.com/in/YOUR_LINKEDIN/\n- icon: fa fa-twitter\n  href: https://twitter.com/YOUR_TWITTER\nOm een gepersonaliseerde favicon toe te voegen, voeg het volgende toe na navbar:, waar ll.png de persoonlijke favicon is. Je kunt ook een link naar een website toevoegen waar hij naartoe gaat als je er op klikt. Nogmaals, wees voorzichtig met inspringen.\n  logo:\n    image: ll.png\nVolg de video hieronder (tot minuut 44:22). Toen ik dit de eerste keer deed, maakte ik wat fouten, dus ik laat je dat deel van de video overslaan."
  },
  {
    "objectID": "posts/2021-02-19-website-met-distill/website-met-distill.html#netlify",
    "href": "posts/2021-02-19-website-met-distill/website-met-distill.html#netlify",
    "title": "Website met distill",
    "section": "Publiseer de site via netlify",
    "text": "Publiseer de site via netlify\nNu je een website hebt, kun je die gemakkelijk publiceren via netlify. Ik zal je laten zien hoe je deze aan je GitHub repo kunt koppelen, zodat iedere keer dat je wijzigingen naar GitHub stuurt, je website die wijzigingen zal weergeven. Ik raad aan om eerst een account op netlify aan te maken.\nBekijk de video om te zien hoe ik het doe (tot minuut 48:22):"
  },
  {
    "objectID": "posts/2021-02-19-website-met-distill/website-met-distill.html#maak-het-je-eigen",
    "href": "posts/2021-02-19-website-met-distill/website-met-distill.html#maak-het-je-eigen",
    "title": "Website met distill",
    "section": "Maak het je eigen!",
    "text": "Maak het je eigen!\nHet laatste stuk is om wat aanpassingen te doen. Dankzij de geweldige {distill} auteurs, kunnen we de create_theme() functie gebruiken om ons door het aanpassen van wat css te leiden. Ik ben een echte beginner als het op css aankomt, dus er is van een makkelijkere manier. Ik raad ten zeerste aan om de documentatie over theming en de recente updates door te lezen. En lees grondig de tekst van de website (misschien heb ik dat de eerste keer niet gedaan)!\n\nVoegtheme: \"my_theme.css\" aan de bodem van de _site.yml file toe.\n\nJe kunt de video tot het einde bekijken:"
  },
  {
    "objectID": "posts/2021-03-10-github/github.html",
    "href": "posts/2021-03-10-github/github.html",
    "title": "GitHub voor samenwerking",
    "section": "",
    "text": "GitHub\nIk was al vaker van plan hier een stukje over te schrijven. Lisa Lendways tekst hierover vind ik heel duidelijk. Lisa, ik hoop dat je het goed vindt dat ik mij aan jou op trek zo. Dank je wel.\n\n\n\nMijn eigen GitHub account\n\n\nLisa Lendway heeft veel van haar materiaal weghaald uit Happy git with R by Jenny Bryan. Dat is inderdaad een uitstekende bron, maar bevat ook veel informatie die we niet altijd nodig hebben. Als je Git en GitHub op meer geavanceerde manieren wilt gebruiken of als dit stuk onduidelijk is voor je, dan moet je het zeker bekijken. Het is trouwens überhaupt een goede bron.\n\n\nVideo uitleg\n\n\nVoicethread tutorial\n\n\nGit en GitHub\nGit is een versie controle systeem. Het is net zoiets als Googledocs, maar het biedt ruimte aan veel soorten bestanden, ook bestanden waar Google docs niets mee kan … zoals .rmd bestanden! GitHub is een online interface om met Git te werken.\nWaarom leren we deze dingen?!\n\nGitHub is goed geïntegreerd met R Studio. Dus, we zullen geen command-line functies hoeven te gebruiken, tenminste niet nadat we alles hebben ingesteld.\nJe bent verplicht om R te gebruiken voor je eindproject. De presentatie of paper moet worden opgeslagen als een .rmd document dat kan worden ‘geknit’ tot een html document. Door GitHub te gebruiken, kun je gemakkelijk met je groep samenwerken, ook als je niet bij elkaar bent.\nGitHub leert je een aantal goede gewoontes aan. Je wordt gedwongen om na te denken over wanneer je ieets opslaat en om notities te maken over welke wijzigingen je hebt gemaakt, bijvoorbeeld.\n\n\n\nMaak eerst een GitHub account aan\n\nGa naar http://github.com\nGebruik een username … zie Jenny Bryan’s tips. Incorporeer jouw eigen naam, maar gebruik een andere usernaam die je verder gebrukkt, neem iets waar jouw toekomstige baas zich prettig bij voelt. De username van de universiteit is misschien een goede optie.\n\n\n\nInstalleer Git\n1. Controleer of je Git al geïnstalleerd hebt. Dit zal alleen het geval zijn als je het ergens anders gebruikt hebt. Om dit te doen, open je de commandoregel of, in R Studio, vouw je de Console uit. Er zou een tabblad moeten zijn dat Terminal zegt. In dat gebied type je\nwhich git\nHet geeft iets terug als\n/usr/bin/git\ndan ben je klaar en hoef je Git niet meer te installeren. Op een Windows machine kun je misschien niet eens het which git commando succesvol intypen. Dit zou verholpen moeten zijn door git te installeren. Of je zult de shell moeten gebruiken.\n\nAls je Git niet geïnstalleerd hebt, moet je het installeren. De instructies zijn iets anders voor Windows en Macs.\n\nVoor een Windows machine:\n\nInstalleer Git for Windows. Als er gevraagd wordt naar “Aanpassen van uw PATH omgeving”, zorg er dan voor dat u “Git vanaf de commandoregel en ook van software van derden” selecteert. Anders denken we dat het goed is om de standaardinstellingen te accepteren.\nR Studio voor Windows geeft er de voorkeur aan dat Git geïnstalleerd wordt onder C:/Program Files en dit lijkt de standaard te zijn. Dit houdt bijvoorbeeld in dat de Git executable op mijn Windows systeem te vinden is in C:/Program Files/Git/bin/git.exe. Tenzij je specifieke redenen hebt om anders te doen, volg deze conventie.\n\nVoor een Mac machine:\n\nGa naar jouw shell/terminal en voer één van deze commando’s in om een aanbod te krijgen om developer command line tools te installeren. Accepteer het aanbod … klik op installeren.\n\ngit --version\ngit config\n\nSommigen van jullie die op een Mac werken moeten misschien eerst het volgende doen in de terminal als je een project zonder succes probeert te openen.\n\nxcode-select --install\nJe komt er zo achter of dit het geval is.\n\nGa nu terug naar de Console in R Studio en installeer het usethis pakket in R Studio. Sluit vervolgens R Studio en open het opnieuw.\nLaad de usethis bibliotheek door het volgende stukje code in de console uit te voeren:\n\nlibrary(usethis)\n\nVoer de volgende code uit in de console met enkele kleine wijzigingen. De user.name is je Git gebruikersnaam. Dit kan anders zijn dan je GitHub gebruikersnaam, hoewel het misschien een goed idee is om het gewoon hetzelfde te houden. De user.email MOET hetzelfde zijn als je GitHub gebruikers email.\n\nuse_git_config(user.name = \"Jane Doe\", user.email =       \"jane@example.org\")\n\n\nMaak een eerste repo (repository) en gebruik RStudio daarbij\nHet woord “repo” is een afkorting van “repository”, en dat is precies wat het is: een plaats waar dingen (onze bestanden, in dit geval) worden opgeslagen. Het is als de map die je gemaakt hebt om al je werk voor deze les in op te slaan.\nLaten we naar GitHub gaan en inloggen. Nadat je ingelogd bent, zou je een klein icoontje in de rechter bovenhoek moeten zien. Het mijne is een afbeelding van mij. Als ik daar op klik verschijnt er een drop-down en kan ik “Your repositories” kiezen. Doe dat. Je zou nu zoiets als dit moeten zien:\n\nKlik op de “New” knop. Geef jouw repository een naam, bv NAME_test_repo, waar NAME eigenlijk jouw naam is. Kies Public en klik de README file aan. Klik dan op Create repository.\n\nEr zijn dingen die je direct binnen GitHub kunt doen, maar we zullen ons richten op de integratie met R Studio.\n\n\nKlonen van een repo\nDenk aan het klonen van een repo als het “kopiëren” van de repository naar je computer. Maar als met het kopiëren doet, houdt het de verbinding met de online repo.\nLaten we dit doen. Op je mijn_test_repo pagina, kies je de groene knop met Code en kopieer je het pad door het icoontje met een pijl erop te selecteren en erop te klikken.\nGa nu naar R Studio. Klik op Bestand –&gt; Nieuw Project … Je zou nu een venster moeten zien dat er als volgt uitziet:\n\nKies Version Control. Dan zie je een scherm dat er zo’n beetje zo uitziet:\n\nKies Git. Dan zou je een scherm moeten zien dat er uitziet als dit, zonder alle details ingevuld. De Repository URL is waar je de repo URL moet plakken die je gekloond hebt van github. Het zal ook de Project mapnaam invullen. Laat die gewoon staan. Let op waar de project directory zich bevindt en verander het naar een betere directory indien nodig. Klik op Create Project.\n\nAls je in de Bestanden tab kijkt in het rechter ondervenster van R Studio, dan zou je het .gitignore bestand moeten zien, het project bestand (eindigt op .Rproj), en het README.md bestand. Je zou ook een Git tab moeten zien in het rechter bovenvenster van R Studio. Als je nu op de Git tab klikt, zul je daar niets zien.\nMet de Git tab open, laten we het README.md bestand in R Studio openen. Maak een kleine wijziging in het bestand door de zin “Ik verander iets in dit bestand.” toe te voegen. Klik dan op het save icoon. Als je dit doet, zul je README.md zien verschijnen in de Git tab.\nKlik nu op de Commit knop in de Git tab. Zet een vinkje in het vakje naast het README.md bestand onder het woord Staged (in de toekomst kun je meerdere bestanden tegelijk stagen door de vakjes naast meerdere bestanden aan te vinken) en voeg een commentaar toe aan het commit vakje.\nHet zou er ongeveer zo uit moeten zien:\n\nKlik tenslotte op commit. Je krijgt dan een bericht dat het voltooid is. Het bericht kan cryptisch overkomen als je er niet aan gewend bent. Het ziet er ongeveer zo uit:\n\nDe wijziging die je hebt gemaakt is nu gecommit in het lokale geheugen. Het gewijzigde bestand is alleen gewijzigd op je computer, NIET online als je op GitHub kijkt … ga maar eens kijken. Klik op de Diff knop in de Git tab en je kunt de geschiedenis van je commits zien.\nVervolgens gaan we die wijzigingen naar GitHub pushen door op de groene pijl omhoog in de Git tab te klikken. Dit zal je een bericht geven dat er ongeveer zo uitziet:\n\nWerd je gevraagd om een gebruikersnaam en wachtwoord? Probeer een andere wijziging te maken, vast te leggen en te pushen. Wordt er nog steeds om een gebruikersnaam en wachtwoord gevraagd? Zo ja, dan kun je hier zien hoe je dat doet Jenny Bryans bron.\nPROBEER HET EENS!!\n\nVoeg een .rmd bestand toe aan je project. Doe dit door te gaan naar Bestand –&gt; Nieuw bestand –&gt; R Markdown … Voeg wat woorden en een R code chunk toe aan het .rmd bestand. Sla het op, commit het (vergeet het bericht niet!), en push het. Controleer GitHub online om er zeker van te zijn dat je het .rmd bestand daar ziet.\nNu, knit je het bestand lokaal. Commit de wijzigingen (zorg ervoor dat je een vinkje zet naast alles wat je ge-staged wilt hebben - .rmd, .html, etc.) en push ze naar GitHub. Controleer GitHub online om er zeker van te zijn dat je alles ziet wat je verwacht.\n\n\n\nPartners toevoegen\nTot nu toe hebben we eigenlijk alleen technieken geleerd om GitHub te gebruiken om onze eigen bestanden te beheren, maar het coolste eraan zijn de samenwerkingsmogelijkheden. De manier waarop we dit gaan leren is door medewerkers aan de repo toe te voegen.\nZoek iemand om mee samen te werken. Als er een oneven aantal is, maak dan een groepje van drie. In je groepje, voeg elkaar toe als medewerkers aan je project. In GitHub, op de repo pagina, ga naar Instellingen. Een van de opties aan de linkerkant is Collaborators. Klik daarop en doe wat er staat.\nDe persoon die is uitgenodigd om samen te werken zal een email ontvangen en zou ook in staat moeten zijn om de uitnodiging op GitHub te zien. Ze moeten deze accepteren. Eenmaal geaccepteerd, zouden jullie beiden (of alle drie) toegang moeten hebben om wijzigingen in het bestand vast te leggen.\n\n\nCommit –&gt; Push –&gt; Pull –&gt; … (en Communicatie)\nZodra je medewerkers hebt toegevoegd, kunnen alle medewerkers committen en pushen. Maar, wat gebeurt er als iemand iets commit en terugzet en jij gaat er dan aan werken op je computer… hoe krijg je dan die wijzigingen? … PULL!\nProbeer in jullie groepen het volgende. Jullie moeten allemaal meewerken aan elkaars projecten, dus je kunt van rol wisselen nadat je het één keer gedaan hebt.\n\nDe medewerker moet eerst de repo klonen waaraan hij gevraagd is om mee te werken. Als ze een ander project open hebben, sla dan op, commit, en push alle wijzigingen. Sluit dan dat project en open het project waar ze om gevraagd is om aan mee te werken door de GitHub repo te klonen. De medewerker moet het project open hebben in R Studio.\nDe medewerker moet proberen te trekken (pullen) door op de aqua pijl omlaag te klikken in de Git tab. Je zou een bericht moeten krijgen dat er als volgt uitziet:\n\n\n\nDe persoon die de repo heeft aangemaakt, maakt een wijziging in zijn .rmd bestand. Het kan een kleine wijziging zijn, zoals het toevoegen van een zin. Diezelfde persoon slaat het bestand op, commit (staged en schrijft een commit boodschap), en pushed het naar GitHub. Controleer online om er zeker van te zijn dat de meest recente wijzigingen zijn gepushed.\nDe medewerker haalt nu die wijzigingen naar zijn lokale map (naar zijn computer). Klik op het pull icoon. Je zou een bericht moeten zien dat er ongeveer zo uitziet:\n\n\nEn controleer het bestand waarin een wijziging is aangebracht om er zeker van te zijn dat de wijziging wordt weerspiegeld in het bestand op uw computer.\n\nGa nog een paar keer heen en weer en breng kleine wijzigingen aan. Degene die eigenaar is van de repo zou de wijziging moeten maken en de medewerker zou het moeten binnenhalen. Wissel dan van rol. Als je wisselt, wees er dan zeker van dat je aan het juiste project werkt.\n\n\n\nConflicten samenvoegen\nAls je samen aan een project werkt, is de kans groot dat je tegen een moment aanloopt waarop twee van jullie hetzelfde bestand tegelijkertijd aan het bewerken zijn. Soms, als je allebei je wijzigingen probeert te pushen, zul je wat genoemd wordt een “merge conflict” krijgen. GitHub zal niet weten welke te gebruiken. Dus, zal het je dwingen om te beslissen.\nAls je probeert je wijzigingen naar GitHub te pushen en iemand anders heeft zijn wijzigingen met betrekking tot hetzelfde bestand al gepushed, dan zul je een bericht als dit krijgen:\n\nDan, wanneer je de wijzigingen binnenhaalt, krijg je een bericht zoals dit:\n\nMerk op dat het je vertelt in welk bestand het samenvoegconflict zich voordeed. Je moet dat bestand openen en beslissen hoe de conflicterende informatie samengevoegd moet worden. In het begin zal het er ongeveer zo uitzien:\n\nHet deel na het woord HEAD is wat in je lokale bestand staat. Alles na de ====== is wat in het bestand op afstand staat (d.w.z. de wijzigingen die je medewerker heeft gemaakt). Je kunt besluiten om dit op te lossen op elke manier die je wilt: combineer de twee ideeën, verwijder ze allebei, houd er maar een over, etc. Als je klaar bent, zorg er dan voor dat je de &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD en &gt;&gt;&gt;&gt;&gt;&gt;&gt; verwijdert, gevolgd door de alfanumerieke string, plus alle andere vreemde tekens.\nSla het bestand dan op en doe de gebruikelijke commit en push. Je zou de wijzigingen naar GitHub gepushed moeten zien worden.\n\n\nLaten we dit eens proberen!\n\nIn groepjes van 3-4, oefen je GitHub vaardigheden.\n\n\nKies iemand om een nieuwe repo aan te maken op GitHub genaamd our_collaborative_graph.\n\nDe maker voegt de anderen toe als collaborators.\n\nDe medewerkers moeten hun e-mail controleren en accepteren dat ze medewerkers zijn.\n\nDe maker en de medewerkers klonen de repo lokaal.\n\nEen medewerker voegt lokaal een .rmd bestand aan het project toe. De titel moet zijn “Onze grafiek” en voeg alle groepsleden toe als auteurs. Voeg een R code stuk toe dat de tidyverse bibliotheek laadt. Sla het bestand op, commit met bericht, en push naar GitHub. Controleer online om er zeker van te zijn dat het goed gepushed is.\n\nAlle andere groepsleden halen de wijzigingen lokaal op.\n\nEen andere medewerker voegt een ander R code stuk toe. Maak met de mpg dataset een scatterplot met hwy op de y-as, displ op de x-as en kleur de punten met drv. Sla de wijzigingen op. Brei het bestand. commit dan met bericht en push naar GitHub. Zorg ervoor dat je alle bestanden in de commit staged. Controleer online om er zeker van te zijn dat je de wijzigingen ziet.\n\nAlle andere groepsleden halen de wijzigingen lokaal op.\n\nEen ander groepslid (medewerker of maker als je maar 3 groepsleden hebt) wijzigt de R code chunk die de grafiek maakt, door mooie x en y labels toe te voegen en te veranderen naar theme_minimal(). Sla de wijzigingen op. Brei het bestand. commit dan met bericht en push naar GitHub. Zorg ervoor dat je alle bestanden in de commit staged. Controleer online om er zeker van te zijn dat je de veranderingen ziet.\n\nAlle grop leden trekken. Degene die net gepushed heeft zou moeten zien dat ze al up to date zijn. Alle anderen zouden de wijzigingen lokaal terug moeten zien.\n\nNu moeten alle groepsleden iets toevoegen aan het .rmd bestand. Vertel elkaar niet wat je toevoegt. Als je klaar bent, sla op, brei, commit, en push naar GitHub. Ten minste één van jullie zal een samenvoeg conflict krijgen, dus zal het je vragen om wijzigingen van GitHub binnen te halen en het conflict op te lossen. Doe dat. Deze keer zul je het .rmd bestand moeten aanpassen in plaats van de README zoals ik eerder liet zien.\n\n\nAls je klaar bent, moeten 112 leerlingen een project opzetten met hun eigenlijke groepsprojectleden. Neem een .rmd-bestand op met de naam “ideas.rmd” waarin je ideeën kunt uitwisselen, inclusief onderwerpen en gegevens die je misschien zou willen analyseren. 155 leerlingen moeten de enquête over het groepsproject op de Moodle-pagina invullen.\n\n\n\nBron\nHappy git with R door Jenny Bryan. Leer meer over Distill via https://rstudio.github.io/distill."
  },
  {
    "objectID": "posts/2021-04-22-tidymodels-opnieuw/tidymodels-opnieuw.html",
    "href": "posts/2021-04-22-tidymodels-opnieuw/tidymodels-opnieuw.html",
    "title": "Tidymodels opnieuw",
    "section": "",
    "text": "Tidyverse is misschien wel een van de grootste successen van R de laatste jaren. Het is een basispakket (een suite van pakketten) waarmee je heel veel statistiscche bewerkingen goed en betrekkelijk eenvoudig kunt uitvoeren. De laatste jaren is tidymodels ontwikkeld dat voor het modelleren van data het basispakket moet worden en het ontwikkelt zich vergelijkbaar de gereedschapskist van tidyverse maar dan op het gebied van machine learning.\nWaarom tidymodels? Nou, het blijkt dat R een consistentieprobleem heeft. Omdat alles rondom machine learning door verschillende mensen is gemaakt, allemaal met verschillende principes, heeft alles een net iets andere interface gekregenen. Om de boel in lijn te houden is onderhand een frustrerende bezigheid. Enkele jaren geleden ontwikkelde Max Kuhn (nu bij RStudio in dienst) het caret R-pakket, dat is zo’n uniforme interface voor een groot aantal machine learning-modellen die er in R zijn. Het programma caret bestaat nog steeds, was in veel opzichten geweldig en is nog steeds goed te gebruiken. Maar in andere opzichten is het beperkt. Zo kan het vrij traag zijn, zelfs bij gebuik van data in bescheiden omvang.\ncaret was een geweldig uitgangspunt, dus RStudio heeft Max Kuhn ingehuurd om te werken aan een tidy versie van caret. Hij en veel anderen ontwikkelden de afgelopen jaren tidymodels.tidymodels is al een paar jaar in ontwikkeling en delen ervan waren al eerder uitgebracht. Die volledige versie is in het voorjaar van 2020 gepresenteerd en Barter schreef vlak daarvoor deze tutoriol. Ondertussen is het voldoende ontwikkeld als je het wil leren! Terwijl caret niet verder ontwikkeld wordt (je kunt caret blijven gebruiken en je bestaande caret-code werkt nog steeds, het pakket wordt alleen niet onderhouden), zal tidymodels het uiteindelijk overbodig maken.\nDeze tutorial van Barter is gebaseerd op Alison Hill’s dia’s van Introduction to Machine Learning with the Tidyverse, die alle dia’s bevat voor de cursus die ze met Garrett Grolemund voor RStudio heeft voorbereid::conf(2020), en Edgar Ruiz’s Gentle introduction to tidymodels op de website van RStudio. In deze tutorial gaat zij ervan uit dat de gebruiker bepaalde basiskennis heeft, voornamelijk omgaan met dplyr (b.v. piping %&gt;% en een functie zoals mutate()).\n\n\nNet als tidyverse, dat uit verschillende pakketten bestaat zoals ggplot2 en dplyr, zitten er ook in tidymodels enkele kernpakketten, zoals\n\nrsample: voor het uit elkaar halen van een datasample (b.v. train/test of cross-validatie);\nrecipes: voor pre-procesfuncties;\nparsnip: voor het specificeren van het model;\nyardstick: voor het evalueren van van het model;\ntune: voor het afstemmen van parameters;\nworkflow: om alles samen te brengen.\n\nNet zoals je de hele suite aan pakketten van tidyverse kunt binnenhalen door library(tidyverse) in te tikken. tidymodels bestaat dus uit verschillende pakketten en soms zal ik hieronder individuele pakketten noemen.\n\n\n\nAls je deze pakketten nog niet hebt geïnstalleerd, moet je dat wel eerst doen (slechts één keer) door install.packages(\"tidymodels\") te gebruiken. Vervolgens laad je bepaalde bibliotheken: tidymodels en tidyverse.\n\n# laad de relevante tidymodels pakketten\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.1.3\n\n\n-- Attaching packages -------------------------------------- tidymodels 0.2.0 --\n\n\nv broom        0.8.0     v recipes      0.2.0\nv dials        1.0.0     v rsample      0.1.1\nv dplyr        1.0.9     v tibble       3.1.7\nv ggplot2      3.3.6     v tidyr        1.2.0\nv infer        1.0.2     v tune         0.2.0\nv modeldata    0.1.1     v workflows    0.2.6\nv parsnip      1.0.0     v workflowsets 0.2.1\nv purrr        0.3.4     v yardstick    1.0.0\n\n\nWarning: package 'broom' was built under R version 4.1.3\n\n\nWarning: package 'dials' was built under R version 4.1.3\n\n\nWarning: package 'scales' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'infer' was built under R version 4.1.3\n\n\nWarning: package 'parsnip' was built under R version 4.1.3\n\n\nWarning: package 'recipes' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'tune' was built under R version 4.1.3\n\n\nWarning: package 'workflows' was built under R version 4.1.3\n\n\nWarning: package 'workflowsets' was built under R version 4.1.3\n\n\nWarning: package 'yardstick' was built under R version 4.1.3\n\n\n-- Conflicts ----------------------------------------- tidymodels_conflicts() --\nx purrr::discard() masks scales::discard()\nx dplyr::filter()  masks stats::filter()\nx dplyr::lag()     masks stats::lag()\nx recipes::step()  masks stats::step()\n* Use suppressPackageStartupMessages() to eliminate package startup messages\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv readr   2.1.2     v forcats 0.5.1\nv stringr 1.4.1     \n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'stringr' was built under R version 4.1.3\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx readr::col_factor() masks scales::col_factor()\nx purrr::discard()    masks scales::discard()\nx dplyr::filter()     masks stats::filter()\nx stringr::fixed()    masks recipes::fixed()\nx dplyr::lag()        masks stats::lag()\nx readr::spec()       masks yardstick::spec()\n\n\n\n# laad de Pima Indians dataset van de mlbench dataset\nlibrary(mlbench)\ndata(PimaIndiansDiabetes)\n#Wat zit erin\nglimpse(PimaIndiansDiabetes)\n\nRows: 768\nColumns: 9\n$ pregnant &lt;dbl&gt; 6, 1, 8, 1, 0, 5, 3, 10, 2, 8, 4, 10, 10, 1, 5, 7, 0, 7, 1, 1~\n$ glucose  &lt;dbl&gt; 148, 85, 183, 89, 137, 116, 78, 115, 197, 125, 110, 168, 139,~\n$ pressure &lt;dbl&gt; 72, 66, 64, 66, 40, 74, 50, 0, 70, 96, 92, 74, 80, 60, 72, 0,~\n$ triceps  &lt;dbl&gt; 35, 29, 0, 23, 35, 0, 32, 0, 45, 0, 0, 0, 0, 23, 19, 0, 47, 0~\n$ insulin  &lt;dbl&gt; 0, 0, 0, 94, 168, 0, 88, 0, 543, 0, 0, 0, 0, 846, 175, 0, 230~\n$ mass     &lt;dbl&gt; 33.6, 26.6, 23.3, 28.1, 43.1, 25.6, 31.0, 35.3, 30.5, 0.0, 37~\n$ pedigree &lt;dbl&gt; 0.627, 0.351, 0.672, 0.167, 2.288, 0.201, 0.248, 0.134, 0.158~\n$ age      &lt;dbl&gt; 50, 31, 32, 21, 33, 30, 26, 29, 53, 54, 30, 34, 57, 59, 51, 3~\n$ diabetes &lt;fct&gt; pos, neg, pos, neg, pos, neg, pos, neg, pos, pos, neg, pos, n~\n\n\nWe zullen gebruik maken van de Pima Indian Women’s diabetes-dataset dat informatie bevat over de diabetes status van 768 Pima Indian vrouwen(diabetes). In de dataset zitten daarnaast enkele predictoren zoals het aantal zwangerschappen (pregnant), concentratie glucose (glucose), diastolische bloeddruk (pressure), triceps huidplooidikte (triceps), 2 uur serum insuline (insuline), BMI (mass), diabetes stamboom functie (pedigree) en hun leeftijd (age). Voor het geval je het je afvraagt, de Pima Indianen zijn een groep indianen die leven in een gebied dat bestaat uit wat nu centraal en zuidelijk Arizona is. De korte naam “Pima” zou afkomstig zijn van een zinsnede die “ik weet het niet” betekent, die ze herhaaldelijk gebruikten in hun eerste ontmoetingen met Spaanse kolonisten. Wikipedia bedankt!\n\n# Geef de dataset een kortere naam omdat we wat lui zijn\ndiabetes_orig &lt;- PimaIndiansDiabetes\n\n\ndiabetes_orig\n\n    pregnant glucose pressure triceps insulin mass pedigree age diabetes\n1          6     148       72      35       0 33.6    0.627  50      pos\n2          1      85       66      29       0 26.6    0.351  31      neg\n3          8     183       64       0       0 23.3    0.672  32      pos\n4          1      89       66      23      94 28.1    0.167  21      neg\n5          0     137       40      35     168 43.1    2.288  33      pos\n6          5     116       74       0       0 25.6    0.201  30      neg\n7          3      78       50      32      88 31.0    0.248  26      pos\n8         10     115        0       0       0 35.3    0.134  29      neg\n9          2     197       70      45     543 30.5    0.158  53      pos\n10         8     125       96       0       0  0.0    0.232  54      pos\n11         4     110       92       0       0 37.6    0.191  30      neg\n12        10     168       74       0       0 38.0    0.537  34      pos\n13        10     139       80       0       0 27.1    1.441  57      neg\n14         1     189       60      23     846 30.1    0.398  59      pos\n15         5     166       72      19     175 25.8    0.587  51      pos\n16         7     100        0       0       0 30.0    0.484  32      pos\n17         0     118       84      47     230 45.8    0.551  31      pos\n18         7     107       74       0       0 29.6    0.254  31      pos\n19         1     103       30      38      83 43.3    0.183  33      neg\n20         1     115       70      30      96 34.6    0.529  32      pos\n21         3     126       88      41     235 39.3    0.704  27      neg\n22         8      99       84       0       0 35.4    0.388  50      neg\n23         7     196       90       0       0 39.8    0.451  41      pos\n24         9     119       80      35       0 29.0    0.263  29      pos\n25        11     143       94      33     146 36.6    0.254  51      pos\n26        10     125       70      26     115 31.1    0.205  41      pos\n27         7     147       76       0       0 39.4    0.257  43      pos\n28         1      97       66      15     140 23.2    0.487  22      neg\n29        13     145       82      19     110 22.2    0.245  57      neg\n30         5     117       92       0       0 34.1    0.337  38      neg\n31         5     109       75      26       0 36.0    0.546  60      neg\n32         3     158       76      36     245 31.6    0.851  28      pos\n33         3      88       58      11      54 24.8    0.267  22      neg\n34         6      92       92       0       0 19.9    0.188  28      neg\n35        10     122       78      31       0 27.6    0.512  45      neg\n36         4     103       60      33     192 24.0    0.966  33      neg\n37        11     138       76       0       0 33.2    0.420  35      neg\n38         9     102       76      37       0 32.9    0.665  46      pos\n39         2      90       68      42       0 38.2    0.503  27      pos\n40         4     111       72      47     207 37.1    1.390  56      pos\n41         3     180       64      25      70 34.0    0.271  26      neg\n42         7     133       84       0       0 40.2    0.696  37      neg\n43         7     106       92      18       0 22.7    0.235  48      neg\n44         9     171      110      24     240 45.4    0.721  54      pos\n45         7     159       64       0       0 27.4    0.294  40      neg\n46         0     180       66      39       0 42.0    1.893  25      pos\n47         1     146       56       0       0 29.7    0.564  29      neg\n48         2      71       70      27       0 28.0    0.586  22      neg\n49         7     103       66      32       0 39.1    0.344  31      pos\n50         7     105        0       0       0  0.0    0.305  24      neg\n51         1     103       80      11      82 19.4    0.491  22      neg\n52         1     101       50      15      36 24.2    0.526  26      neg\n53         5      88       66      21      23 24.4    0.342  30      neg\n54         8     176       90      34     300 33.7    0.467  58      pos\n55         7     150       66      42     342 34.7    0.718  42      neg\n56         1      73       50      10       0 23.0    0.248  21      neg\n57         7     187       68      39     304 37.7    0.254  41      pos\n58         0     100       88      60     110 46.8    0.962  31      neg\n59         0     146       82       0       0 40.5    1.781  44      neg\n60         0     105       64      41     142 41.5    0.173  22      neg\n61         2      84        0       0       0  0.0    0.304  21      neg\n62         8     133       72       0       0 32.9    0.270  39      pos\n63         5      44       62       0       0 25.0    0.587  36      neg\n64         2     141       58      34     128 25.4    0.699  24      neg\n65         7     114       66       0       0 32.8    0.258  42      pos\n66         5      99       74      27       0 29.0    0.203  32      neg\n67         0     109       88      30       0 32.5    0.855  38      pos\n68         2     109       92       0       0 42.7    0.845  54      neg\n69         1      95       66      13      38 19.6    0.334  25      neg\n70         4     146       85      27     100 28.9    0.189  27      neg\n71         2     100       66      20      90 32.9    0.867  28      pos\n72         5     139       64      35     140 28.6    0.411  26      neg\n73        13     126       90       0       0 43.4    0.583  42      pos\n74         4     129       86      20     270 35.1    0.231  23      neg\n75         1      79       75      30       0 32.0    0.396  22      neg\n76         1       0       48      20       0 24.7    0.140  22      neg\n77         7      62       78       0       0 32.6    0.391  41      neg\n78         5      95       72      33       0 37.7    0.370  27      neg\n79         0     131        0       0       0 43.2    0.270  26      pos\n80         2     112       66      22       0 25.0    0.307  24      neg\n81         3     113       44      13       0 22.4    0.140  22      neg\n82         2      74        0       0       0  0.0    0.102  22      neg\n83         7      83       78      26      71 29.3    0.767  36      neg\n84         0     101       65      28       0 24.6    0.237  22      neg\n85         5     137      108       0       0 48.8    0.227  37      pos\n86         2     110       74      29     125 32.4    0.698  27      neg\n87        13     106       72      54       0 36.6    0.178  45      neg\n88         2     100       68      25      71 38.5    0.324  26      neg\n89        15     136       70      32     110 37.1    0.153  43      pos\n90         1     107       68      19       0 26.5    0.165  24      neg\n91         1      80       55       0       0 19.1    0.258  21      neg\n92         4     123       80      15     176 32.0    0.443  34      neg\n93         7      81       78      40      48 46.7    0.261  42      neg\n94         4     134       72       0       0 23.8    0.277  60      pos\n95         2     142       82      18      64 24.7    0.761  21      neg\n96         6     144       72      27     228 33.9    0.255  40      neg\n97         2      92       62      28       0 31.6    0.130  24      neg\n98         1      71       48      18      76 20.4    0.323  22      neg\n99         6      93       50      30      64 28.7    0.356  23      neg\n100        1     122       90      51     220 49.7    0.325  31      pos\n101        1     163       72       0       0 39.0    1.222  33      pos\n102        1     151       60       0       0 26.1    0.179  22      neg\n103        0     125       96       0       0 22.5    0.262  21      neg\n104        1      81       72      18      40 26.6    0.283  24      neg\n105        2      85       65       0       0 39.6    0.930  27      neg\n106        1     126       56      29     152 28.7    0.801  21      neg\n107        1      96      122       0       0 22.4    0.207  27      neg\n108        4     144       58      28     140 29.5    0.287  37      neg\n109        3      83       58      31      18 34.3    0.336  25      neg\n110        0      95       85      25      36 37.4    0.247  24      pos\n111        3     171       72      33     135 33.3    0.199  24      pos\n112        8     155       62      26     495 34.0    0.543  46      pos\n113        1      89       76      34      37 31.2    0.192  23      neg\n114        4      76       62       0       0 34.0    0.391  25      neg\n115        7     160       54      32     175 30.5    0.588  39      pos\n116        4     146       92       0       0 31.2    0.539  61      pos\n117        5     124       74       0       0 34.0    0.220  38      pos\n118        5      78       48       0       0 33.7    0.654  25      neg\n119        4      97       60      23       0 28.2    0.443  22      neg\n120        4      99       76      15      51 23.2    0.223  21      neg\n121        0     162       76      56     100 53.2    0.759  25      pos\n122        6     111       64      39       0 34.2    0.260  24      neg\n123        2     107       74      30     100 33.6    0.404  23      neg\n124        5     132       80       0       0 26.8    0.186  69      neg\n125        0     113       76       0       0 33.3    0.278  23      pos\n126        1      88       30      42      99 55.0    0.496  26      pos\n127        3     120       70      30     135 42.9    0.452  30      neg\n128        1     118       58      36      94 33.3    0.261  23      neg\n129        1     117       88      24     145 34.5    0.403  40      pos\n130        0     105       84       0       0 27.9    0.741  62      pos\n131        4     173       70      14     168 29.7    0.361  33      pos\n132        9     122       56       0       0 33.3    1.114  33      pos\n133        3     170       64      37     225 34.5    0.356  30      pos\n134        8      84       74      31       0 38.3    0.457  39      neg\n135        2      96       68      13      49 21.1    0.647  26      neg\n136        2     125       60      20     140 33.8    0.088  31      neg\n137        0     100       70      26      50 30.8    0.597  21      neg\n138        0      93       60      25      92 28.7    0.532  22      neg\n139        0     129       80       0       0 31.2    0.703  29      neg\n140        5     105       72      29     325 36.9    0.159  28      neg\n141        3     128       78       0       0 21.1    0.268  55      neg\n142        5     106       82      30       0 39.5    0.286  38      neg\n143        2     108       52      26      63 32.5    0.318  22      neg\n144       10     108       66       0       0 32.4    0.272  42      pos\n145        4     154       62      31     284 32.8    0.237  23      neg\n146        0     102       75      23       0  0.0    0.572  21      neg\n147        9      57       80      37       0 32.8    0.096  41      neg\n148        2     106       64      35     119 30.5    1.400  34      neg\n149        5     147       78       0       0 33.7    0.218  65      neg\n150        2      90       70      17       0 27.3    0.085  22      neg\n151        1     136       74      50     204 37.4    0.399  24      neg\n152        4     114       65       0       0 21.9    0.432  37      neg\n153        9     156       86      28     155 34.3    1.189  42      pos\n154        1     153       82      42     485 40.6    0.687  23      neg\n155        8     188       78       0       0 47.9    0.137  43      pos\n156        7     152       88      44       0 50.0    0.337  36      pos\n157        2      99       52      15      94 24.6    0.637  21      neg\n158        1     109       56      21     135 25.2    0.833  23      neg\n159        2      88       74      19      53 29.0    0.229  22      neg\n160       17     163       72      41     114 40.9    0.817  47      pos\n161        4     151       90      38       0 29.7    0.294  36      neg\n162        7     102       74      40     105 37.2    0.204  45      neg\n163        0     114       80      34     285 44.2    0.167  27      neg\n164        2     100       64      23       0 29.7    0.368  21      neg\n165        0     131       88       0       0 31.6    0.743  32      pos\n166        6     104       74      18     156 29.9    0.722  41      pos\n167        3     148       66      25       0 32.5    0.256  22      neg\n168        4     120       68       0       0 29.6    0.709  34      neg\n169        4     110       66       0       0 31.9    0.471  29      neg\n170        3     111       90      12      78 28.4    0.495  29      neg\n171        6     102       82       0       0 30.8    0.180  36      pos\n172        6     134       70      23     130 35.4    0.542  29      pos\n173        2      87        0      23       0 28.9    0.773  25      neg\n174        1      79       60      42      48 43.5    0.678  23      neg\n175        2      75       64      24      55 29.7    0.370  33      neg\n176        8     179       72      42     130 32.7    0.719  36      pos\n177        6      85       78       0       0 31.2    0.382  42      neg\n178        0     129      110      46     130 67.1    0.319  26      pos\n179        5     143       78       0       0 45.0    0.190  47      neg\n180        5     130       82       0       0 39.1    0.956  37      pos\n181        6      87       80       0       0 23.2    0.084  32      neg\n182        0     119       64      18      92 34.9    0.725  23      neg\n183        1       0       74      20      23 27.7    0.299  21      neg\n184        5      73       60       0       0 26.8    0.268  27      neg\n185        4     141       74       0       0 27.6    0.244  40      neg\n186        7     194       68      28       0 35.9    0.745  41      pos\n187        8     181       68      36     495 30.1    0.615  60      pos\n188        1     128       98      41      58 32.0    1.321  33      pos\n189        8     109       76      39     114 27.9    0.640  31      pos\n190        5     139       80      35     160 31.6    0.361  25      pos\n191        3     111       62       0       0 22.6    0.142  21      neg\n192        9     123       70      44      94 33.1    0.374  40      neg\n193        7     159       66       0       0 30.4    0.383  36      pos\n194       11     135        0       0       0 52.3    0.578  40      pos\n195        8      85       55      20       0 24.4    0.136  42      neg\n196        5     158       84      41     210 39.4    0.395  29      pos\n197        1     105       58       0       0 24.3    0.187  21      neg\n198        3     107       62      13      48 22.9    0.678  23      pos\n199        4     109       64      44      99 34.8    0.905  26      pos\n200        4     148       60      27     318 30.9    0.150  29      pos\n201        0     113       80      16       0 31.0    0.874  21      neg\n202        1     138       82       0       0 40.1    0.236  28      neg\n203        0     108       68      20       0 27.3    0.787  32      neg\n204        2      99       70      16      44 20.4    0.235  27      neg\n205        6     103       72      32     190 37.7    0.324  55      neg\n206        5     111       72      28       0 23.9    0.407  27      neg\n207        8     196       76      29     280 37.5    0.605  57      pos\n208        5     162      104       0       0 37.7    0.151  52      pos\n209        1      96       64      27      87 33.2    0.289  21      neg\n210        7     184       84      33       0 35.5    0.355  41      pos\n211        2      81       60      22       0 27.7    0.290  25      neg\n212        0     147       85      54       0 42.8    0.375  24      neg\n213        7     179       95      31       0 34.2    0.164  60      neg\n214        0     140       65      26     130 42.6    0.431  24      pos\n215        9     112       82      32     175 34.2    0.260  36      pos\n216       12     151       70      40     271 41.8    0.742  38      pos\n217        5     109       62      41     129 35.8    0.514  25      pos\n218        6     125       68      30     120 30.0    0.464  32      neg\n219        5      85       74      22       0 29.0    1.224  32      pos\n220        5     112       66       0       0 37.8    0.261  41      pos\n221        0     177       60      29     478 34.6    1.072  21      pos\n222        2     158       90       0       0 31.6    0.805  66      pos\n223        7     119        0       0       0 25.2    0.209  37      neg\n224        7     142       60      33     190 28.8    0.687  61      neg\n225        1     100       66      15      56 23.6    0.666  26      neg\n226        1      87       78      27      32 34.6    0.101  22      neg\n227        0     101       76       0       0 35.7    0.198  26      neg\n228        3     162       52      38       0 37.2    0.652  24      pos\n229        4     197       70      39     744 36.7    2.329  31      neg\n230        0     117       80      31      53 45.2    0.089  24      neg\n231        4     142       86       0       0 44.0    0.645  22      pos\n232        6     134       80      37     370 46.2    0.238  46      pos\n233        1      79       80      25      37 25.4    0.583  22      neg\n234        4     122       68       0       0 35.0    0.394  29      neg\n235        3      74       68      28      45 29.7    0.293  23      neg\n236        4     171       72       0       0 43.6    0.479  26      pos\n237        7     181       84      21     192 35.9    0.586  51      pos\n238        0     179       90      27       0 44.1    0.686  23      pos\n239        9     164       84      21       0 30.8    0.831  32      pos\n240        0     104       76       0       0 18.4    0.582  27      neg\n241        1      91       64      24       0 29.2    0.192  21      neg\n242        4      91       70      32      88 33.1    0.446  22      neg\n243        3     139       54       0       0 25.6    0.402  22      pos\n244        6     119       50      22     176 27.1    1.318  33      pos\n245        2     146       76      35     194 38.2    0.329  29      neg\n246        9     184       85      15       0 30.0    1.213  49      pos\n247       10     122       68       0       0 31.2    0.258  41      neg\n248        0     165       90      33     680 52.3    0.427  23      neg\n249        9     124       70      33     402 35.4    0.282  34      neg\n250        1     111       86      19       0 30.1    0.143  23      neg\n251        9     106       52       0       0 31.2    0.380  42      neg\n252        2     129       84       0       0 28.0    0.284  27      neg\n253        2      90       80      14      55 24.4    0.249  24      neg\n254        0      86       68      32       0 35.8    0.238  25      neg\n255       12      92       62       7     258 27.6    0.926  44      pos\n256        1     113       64      35       0 33.6    0.543  21      pos\n257        3     111       56      39       0 30.1    0.557  30      neg\n258        2     114       68      22       0 28.7    0.092  25      neg\n259        1     193       50      16     375 25.9    0.655  24      neg\n260       11     155       76      28     150 33.3    1.353  51      pos\n261        3     191       68      15     130 30.9    0.299  34      neg\n262        3     141        0       0       0 30.0    0.761  27      pos\n263        4      95       70      32       0 32.1    0.612  24      neg\n264        3     142       80      15       0 32.4    0.200  63      neg\n265        4     123       62       0       0 32.0    0.226  35      pos\n266        5      96       74      18      67 33.6    0.997  43      neg\n267        0     138        0       0       0 36.3    0.933  25      pos\n268        2     128       64      42       0 40.0    1.101  24      neg\n269        0     102       52       0       0 25.1    0.078  21      neg\n270        2     146        0       0       0 27.5    0.240  28      pos\n271       10     101       86      37       0 45.6    1.136  38      pos\n272        2     108       62      32      56 25.2    0.128  21      neg\n273        3     122       78       0       0 23.0    0.254  40      neg\n274        1      71       78      50      45 33.2    0.422  21      neg\n275       13     106       70       0       0 34.2    0.251  52      neg\n276        2     100       70      52      57 40.5    0.677  25      neg\n277        7     106       60      24       0 26.5    0.296  29      pos\n278        0     104       64      23     116 27.8    0.454  23      neg\n279        5     114       74       0       0 24.9    0.744  57      neg\n280        2     108       62      10     278 25.3    0.881  22      neg\n281        0     146       70       0       0 37.9    0.334  28      pos\n282       10     129       76      28     122 35.9    0.280  39      neg\n283        7     133       88      15     155 32.4    0.262  37      neg\n284        7     161       86       0       0 30.4    0.165  47      pos\n285        2     108       80       0       0 27.0    0.259  52      pos\n286        7     136       74      26     135 26.0    0.647  51      neg\n287        5     155       84      44     545 38.7    0.619  34      neg\n288        1     119       86      39     220 45.6    0.808  29      pos\n289        4      96       56      17      49 20.8    0.340  26      neg\n290        5     108       72      43      75 36.1    0.263  33      neg\n291        0      78       88      29      40 36.9    0.434  21      neg\n292        0     107       62      30      74 36.6    0.757  25      pos\n293        2     128       78      37     182 43.3    1.224  31      pos\n294        1     128       48      45     194 40.5    0.613  24      pos\n295        0     161       50       0       0 21.9    0.254  65      neg\n296        6     151       62      31     120 35.5    0.692  28      neg\n297        2     146       70      38     360 28.0    0.337  29      pos\n298        0     126       84      29     215 30.7    0.520  24      neg\n299       14     100       78      25     184 36.6    0.412  46      pos\n300        8     112       72       0       0 23.6    0.840  58      neg\n301        0     167        0       0       0 32.3    0.839  30      pos\n302        2     144       58      33     135 31.6    0.422  25      pos\n303        5      77       82      41      42 35.8    0.156  35      neg\n304        5     115       98       0       0 52.9    0.209  28      pos\n305        3     150       76       0       0 21.0    0.207  37      neg\n306        2     120       76      37     105 39.7    0.215  29      neg\n307       10     161       68      23     132 25.5    0.326  47      pos\n308        0     137       68      14     148 24.8    0.143  21      neg\n309        0     128       68      19     180 30.5    1.391  25      pos\n310        2     124       68      28     205 32.9    0.875  30      pos\n311        6      80       66      30       0 26.2    0.313  41      neg\n312        0     106       70      37     148 39.4    0.605  22      neg\n313        2     155       74      17      96 26.6    0.433  27      pos\n314        3     113       50      10      85 29.5    0.626  25      neg\n315        7     109       80      31       0 35.9    1.127  43      pos\n316        2     112       68      22      94 34.1    0.315  26      neg\n317        3      99       80      11      64 19.3    0.284  30      neg\n318        3     182       74       0       0 30.5    0.345  29      pos\n319        3     115       66      39     140 38.1    0.150  28      neg\n320        6     194       78       0       0 23.5    0.129  59      pos\n321        4     129       60      12     231 27.5    0.527  31      neg\n322        3     112       74      30       0 31.6    0.197  25      pos\n323        0     124       70      20       0 27.4    0.254  36      pos\n324       13     152       90      33      29 26.8    0.731  43      pos\n325        2     112       75      32       0 35.7    0.148  21      neg\n326        1     157       72      21     168 25.6    0.123  24      neg\n327        1     122       64      32     156 35.1    0.692  30      pos\n328       10     179       70       0       0 35.1    0.200  37      neg\n329        2     102       86      36     120 45.5    0.127  23      pos\n330        6     105       70      32      68 30.8    0.122  37      neg\n331        8     118       72      19       0 23.1    1.476  46      neg\n332        2      87       58      16      52 32.7    0.166  25      neg\n333        1     180        0       0       0 43.3    0.282  41      pos\n334       12     106       80       0       0 23.6    0.137  44      neg\n335        1      95       60      18      58 23.9    0.260  22      neg\n336        0     165       76      43     255 47.9    0.259  26      neg\n337        0     117        0       0       0 33.8    0.932  44      neg\n338        5     115       76       0       0 31.2    0.343  44      pos\n339        9     152       78      34     171 34.2    0.893  33      pos\n340        7     178       84       0       0 39.9    0.331  41      pos\n341        1     130       70      13     105 25.9    0.472  22      neg\n342        1      95       74      21      73 25.9    0.673  36      neg\n343        1       0       68      35       0 32.0    0.389  22      neg\n344        5     122       86       0       0 34.7    0.290  33      neg\n345        8      95       72       0       0 36.8    0.485  57      neg\n346        8     126       88      36     108 38.5    0.349  49      neg\n347        1     139       46      19      83 28.7    0.654  22      neg\n348        3     116        0       0       0 23.5    0.187  23      neg\n349        3      99       62      19      74 21.8    0.279  26      neg\n350        5       0       80      32       0 41.0    0.346  37      pos\n351        4      92       80       0       0 42.2    0.237  29      neg\n352        4     137       84       0       0 31.2    0.252  30      neg\n353        3      61       82      28       0 34.4    0.243  46      neg\n354        1      90       62      12      43 27.2    0.580  24      neg\n355        3      90       78       0       0 42.7    0.559  21      neg\n356        9     165       88       0       0 30.4    0.302  49      pos\n357        1     125       50      40     167 33.3    0.962  28      pos\n358       13     129        0      30       0 39.9    0.569  44      pos\n359       12      88       74      40      54 35.3    0.378  48      neg\n360        1     196       76      36     249 36.5    0.875  29      pos\n361        5     189       64      33     325 31.2    0.583  29      pos\n362        5     158       70       0       0 29.8    0.207  63      neg\n363        5     103      108      37       0 39.2    0.305  65      neg\n364        4     146       78       0       0 38.5    0.520  67      pos\n365        4     147       74      25     293 34.9    0.385  30      neg\n366        5      99       54      28      83 34.0    0.499  30      neg\n367        6     124       72       0       0 27.6    0.368  29      pos\n368        0     101       64      17       0 21.0    0.252  21      neg\n369        3      81       86      16      66 27.5    0.306  22      neg\n370        1     133      102      28     140 32.8    0.234  45      pos\n371        3     173       82      48     465 38.4    2.137  25      pos\n372        0     118       64      23      89  0.0    1.731  21      neg\n373        0      84       64      22      66 35.8    0.545  21      neg\n374        2     105       58      40      94 34.9    0.225  25      neg\n375        2     122       52      43     158 36.2    0.816  28      neg\n376       12     140       82      43     325 39.2    0.528  58      pos\n377        0      98       82      15      84 25.2    0.299  22      neg\n378        1      87       60      37      75 37.2    0.509  22      neg\n379        4     156       75       0       0 48.3    0.238  32      pos\n380        0      93      100      39      72 43.4    1.021  35      neg\n381        1     107       72      30      82 30.8    0.821  24      neg\n382        0     105       68      22       0 20.0    0.236  22      neg\n383        1     109       60       8     182 25.4    0.947  21      neg\n384        1      90       62      18      59 25.1    1.268  25      neg\n385        1     125       70      24     110 24.3    0.221  25      neg\n386        1     119       54      13      50 22.3    0.205  24      neg\n387        5     116       74      29       0 32.3    0.660  35      pos\n388        8     105      100      36       0 43.3    0.239  45      pos\n389        5     144       82      26     285 32.0    0.452  58      pos\n390        3     100       68      23      81 31.6    0.949  28      neg\n391        1     100       66      29     196 32.0    0.444  42      neg\n392        5     166       76       0       0 45.7    0.340  27      pos\n393        1     131       64      14     415 23.7    0.389  21      neg\n394        4     116       72      12      87 22.1    0.463  37      neg\n395        4     158       78       0       0 32.9    0.803  31      pos\n396        2     127       58      24     275 27.7    1.600  25      neg\n397        3      96       56      34     115 24.7    0.944  39      neg\n398        0     131       66      40       0 34.3    0.196  22      pos\n399        3      82       70       0       0 21.1    0.389  25      neg\n400        3     193       70      31       0 34.9    0.241  25      pos\n401        4      95       64       0       0 32.0    0.161  31      pos\n402        6     137       61       0       0 24.2    0.151  55      neg\n403        5     136       84      41      88 35.0    0.286  35      pos\n404        9      72       78      25       0 31.6    0.280  38      neg\n405        5     168       64       0       0 32.9    0.135  41      pos\n406        2     123       48      32     165 42.1    0.520  26      neg\n407        4     115       72       0       0 28.9    0.376  46      pos\n408        0     101       62       0       0 21.9    0.336  25      neg\n409        8     197       74       0       0 25.9    1.191  39      pos\n410        1     172       68      49     579 42.4    0.702  28      pos\n411        6     102       90      39       0 35.7    0.674  28      neg\n412        1     112       72      30     176 34.4    0.528  25      neg\n413        1     143       84      23     310 42.4    1.076  22      neg\n414        1     143       74      22      61 26.2    0.256  21      neg\n415        0     138       60      35     167 34.6    0.534  21      pos\n416        3     173       84      33     474 35.7    0.258  22      pos\n417        1      97       68      21       0 27.2    1.095  22      neg\n418        4     144       82      32       0 38.5    0.554  37      pos\n419        1      83       68       0       0 18.2    0.624  27      neg\n420        3     129       64      29     115 26.4    0.219  28      pos\n421        1     119       88      41     170 45.3    0.507  26      neg\n422        2      94       68      18      76 26.0    0.561  21      neg\n423        0     102       64      46      78 40.6    0.496  21      neg\n424        2     115       64      22       0 30.8    0.421  21      neg\n425        8     151       78      32     210 42.9    0.516  36      pos\n426        4     184       78      39     277 37.0    0.264  31      pos\n427        0      94        0       0       0  0.0    0.256  25      neg\n428        1     181       64      30     180 34.1    0.328  38      pos\n429        0     135       94      46     145 40.6    0.284  26      neg\n430        1      95       82      25     180 35.0    0.233  43      pos\n431        2      99        0       0       0 22.2    0.108  23      neg\n432        3      89       74      16      85 30.4    0.551  38      neg\n433        1      80       74      11      60 30.0    0.527  22      neg\n434        2     139       75       0       0 25.6    0.167  29      neg\n435        1      90       68       8       0 24.5    1.138  36      neg\n436        0     141        0       0       0 42.4    0.205  29      pos\n437       12     140       85      33       0 37.4    0.244  41      neg\n438        5     147       75       0       0 29.9    0.434  28      neg\n439        1      97       70      15       0 18.2    0.147  21      neg\n440        6     107       88       0       0 36.8    0.727  31      neg\n441        0     189      104      25       0 34.3    0.435  41      pos\n442        2      83       66      23      50 32.2    0.497  22      neg\n443        4     117       64      27     120 33.2    0.230  24      neg\n444        8     108       70       0       0 30.5    0.955  33      pos\n445        4     117       62      12       0 29.7    0.380  30      pos\n446        0     180       78      63      14 59.4    2.420  25      pos\n447        1     100       72      12      70 25.3    0.658  28      neg\n448        0      95       80      45      92 36.5    0.330  26      neg\n449        0     104       64      37      64 33.6    0.510  22      pos\n450        0     120       74      18      63 30.5    0.285  26      neg\n451        1      82       64      13      95 21.2    0.415  23      neg\n452        2     134       70       0       0 28.9    0.542  23      pos\n453        0      91       68      32     210 39.9    0.381  25      neg\n454        2     119        0       0       0 19.6    0.832  72      neg\n455        2     100       54      28     105 37.8    0.498  24      neg\n456       14     175       62      30       0 33.6    0.212  38      pos\n457        1     135       54       0       0 26.7    0.687  62      neg\n458        5      86       68      28      71 30.2    0.364  24      neg\n459       10     148       84      48     237 37.6    1.001  51      pos\n460        9     134       74      33      60 25.9    0.460  81      neg\n461        9     120       72      22      56 20.8    0.733  48      neg\n462        1      71       62       0       0 21.8    0.416  26      neg\n463        8      74       70      40      49 35.3    0.705  39      neg\n464        5      88       78      30       0 27.6    0.258  37      neg\n465       10     115       98       0       0 24.0    1.022  34      neg\n466        0     124       56      13     105 21.8    0.452  21      neg\n467        0      74       52      10      36 27.8    0.269  22      neg\n468        0      97       64      36     100 36.8    0.600  25      neg\n469        8     120        0       0       0 30.0    0.183  38      pos\n470        6     154       78      41     140 46.1    0.571  27      neg\n471        1     144       82      40       0 41.3    0.607  28      neg\n472        0     137       70      38       0 33.2    0.170  22      neg\n473        0     119       66      27       0 38.8    0.259  22      neg\n474        7     136       90       0       0 29.9    0.210  50      neg\n475        4     114       64       0       0 28.9    0.126  24      neg\n476        0     137       84      27       0 27.3    0.231  59      neg\n477        2     105       80      45     191 33.7    0.711  29      pos\n478        7     114       76      17     110 23.8    0.466  31      neg\n479        8     126       74      38      75 25.9    0.162  39      neg\n480        4     132       86      31       0 28.0    0.419  63      neg\n481        3     158       70      30     328 35.5    0.344  35      pos\n482        0     123       88      37       0 35.2    0.197  29      neg\n483        4      85       58      22      49 27.8    0.306  28      neg\n484        0      84       82      31     125 38.2    0.233  23      neg\n485        0     145        0       0       0 44.2    0.630  31      pos\n486        0     135       68      42     250 42.3    0.365  24      pos\n487        1     139       62      41     480 40.7    0.536  21      neg\n488        0     173       78      32     265 46.5    1.159  58      neg\n489        4      99       72      17       0 25.6    0.294  28      neg\n490        8     194       80       0       0 26.1    0.551  67      neg\n491        2      83       65      28      66 36.8    0.629  24      neg\n492        2      89       90      30       0 33.5    0.292  42      neg\n493        4      99       68      38       0 32.8    0.145  33      neg\n494        4     125       70      18     122 28.9    1.144  45      pos\n495        3      80        0       0       0  0.0    0.174  22      neg\n496        6     166       74       0       0 26.6    0.304  66      neg\n497        5     110       68       0       0 26.0    0.292  30      neg\n498        2      81       72      15      76 30.1    0.547  25      neg\n499        7     195       70      33     145 25.1    0.163  55      pos\n500        6     154       74      32     193 29.3    0.839  39      neg\n501        2     117       90      19      71 25.2    0.313  21      neg\n502        3      84       72      32       0 37.2    0.267  28      neg\n503        6       0       68      41       0 39.0    0.727  41      pos\n504        7      94       64      25      79 33.3    0.738  41      neg\n505        3      96       78      39       0 37.3    0.238  40      neg\n506       10      75       82       0       0 33.3    0.263  38      neg\n507        0     180       90      26      90 36.5    0.314  35      pos\n508        1     130       60      23     170 28.6    0.692  21      neg\n509        2      84       50      23      76 30.4    0.968  21      neg\n510        8     120       78       0       0 25.0    0.409  64      neg\n511       12      84       72      31       0 29.7    0.297  46      pos\n512        0     139       62      17     210 22.1    0.207  21      neg\n513        9      91       68       0       0 24.2    0.200  58      neg\n514        2      91       62       0       0 27.3    0.525  22      neg\n515        3      99       54      19      86 25.6    0.154  24      neg\n516        3     163       70      18     105 31.6    0.268  28      pos\n517        9     145       88      34     165 30.3    0.771  53      pos\n518        7     125       86       0       0 37.6    0.304  51      neg\n519       13      76       60       0       0 32.8    0.180  41      neg\n520        6     129       90       7     326 19.6    0.582  60      neg\n521        2      68       70      32      66 25.0    0.187  25      neg\n522        3     124       80      33     130 33.2    0.305  26      neg\n523        6     114        0       0       0  0.0    0.189  26      neg\n524        9     130       70       0       0 34.2    0.652  45      pos\n525        3     125       58       0       0 31.6    0.151  24      neg\n526        3      87       60      18       0 21.8    0.444  21      neg\n527        1      97       64      19      82 18.2    0.299  21      neg\n528        3     116       74      15     105 26.3    0.107  24      neg\n529        0     117       66      31     188 30.8    0.493  22      neg\n530        0     111       65       0       0 24.6    0.660  31      neg\n531        2     122       60      18     106 29.8    0.717  22      neg\n532        0     107       76       0       0 45.3    0.686  24      neg\n533        1      86       66      52      65 41.3    0.917  29      neg\n534        6      91        0       0       0 29.8    0.501  31      neg\n535        1      77       56      30      56 33.3    1.251  24      neg\n536        4     132        0       0       0 32.9    0.302  23      pos\n537        0     105       90       0       0 29.6    0.197  46      neg\n538        0      57       60       0       0 21.7    0.735  67      neg\n539        0     127       80      37     210 36.3    0.804  23      neg\n540        3     129       92      49     155 36.4    0.968  32      pos\n541        8     100       74      40     215 39.4    0.661  43      pos\n542        3     128       72      25     190 32.4    0.549  27      pos\n543       10      90       85      32       0 34.9    0.825  56      pos\n544        4      84       90      23      56 39.5    0.159  25      neg\n545        1      88       78      29      76 32.0    0.365  29      neg\n546        8     186       90      35     225 34.5    0.423  37      pos\n547        5     187       76      27     207 43.6    1.034  53      pos\n548        4     131       68      21     166 33.1    0.160  28      neg\n549        1     164       82      43      67 32.8    0.341  50      neg\n550        4     189      110      31       0 28.5    0.680  37      neg\n551        1     116       70      28       0 27.4    0.204  21      neg\n552        3      84       68      30     106 31.9    0.591  25      neg\n553        6     114       88       0       0 27.8    0.247  66      neg\n554        1      88       62      24      44 29.9    0.422  23      neg\n555        1      84       64      23     115 36.9    0.471  28      neg\n556        7     124       70      33     215 25.5    0.161  37      neg\n557        1      97       70      40       0 38.1    0.218  30      neg\n558        8     110       76       0       0 27.8    0.237  58      neg\n559       11     103       68      40       0 46.2    0.126  42      neg\n560       11      85       74       0       0 30.1    0.300  35      neg\n561        6     125       76       0       0 33.8    0.121  54      pos\n562        0     198       66      32     274 41.3    0.502  28      pos\n563        1      87       68      34      77 37.6    0.401  24      neg\n564        6      99       60      19      54 26.9    0.497  32      neg\n565        0      91       80       0       0 32.4    0.601  27      neg\n566        2      95       54      14      88 26.1    0.748  22      neg\n567        1      99       72      30      18 38.6    0.412  21      neg\n568        6      92       62      32     126 32.0    0.085  46      neg\n569        4     154       72      29     126 31.3    0.338  37      neg\n570        0     121       66      30     165 34.3    0.203  33      pos\n571        3      78       70       0       0 32.5    0.270  39      neg\n572        2     130       96       0       0 22.6    0.268  21      neg\n573        3     111       58      31      44 29.5    0.430  22      neg\n574        2      98       60      17     120 34.7    0.198  22      neg\n575        1     143       86      30     330 30.1    0.892  23      neg\n576        1     119       44      47      63 35.5    0.280  25      neg\n577        6     108       44      20     130 24.0    0.813  35      neg\n578        2     118       80       0       0 42.9    0.693  21      pos\n579       10     133       68       0       0 27.0    0.245  36      neg\n580        2     197       70      99       0 34.7    0.575  62      pos\n581        0     151       90      46       0 42.1    0.371  21      pos\n582        6     109       60      27       0 25.0    0.206  27      neg\n583       12     121       78      17       0 26.5    0.259  62      neg\n584        8     100       76       0       0 38.7    0.190  42      neg\n585        8     124       76      24     600 28.7    0.687  52      pos\n586        1      93       56      11       0 22.5    0.417  22      neg\n587        8     143       66       0       0 34.9    0.129  41      pos\n588        6     103       66       0       0 24.3    0.249  29      neg\n589        3     176       86      27     156 33.3    1.154  52      pos\n590        0      73        0       0       0 21.1    0.342  25      neg\n591       11     111       84      40       0 46.8    0.925  45      pos\n592        2     112       78      50     140 39.4    0.175  24      neg\n593        3     132       80       0       0 34.4    0.402  44      pos\n594        2      82       52      22     115 28.5    1.699  25      neg\n595        6     123       72      45     230 33.6    0.733  34      neg\n596        0     188       82      14     185 32.0    0.682  22      pos\n597        0      67       76       0       0 45.3    0.194  46      neg\n598        1      89       24      19      25 27.8    0.559  21      neg\n599        1     173       74       0       0 36.8    0.088  38      pos\n600        1     109       38      18     120 23.1    0.407  26      neg\n601        1     108       88      19       0 27.1    0.400  24      neg\n602        6      96        0       0       0 23.7    0.190  28      neg\n603        1     124       74      36       0 27.8    0.100  30      neg\n604        7     150       78      29     126 35.2    0.692  54      pos\n605        4     183        0       0       0 28.4    0.212  36      pos\n606        1     124       60      32       0 35.8    0.514  21      neg\n607        1     181       78      42     293 40.0    1.258  22      pos\n608        1      92       62      25      41 19.5    0.482  25      neg\n609        0     152       82      39     272 41.5    0.270  27      neg\n610        1     111       62      13     182 24.0    0.138  23      neg\n611        3     106       54      21     158 30.9    0.292  24      neg\n612        3     174       58      22     194 32.9    0.593  36      pos\n613        7     168       88      42     321 38.2    0.787  40      pos\n614        6     105       80      28       0 32.5    0.878  26      neg\n615       11     138       74      26     144 36.1    0.557  50      pos\n616        3     106       72       0       0 25.8    0.207  27      neg\n617        6     117       96       0       0 28.7    0.157  30      neg\n618        2      68       62      13      15 20.1    0.257  23      neg\n619        9     112       82      24       0 28.2    1.282  50      pos\n620        0     119        0       0       0 32.4    0.141  24      pos\n621        2     112       86      42     160 38.4    0.246  28      neg\n622        2      92       76      20       0 24.2    1.698  28      neg\n623        6     183       94       0       0 40.8    1.461  45      neg\n624        0      94       70      27     115 43.5    0.347  21      neg\n625        2     108       64       0       0 30.8    0.158  21      neg\n626        4      90       88      47      54 37.7    0.362  29      neg\n627        0     125       68       0       0 24.7    0.206  21      neg\n628        0     132       78       0       0 32.4    0.393  21      neg\n629        5     128       80       0       0 34.6    0.144  45      neg\n630        4      94       65      22       0 24.7    0.148  21      neg\n631        7     114       64       0       0 27.4    0.732  34      pos\n632        0     102       78      40      90 34.5    0.238  24      neg\n633        2     111       60       0       0 26.2    0.343  23      neg\n634        1     128       82      17     183 27.5    0.115  22      neg\n635       10      92       62       0       0 25.9    0.167  31      neg\n636       13     104       72       0       0 31.2    0.465  38      pos\n637        5     104       74       0       0 28.8    0.153  48      neg\n638        2      94       76      18      66 31.6    0.649  23      neg\n639        7      97       76      32      91 40.9    0.871  32      pos\n640        1     100       74      12      46 19.5    0.149  28      neg\n641        0     102       86      17     105 29.3    0.695  27      neg\n642        4     128       70       0       0 34.3    0.303  24      neg\n643        6     147       80       0       0 29.5    0.178  50      pos\n644        4      90        0       0       0 28.0    0.610  31      neg\n645        3     103       72      30     152 27.6    0.730  27      neg\n646        2     157       74      35     440 39.4    0.134  30      neg\n647        1     167       74      17     144 23.4    0.447  33      pos\n648        0     179       50      36     159 37.8    0.455  22      pos\n649       11     136       84      35     130 28.3    0.260  42      pos\n650        0     107       60      25       0 26.4    0.133  23      neg\n651        1      91       54      25     100 25.2    0.234  23      neg\n652        1     117       60      23     106 33.8    0.466  27      neg\n653        5     123       74      40      77 34.1    0.269  28      neg\n654        2     120       54       0       0 26.8    0.455  27      neg\n655        1     106       70      28     135 34.2    0.142  22      neg\n656        2     155       52      27     540 38.7    0.240  25      pos\n657        2     101       58      35      90 21.8    0.155  22      neg\n658        1     120       80      48     200 38.9    1.162  41      neg\n659       11     127      106       0       0 39.0    0.190  51      neg\n660        3      80       82      31      70 34.2    1.292  27      pos\n661       10     162       84       0       0 27.7    0.182  54      neg\n662        1     199       76      43       0 42.9    1.394  22      pos\n663        8     167      106      46     231 37.6    0.165  43      pos\n664        9     145       80      46     130 37.9    0.637  40      pos\n665        6     115       60      39       0 33.7    0.245  40      pos\n666        1     112       80      45     132 34.8    0.217  24      neg\n667        4     145       82      18       0 32.5    0.235  70      pos\n668       10     111       70      27       0 27.5    0.141  40      pos\n669        6      98       58      33     190 34.0    0.430  43      neg\n670        9     154       78      30     100 30.9    0.164  45      neg\n671        6     165       68      26     168 33.6    0.631  49      neg\n672        1      99       58      10       0 25.4    0.551  21      neg\n673       10      68      106      23      49 35.5    0.285  47      neg\n674        3     123      100      35     240 57.3    0.880  22      neg\n675        8      91       82       0       0 35.6    0.587  68      neg\n676        6     195       70       0       0 30.9    0.328  31      pos\n677        9     156       86       0       0 24.8    0.230  53      pos\n678        0      93       60       0       0 35.3    0.263  25      neg\n679        3     121       52       0       0 36.0    0.127  25      pos\n680        2     101       58      17     265 24.2    0.614  23      neg\n681        2      56       56      28      45 24.2    0.332  22      neg\n682        0     162       76      36       0 49.6    0.364  26      pos\n683        0      95       64      39     105 44.6    0.366  22      neg\n684        4     125       80       0       0 32.3    0.536  27      pos\n685        5     136       82       0       0  0.0    0.640  69      neg\n686        2     129       74      26     205 33.2    0.591  25      neg\n687        3     130       64       0       0 23.1    0.314  22      neg\n688        1     107       50      19       0 28.3    0.181  29      neg\n689        1     140       74      26     180 24.1    0.828  23      neg\n690        1     144       82      46     180 46.1    0.335  46      pos\n691        8     107       80       0       0 24.6    0.856  34      neg\n692       13     158      114       0       0 42.3    0.257  44      pos\n693        2     121       70      32      95 39.1    0.886  23      neg\n694        7     129       68      49     125 38.5    0.439  43      pos\n695        2      90       60       0       0 23.5    0.191  25      neg\n696        7     142       90      24     480 30.4    0.128  43      pos\n697        3     169       74      19     125 29.9    0.268  31      pos\n698        0      99        0       0       0 25.0    0.253  22      neg\n699        4     127       88      11     155 34.5    0.598  28      neg\n700        4     118       70       0       0 44.5    0.904  26      neg\n701        2     122       76      27     200 35.9    0.483  26      neg\n702        6     125       78      31       0 27.6    0.565  49      pos\n703        1     168       88      29       0 35.0    0.905  52      pos\n704        2     129        0       0       0 38.5    0.304  41      neg\n705        4     110       76      20     100 28.4    0.118  27      neg\n706        6      80       80      36       0 39.8    0.177  28      neg\n707       10     115        0       0       0  0.0    0.261  30      pos\n708        2     127       46      21     335 34.4    0.176  22      neg\n709        9     164       78       0       0 32.8    0.148  45      pos\n710        2      93       64      32     160 38.0    0.674  23      pos\n711        3     158       64      13     387 31.2    0.295  24      neg\n712        5     126       78      27      22 29.6    0.439  40      neg\n713       10     129       62      36       0 41.2    0.441  38      pos\n714        0     134       58      20     291 26.4    0.352  21      neg\n715        3     102       74       0       0 29.5    0.121  32      neg\n716        7     187       50      33     392 33.9    0.826  34      pos\n717        3     173       78      39     185 33.8    0.970  31      pos\n718       10      94       72      18       0 23.1    0.595  56      neg\n719        1     108       60      46     178 35.5    0.415  24      neg\n720        5      97       76      27       0 35.6    0.378  52      pos\n721        4      83       86      19       0 29.3    0.317  34      neg\n722        1     114       66      36     200 38.1    0.289  21      neg\n723        1     149       68      29     127 29.3    0.349  42      pos\n724        5     117       86      30     105 39.1    0.251  42      neg\n725        1     111       94       0       0 32.8    0.265  45      neg\n726        4     112       78      40       0 39.4    0.236  38      neg\n727        1     116       78      29     180 36.1    0.496  25      neg\n728        0     141       84      26       0 32.4    0.433  22      neg\n729        2     175       88       0       0 22.9    0.326  22      neg\n730        2      92       52       0       0 30.1    0.141  22      neg\n731        3     130       78      23      79 28.4    0.323  34      pos\n732        8     120       86       0       0 28.4    0.259  22      pos\n733        2     174       88      37     120 44.5    0.646  24      pos\n734        2     106       56      27     165 29.0    0.426  22      neg\n735        2     105       75       0       0 23.3    0.560  53      neg\n736        4      95       60      32       0 35.4    0.284  28      neg\n737        0     126       86      27     120 27.4    0.515  21      neg\n738        8      65       72      23       0 32.0    0.600  42      neg\n739        2      99       60      17     160 36.6    0.453  21      neg\n740        1     102       74       0       0 39.5    0.293  42      pos\n741       11     120       80      37     150 42.3    0.785  48      pos\n742        3     102       44      20      94 30.8    0.400  26      neg\n743        1     109       58      18     116 28.5    0.219  22      neg\n744        9     140       94       0       0 32.7    0.734  45      pos\n745       13     153       88      37     140 40.6    1.174  39      neg\n746       12     100       84      33     105 30.0    0.488  46      neg\n747        1     147       94      41       0 49.3    0.358  27      pos\n748        1      81       74      41      57 46.3    1.096  32      neg\n749        3     187       70      22     200 36.4    0.408  36      pos\n750        6     162       62       0       0 24.3    0.178  50      pos\n751        4     136       70       0       0 31.2    1.182  22      pos\n752        1     121       78      39      74 39.0    0.261  28      neg\n753        3     108       62      24       0 26.0    0.223  25      neg\n754        0     181       88      44     510 43.3    0.222  26      pos\n755        8     154       78      32       0 32.4    0.443  45      pos\n756        1     128       88      39     110 36.5    1.057  37      pos\n757        7     137       90      41       0 32.0    0.391  39      neg\n758        0     123       72       0       0 36.3    0.258  52      pos\n759        1     106       76       0       0 37.5    0.197  26      neg\n760        6     190       92       0       0 35.5    0.278  66      pos\n761        2      88       58      26      16 28.4    0.766  22      neg\n762        9     170       74      31       0 44.0    0.403  43      pos\n763        9      89       62       0       0 22.5    0.142  33      neg\n764       10     101       76      48     180 32.9    0.171  63      neg\n765        2     122       70      27       0 36.8    0.340  27      neg\n766        5     121       72      23     112 26.2    0.245  30      neg\n767        1     126       60       0       0 30.1    0.349  47      pos\n768        1      93       70      31       0 30.4    0.315  23      neg\n\n\nEen snelle verkenning van de dataset toont aan dat er meer nullen in de gegevens zitten dan verwacht (vooral omdat een BMI of tricep huiddikte van 0 onmogelijk is), wat betekent dat ontbrekende waarden als nullen worden geregistreerd. Zie bijvoorbeeld het histogram van de tricep huidplooidikte, waar de nullen voor dikte opvallen.\n\nggplot(diabetes_orig) +\n  geom_histogram(aes(x = triceps))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nDit fenomeen is ook te zien in de glucose-, druk-, insuline- en massavariabelen. We zetten eerst de 0-scores in alle variabelen (behalve “zwanger”) over naar NA (missende waarde). Daarvoor gebruiken we de mutate_at()functie (die binnenkort wordt vervangen door mutate() met across()) om aan te geven op welke variabelen we onze muterende functie willen toepassen. We gebruiken de if_else()functie om aan te geven waar we de waarde mee moeten vervangen als de voorwaarde waar of onwaar is.\n\ndiabetes_clean &lt;- diabetes_orig %&gt;%\n  mutate_at(vars(triceps, glucose, pressure, insulin, mass), \n            function(.var) { \n              if_else(condition = (.var == 0), # als waar (bv als het 0 is)\n                      true = as.numeric(NA),  # zet er de waarde NA voor in de plaats\n                      false = .var # anders laat het zoals het is\n                      )\n            })\n\nOnze gegevens zijn klaar. Laten we beginnen met het maken van een aantal tidymodels!\n\n\n\nLaten we onze data verdelen in trainings- en testdata. De trainingsdata worden gebruikt om ons model te vinden en de parameters in te stellen (tune). De testdata gebruiken we alleen om de werking van het finale model vast te stellen. Dat splitten kunnen we doen door de inital_split() functie (van het rsample pakket). Dat creëert een speciaal “split” object.\n\nset.seed(234589)\n# deel de data op in trainng (75%) en testing (25%)\ndiabetes_split &lt;- initial_split(diabetes_clean, \n                                prop = 3/4)\ndiabetes_split\n\n&lt;Analysis/Assess/Total&gt;\n&lt;576/192/768&gt;\n\n\ndiabetes_split, ons gesplitste object, vertelt ons hoeveel waarnemingen we hebben in de trainingsset, de testset en de gehele dataset: &lt;train/test/totaal&gt; (576/192/768).\nDe trainings- en testsets kunnen uit het “split”-object worden gehaald met behulp van de training() en testing() functies. Hoewel we deze objecten niet echt zullen gebruiken in de pipeline (daarvoor zullen we het diabetes_split-object zelf gebruiken).\n\n# haal training en testing sets uit elkaar\ndiabetes_train &lt;- training(diabetes_split)\ndiabetes_test &lt;- testing(diabetes_split)\n\nOp een gegeven moment zullen we de parameters hiervan wat willen tuenen (afstemmen). Dat doen we met cross-validatie. Zo ontstaat er met vfold_cv() een cross-validatie versie van de trainingsset waar we zo op terugkomen.\n\n# creeer CV object van training data\ndiabetes_cv &lt;- vfold_cv(diabetes_train)\n\n\n\n\nMet het pakket recipes kun je de variabelen een rol geven, als uitkomst of voorspellende variabele (gebruik een “formule”) b.v.. Maar met recipe kun je ook andere voorbereidingsstappen zetten die je nodig acht (zoals standaardiseren, imputeren, PCA, etc). Een recipe voer je uit in delen (gelaagd op elkaar door pipes %&gt;% te gebruiken):\n\nSpecificeer de formule (recipe()): specificeer eerst wat is de uitkomstvariabele en wat zijn de predictoren;\nSpecificeer pre-processing steps (step_zzz()): defineer voorbereidingsstappen, zoals imputatie, creëren van dummy variabelen, schalen en wat al niet meer\n\nZo kunnen we bijvoorbeeld de volgende recipe maken.\n\n# defineer de `recipe`\ndiabetes_recipe &lt;- \n  # dat bestaat uit de volgende formule (uitkomst ~ predictoren)\n  recipe(diabetes ~ pregnant + glucose + pressure + triceps + \n           insulin + mass + pedigree + age, \n         data = diabetes_clean) %&gt;%\n  # en voeren we enkele voorbereidingsstappen uit (normaliseren en imputeren)\n  step_normalize(all_numeric()) %&gt;%\n  step_knnimpute(all_predictors())\n\nWarning: `step_knnimpute()` was deprecated in recipes 0.1.16.\nPlease use `step_impute_knn()` instead.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\n\nAls je ooit eerder formules hebt gezien (bijvoorbeeld met behulp van de lm() functie in R), dan weet je misschien dat we onze formule veel efficiënter hadden kunnen schrijven met behulp van een shortcut, waarbij de . alle variabelen in de gegevens vertegenwoordigt: outcome ~ .\nDe volledige lijst van beschikbare voorbewerkingsstappen is hier te vinden. In de bovenstaande chunck hebben we de functies all_numeric() en all_predictors() gebruikt als argumenten van voorbereiding. Deze worden “rolselecties” genoemd en geven aan dat we de stap willen toepassen op “alle numerieke” variabelen of “alle predictoren”. De lijst van alle potentiële rolselectoren kan worden gevonden door ?selectis in je console te typen.\nMerk op dat we het originele diabetes_clean data-object hebben gebruikt (we stellen recipe(..., data = diabetes_clean)), in plaats van het diabetes_train-object of het diabetes_split-object. Het blijkt dat we deze allemaal hadden kunnen gebruiken. Alle recipes die op dit punt uit het dataobject worden gehaald zijn de namen en rollen van de uitkomst en de voorspellende variabelen. We zullen deze recipe later toepassen op specifieke datasets. Dit betekent dat voor grote datasets een kleinere dataset gebruikt wordt om tijd en geheugen te besparen.\nInderdaad, als we een samenvatting van het diabetes_recipe object printen, dan laat het ons gewoon zien hoeveel voorspellingsvariabelen we hebben gespecificeerd en welke stappen we hebben gespecificeerd (maar het implementeert ze eigenlijk nog niet!).\n\ndiabetes_recipe\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          8\n\nOperations:\n\nCentering and scaling for all_numeric()\nK-nearest neighbor imputation for all_predictors()\n\n\nAls je de voorbewerkte dataset zelf wilt extraheren, kunt je eerst prep() het recept voor een specifieke dataset en juice() het voorbewerkte recept om de voorbewerkte gegevens te extraheren. Het blijkt dat het extraheren van de voorbewerkte data eigenlijk niet nodig is voor de pipeline, omdat dit onder de motorkap gebeurt als het model geschikt is. Soms is het toch nuttig.\n\ndiabetes_train_preprocessed &lt;- diabetes_recipe %&gt;%\n  # apply the recipe to the training data\n  prep(diabetes_train) %&gt;%\n  # extract the pre-processed training dataset\n  juice()\ndiabetes_train_preprocessed\n\n# A tibble: 576 x 9\n   pregnant glucose pressure triceps insulin     mass pedigree     age diabetes\n      &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;   \n 1   1.23   -0.390    0.262   0.892   -0.317 -0.656      0.502 -0.201  pos     \n 2   0.0447  1.09    -0.0616 -0.0348  -0.219 -0.168     -0.400  0.301  neg     \n 3   1.82    1.91    -0.224   0.595    0.146  0.379     -0.813  0.301  neg     \n 4  -1.14   -0.0948  -0.710  -0.591   -0.522 -0.311      3.76  -1.04   neg     \n 5  -0.548   0.233    0.424   0.706    0.239  1.56       2.25  -0.201  pos     \n 6   0.637  -0.0620  -1.84   -0.683    0.190 -0.771      2.53  -0.0335 pos     \n 7  -0.844  -1.64    -2.01   -1.05    -0.629 -1.73      -0.445 -0.953  neg     \n 8  -0.548  -0.423   -1.68   -0.313   -0.735  0.00505   -0.460 -0.953  neg     \n 9  -1.14    0.463   -0.386   1.17     0.796  1.41      -0.320 -0.786  pos     \n10   1.53    1.41     0.424   0.150    0.877  0.0482    -0.968  0.969  pos     \n# ... with 566 more rows\n\n\n\n\n\nTot nu toe hebben we onze data verdeeld in training en test-sets en onze pre-proces stappen gespecificeerd door een recipe te gebruiken. Nu willen we ons model definiëren en daarvoor gebruiken we het parsnip pakket dat in tidymodels zit.\nParsnip biedt een uniforme interface voor de enorme verscheidenheid aan modellen die er in R bestaan. Dit betekent dat je slechts één manier hoeft te leren om een model te specificeren en dan kun je dit gebruiken voor allerlei verschillende modellen, vaak met enkele coderegel.\nEr zijn een paar primaire componenten in de modelspecificatie opgeslagen:\n\nHet model type: wat voor soort model wil je gebruiken, zoals rand_forest() voor het random forest-model, logistic_reg() voor het logistisch regressie-model, svm_poly() voor een polynomiaal SVM-model, enz. De volledige lijst van modellen die beschikbaar zijn via parsnip kan [hier] (link naar website) vinden.\nDe arguments: de model parameter waarden (de benaming is consistent over verschillende modellen), door het gebruik van set_args().\nDe engine: het onderliggende pakket waar het model van wegkomt (bv. “ranger” voor implementatie van Random Forest), door het gebuik van set_engine().\nDe mode: het type voorspelling - omdat verschillende pakketten zowel classificatie (binaire/categoriale voorspelling) en regressie (continue voorspelling) kunnen uitvoeren, door het gebruik van set_mode().\n\nAls we bijvoorbeeld een random forest model willen gebruiken, zoals dat in het ranger pakket zit, met als doel classificatie en we willen de try parameter tunen (het afstemmen van het aantal willekeurig gekozen variabelen dat bij elke splitsing in aanmerking moet worden genomen), dan moeten we de volgende modelspecificatie definiëren:\n\nrf_model &lt;- \n  # specificeren dat het model random forest is\n  rand_forest() %&gt;%\n  # specificeren dat we de `mtry` parameter moeten afstemmen\n  set_args(mtry = tune()) %&gt;%\n  # selecteren van de motor van het pakket dat onder het model zit\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  # kiezen dat je voor continue analyse (regressie) of categoriale analyse (classificatie) gaat\n  set_mode(\"classification\") \n\nAls je later het variabele belang van jouw uiteindelijke model wilt kunnen onderzoeken, moet je het engine argument opnieuw instellen. De volgende code specificeert bijvoorbeeld een logistisch regressiemodel uit het glm pakket.\n\nlr_model &lt;- \n  # specificeer een logistisch regressiemodel\n  logistic_reg() %&gt;%\n  # selecteer het pakket dat bij dit model hoort\n  set_engine(\"glm\") %&gt;%\n  # kies voor een continue regressie of binaire classificatie wijze\n  set_mode(\"classification\") \n\nDeze code draait niet het model. Net als de recipe, is het veel meer een beschrijving van het model. Echter, wanneer je een parameter op tune() zet wordt het later gestemd in de stemfase van de pipeline (bv. om de waarde vast te stellen van de parameter die de beste performance geeft). Je kunt ook zelf een bepaalde waarde aan de parameter geven wanneer je het niet wilt afstemmen, bv door set_args(mtry = 4) te gebruiken. Een ander ding om op te merken is dat niets wat deze modelspecificatie betreft specifiek is voor de diabetes-dataset.\n\n\n\nWe zijn klaar om het model en de recipes in een workflow te plaatsen. Een workflow zet je op door het gebruik van workflow() (van het workflows pakket) en dan kun je een recipe en een model toevoegen.\n\n# zet de workflow op\nrf_workflow &lt;- workflow() %&gt;%\n  # voeg de `recipe` toe\n  add_recipe(diabetes_recipe) %&gt;%\n  # voeg het `model` toe\n  add_model(rf_model)\n\nMerk op dat we de voorbewerkingsstappen nog niet in de recipe hebben geïmplementeerd noch dat we het model hebben gepast. We hebben alleen maar het raamwerk geschreven. Pas als we de parameters hebben afgestemd of in het model hebben gepast, worden het recept en het model daadwerkelijk geïmplementeerd.\n\n\n\nOmdat er een parameter is ontwikkeld om af te stemmen (mtry), moeten we dat daar voor gebruiken (bv. de waarde kiezen die de beste performance laat zien) voordat we het model passen. Als je geen parameters hebt om af te stemmen, kun je dit deel overslaan.\nDat afstemmen doen we door een cross-validation object (diabetes_cv) te kiezen. Om dat te doen specificeren we de range van mtry waarden die we willen gebruiken en dan voegen we een stemmingslaag toe aan onze workflow door tune_grid() te gebruiken (van het tune pakket). We richten ons op twee maten: accuracy en roc_auc (van het yardstick pakket). Die vertellen ons welke maten we het beste kunnen gebruiken.\n\n# specificeer de waarden die je wilt gebruiken\nrf_grid &lt;- expand.grid(mtry = c(3, 4, 5))\n# extraheer resultaten\nrf_tune_results &lt;- rf_workflow %&gt;%\n  tune_grid(resamples = diabetes_cv, #CV object\n            grid = rf_grid, # grid van waarden om te proberen\n            metrics = metric_set(accuracy, roc_auc) # maten waar we naar moeten kijken\n            )\n\nWarning: package 'ranger' was built under R version 4.1.3\n\n\nJe kunt verschillende parameters afstemmen door verschillende parameters aan de expand.grid() functie toe te voegen, bv. expand.grid(mtry = c(3, 4, 5), trees = c(100, 500)).\nHet is altijd goed om de resultaten van de cross-validatie goed te onderzoeken. collect_metrics() is echt een handige functie die in verschillende omstandigheden kan worden gebruikt om te vergelijken die zijn berekend in het object dat is gebruikt. In dit geval komen de maten van de cross-validatie performance over de verschillende waarden van de performance.\n\n# print results\nrf_tune_results %&gt;%\n  collect_metrics()\n\n# A tibble: 6 x 7\n   mtry .metric  .estimator  mean     n std_err .config             \n  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1     3 accuracy binary     0.766    10 0.00832 Preprocessor1_Model1\n2     3 roc_auc  binary     0.842    10 0.0116  Preprocessor1_Model1\n3     4 accuracy binary     0.774    10 0.0127  Preprocessor1_Model2\n4     4 roc_auc  binary     0.842    10 0.0123  Preprocessor1_Model2\n5     5 accuracy binary     0.771    10 0.0121  Preprocessor1_Model3\n6     5 roc_auc  binary     0.841    10 0.0134  Preprocessor1_Model3\n\n\nTen opzichte van accuracy en AUC laat mtry = 4 de beste performance zien (hoogste gemiddelde waarden).\n\n\n\nWe willen een laag aan onze workflow toevoegen die overeenkomt met de afgestemde parameter, d.w.z. dat we mtry instellen als de waarde die de beste resultaten opleverde. Als je geen parameters hebt afgestemd, kun je deze stap overslaan.\nWe kunnen de beste waarde voor de nauwkeurigheidsmetriek extraheren door de select_best()functie toe te passen op het afstemmingsobject.\n\nparam_final &lt;- rf_tune_results %&gt;%\n  select_best(metric = \"accuracy\")\nparam_final\n\n# A tibble: 1 x 2\n   mtry .config             \n  &lt;dbl&gt; &lt;chr&gt;               \n1     4 Preprocessor1_Model2\n\n\nDan kunnen we deze parameter aan de workflow toevoegen door de finalize_workflow() functie te gebruiken.\n\nrf_workflow &lt;- rf_workflow %&gt;%\n  finalize_workflow(param_final)\n\n\n\n\nNu we ons recipe en ons model hebben gedefinieerd en de parameters van het model hebben getuned, zijn we klaar om daadwerkelijk het uiteindelijke model te draaien. Aangezien al deze informatie in het workflow-object zit, zullen we de last_fit() functie toepassen op onze workflow en ons train/test-splitsingsobject. Dit zal automatisch het door de workflow gespecificeerde model trainen met behulp van de trainingsgegevens en evaluaties produceren op basis van de testset.\n\nrf_fit &lt;- rf_workflow %&gt;%\n  # draaien op de trainingsset en evalueren op de test set\n  last_fit(diabetes_split)\n\nMerk op dat het object dat wordt gecreëerd een data-frame-achtig object is; het is een tibble met listkolommen.\n\nrf_fit\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 x 6\n  splits            id               .metrics .notes   .predictions .workflow \n  &lt;list&gt;            &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [576/192]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\n\nDit is echt een aardige eigenschap van tidymodels (en ook waarom je zo goed kunt werken met tidyverse) omdat je al je nette handelingen op het modelobject kunt uitvoeren.\nAangezien we het trainings/testobject al hebben geleverd op het moment dat we in de workflow werken, worden de maten geëvalueerd op de testset. Wanneer we nu de collect_metrics() functie gebruiken (herinner ons dat we deze hebben gebruikt bij het afstemmen van onze parameters), haalt deze de prestaties van het uiteindelijke model (aangezien rf_fit nu bestaat uit een enkel definitief model) toegepast op de test set.\n\ntest_performance &lt;- rf_fit %&gt;% collect_metrics()\ntest_performance\n\n# A tibble: 2 x 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.781 Preprocessor1_Model1\n2 roc_auc  binary         0.847 Preprocessor1_Model1\n\n\nOverall is de performance heel goed, met een accuracy van 0.74 en een AUC van 0.82. Maar deze waarden zijn vaak lager dan in de trainingsset.\nJe kunt de test set voorspellingen zelf gebruiken met de collect_predictions() functie. Let op dat er 192 rijen in het voorspellingsobject zitten dat overeenkomt met de test set observaties (juist om jou te laten zien dat deze gebaseerd zijn op de testset meer dan op de trainingsset).\n\n# genereer voorspellingen vanuit de test set\ntest_predictions &lt;- rf_fit %&gt;% collect_predictions()\ntest_predictions\n\n# A tibble: 192 x 7\n   id               .pred_neg .pred_pos  .row .pred_class diabetes .config      \n   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;    &lt;chr&gt;        \n 1 train/test split     0.347    0.653      3 pos         pos      Preprocessor~\n 2 train/test split     0.210    0.790      9 pos         pos      Preprocessor~\n 3 train/test split     0.783    0.217     11 neg         neg      Preprocessor~\n 4 train/test split     0.508    0.492     13 neg         neg      Preprocessor~\n 5 train/test split     0.624    0.376     18 neg         pos      Preprocessor~\n 6 train/test split     0.364    0.636     26 pos         pos      Preprocessor~\n 7 train/test split     0.218    0.782     32 pos         pos      Preprocessor~\n 8 train/test split     0.673    0.327     50 neg         neg      Preprocessor~\n 9 train/test split     0.976    0.0240    53 neg         neg      Preprocessor~\n10 train/test split     0.166    0.834     55 pos         neg      Preprocessor~\n# ... with 182 more rows\n\n\nOmndat dit een normaal data frame/tibble object is, kunnen we de samenvattingen genereren en een confusie matrix plotten.\n\n# genereer een confusie matrix\ntest_predictions %&gt;% \n  conf_mat(truth = diabetes, estimate = .pred_class)\n\n          Truth\nPrediction neg pos\n       neg 107  19\n       pos  23  43\n\n\nWe kunnen ook de voorspelde kansverdelingen voor elke klasse in kaart brengen.\n\ntest_predictions %&gt;%\n  ggplot() +\n  geom_density(aes(x = .pred_pos, fill = diabetes), \n               alpha = 0.5)\n\n\n\n\nDe voorspellingen kun je ook als volgt laten zien:\n\ntest_predictions &lt;- rf_fit %&gt;% pull(.predictions)\ntest_predictions\n\n[[1]]\n# A tibble: 192 x 6\n   .pred_neg .pred_pos  .row .pred_class diabetes .config             \n       &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;    &lt;chr&gt;               \n 1     0.347    0.653      3 pos         pos      Preprocessor1_Model1\n 2     0.210    0.790      9 pos         pos      Preprocessor1_Model1\n 3     0.783    0.217     11 neg         neg      Preprocessor1_Model1\n 4     0.508    0.492     13 neg         neg      Preprocessor1_Model1\n 5     0.624    0.376     18 neg         pos      Preprocessor1_Model1\n 6     0.364    0.636     26 pos         pos      Preprocessor1_Model1\n 7     0.218    0.782     32 pos         pos      Preprocessor1_Model1\n 8     0.673    0.327     50 neg         neg      Preprocessor1_Model1\n 9     0.976    0.0240    53 neg         neg      Preprocessor1_Model1\n10     0.166    0.834     55 pos         neg      Preprocessor1_Model1\n# ... with 182 more rows\n\n\n\n\n\nIn de vorige paragraaf is het model dat is getraind op de trainingsgegevens geëvalueerd aan de hand van de testgegevens. Maar als je eenmaal jouw definitieve model hebt bepaald, wil je het vaak trainen op je volledige dataset en het dan gebruiken om de respons voor nieuwe gegevens te voorspellen.\nAls je jouw model wilt gebruiken om de respons voor nieuwe waarnemingen te voorspellen, moet je de fit()functie op jouw workflow gebruiken en de dataset waarop je het uiteindelijke model wilt laten passen (bijvoorbeeld de volledige training + testdataset).\n\nfinal_model &lt;- fit(rf_workflow, diabetes_clean)\n\nHet final_model object bevat een aantal zaken, waaronder het ranger-object dat getraind is met de parameters die via de workflow in rf_workflow zijn vastgelegd op basis van de gegevens in diabetes_clean (de gecombineerde trainings- en testgegevens).\n\nfinal_model\n\n== Workflow [trained] ==========================================================\nPreprocessor: Recipe\nModel: rand_forest()\n\n-- Preprocessor ----------------------------------------------------------------\n2 Recipe Steps\n\n* step_normalize()\n* step_impute_knn()\n\n-- Model -----------------------------------------------------------------------\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~4,      x), importance = ~\"impurity\", num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  500 \nSample size:                      768 \nNumber of independent variables:  8 \nMtry:                             4 \nTarget node size:                 10 \nVariable importance mode:         impurity \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.1595594 \n\n\nAls we de diabetes status van een nieuwe vrouw willen voorspellen, kunnen we de predict() functie gebruiken.\nBijvoorbeeld, definieren we de data voor een nieuwe vrouw.\n\nnew_woman &lt;- tribble(~pregnant, ~glucose, ~pressure, ~triceps, ~insulin, ~mass, ~pedigree, ~age,\n                     2, 95, 70, 31, 102, 28.2, 0.67, 47)\nnew_woman\n\n# A tibble: 1 x 8\n  pregnant glucose pressure triceps insulin  mass pedigree   age\n     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1        2      95       70      31     102  28.2     0.67    47\n\n\nDe voorspelde diabetes status van deze nieuwe vrouw is “negatief”.\n\npredict(final_model, new_data = new_woman)\n\n# A tibble: 1 x 1\n  .pred_class\n  &lt;fct&gt;      \n1 neg        \n\n\n\n\n\nAls je de belangrijkheid van een variabele uit je model wilt vaststellen, voor zover je dat kan zien, moet je het modelobject uit het fit() object halen (dat voor ons final_model heet). De functie die het model extraheert is pull_workflow_fit() en dan moet je het fit-object pakken dat de output bevat.\n\nranger_obj &lt;- pull_workflow_fit(final_model)$fit\n\nWarning: `pull_workflow_fit()` was deprecated in workflows 0.2.3.\nPlease use `extract_fit_parsnip()` instead.\n\nranger_obj\n\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~4,      x), importance = ~\"impurity\", num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  500 \nSample size:                      768 \nNumber of independent variables:  8 \nMtry:                             4 \nTarget node size:                 10 \nVariable importance mode:         impurity \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.1595594 \n\n\nVervolgens kun je het belang van de variabele uit het ranger-object zelf halen (variable.importance is een specifiek object in de ranger-output - dit zal moeten worden aangepast voor het specifieke objecttype van andere modellen).\n\nranger_obj$variable.importance\n\npregnant  glucose pressure  triceps  insulin     mass pedigree      age \n15.64236 79.84557 17.51228 21.64400 53.15198 43.77916 29.70545 32.32099"
  },
  {
    "objectID": "posts/2021-07-18-classificeren-met-tidymodels/classificeren-met-tidymodels.html",
    "href": "posts/2021-07-18-classificeren-met-tidymodels/classificeren-met-tidymodels.html",
    "title": "Classificeren met Tidymodels",
    "section": "",
    "text": "Inleiding\nEen gids om stap voor stap een logissche regressie uit te voeren met gebruik van het tidymodels pakket\nDit is bewerking van een blog die Rahul Raoniar, Towards data science begin 2021 schreef.\nIn de wereld van ‘supervised machine learning’ worden vaak twee soorten analyses uitgevoerd. De ene heet regressie (voorspellen van continue waarden), de andere heet classificatie (voorspellen van discrete waarden). In deze blog geef ik een voorbeeld van een binair classificatiealgoritme, “Binaire Logistische Regressie” genaamd. Dat valt onder de Binomiale familie met een logit koppelingsfunctie. Binaire logistische regressie wordt gebruikt voor het voorspellen van binaire klassen. Bijvoorbeeld in gevallen waarin je ja/nee, winst/verlies, negatief/positief, waar/onwaar enzovoort wilt voorspellen.\nDeze blog leidt jou door een proces van hoe het ‘tidymodels’-pakket te gebruiken om een model toe te passen en te evalueren met heel weinig en eenvoudige stappen.\n\n\nAchtergrond van de data\nIn dit voorbeeld maak je gebruik maken van de Pima Indian Diabetes 2 data, verkregen uit de UCI Repository van de machine learning data (Newman et al. 1998).\nDeze data zijn oorspronkelijk afkomstig van het ‘National Institute of Diabetes and Digestive and Kidney Diseases’. Het doel van de dataset is diagnostisch te voorspellen of een patiënt al dan niet diabetes heeft, op basis bepaalde diagnostische metingen die in de dataset zijn opgenomen. Bij de selectie van deze data uit een grotere databank werden verschillende beperkingen opgelegd. In het bijzonder zijn alle patiënten hier vrouwen van ten minste 21 jaar oud van Pima Indiaanse afkomst. De Pima Indian Diabetes 2-data is de verfijnde versie (alle ontbrekende waarden zijn toegewezen als NA) van de Pima Indian diabetes-gegevens. De dataset bevat de volgende onafhankelijke en afhankelijke variabelen.\nOnafhankelijke variabelen (met symbool: O) - O1: pregnant: Aantal keren zwanger\n- O2: glucose: Plasma glucose concentratie (glucose tolerantie test)\n- O3: pressure: Diastolische bloed druk (mm Hg)\n- O4: triceps: Triceps huidplooidikte (mm)\n- O5: insulin: 2-uur serum insuline (mu U/ml)\n- O6: mass: Body mass index (gewicht in kg/(lengte in m)\\²)\n- O7: pedigree: Diabetes pedigree functie\n- O8: age: Leeftijd (jaren)\nDependent Variable (met symbool: A)\n- A1: diabetes: diabetes geval (pos/neg)\n\n\nDoel van de modellering\n\naanpassen van een binair logistisch regressie-machineleermodel met behulp van de bibliotheek tidymodels\n\nhet testen van de voorspellingskracht van het getrainde model (evaluatie van het model) op de ongeziene/geteste dataset met behulp van verschillende evaluatiemetrieken.\n\n\n\nBibliotheken en Datasets laden\nStap1: Eerst moeten we de volgende pakketten worden geïnstalleerd met de install.packages( ) functie (als ze al niet zijn geïnstalleerd en ze laden met de library( ) functie.\n\nlibrary(mlbench)     # voor de PimaIndiansDiabetes2 dataset\nlibrary(tidymodels)  # voor modelpreparatie en fitten van modellen\n\nWarning: package 'tidymodels' was built under R version 4.1.3\n\n\n-- Attaching packages -------------------------------------- tidymodels 0.2.0 --\n\n\nv broom        0.8.0     v recipes      0.2.0\nv dials        1.0.0     v rsample      0.1.1\nv dplyr        1.0.9     v tibble       3.1.7\nv ggplot2      3.3.6     v tidyr        1.2.0\nv infer        1.0.2     v tune         0.2.0\nv modeldata    0.1.1     v workflows    0.2.6\nv parsnip      1.0.0     v workflowsets 0.2.1\nv purrr        0.3.4     v yardstick    1.0.0\n\n\nWarning: package 'broom' was built under R version 4.1.3\n\n\nWarning: package 'dials' was built under R version 4.1.3\n\n\nWarning: package 'scales' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'infer' was built under R version 4.1.3\n\n\nWarning: package 'parsnip' was built under R version 4.1.3\n\n\nWarning: package 'recipes' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'tune' was built under R version 4.1.3\n\n\nWarning: package 'workflows' was built under R version 4.1.3\n\n\nWarning: package 'workflowsets' was built under R version 4.1.3\n\n\nWarning: package 'yardstick' was built under R version 4.1.3\n\n\n-- Conflicts ----------------------------------------- tidymodels_conflicts() --\nx purrr::discard() masks scales::discard()\nx dplyr::filter()  masks stats::filter()\nx dplyr::lag()     masks stats::lag()\nx recipes::step()  masks stats::step()\n* Use tidymodels_prefer() to resolve common conflicts.\n\n\nStap2: Vervolgens moet je de dataset binnen halen uit het mlbench pakket met behulp van de data( ) functie.\nNa het laden van de data, is de volgende essentiële stap het uitvoeren van een verkennende data-analyse, die zal helpen bij het vertrouwd raken met de data. Gebruik de head( ) functie om de bovenste zes rijen van de data te bekijken.\n\ndata(PimaIndiansDiabetes2)\nhead(PimaIndiansDiabetes2)\n\n  pregnant glucose pressure triceps insulin mass pedigree age diabetes\n1        6     148       72      35      NA 33.6    0.627  50      pos\n2        1      85       66      29      NA 26.6    0.351  31      neg\n3        8     183       64      NA      NA 23.3    0.672  32      pos\n4        1      89       66      23      94 28.1    0.167  21      neg\n5        0     137       40      35     168 43.1    2.288  33      pos\n6        5     116       74      NA      NA 25.6    0.201  30      neg\n\n\nDe Diabetes-gegevensreeks telt 768 waarnemingen en negen variabelen. De eerste acht variabelen zijn van het numerieke type en de afhankelijke/output variabele (diabetes) is een factor/categorische variabele. Het is ook merkbaar dat veel variabelen NA waarden bevatten (missende waarde). Onze volgende taak is het de gegevens te verfijnen/wijzigen, zodat ze compatibel worden met het modelleeralgoritme. Eerst nog eens beter naar de data kijken.\n\n# Een blik op de datastructuur\nglimpse(PimaIndiansDiabetes2)\n\nRows: 768\nColumns: 9\n$ pregnant &lt;dbl&gt; 6, 1, 8, 1, 0, 5, 3, 10, 2, 8, 4, 10, 10, 1, 5, 7, 0, 7, 1, 1~\n$ glucose  &lt;dbl&gt; 148, 85, 183, 89, 137, 116, 78, 115, 197, 125, 110, 168, 139,~\n$ pressure &lt;dbl&gt; 72, 66, 64, 66, 40, 74, 50, NA, 70, 96, 92, 74, 80, 60, 72, N~\n$ triceps  &lt;dbl&gt; 35, 29, NA, 23, 35, NA, 32, NA, 45, NA, NA, NA, NA, 23, 19, N~\n$ insulin  &lt;dbl&gt; NA, NA, NA, 94, 168, NA, 88, NA, 543, NA, NA, NA, NA, 846, 17~\n$ mass     &lt;dbl&gt; 33.6, 26.6, 23.3, 28.1, 43.1, 25.6, 31.0, 35.3, 30.5, NA, 37.~\n$ pedigree &lt;dbl&gt; 0.627, 0.351, 0.672, 0.167, 2.288, 0.201, 0.248, 0.134, 0.158~\n$ age      &lt;dbl&gt; 50, 31, 32, 21, 33, 30, 26, 29, 53, 54, 30, 34, 57, 59, 51, 3~\n$ diabetes &lt;fct&gt; pos, neg, pos, neg, pos, neg, pos, neg, pos, pos, neg, pos, n~\n\n\n\n\nVoorbereiding van de gegevens\nDe eerste stap is het verwijderen van data rijen met NA waarden met behulp van na.omit( ) functie. De volgende stap is nogmaals het controleren van de gegevens met behulp van de glimpse( ) functie.\n\nDiabetes &lt;- na.omit(PimaIndiansDiabetes2) #weghalen van NA waarden\nglimpse(Diabetes)\n\nRows: 392\nColumns: 9\n$ pregnant &lt;dbl&gt; 1, 0, 3, 2, 1, 5, 0, 1, 1, 3, 11, 10, 1, 13, 3, 3, 4, 4, 3, 9~\n$ glucose  &lt;dbl&gt; 89, 137, 78, 197, 189, 166, 118, 103, 115, 126, 143, 125, 97,~\n$ pressure &lt;dbl&gt; 66, 40, 50, 70, 60, 72, 84, 30, 70, 88, 94, 70, 66, 82, 76, 5~\n$ triceps  &lt;dbl&gt; 23, 35, 32, 45, 23, 19, 47, 38, 30, 41, 33, 26, 15, 19, 36, 1~\n$ insulin  &lt;dbl&gt; 94, 168, 88, 543, 846, 175, 230, 83, 96, 235, 146, 115, 140, ~\n$ mass     &lt;dbl&gt; 28.1, 43.1, 31.0, 30.5, 30.1, 25.8, 45.8, 43.3, 34.6, 39.3, 3~\n$ pedigree &lt;dbl&gt; 0.167, 2.288, 0.248, 0.158, 0.398, 0.587, 0.551, 0.183, 0.529~\n$ age      &lt;dbl&gt; 21, 33, 26, 53, 59, 51, 31, 33, 32, 27, 51, 41, 22, 57, 28, 2~\n$ diabetes &lt;fct&gt; neg, pos, pos, pos, pos, pos, pos, neg, pos, neg, pos, pos, n~\n\n\nDe uiteindelijke (voorbereide) gegevens bevatten 392 waarnemingen en 9 kolommen. De onafhankelijke variabelen zijn van het type numeriek/dubbel, terwijl de afhankelijke/uitgaande binaire variabele van het type factor/categorie is (neg/ pos).\n\n\nGegevensniveaus\nWe kunnen het referentieniveau van de afhankelijke variabele controleren met de functie levels( ). We kunnen zien dat het referentieniveau neg is (het allereerste niveau).\n\nlevels(Diabetes$diabetes)\n\n[1] \"neg\" \"pos\"\n\n\n\n\nInstellen referentieniveau\nVoor een betere interpretatie (later voor het uitzetten van de ROC curve) moeten we het referentieniveau van onze afhankelijke variabele “diabetes” op positief (pos) zetten met de relevel( ) functie.\n\nDiabetes$diabetes &lt;- relevel(Diabetes$diabetes, ref = \"pos\")\nlevels(Diabetes$diabetes)\n\n[1] \"pos\" \"neg\"\n\n\n\n\nSplitsing training en testset\nDe volledige dataset wordt in het algemeen opgesplitst in 75% train en 25% test data set (algemene vuistregel). 75% van de trainingsdata wordt gebruikt om het model te trainen, terwijl de overige 25% wordt gebruikt om te controleren hoe het model generaliseerde op ongeziene/test data set.\nOm een split object te maken kun je de initial_split( ) functie gebruiken waar je de dataset, proportie en een strata argument voor moet opgeven. Door de afhankelijke variabele in het strata-attribuut op te geven, wordt gestratificeerde steekproeftrekking uitgevoerd. Gestratificeerde steekproeftrekking is nuttig als je afhankelijke variabele een ongelijke klasse heeft.\nDe volgende stap is het aanroepen van de training( ) en testing( ) functies op het split object (d.w.z. diabetes_split) om de trainings- (diabetes_train) en test- (diabetes_test) datasets op te slaan.\nDe training set bevat 295 waarnemingen, terwijl de test set 97 waarnemingen bevat.\n\nset.seed(123)\n# Creëer datasplit voor training en test\ndiabetes_split &lt;- initial_split(Diabetes,\n                                prop = 0.75,\n                                strata = diabetes)\n\n# Creëer trainingsdata\ndiabetes_train &lt;- diabetes_split %&gt;%\n                    training()\n\n# Creëer testdata\ndiabetes_test &lt;- diabetes_split %&gt;%\n                    testing()\n# Aantal rijen in trainings- en testset\nnrow(diabetes_train)\n\n[1] 293\n\nnrow(diabetes_test)\n\n[1] 99\n\n\n\n\nFitten van logistische regressie\nJe kunt met tidymodels elk type model pasklaar maken met behulp van de volgende stappen. l Stap 1: roep de modelfunctie op: hier gebruiken we logistic_reg( ) omdat we een logistisch regressiemodel willen draaien.\nStap 2: gebruik de set_engine( ) functie om de familie van het model op te geven. We geven het glm argument op, omdat logistische regressie onder de ‘Generalized Linear Regression’-familie valt.\nStap 3: gebruik de set_mode( ) functie en geef het type model op dat je wilt toepassen. Hier willen we pos vs neg classificeren, dus het is een classificatie.\nStap 4: Vervolgens moet je de fit( ) functie gebruiken om het model te fitten en daarbinnen moet je de formule notatie en de dataset (diabetes_train) opgeven.\nplus notatie → diabetes ~ ind_variable 1 + ind_variable 2 + …….so on\ntilde punt notatioe →\ndiabetes~. betekent dat diabetes wordt voorspeld door de rest van de variabelen in het gegevensbestand (d.w.z. alle onafhankelijke variabelen), behalve de afhankelijke variabele, d.w.z. diabetes.\nNa het draaien van het model is de volgende stap het genereren van de modeloverzichtstabel. Je kunt een mooie tabel maken met behulp van de tidy( ) functie van de broom bibliotheek (die is ingebouwd in de tidymodels bibliotheek). De gerapporteerde coëfficiënten zijn in log-odds termen.\n\nfitted_logistic_model&lt;- logistic_reg() %&gt;%\n        # Set the engine\n        set_engine(\"glm\") %&gt;%\n        # Set the mode\n        set_mode(\"classification\") %&gt;%\n        # Fit the model\n        fit(diabetes~., data = diabetes_train)\ntidy(fitted_logistic_model)    # Generate Summary Table\n\n# A tibble: 9 x 5\n  term         estimate std.error statistic  p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept) 10.3        1.47       6.96   3.38e-12\n2 pregnant    -0.0788     0.0647    -1.22   2.24e- 1\n3 glucose     -0.0390     0.00681   -5.72   1.04e- 8\n4 pressure     0.000634   0.0144     0.0440 9.65e- 1\n5 triceps     -0.0295     0.0208    -1.42   1.56e- 1\n6 insulin     -0.00166    0.00163   -1.02   3.09e- 1\n7 mass        -0.0587     0.0328    -1.79   7.37e- 2\n8 pedigree    -1.42       0.524     -2.71   6.65e- 3\n9 age         -0.0152     0.0209    -0.724  4.69e- 1\n\n\nOpgelet: Het teken en de waarde van de coëfficiënten veranderen afhankelijk van de referentie die u voor de afhankelijke variabele hebt ingesteld (in ons geval is pos het referentieniveau) en de waarneming die u op basis van de aselecte steekproefselectie in de opleidingssteekproef hebt opgenomen [bovenstaande resultaten zijn slechts een voorbeeld].\nDe interpretatie van coëfficiënten in de log-odds term heeft niet veel zin als je die moet rapporteren in je artikel of publicatie. Daarom werd het begrip odds ratio geïntroduceerd.\nDe ODDS is de verhouding van de kans dat een gebeurtenis zich voordoet tot de kans dat de gebeurtenis zich niet voordoet. Wanneer we een verhouding van twee zulke kansen nemen, noemen we dat Odds Ratio.\n\n\n\nOdds ratio\n\n\nWiskundig kan men de odds ratio berekenen door de exponent van de geschatte coëfficiënten te nemen. Je kunt bijvoorbeeld direct de odds ratio’s van de coëfficiënten krijgen door de exponentiate = True mee te geven in de tidy( ) functie.\nHet resultaat is alleen afhankelijk van de steekproeven die we hebben verkregen tijdens het splitsen. Je kunt een ander resultaat krijgen (odds ratio waarden).\n\ntidy(fitted_logistic_model, exponentiate = TRUE)\n\n# A tibble: 9 x 5\n  term         estimate std.error statistic  p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept) 28591.      1.47       6.96   3.38e-12\n2 pregnant        0.924   0.0647    -1.22   2.24e- 1\n3 glucose         0.962   0.00681   -5.72   1.04e- 8\n4 pressure        1.00    0.0144     0.0440 9.65e- 1\n5 triceps         0.971   0.0208    -1.42   1.56e- 1\n6 insulin         0.998   0.00163   -1.02   3.09e- 1\n7 mass            0.943   0.0328    -1.79   7.37e- 2\n8 pedigree        0.242   0.524     -2.71   6.65e- 3\n9 age             0.985   0.0209    -0.724  4.69e- 1\n\n\n\n\nSignificante kansen\nDe tabel geproduceerd door tidy( ) functie kan worden gefilterd. Hier hebben we de variabelen uitgefilterd waarvan de p-waarden lager zijn dan 0.05 (5%) significant niveau. Voor onze steekproef hebben glucose en massa een significante invloed op diabetes.\n\ntidy(fitted_logistic_model, exponentiate = TRUE) %&gt;%\n  filter(p.value &lt; 0.05)\n\n# A tibble: 3 x 5\n  term         estimate std.error statistic  p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept) 28591.      1.47         6.96 3.38e-12\n2 glucose         0.962   0.00681     -5.72 1.04e- 8\n3 pedigree        0.242   0.524       -2.71 6.65e- 3\n\n\n\n\nModel voorspelling\n\n\nVoorspelling van de testgegevensklasse\nDe volgende stap is het genereren van de testvoorspellingen die we kunnen gebruiken voor de evaluatie van het model. Om de klassevoorspelling (pos/neg) te genereren kunnen wij de predict-functie gebruiken en het getrainde modelobject, de testdataset en het type opgeven, dat hier “klasse” is, aangezien wij de klassevoorspelling willen, geen waarschijnlijkheden.\n\n# Class prediction\npred_class &lt;- predict(fitted_logistic_model,\n                      new_data = diabetes_test,\n                      type = \"class\")\n\npred_class[1:5,]\n\n# A tibble: 5 x 1\n  .pred_class\n  &lt;fct&gt;      \n1 neg        \n2 pos        \n3 neg        \n4 pos        \n5 pos        \n\n\n\n\nTestdata klasse waarschijnlijkheden\nWe kunnen ook voorspellingen genereren voor de klassenwaarschijnlijkheden door het argument “prob” in het type-attribuut mee te geven.\n\n# Voorspelling waarschijnlijkheden\npred_proba &lt;- predict(fitted_logistic_model,\n                      new_data = diabetes_test,\n                      type = \"prob\")\n\npred_proba[1:5,]\n\n# A tibble: 5 x 2\n  .pred_pos .pred_neg\n      &lt;dbl&gt;     &lt;dbl&gt;\n1     0.164     0.836\n2     0.536     0.464\n3     0.279     0.721\n4     0.739     0.261\n5     0.831     0.169\n\n\n\n\nVoorbereiding van de uiteindelijke gegevens voor de evaluatie van het model\nDe volgende stap is het voorbereiden van een gegevensframe dat de kolom diabetes uit de oorspronkelijke testdataset, de voorspelde klasse en de klassevoorspellingswaarschijnlijkheid bevat. We gaan dit dataframe gebruiken voor de evaluatie van het model.\n\ndiabetes_results &lt;- diabetes_test %&gt;%\n  select(diabetes) %&gt;%\n  bind_cols(pred_class, pred_proba)\n\ndiabetes_results[1:5, ]\n\n   diabetes .pred_class .pred_pos .pred_neg\n19      neg         neg 0.1640730 0.8359270\n21      neg         pos 0.5360718 0.4639282\n26      pos         neg 0.2786670 0.7213330\n32      pos         pos 0.7389338 0.2610662\n55      neg         pos 0.8311350 0.1688650\n\n\n\n\nModelevaluatie\n\n\nConfusiematrix\nWe kunnen een confusiematrix genereren met de conf_mat( )-functie door het uiteindelijke dataframe, diabetes_results, de waarheidskolom, diabetes en voorspelde klasse (.pred_class) in het schattingsattribuut op te geven.\nUit de confusiematrix blijkt dat de testdataset 65 gevallen van negatieve (neg) en 32 gevallen van positieve (pos) waarnemingen bevat. Het getrainde model classificeert 61 negatieven (neg) en 18 positieven (pos) accuraat.\n\nconf_mat(diabetes_results, truth = diabetes,\n         estimate = .pred_class)\n\n          Truth\nPrediction pos neg\n       pos  21  12\n       neg  12  54\n\n\nWe kunnen ook het yardstick pakket gebruiken dat bij het tidymodels pakket hoort om verschillende evaluatie metrieken te genereren voor de testdata set.\n\n\nNauwkeurigheid\nWe kunnen de classificatienauwkeurigheid berekenen met de accuracy( )-functie door het uiteindelijke dataframe, diabetes_results, de waarheidskolom, diabetes en voorspelde klasse (.pred_class) in het schattingsattribuut op te geven. De classificatienauwkeurigheid van het model op de testdataset is ongeveer 81,4%.\n\naccuracy(diabetes_results, truth = diabetes,\n         estimate = .pred_class)\n\n# A tibble: 1 x 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.758\n\n\n\n\nSensitiviteit\nDe sensitiviteit van een classificator is de verhouding tussen het aantal dat correct als positief wordt geïdentificeerd (TP) en het aantal dat daadwerkelijk positief is (FN+TP).\nSensitivity = TP / FN+TP\nDe geschatte sensitiviteitswaarde is 0,562, wat wijst op een slechte detectie van positieve klassen in de testdataset.\n\nsens(diabetes_results, truth = diabetes,\n    estimate = .pred_class)\n\n# A tibble: 1 x 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 sens    binary         0.636\n\n\n\n\nSpecificiteit\nSpecificiteit van een classificator is de verhouding tussen het aantal dat correct als negatief werd geclassificeerd (TN) en het aantal dat werkelijk negatief was (FP+TN).\nSpecificity = TN/FP+TN\nDe geschatte specificiteitswaarde is 0,938, wat wijst op een algemeen goede detectie van negatieve klassen in de testdataset.\n\nspec(diabetes_results, truth = diabetes,\n    estimate = .pred_class)\n\n# A tibble: 1 x 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 spec    binary         0.818\n\n\n\n\nPrecisie\nHoeveel van alle positieven werden correct als positief geclassificeerd?\nPrecisie = TP/TP+FP\nDe geschatte precisie waarde is 0.818.\n\nprecision(diabetes_results, truth = diabetes,\n    estimate = .pred_class)\n\n# A tibble: 1 x 3\n  .metric   .estimator .estimate\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 precision binary         0.636\n\n\n\n\nRecall\nRecall en sensitiviteit zijn hetzelfde.\nRecall = TP / FN+TP\nDe geschatte recall-waarde is 0.562.\n\nrecall(diabetes_results, truth = diabetes,\n      estimate = .pred_class)\n\n# A tibble: 1 x 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 recall  binary         0.636\n\n\n\n\nF-maat\nF-maat is een gewogen harmonisch gemiddelde van precisie en recall met de beste score 1 en de slechtste score 0. De F-maatscore geeft het evenwicht tussen precisie en recall weer. De F1-score is ongeveer 0,667, wat betekent dat het getrainde model een classificatiekracht van 66,7% heeft.\n\nf_meas(diabetes_results, truth = diabetes,\n       estimate = .pred_class)\n\n# A tibble: 1 x 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 f_meas  binary         0.636\n\n\n\n\nKappa\nCohen Kappa geeft informatie over hoeveel beter een model is dan de willekeurige classificator. Kappa kan gaan van -1 tot +1. De waarde &lt;0 betekent geen overeenstemming, terwijl 1,0 een perfecte overeenstemming aangeeft. Uit de geschatte kappastatistieken bleek een matige overeenkomst.\n\nkap(diabetes_results, truth = diabetes,\n    estimate = .pred_class)\n\n# A tibble: 1 x 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 kap     binary         0.455\n\n\n\n\nMatthews Correlatie Coefficient (MCC)\nDe Matthews correlatiecoëfficiënt (MCC) wordt gebruikt als maatstaf voor de kwaliteit van een binaire classificator. De waarde varieert van -1 tot +1.\nMCC: -1 wijst op totale onenigheid MCC: 0 wijst op geen overeenstemming MCC: +1 wijst op totale overeenstemming\nUit de geschatte MCC-statistieken bleek een matige overeenstemming.\n\nmcc(diabetes_results, truth = diabetes,\n    estimate = .pred_class)\n\n# A tibble: 1 x 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mcc     binary         0.455\n\n\n\n\nEvaluatiematen genereren\nWe kunnen de custom_metrics( )-functie gebruiken om verschillende metrieken tegelijk te genereren.\nStap 1: laat eerst zien wat je wilt laten zien door metric_set( ) te gebruiken Step 2: gebruik decustom_metrics( ) functie en betrek dit op de diabetes_results dataframe, diabaets kolom en op de voorspelde klasse (.pred_class).\n\ncustom_metrics &lt;- metric_set(accuracy, sens, spec, precision, recall, f_meas, kap, mcc)\ncustom_metrics(diabetes_results,\n               truth = diabetes,\n               estimate = .pred_class)\n\n# A tibble: 8 x 3\n  .metric   .estimator .estimate\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy  binary         0.758\n2 sens      binary         0.636\n3 spec      binary         0.818\n4 precision binary         0.636\n5 recall    binary         0.636\n6 f_meas    binary         0.636\n7 kap       binary         0.455\n8 mcc       binary         0.455\n\n\n\n\nROC-AUC\nROC-AUC is a performance measurement for the classification problem at various thresholds settings. ROC_AUC tells how much the model is capable of distinguishing between classes. The trained logistic regression model has a ROC-AUC of 0.921 indicating overall good predictive performance.\n\nroc_auc(diabetes_results,\n        truth = diabetes,\n        .pred_pos)\n\n# A tibble: 1 x 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.828\n\n\n\n\nROC-curve\nROC-AUC is een evaluatiemaat voor het classificatieprobleem bij verschillende drempelinstellingen. ROC-AUC geeft aan in welke mate het model in staat is een onderscheid te maken tussen de klassen. Het getrainde logistische regressiemodel heeft een ROC-AUC van 0,921, wat wijst op een algemeen goede voorspellende prestatie.\nDe ROC-curve wordt uitgezet met TPR (Sensitiviteit) tegen de FPR/ (1- Specificiteit), waarbij Sensitiviteit op de y-as staat en 1-Specificiteit op de x-as. Een lijn wordt diagonaal getrokken om de 50-50 verdeling van de grafiek aan te geven. Als de kromme dichter bij de lijn ligt, is de prestatie van de classificeerder lager en dan niet beter dan een toevallige gok.\nJe kunt een ROC Curve genereren met de roc_curve( ) functie waarbij je de waarheidskolom (diabetes) en de voorspelde kansen voor de positieve klasse (.pred_pos) moet opgeven.\nOns model heeft een ROC-AUC score van 0.921 wat aangeeft dat het een goed model is dat onderscheid kan maken tussen patiënten met diabetes en zonder diabetes.\n\ndiabetes_results %&gt;%\n  roc_curve(truth = diabetes, .pred_pos) %&gt;%\n  autoplot()\n\n\n\n\n\n# Schatting van verschillende evaluatiematenmet behulp van het caret-pakket\nlibrary(caret)\n\nWarning: package 'caret' was built under R version 4.1.3\n\n\nLoading required package: lattice\n\n\n\nAttaching package: 'caret'\n\n\nThe following objects are masked from 'package:yardstick':\n\n    precision, recall, sensitivity, specificity\n\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nconfusionMatrix(diabetes_results$.pred_class,\n                diabetes_results$diabetes,\n                positive=\"pos\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction pos neg\n       pos  21  12\n       neg  12  54\n                                          \n               Accuracy : 0.7576          \n                 95% CI : (0.6611, 0.8381)\n    No Information Rate : 0.6667          \n    P-Value [Acc &gt; NIR] : 0.0325          \n                                          \n                  Kappa : 0.4545          \n                                          \n Mcnemar's Test P-Value : 1.0000          \n                                          \n            Sensitivity : 0.6364          \n            Specificity : 0.8182          \n         Pos Pred Value : 0.6364          \n         Neg Pred Value : 0.8182          \n             Prevalence : 0.3333          \n         Detection Rate : 0.2121          \n   Detection Prevalence : 0.3333          \n      Balanced Accuracy : 0.7273          \n                                          \n       'Positive' Class : pos             \n                                          \n\n\nBinaire logistische regressie is nog steeds een enorm populair ML-algoritme (voor binaire classificatie) in het bèta/technische onderzoeksdomein. Het is nog steeds zeer eenvoudig te trainen en te interpreteren, in vergelijking met veel complexere modellen.\n\n\nReferenties\nNewman, C. B. D. & Merz, C. (1998). UCI Repository of machine learning databases, Technical report, University of California, Irvine, Dept. of Information and Computer Sciences.\nShrikant I. Bangdiwala (2018). Regression: binary logistic, International Journal of Injury Control and Safety Promotion, DOI: 10.1080/17457300.2018.1486503"
  },
  {
    "objectID": "posts/2021-07-18-verzekeringskosten/verzekeringskosten.html",
    "href": "posts/2021-07-18-verzekeringskosten/verzekeringskosten.html",
    "title": "Verzekeringskosten voorspellen",
    "section": "",
    "text": "Verzekeringskosten voorspellen met behulp van lineaire regressie\nDit is de blog die Arta Seyedan op 14 februari 2021 R-bloggers schreef en die ik wat bewerkt en vertaald heb.\nRond eind oktober 2020 woonde AS de Open Data Science Conferentie bij, voornamelijk voor de workshops en trainingssessies die daar werden aangeboden. De eerste workshop die hij bijwoonde was een demonstratie door Jared Lander over hoe je machine learning methoden in R kunt implementeren met behulp van een nieuw pakket genaamd tidymodels. Hij ging die training in en wist bijna niets over machine learning en heeft vervolgens uitsluitend gebruik gemaakt van gratis online materiaal om te begrijpen hoe je data analyseert met behulp van dit “meta-pakket”.\ntidymodels is net als tidyverse niet een enkel pakket. Het is eerder een verzameling van data science pakketten (een suite zeg maar) ontworpen volgens principes van tidyverse. Er is overeenkomst tussen tidymodels en tidyverse. Wat tidymodels echter anders maakt dan tidyverse, is dat veel van deze pakketten bedoeld zijn voor voorspellend modelleren. Het biedt een universele standaard interface voor alle verschillende machine learning methoden die beschikbaar zijn in R.\nOm te laten zien hoe het werkt, wordt hier een dataset aangeboden met informatie van ziektekostenverzekering van ~1300 klanten van een ziektekostenverzekeringsmaatschappij. Deze dataset is afkomstig uit een boek getiteld Machine Learning with R van Brett Lantz. Laten we tegelijk enkele pakketten openen.\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv ggplot2 3.3.6     v purrr   0.3.4\nv tibble  3.1.7     v dplyr   1.0.9\nv tidyr   1.2.0     v stringr 1.4.1\nv readr   2.1.2     v forcats 0.5.1\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'stringr' was built under R version 4.1.3\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.1.3\n\n\n-- Attaching packages -------------------------------------- tidymodels 0.2.0 --\n\n\nv broom        0.8.0     v rsample      0.1.1\nv dials        1.0.0     v tune         0.2.0\nv infer        1.0.2     v workflows    0.2.6\nv modeldata    0.1.1     v workflowsets 0.2.1\nv parsnip      1.0.0     v yardstick    1.0.0\nv recipes      0.2.0     \n\n\nWarning: package 'broom' was built under R version 4.1.3\n\n\nWarning: package 'dials' was built under R version 4.1.3\n\n\nWarning: package 'scales' was built under R version 4.1.3\n\n\nWarning: package 'infer' was built under R version 4.1.3\n\n\nWarning: package 'parsnip' was built under R version 4.1.3\n\n\nWarning: package 'recipes' was built under R version 4.1.3\n\n\nWarning: package 'tune' was built under R version 4.1.3\n\n\nWarning: package 'workflows' was built under R version 4.1.3\n\n\nWarning: package 'workflowsets' was built under R version 4.1.3\n\n\nWarning: package 'yardstick' was built under R version 4.1.3\n\n\n-- Conflicts ----------------------------------------- tidymodels_conflicts() --\nx scales::discard() masks purrr::discard()\nx dplyr::filter()   masks stats::filter()\nx recipes::fixed()  masks stringr::fixed()\nx dplyr::lag()      masks stats::lag()\nx yardstick::spec() masks readr::spec()\nx recipes::step()   masks stats::step()\n* Dig deeper into tidy modeling with R at https://www.tmwr.org\n\nlibrary(data.table)\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\ndownload.file(\"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv\", \n              \"insurance.csv\")\n\ninsur_dt &lt;- fread(\"insurance.csv\")\n\nDit zijn de zeven variabelen die erin zitten.\n\ninsur_dt %&gt;% colnames()\n\n[1] \"age\"      \"sex\"      \"bmi\"      \"children\" \"smoker\"   \"region\"   \"charges\" \n\n\nZo zien de variabelen er vervolgens uit.\n\ninsur_dt$age %&gt;% summary()\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  18.00   27.00   39.00   39.21   51.00   64.00 \n\n\n\ninsur_dt$sex %&gt;% table()\n\n.\nfemale   male \n   662    676 \n\n\n\ninsur_dt$bmi %&gt;% summary()\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  15.96   26.30   30.40   30.66   34.69   53.13 \n\n\n\ninsur_dt$smoker %&gt;% table()\n\n.\n  no  yes \n1064  274 \n\n\n\ninsur_dt$charges %&gt;% summary()\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1122    4740    9382   13270   16640   63770 \n\n\nHierboven zie je dat je pakketten als parsnip en recipes hebt geladen. Deze pakketten vormen dus, samen met andere pakketten, het meta-pakket tidymodels, dat gebruikt wordt voor modelleren en statistische analyse en machine learning.\nZoals je kunt zien, zijn er 7 verschillende, relatief voor zichzelf sprekende variabelen in deze dataset, waarvan sommige vermoedelijk worden gebruikt door de particuliere ziektekostenverzekeraar in kwestie om te bepalen hoeveel een bepaald individu uiteindelijk in rekening wordt gebracht. age(Leeftijd), sex (geslacht) en region (regio) lijken demografische achtergrondvariabelen te zijn, waarbij de leeftijd niet lager dan 18 en niet hoger dan 64 jaar is, met een gemiddelde van ongeveer 40 jaar. Het aantal mannen en vrouwen is vrijwel hetzelfde.\nErvan uitgaande dat de variabele bmi overeenkomt met Body Mass Index, wordt een BMI van 30 of hoger als klinisch zwaarlijvig beschouwd. In onze huidige gegevensverzameling ligt het gemiddelde net boven de grens van zwaarlijvigheid.\nVervolgens hebben we het aantal rokers versus niet-rokers. Nu kan ik je zeker al vertellen dat het al of nietroker zijn belangrijk zal zijn bij het bepalen van de kosten van een bepaalde ziektekostenverzekeraar.\nTenslotte, hebben we charge (kosten). De gemiddelde jaarlijkse kosten voor een ziektekostenverzekering zijn een bescheiden 13.000 dollar."
  },
  {
    "objectID": "posts/2021-07-18-verzekeringskosten/verzekeringskosten.html#lineaire-regressie",
    "href": "posts/2021-07-18-verzekeringskosten/verzekeringskosten.html#lineaire-regressie",
    "title": "Verzekeringskosten voorspellen",
    "section": "Lineaire Regressie",
    "text": "Lineaire Regressie\nWe hebben het recept (recipe) al. Nu moeten we alleen nog een lineair model specificeren en het model kruisvalideren om het te testen op de testgegevens.\n\nlm_spec &lt;- linear_reg() %&gt;% \n    set_engine(\"lm\")\n\nlm_fit &lt;- lm_spec %&gt;%\n    fit(charges ~ age + bmi + smoker_yes + bmi_x_smoker_yes,\n        data = juice(insur_rec %&gt;% prep()))\n\ninsur_lm_wf &lt;- workflow() %&gt;%\n    add_recipe(insur_rec) %&gt;%\n    add_model(lm_spec)\n\nWe herhalen sommige van dezelfde stappen die we voor KNN deden, maar dan nu voor het lineaire model. We kunnen zelfs crossvalideren door (bijna) hetzelfde commando te gebruiken:\n\ninsur_lm_rsmpl &lt;- fit_resamples(insur_lm_wf,\n                           insur_cv,\n                           control = control_resamples(save_pred = TRUE))\n\ninsur_lm_rsmpl %&gt;% \n    collect_metrics()\n\n# A tibble: 2 x 6\n  .metric .estimator     mean     n  std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   4821.       10 238.     Preprocessor1_Model1\n2 rsq     standard      0.837    10   0.0199 Preprocessor1_Model1\n\ninsur_rsmpl %&gt;% \n      collect_metrics()\n\n# A tibble: 2 x 6\n  .metric .estimator     mean     n  std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   4829.       10 242.     Preprocessor1_Model1\n2 rsq     standard      0.837    10   0.0217 Preprocessor1_Model1\n\n\nFascinerend! Het blijkt dat het goede, ouderwetse lineaire model k-Nearest Neighbors verslaat zowel in termen van RMSE als van R^2 over 10 kruisvalidatie-voudigingen.\n\ninsur_test_lm_res &lt;- predict(lm_fit, new_data = test_proc %&gt;% select(-charges))\n\ninsur_test_lm_res &lt;- bind_cols(insur_test_lm_res, insur_test %&gt;% select(charges))\n\ninsur_test_lm_res\n\n# A tibble: 335 x 2\n    .pred charges\n    &lt;dbl&gt;   &lt;dbl&gt;\n 1 23455.  16885.\n 2  2911.   1726.\n 3  6385.  21984.\n 4  7586.   7282.\n 5 13430.  28923.\n 6 12883.  11091.\n 7 11544.  10797.\n 8  3855.   2395.\n 9 36734.  36837.\n10 14106.  14452.\n# ... with 325 more rows\n\n\nNu we onze voorspellingen hebben, laten we eens kijken hoe goed het lineaire model het deed:\n\nggplot(insur_test_lm_res, aes(x = charges, y = .pred)) +\n  # Create a diagonal line:\n  geom_abline(lty = 2) +\n  geom_point(alpha = 0.5) +\n  labs(y = \"Predicted Charges\", x = \"Charges\") +\n  # Scale and size the x- and y-axis uniformly:\n  coord_obs_pred()\n\n\n\n\nHet lijkt erop dat het gebied linksonder de grootste concentratie ladingen had, en het grootste deel van de lm fit verklaart. Kijkend naar deze beide plots vraag ik me af of er een beter model was dat we hadden kunnen gebruiken, maar ons model voldeed gezien onze doelstellingen en nauwkeurigheidsniveau.\n\ncombind_dt &lt;- mutate(insur_test_lm_res,\n      lm_pred = .pred,\n      charges = charges\n      ) %&gt;% select(-.pred) %&gt;%\n    add_column(knn_pred = insur_test_res$.pred)\n\nggplot(combind_dt, aes(x = charges)) +\n    geom_line(aes(y = knn_pred, color = \"kNN Fit\"), size = 1) +\n    geom_line(aes(y = lm_pred, color = \"lm Fit\"), size = 1) +\n    geom_point(aes(y = knn_pred, alpha = 0.5), color = \"#F99E9E\") +\n    geom_point(aes(y = lm_pred, alpha = 0.5), color = \"#809BF4\") +\n    geom_abline(size = 0.5, linetype = \"dashed\") +\n    xlab('Charges') +\n    ylab('Predicted Charges') +\n    guides(alpha = FALSE)\n\nWarning: `guides(&lt;scale&gt; = FALSE)` is deprecated. Please use `guides(&lt;scale&gt; =\n\"none\")` instead.\n\n\n\n\n\nHierboven is een vergelijking van de twee methoden met hun respectieve voorspellingen, en met de stippellijn die de “juiste” waarden weergeeft. In dit geval verschilden de twee modellen niet zo veel van elkaar dat hun verschillen gemakkelijk konden worden waargenomen wanneer ze tegen elkaar werden uitgezet. Maar er zullen zich in de toekomst gevallen voordoen waarin uw twee modellen toch aanzienlijk verschillen. Het zo doen zal je helpen het ene model boven het andere te verkiezen.\n\nConclusie\nHier konden wij een KNN-model bouwen met onze trainingsgegevens en het gebruiken om waarden in onze testgegevens te voorspellen. Om dit te doen, hebben we:\n\neen EDA uitgevoerd;\nonze gegevens voorbewerkt en met recipe ons model gespecificeerd als KNN;\nhet toegepast op onze trainingsgegevens;\ncrossvalidatie uitgevoerd om nauwkeurige foutstatistieken te produceren;\nvoorspelde waarden vastgesteld in onze testset;\nde waargenomen testwaarden met onze voorspellingen vergeleken;\neen ander model gespecificeerd (lm);\nook hier een crossvalidatie uitgevoerd;\nontdekt dat lm het betere model was.\n\nHij is zeer enthousiast om door te gaan met het gebruik van tidymodels in R als een manier om machine-learning methoden toe te passen. Als je geïnteresseerd bent, raad ik je aan om Tidy Modeling with R by Max Kuhn and Julia Silge te bekijken."
  },
  {
    "objectID": "posts/2021-07-18-classificeren-van-palmer-penguins/classificeren-van-palmer-penguins.html",
    "href": "posts/2021-07-18-classificeren-van-palmer-penguins/classificeren-van-palmer-penguins.html",
    "title": "Classificeren van Palmer penguins",
    "section": "",
    "text": "Hier kun je overigen haar opnmame vinden. Julia Silge on youtube\nDe laatste tijd heeft Julia Silge een aantal videoopnamen gemaakt die laten zien hoe het tidymodels raamwerk is te gebruiken.Het zijn opnamen over de eerste stappen in het modelleren tot hoe complexe modellen zijn te evalueren. Deze videoopname is goed voor mensen die net beginnen met tidymodels. Ze maakt daarbij gebruik van een #TidyTuesday dataset over pinguïns. Hier gaat het om classificeren.\nHier kun je haar opnmame vinden. Julia Silge on youtube\nEerst maar eens enkele pakketten laden en het databestand openen.\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv ggplot2 3.3.6     v purrr   0.3.4\nv tibble  3.1.7     v dplyr   1.0.9\nv tidyr   1.2.0     v stringr 1.4.1\nv readr   2.1.2     v forcats 0.5.1\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'stringr' was built under R version 4.1.3\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(palmerpenguins)\n\nWarning: package 'palmerpenguins' was built under R version 4.1.3\n\npenguins\n\n# A tibble: 344 x 8\n   species island    bill_length_mm bill_depth_mm flipper_~1 body_~2 sex    year\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;      &lt;int&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7        181    3750 male   2007\n 2 Adelie  Torgersen           39.5          17.4        186    3800 fema~  2007\n 3 Adelie  Torgersen           40.3          18          195    3250 fema~  2007\n 4 Adelie  Torgersen           NA            NA           NA      NA &lt;NA&gt;   2007\n 5 Adelie  Torgersen           36.7          19.3        193    3450 fema~  2007\n 6 Adelie  Torgersen           39.3          20.6        190    3650 male   2007\n 7 Adelie  Torgersen           38.9          17.8        181    3625 fema~  2007\n 8 Adelie  Torgersen           39.2          19.6        195    4675 male   2007\n 9 Adelie  Torgersen           34.1          18.1        193    3475 &lt;NA&gt;   2007\n10 Adelie  Torgersen           42            20.2        190    4250 &lt;NA&gt;   2007\n# ... with 334 more rows, and abbreviated variable names 1: flipper_length_mm,\n#   2: body_mass_g\n\n\nAls je een classificatiemodel voor soorten pinquins probeert op te stellen, zul je waarschijnlijk een bijna perfecte pasvorm vinden, omdat dit soort waarnemingen in feite de verschillende soorten onderscheiden. sex (geslacht) daarentegen geeft een wat rommeliger beeld, vandaar dat hier deze uitkomstvariabelen op basis van predictoren wordt voorspeld.\n\npenguins %&gt;%\n  filter(!is.na(sex)) %&gt;%\n  ggplot(aes(flipper_length_mm, bill_length_mm, color = sex, size = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~species)\n\n\n\n\nHet ziet er naar uit dat de vrouwelijke pinguïnflippers kleiner zijn met kleinere snavels, maar laten we ons klaarmaken voor het modelleren om meer te weten te komen! De informatie over het eiland of het jaar zullen we niet gebruiken in ons model. Die halen we eruit.\n\npenguins_df &lt;- penguins %&gt;%\n  filter(!is.na(sex)) %&gt;%\n  select(-year, -island)\n\n\n\nWe zullen ook het tidymodels metapakket laden en vervolgens onze gegevens splitsen in een trainings- en testingssets.\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.1.3\n\n\n-- Attaching packages -------------------------------------- tidymodels 0.2.0 --\n\n\nv broom        0.8.0     v rsample      0.1.1\nv dials        1.0.0     v tune         0.2.0\nv infer        1.0.2     v workflows    0.2.6\nv modeldata    0.1.1     v workflowsets 0.2.1\nv parsnip      1.0.0     v yardstick    1.0.0\nv recipes      0.2.0     \n\n\nWarning: package 'broom' was built under R version 4.1.3\n\n\nWarning: package 'dials' was built under R version 4.1.3\n\n\nWarning: package 'scales' was built under R version 4.1.3\n\n\nWarning: package 'infer' was built under R version 4.1.3\n\n\nWarning: package 'parsnip' was built under R version 4.1.3\n\n\nWarning: package 'recipes' was built under R version 4.1.3\n\n\nWarning: package 'tune' was built under R version 4.1.3\n\n\nWarning: package 'workflows' was built under R version 4.1.3\n\n\nWarning: package 'workflowsets' was built under R version 4.1.3\n\n\nWarning: package 'yardstick' was built under R version 4.1.3\n\n\n-- Conflicts ----------------------------------------- tidymodels_conflicts() --\nx scales::discard() masks purrr::discard()\nx dplyr::filter()   masks stats::filter()\nx recipes::fixed()  masks stringr::fixed()\nx dplyr::lag()      masks stats::lag()\nx yardstick::spec() masks readr::spec()\nx recipes::step()   masks stats::step()\n* Learn how to get started at https://www.tidymodels.org/start/\n\nset.seed(123)\npenguin_split &lt;- initial_split(penguins_df, strata = sex)\npenguin_train &lt;- training(penguin_split)\npenguin_test &lt;- testing(penguin_split)\n\nOmdat het een relatieve kleine dataset betreft (zeker de testset), maken we vervolgens hier gebruik van bootstrap-resamples van de trainingsgegevens, om onze modellen te evalueren.\n\nset.seed(123)\npenguin_boot &lt;- bootstraps(penguin_train)\npenguin_boot\n\n# Bootstrap sampling \n# A tibble: 25 x 2\n   splits           id         \n   &lt;list&gt;           &lt;chr&gt;      \n 1 &lt;split [249/93]&gt; Bootstrap01\n 2 &lt;split [249/91]&gt; Bootstrap02\n 3 &lt;split [249/90]&gt; Bootstrap03\n 4 &lt;split [249/91]&gt; Bootstrap04\n 5 &lt;split [249/85]&gt; Bootstrap05\n 6 &lt;split [249/87]&gt; Bootstrap06\n 7 &lt;split [249/94]&gt; Bootstrap07\n 8 &lt;split [249/88]&gt; Bootstrap08\n 9 &lt;split [249/95]&gt; Bootstrap09\n10 &lt;split [249/89]&gt; Bootstrap10\n# ... with 15 more rows\n\n\nLaten we eens twee verschillende modellen vergelijken, een logistisch regressiemodel en een random forest model. We beginnen met het maken van de modelspecificaties voor beide modellen.\n\nglm_spec &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\")\n\nglm_spec\n\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\n\nrf_spec &lt;- rand_forest() %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"ranger\")\n\nrf_spec\n\nRandom Forest Model Specification (classification)\n\nComputational engine: ranger \n\n\nLaten we nu beginnen met het samenstellen van een tidymodels workflow(), een object dat helpt om modelleer-pijplijnen te beheren met stukjes die in elkaar passen als Lego-blokjes. Merk op dat er nog geen model is:\n\npenguin_wf &lt;- workflow() %&gt;%\n  add_formula(sex ~ .)\n\npenguin_wf\n\n== Workflow ====================================================================\nPreprocessor: Formula\nModel: None\n\n-- Preprocessor ----------------------------------------------------------------\nsex ~ .\n\n\nNu kunnen we een model toevoegen, en de fit voor elk van de resamples. Eerst kunnen we het logistische regressiemodel passen.\n\nglm_rs &lt;- penguin_wf %&gt;%\n  add_model(glm_spec) %&gt;%\n  fit_resamples(\n    resamples = penguin_boot,\n    control = control_resamples(save_pred = TRUE)\n  )\n\n! Bootstrap05: preprocessor 1/1, model 1/1: glm.fit: fitted probabilities numerically 0...\n\n\n! Bootstrap08: preprocessor 1/1, model 1/1: glm.fit: fitted probabilities numerically 0...\n\n\n! Bootstrap23: preprocessor 1/1, model 1/1: glm.fit: fitted probabilities numerically 0...\n\nglm_rs\n\n# Resampling results\n# Bootstrap sampling \n# A tibble: 25 x 5\n   splits           id          .metrics         .notes           .predictions\n   &lt;list&gt;           &lt;chr&gt;       &lt;list&gt;           &lt;list&gt;           &lt;list&gt;      \n 1 &lt;split [249/93]&gt; Bootstrap01 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 3]&gt; &lt;tibble&gt;    \n 2 &lt;split [249/91]&gt; Bootstrap02 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 3]&gt; &lt;tibble&gt;    \n 3 &lt;split [249/90]&gt; Bootstrap03 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 3]&gt; &lt;tibble&gt;    \n 4 &lt;split [249/91]&gt; Bootstrap04 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 3]&gt; &lt;tibble&gt;    \n 5 &lt;split [249/85]&gt; Bootstrap05 &lt;tibble [2 x 4]&gt; &lt;tibble [1 x 3]&gt; &lt;tibble&gt;    \n 6 &lt;split [249/87]&gt; Bootstrap06 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 3]&gt; &lt;tibble&gt;    \n 7 &lt;split [249/94]&gt; Bootstrap07 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 3]&gt; &lt;tibble&gt;    \n 8 &lt;split [249/88]&gt; Bootstrap08 &lt;tibble [2 x 4]&gt; &lt;tibble [1 x 3]&gt; &lt;tibble&gt;    \n 9 &lt;split [249/95]&gt; Bootstrap09 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 3]&gt; &lt;tibble&gt;    \n10 &lt;split [249/89]&gt; Bootstrap10 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 3]&gt; &lt;tibble&gt;    \n# ... with 15 more rows\n\nThere were issues with some computations:\n\n  - Warning(s) x3: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nUse `collect_notes(object)` for more information.\n\n\nTen tweede kunnen we het random forest model toepassen.\n\nrf_rs &lt;- penguin_wf %&gt;%\n  add_model(rf_spec) %&gt;%\n  fit_resamples(\n    resamples = penguin_boot,\n    control = control_resamples(save_pred = TRUE)\n  )\n\nWarning: package 'ranger' was built under R version 4.1.3\n\nrf_rs\n\n# Resampling results\n# Bootstrap sampling \n# A tibble: 25 x 5\n   splits           id          .metrics         .notes           .predictions\n   &lt;list&gt;           &lt;chr&gt;       &lt;list&gt;           &lt;list&gt;           &lt;list&gt;      \n 1 &lt;split [249/93]&gt; Bootstrap01 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 3]&gt; &lt;tibble&gt;    \n 2 &lt;split [249/91]&gt; Bootstrap02 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 3]&gt; &lt;tibble&gt;    \n 3 &lt;split [249/90]&gt; Bootstrap03 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 3]&gt; &lt;tibble&gt;    \n 4 &lt;split [249/91]&gt; Bootstrap04 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 3]&gt; &lt;tibble&gt;    \n 5 &lt;split [249/85]&gt; Bootstrap05 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 3]&gt; &lt;tibble&gt;    \n 6 &lt;split [249/87]&gt; Bootstrap06 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 3]&gt; &lt;tibble&gt;    \n 7 &lt;split [249/94]&gt; Bootstrap07 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 3]&gt; &lt;tibble&gt;    \n 8 &lt;split [249/88]&gt; Bootstrap08 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 3]&gt; &lt;tibble&gt;    \n 9 &lt;split [249/95]&gt; Bootstrap09 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 3]&gt; &lt;tibble&gt;    \n10 &lt;split [249/89]&gt; Bootstrap10 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 3]&gt; &lt;tibble&gt;    \n# ... with 15 more rows\n\n\nWij hebben elk van onze kandidaat-modellen aangepast aan onze opnieuw bemonsterde trainingsreeks!\n\n\n\nLaten we nu eens kijken hoe we het gedaan hebben. Eerst het logistisch regressiemodel.\n\ncollect_metrics(glm_rs)\n\n# A tibble: 2 x 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.918    25 0.00639 Preprocessor1_Model1\n2 roc_auc  binary     0.979    25 0.00254 Preprocessor1_Model1\n\n\nGoed zo! De functie collect_metrics() extraheert en formatteert de .metrics kolom van resampling resultaten zoals hierboven voor het glm-model. Nu het random-forest model.\n\ncollect_metrics(rf_rs)\n\n# A tibble: 2 x 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.912    25 0.00547 Preprocessor1_Model1\n2 roc_auc  binary     0.977    25 0.00202 Preprocessor1_Model1\n\n\nDus… ook geweldig! Als ik in een situatie zit waarin een complexer model (zoals een random forest) hetzelfde presteert als een eenvoudiger model (zoals logistische regressie), dan kies ik het eenvoudiger model. Laten we eens dieper ingaan op hoe het het doet. Bijvoorbeeld, hoe voorspelt het glm-model de twee klassen?\n\nglm_rs %&gt;%\n  conf_mat_resampled()\n\n# A tibble: 4 x 3\n  Prediction Truth   Freq\n  &lt;fct&gt;      &lt;fct&gt;  &lt;dbl&gt;\n1 female     female  41.1\n2 female     male     3  \n3 male       female   4.4\n4 male       male    42.3\n\n\nOngeveer hetzelfde, wat goed is. We kunnen ook een ROC curve maken.\n\nglm_rs %&gt;%\n  collect_predictions() %&gt;%\n  group_by(id) %&gt;%\n  roc_curve(sex, .pred_female) %&gt;%\n  ggplot(aes(1 - specificity, sensitivity, color = id)) +\n  geom_abline(lty = 2, color = \"gray80\", size = 1.5) +\n  geom_path(show.legend = FALSE, alpha = 0.6, size = 1.2) +\n  coord_equal()\n\n\n\n\nDeze ROC-curve is grilliger dan andere die u wellicht hebt gezien omdat de dataset klein is.\nHet is eindelijk tijd om terug te keren naar de testset. Merk op dat we de testset tijdens deze hele analyse nog niet hebben gebruikt; de testset is kostbaar en kan alleen worden gebruikt om de prestaties op nieuwe gegevens in te schatten. Laten we nog een keer passen op de trainingsgegevens en evalueren op de testgegevens met behulp van de functie last_fit().\n\npenguin_final &lt;- penguin_wf %&gt;%\n  add_model(glm_spec) %&gt;%\n  last_fit(penguin_split)\n\npenguin_final\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 x 6\n  splits           id               .metrics .notes   .predictions .workflow \n  &lt;list&gt;           &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [249/84]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\n\nDe metriek en voorspellingen hier zijn op de testgegevens.\n\ncollect_metrics(penguin_final)\n\n# A tibble: 2 x 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.857 Preprocessor1_Model1\n2 roc_auc  binary         0.938 Preprocessor1_Model1\n\n\n\ncollect_predictions(penguin_final) %&gt;%\n  conf_mat(sex, .pred_class)\n\n          Truth\nPrediction female male\n    female     37    7\n    male        5   35\n\n\nDe coëfficiënten (die we eruit kunnen halen met tidy()) zijn geschat met behulp van de trainingsdata. Als we exponentiate = TRUE gebruiken, hebben we odds ratio’s.\n\npenguin_final$.workflow[[1]] %&gt;%\n  tidy(exponentiate = TRUE)\n\n# A tibble: 7 x 5\n  term              estimate std.error statistic     p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 (Intercept)       5.75e-46  19.6        -5.31  0.000000110\n2 speciesChinstrap  1.37e- 4   2.34       -3.79  0.000148   \n3 speciesGentoo     1.14e- 5   3.75       -3.03  0.00243    \n4 bill_length_mm    1.91e+ 0   0.180       3.60  0.000321   \n5 bill_depth_mm     8.36e+ 0   0.478       4.45  0.00000868 \n6 flipper_length_mm 1.06e+ 0   0.0611      0.926 0.355      \n7 body_mass_g       1.01e+ 0   0.00176     4.59  0.00000442 \n\n\n\nDe grootste kansverhouding geldt voor de snaveldiepte, en de op één na grootste voor de snavellengte. Een toename van 1 mm snaveldiepte komt overeen met bijna 4x meer kans om een mannetje te zijn. De kenmerken van de bek van een pinguïn moeten geassocieerd zijn met het geslacht.\n\nWe hebben geen sterke aanwijzingen dat de lengte van de vleugels verschillend is tussen mannelijke en vrouwelijke pinguïns, als we de andere maten controleren; misschien moeten we dat onderzoeken door de eerste grafiek te veranderen!\n\n\npenguins %&gt;%\n  filter(!is.na(sex)) %&gt;%\n  ggplot(aes(bill_depth_mm, bill_length_mm, color = sex, size = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~species)\n\n\n\n\nJa, de mannetjes- en vrouwtjespinguïns zijn nu veel meer gescheiden."
  },
  {
    "objectID": "posts/2021-07-21-kaart-van-zwitserland/kaart-van-zwitserland.html",
    "href": "posts/2021-07-21-kaart-van-zwitserland/kaart-van-zwitserland.html",
    "title": "Kaart van Zwitserland",
    "section": "",
    "text": "Op Medium verscheen eind 2020 dit duidelijke blog van Giulia Ruggeri Hier de link. Ik wilde weer eens met ggplot2 en sf werken en Giulia’s blog vond ik interessant en heb ik vervolgens bewerkt.\nIn de afgelopen jaren is het maken van mooie kaarten in R vrij eenvoudig geworden, dankzij het {sf} pakket. In dit artikel gaan we de ruimtelijke verspreiding van de COVID-19 incidentie van de laatste 14 dagen in Zwitserland visualiseren door een thematische kaart te maken, een choropleth kaart zoals dat heet. We maken daarbij gebruik van {sf} en {ggplot2} als onze belangrijkste hulpmiddelen.\n{sf}, wat staat voor simple feature (eenvoudige eigenschap), is de ‘go-to’ bibliotheek om om te gaan met ruimtelijke vectoriële gegevens, dat zijn gegevens die geografische geometrieën beschrijven als een reeks van punten, die worden beschreven door hun lengteg- en breedtegraad coördinaten. Hiermee kunnen geografische vormen worden geïmporteerd, gemanipuleerd en geplot en kunnen we gegevens verwerken in een tabel-achtig formaat, net als een data.frame. Wat een opluchting!\nIn deze kleine oefening gebruiken we {readxl} om het Excel bestand te importeren en binnen te halen van de website van het Zwitsers Federaal Bureau van Publieke Gezondheid.\n{rcartocolor} is de R bibliotheek die mooi uitziende kleurschalen bevat, die zijn ontwikkeld voor cartografie. Zoals David Letterman zou zeggen, {tidyverse}‘needs no introduction’. Dit zijn de programma’s die we hier binnenhalen.\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(rcartocolor)\nlibrary(readxl)\n\nLaten we beginnen met het laden van de gegevens, met read_excel(), waarin we de exacte naam van het blad dat we willen laden kunnen opgeven, hoeveel regels we mogen overslaan en hoeveel regels we in totaal willen behouden.\nWe hebben een rij per kanton en een rij voor de titel, wat betekent dat we slechts 27 rijen hoeven te behouden (er zijn 26 Zwitserse kantons).\nWe schonen ook de kolomnamen een beetje op met clean_names uit het {janitor} pakket en gebruiken transmute om de gewenste kolommen te hernoemen en de andere te laten vallen.\n\nTot nu toe, eenvoudige dataimport en manipulatie.\n\n\ncovid_incidence &lt;- read_excel(\"resources/200325_dati di base_grafica_COVID-19-rapporto.xlsx\", \n    sheet = \"COVID19 casi per cantone\", skip = 6, \n    n_max = 27) %&gt;% \n  janitor::clean_names() %&gt;% \n  transmute(canton = cantone, incidence = incidenza_100_000_6)\n\nNew names:\n* `Casi confermati` -&gt; `Casi confermati...2`\n* `Incidenza/100 000` -&gt; `Incidenza/100 000...3`\n* `` -&gt; `...4`\n* `Casi confermati` -&gt; `Casi confermati...5`\n* `Incidenza/100 000` -&gt; `Incidenza/100 000...6`\n\nhead(covid_incidence)\n\n# A tibble: 6 x 2\n  canton incidence\n  &lt;chr&gt;      &lt;dbl&gt;\n1 AG          45.1\n2 AI          99.1\n3 AR          92.3\n4 BE          73  \n5 BL          33  \n6 BS          47.7\n\n\nWij hebben nu een variabele die de kantoncodes bevat en een variabele die de incidentie per 100.000, per kanton, van COVID-19 in de laatste 14 dagen bevat.\nWe zijn nu klaar om de shapefiles te laden.\n\nWacht even, wat zijn de shapefiles?\n\nShapefiles, zijn de bestanden die de geografische vormen bevatten die we willen plotten. We willen de data van de kantons plotten, dus hebben we de Zwitserse kantons nodig, die kunnen worden gedownload van [hier] (https://www.bfs.admin.ch/bfs/en/home/services/geostat/swiss-federal-statistics-geodata/administrative-boundaries/generalized-boundaries-local-regional-authorities.html).\nShapefiles zijn eigenlijk een set van bestanden, die verschillende geografische informatie bevatten (b.v. info over de projecties). Een van deze bestanden heeft de extensie .shp en dit is het bestand dat we gaan laden.\n\nLet op dat je alle andere bestanden in dezelfde map hebt staan.\n\nNu kunnen we dus 2 shapefiles laden, één die de vormen van de kantongrenzen bevat en één die de vorm van de grote meren van Zwitserland bevat.\nLaten we ze eens laden en kijken hoe ze eruit zien.\n\nswiss_lakes &lt;- st_read(\"resources/g2s15.shp\")\n\nReading layer `g2s15' from data source \n  `C:\\FilesHarrie\\HHQuarto\\posts\\2021-07-21-kaart-van-zwitserland\\resources\\g2s15.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 22 features and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 500253.8 ymin: 63872.4 xmax: 774495.3 ymax: 297632.2\nProjected CRS: CH1903 / LV03\n\nswiss_cantons &lt;- st_read(\"resources/G1K09.shp\")\n\nReading layer `G1K09' from data source \n  `C:\\FilesHarrie\\HHQuarto\\posts\\2021-07-21-kaart-van-zwitserland\\resources\\G1K09.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 26 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 485414 ymin: 75286 xmax: 833837 ymax: 295935\nProjected CRS: CH1903 / LV03\n\n\n\nclass(swiss_cantons)\n\n[1] \"sf\"         \"data.frame\"\n\n\nswiss_cantons en swiss_lakes, worden opgeslagen als sf data.frames (als dataframe van sf dus), zodat we ze kunnen manipuleren, net zoals we tibbles (of data.frames) kunnen manipuleren. Dit is mogelijk omdat geometrieën worden opgeslagen op een zeer nette manier: als een geneste variabele meestal genaamd geometry. Dit zal je enige speciale variabele zijn, de andere (die attributen worden genoemd) zullen gewoon normale variabelen zijn. Bijvoorbeeld, elk kanton heeft zijn naam en code gekoppeld aan de geometrie die het beschrijft.\n\nhead(swiss_cantons)\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 546871 ymin: 130593 xmax: 768722 ymax: 295935\nProjected CRS: CH1903 / LV03\n  KT         NAME KURZ                       geometry\n1 17   St. Gallen   SG MULTIPOLYGON (((738559 1968...\n2 12  Basel-Stadt   BS MULTIPOLYGON (((608728 2681...\n3  7    Nidwalden   NW MULTIPOLYGON (((671030 1822...\n4  2         Bern   BE MULTIPOLYGON (((572954 1936...\n5 14 Schaffhausen   SH MULTIPOLYGON (((684561 2726...\n6 10     Fribourg   FR MULTIPOLYGON (((584435 1976...\n\n\n\nHet voordeel van {sf} te gebruiken als ons hoofdgereedschap om met deze datatypes om te gaan? We kunnen {ggplot2} gebruiken om ze te plotten!\n\n\nggplot()+\n  geom_sf(data = swiss_cantons)\n\n\n\n\nEn, net als met elke {ggplot2} grafiek, kunnen we het bouwen van de kaart laag voor laag opbouwen. Laten we nu de Zwitserse meren toevoegen bovenop de kantonvormen en theme_void() gebruiken om de achtergrond en de as te verwijderen. In deze stap kunnen we ook de kantons transparant maken door het fill argument op NA te zetten en een lichte groenblauwe kleur toe te voegen om de meren te vullen. geom_sf() werkt inderdaad net als elke andere geom_ functie, geen alarmen en geen verrassingen hier.\n\nggplot()+\n  geom_sf(data = swiss_cantons, fill = NA) +\n  geom_sf(data = swiss_lakes,  fill = \"#d1eeea\", color = \"#d1eeea\") +\n  theme_void()\n\n\n\n\nHoe kunnen we nu elk kanton kleuren naar de grootte van de COVID-19 incidentie per 100’000 mensen?\nWe hoeven alleen maar de covid_incidence tabel en de swiss_cantons tabel samen te voegen, met de kantoncode als verbindingsvariabele. Hiermee kunnen we de variabele incidentie in kaart brengen naar de fill esthetiek en ereen choropleth kaart van maken, d.w.z. een thematische kaart.\n\nswiss_cantons &lt;- swiss_cantons %&gt;% \n  left_join(covid_incidence, c(\"KURZ\" = \"canton\"))\n\nOm onze kaart er mooi te laten uitzien, verdelen wij, in plaats van een numerieke variabele te gebruiken, de incidentie in categorieën. Zo is het voor de gebruiker gemakkelijker te zien in welke categorie elk kanton valt.\nDit is een typische praktijk voor choropleth kaarten en het kan op verschillende manieren worden gedaan. In dit geval kiezen we voor een brute kracht aanpak, we doen het handmatig.\n\nswiss_cantons &lt;- swiss_cantons %&gt;% \n  mutate(incidence_cat = case_when(\n    incidence &lt;= 50 ~ \"0-50\",\n    incidence &lt;= 100 ~ \"51-100\",\n    incidence &lt;= 150 ~ \"101-150\",\n    incidence &lt;= 300 ~ \"251-300\"\n  )) %&gt;% \n  mutate(incidence_cat = factor(incidence_cat, levels = c(\"0-50\", \"51-100\",\"101-150\",\"151-200\",\"251-300\")))\n\nNu kunnen we de kleur toewijzen aan de incidence_cat variabele en de eerste choropleth kaart maken.\n\nggplot(swiss_cantons) +\n  geom_sf(aes(fill = incidence_cat), size = 0.3) +\n  scale_fill_carto_d(palette = \"BrwnYl\") +\n  geom_sf(data = swiss_lakes, fill = \"#d1eeea\", color = \"#d1eeea\")+\n  theme_void() +\n  theme(legend.title = element_blank(),\n        legend.position = \"bottom\") \n\n\n\n\nEn we hebben onze eerste choropleth kaart, gebouwd met alleen {sf} en {ggplot2}. Laten we nog wat puntjes op de i zetten: we zijn niet tevreden met hoe de legende eruit ziet en we kunnen die veranderen met guide_legend().\n\nggplot(swiss_cantons) +\n  geom_sf(aes(fill = incidence_cat), size = 0.3) +\n  scale_fill_carto_d(palette = \"BrwnYl\",\n                     guide = guide_legend(direction = \"horizontal\",\n            keyheight = unit(2, units = \"mm\"),\n            keywidth = unit(70 / 5, units = \"mm\"),\n            title.position = 'top',\n            title.hjust = 0.5,\n            label.hjust = 0.5,\n            nrow = 1,\n            byrow = T,\n            label.position = \"bottom\")) +\n  geom_sf(data = swiss_lakes, fill = \"#d1eeea\", color = \"#d1eeea\")+\n  theme_void() +\n  theme(legend.title = element_blank(),\n        legend.position = \"bottom\") \n\n\n\n\nNu kunnen we een titel, een ondertitel en labels toevoegen aan de bovenkant van elke kanton. We zullen {ggrepel} gebruiken om ervoor te zorgen dat de labels elkaar niet overlappen, we zullen ook {ggtext} gebruiken zodat we markdown syntax kunnen gebruiken voor onze titel en ondertitel.\n\nggplot(swiss_cantons) +\n  geom_sf(aes(fill = incidence_cat), size = 0.3) +\n  scale_fill_carto_d(palette = \"BrwnYl\",\n                     guide = guide_legend(direction = \"horizontal\",\n            keyheight = unit(2, units = \"mm\"),\n            keywidth = unit(70 / 5, units = \"mm\"),\n            title.position = 'top',\n            title.hjust = 0.5,\n            label.hjust = 0.5,\n            nrow = 1,\n            byrow = T,\n            label.position = \"bottom\")) +\n  geom_sf(data = swiss_lakes, fill = \"#d1eeea\", color = \"#d1eeea\")+\n  ggrepel::geom_label_repel(\n    data = swiss_cantons,\n    aes(label = paste0(KURZ,\":\",round(incidence, digits = 0)), \n        geometry = geometry),\n    stat = \"sf_coordinates\",\n    min.segment.length = 0.2,\n    colour = \"#541f3f\",\n    size = 3,\n    segment.alpha = 0.5\n  ) +\n  labs(title = \"&lt;b style='color:#541f3f'&gt; COVID-19 gevallen per kanton, laatste 14 dagen &lt;/b&gt;\",\n       subtitle = \"&lt;span style='font-size:10pt'&gt;Incidentie per 100'000 inwoners,per canton &lt;/span&gt;\",\n       caption = \"Bron: OFSP | updated 12.10.2020\") +\n  theme_void() +\n  theme(legend.title = element_blank(),\n        legend.position = \"bottom\",\n        plot.title = ggtext::element_markdown(),\n        plot.subtitle = ggtext::element_markdown()) \n\n\n\n\nWe hebben nu de code om een choropleth kaart te maken en we hebben gezien hoe we die stap voor stap kunnen bouwen met {ggplot2}. Met een beetje maatwerk hebben we nu een statische kaart die we in een ander formaat opslaan en delen.\nAls je geïnteresseerd bent in het omgaan met geografische gegevens, is een van de beste vrij beschikbare bronnen het Geocomputation with R-boek. De auteurs van het boek maken veel gebruik van verschillende pakketten voor het plotten van thematische kaarten, met name {tmap}, dat ook de moeite waard is om te onderzoeken. Als u pakketten zoals {ggtext} wilt gebruiken om uw plots aan te passen, is {ggplot2} de bibliotheek waarop u wilt vertrouwen, vooral als u al gewend bent om ermee te werken.\nIk hoop dat je dit artikel met plezier las en blijf op de hoogte van meer voorbeelden over hoe kaarten in R zijn te maken."
  },
  {
    "objectID": "posts/2021-10-01-rmarkdown-en-officedown/rmarkdown-en-officedown.html",
    "href": "posts/2021-10-01-rmarkdown-en-officedown/rmarkdown-en-officedown.html",
    "title": "Rmarkdown en Officedown",
    "section": "",
    "text": "Onlangs kwam ik op R-bloggers een blog tegen waar mijn oog op viel. Hierin werden enkele mogelijkheden van rmarkdown en officedown met elkaar vergeleken. Ik dacht ik zet deze blog eens over als manier om voor en nadelen van beide pakketten te vergelijken. Dank je wel, Jakub.\nAccuraatheid, betrouwbaarheid en zichtbaarheid van data staat centraal in het farmaceutische kwaliteitssysteem. Daarvoor zijn er allerlei richlijnen die weer onder controle staan van FDA in Amerika en EMA in Europa.\nGoede documentatie hoort daarbij. De eisen die daaraan gesteld worden richten zich op 3 belangrijke gebieden van naleving:\nTraceerbaarheid: de mogelijkheid om de hele levenscyclus van het product te traceren, compleet met alle veranderingen. Verantwoordingsplicht:de mogelijkheid om alle personen die bijdragen te identificeren en vast te kunnen stellen wanneer belangrijke wijzigingen zijn doorgevoerd. Gegevensintegriteit:de betrouwbaarheid van de door het systeem gegenereerde gegevens.\nVolgens de richtlijnen voor kwaliteit moet elk verslag niet alleen de resultaten en conclusies bevatten, maar ook alle gegevens en gegevensbronnen die zijn gebruikt om ze op te stellen. Daarom is rapportage erg foutgevoelig, vooral als documentatie elke keer met de hand wordt gemaakt op een copy-paste manier. Bovendien kost handmatige rapportage veel tijd en moeite. De invoering van een documentbeheersysteem en programmatisch opgestelde rapporten kunnen deze problemen beperken."
  },
  {
    "objectID": "posts/2021-10-01-rmarkdown-en-officedown/rmarkdown-en-officedown.html#nauwkeurige-controle-van-de-documentstructuur",
    "href": "posts/2021-10-01-rmarkdown-en-officedown/rmarkdown-en-officedown.html#nauwkeurige-controle-van-de-documentstructuur",
    "title": "Rmarkdown en Officedown",
    "section": "Nauwkeurige controle van de documentstructuur",
    "text": "Nauwkeurige controle van de documentstructuur\nVanuit een kwaltiteitsperspectief kan het van cruciaal belang zijn dat de documentatie een specifieke volgorde van secties heeft. Met RMarkdown kun je een inhoudsopgave toevoegen, maar je hebt weinig controle over waar die verschijnt. Met pagedown kun je niet alleen een TOC toevoegen, maar ook een lijst van tabellen en een lijst van figuren. Het is echter moeilijk om de volgorde van deze secties te veranderen en het is niet mogelijk om het rapport te exporteren als een Word Document.\nDit is waar officedown om de hoek komt kijken, het geeft je veel meer flexibiliteit. Het helpt je om secties van het rapport precies te plaatsen waar je ze wilt in het document door gebruik te maken van HTML commentaar tags."
  },
  {
    "objectID": "posts/2021-10-01-rmarkdown-en-officedown/rmarkdown-en-officedown.html#stijl-van-tekst-en-tabellen",
    "href": "posts/2021-10-01-rmarkdown-en-officedown/rmarkdown-en-officedown.html#stijl-van-tekst-en-tabellen",
    "title": "Rmarkdown en Officedown",
    "section": "Stijl van tekst en tabellen",
    "text": "Stijl van tekst en tabellen\nHet maken van bewerkbare documenten in een onderzoeksomgeving kan extra opmaakopties vereisen. U wilt bijvoorbeeld belangrijke gegevens markeren, of dat nu tekst is of cellen in tabellen. Terwijl de opmaak van tabellen met verschillende pakketten kan worden gedaan, is de opmaak van tekstparagrafen alleen mogelijk met officedown. Door een eenvoudige variabele toe te voegen, kunt u ook de opmaak automatiseren, bijv. de oplichtingskleur in cellen alleen veranderen als de waarde lager of hoger is dan de referentiewaarde.\n\n\n\nTekst en tabellen"
  },
  {
    "objectID": "posts/2021-10-01-rmarkdown-en-officedown/rmarkdown-en-officedown.html#stijldocumenten-als-templates",
    "href": "posts/2021-10-01-rmarkdown-en-officedown/rmarkdown-en-officedown.html#stijldocumenten-als-templates",
    "title": "Rmarkdown en Officedown",
    "section": "Stijldocumenten als templates",
    "text": "Stijldocumenten als templates\nMet officedown is het mogelijk om stijlen toe te passen die bekend zijn van Microsoft Word. In plaats van eigen CSS-styl te gebruiken, kun je een template .docx bestand maken met daarin gestylde elementen, dat vervolgens wordt doorgegeven aan de YAML header van het RMarkdown bestand. Dezelfde stijlen zullen worden toegepast op onderdelen in het resulterende .docx bestand. Op deze manier zal de aangepaste opmaak die u nodig hebt voor uw GxP-rapporten of -documentatie automatisch worden opgenomen in alle nieuw gegenereerde bestanden.\n\n\n\nTemplates gebruiken"
  },
  {
    "objectID": "posts/2021-10-01-rmarkdown-en-officedown/rmarkdown-en-officedown.html#meerdere-documenten-combineren",
    "href": "posts/2021-10-01-rmarkdown-en-officedown/rmarkdown-en-officedown.html#meerdere-documenten-combineren",
    "title": "Rmarkdown en Officedown",
    "section": "Meerdere documenten combineren",
    "text": "Meerdere documenten combineren\nWat als u liever een paar kleinere documenten of rapporten maakt en ze later samenvoegt in één bestand? Met officedown kunt u gemakkelijk meerdere documenten samenvoegen tot één. Bovendien blijven alle referenties behouden, terwijl de nummering van secties, figuren en tabellen automatisch wordt bijgewerkt en in een nieuwe inhoudsopgave wordt opgenomen.\n\n\n\nCombineren van documenten"
  },
  {
    "objectID": "posts/2021-10-01-rmarkdown-en-officedown/rmarkdown-en-officedown.html#pagina-in-landschap",
    "href": "posts/2021-10-01-rmarkdown-en-officedown/rmarkdown-en-officedown.html#pagina-in-landschap",
    "title": "Rmarkdown en Officedown",
    "section": "Pagina in landschap",
    "text": "Pagina in landschap\nHet gebruik van officedown geeft je ook meer flexibiliteit in termen van het veranderen van pagina oriëntatie voor geselecteerde delen van het rapport. Met een enkele opmerking in de code, kunt u de liggende modus toepassen op elk blok in het document. Deze eenvoudige functionaliteit komt zeer goed van pas wanneer u brede tabellen of grotere figuren moet opnemen.\n\n\n\nLandschapbreed"
  },
  {
    "objectID": "posts/2021-10-01-rmarkdown-en-officedown/rmarkdown-en-officedown.html#officedown-voor-betere-documentatiemanagement",
    "href": "posts/2021-10-01-rmarkdown-en-officedown/rmarkdown-en-officedown.html#officedown-voor-betere-documentatiemanagement",
    "title": "Rmarkdown en Officedown",
    "section": "officedown voor betere documentatiemanagement",
    "text": "officedown voor betere documentatiemanagement\nMet het naleven van de kwaliteitseisen bereikt het automatiseren van het creatieproces en bijwerken van documenten een nieuw niveau in de farmaceutische industrie. Waar handmatige processen en andere pakketten tekort schieten, biedt officedown een levensvatbare oplossing. Het is een eenvoudige en effectieve manier om de functies van Microsoft Word naar R Markdown te brengen en aangepaste opmaakopties te ontsluiten.\nLaten we eens samenvatten hoe officedown u kan helpen te voldoen aan de kwaliteitseisen:\n\nmeer controle over document lay-out en structuur\n\nrobuuste opmaak van tekst en tabellen\n\nglobale toepassing van aangepaste stijlen in rapporten\n\nhandige compilatie van meerdere documenten\n\nDeze eigenschappen van officedown vertalen zich in een sterker kwaliteitssysteem, beter risico- en wijzigingsbeheer en nauwkeuriger versiebeheer. Het is een win-win situatie voor zowel uw R&D team als alle mensen die vertrouwen op de kwaliteit van uw medische product om te herstellen of een gezonder leven te leiden."
  },
  {
    "objectID": "posts/2020-11-17-testen-met-bayes/testen-met-bayes.html",
    "href": "posts/2020-11-17-testen-met-bayes/testen-met-bayes.html",
    "title": "Testen met Bayes",
    "section": "",
    "text": "De laatste weken lees ik weer regelmatig over de achtergronden, de principes en de voordelen van bayesiaanse onderzoekstechnieken. De update van Statistical Rethinking. A Bayesian Course with Examples in R and Stan (McElreath, 2020) en het nieuwe boek Regression and other stories (Gelman, Hill & Vehtari, 2020) geven veel inspiratie. Daarover later meer. Ondertussen verscheen vorig jaar het R-pakket bayestestR met een hele duidelijke bijbehorende website waarin een aantal uitgangspunten heel duidelijk worden uitgelegd en de voordelen van deze manier van onderzoek doen worden vergeleken met de klassieke onderzoekstechniek. Ik kon het niet laten om een aantal lessen te vertalen. Mogelijk dat ik hier later nog een keer aandacht aan besteed. De website is gebaseerd op twee artikelen waar de wetenschappers naar refereren. Natuurlijk moet ik deze artikelen hier aan het begin noemen.\nMakowski, D., Ben-Shachar, M. S., & Lüdecke, D. (2019). bayestestR: Describing Effects and their Uncertainty, Existence and Significance within the Bayesian Framework. Journal of Open Source Software, 4(40), 1541. 10.21105/joss.01541\nMakowski, D., Ben-Shachar, M. S., Chen, S. H. A., & Lüdecke, D. (2019). Indices of Effect Existence and Significance in the Bayesian Framework. Frontiers in Psychology 2019;10:2767. 10.3389/fpsyg.2019.02767\n\n\nHet Bayesiaanse statistische raamwerk wint snel aan populariteit onder wetenschappers, wat samenhangt met de algemene verschuiving naar open en eerlijke wetenschap. Redenen om de voorkeur te geven aan deze aanpak zijn betrouwbaarheid, nauwkeurigheid (in rommelige data en kleine steekproeven), de mogelijkheid om prior kennis in de analyse te introduceren en, kritisch gezien, de intuïtiviteit van de resultaten en hun rechtstreekse interpretatie (Andrews & Baguley, 2013; Etz & Vandekerckhove, 2016; Kruschke, 2010; Kruschke, Aguinis, & Joo, 2012; Wagenmakers et al., 2018).\nIn het algemeen wordt de frequentistische aanpak geassocieerd met de focus op null hypothesetests en het misbruik van p-waarden blijkt kritisch bij te dragen aan de reproduceerbaarheidscrisis van psychologische wetenschap (Chambers, Feredoes, Muthukumaraswamy, & Etchells, 2014; Szucs & Ioannidis, 2016). Men is het er algemeen over eens dat de veralgemening van de Bayesiaanse aanpak een manier is om deze problemen te overwinnen (Benjamin et al., 2018; Etz & Vandekerckhove, 2016).\nAls we het er eenmaal over eens zijn dat het Bayesiaanse raamwerk de juiste weg is, kun je je vervolgens afvragen wat het Bayesiaanse raamwerk is.\nWaar gaat al dat gedoe over?\n\n\n\nHet aannemen van het Bayesiaanse raamwerk is meer een verschuiving in paradigma dan een verandering in methodologie. Inderdaad, alle gemeenschappelijke statistische procedures (t-tests, correlaties, ANOVA’s, regressies, …) kunnen nog steeds worden uitgevoerd met behulp van het Bayesiaanse raamwerk. Een van de kernverschillen is dat in het frequentische perspectief (de “klassieke” statistiek, met p- en t-waarden, evenals met die rare vrijheidsgraden), de effecten vastliggen (maar onbekend zijn) en data random zijn. Aan de andere kant wordt in het Bayesiaanse inferentieproces, in plaats van schattingen van het “ware effect”, de waarschijnlijkheid van verschillende effecten berekend gegeven de waargenomen gegevens. Dat resulteert in een verdeling van mogelijke waarden voor de parameters, de zogenaamde posterior-distributie.\nDe onzekerheid in de Bayesiaanse inferentie kan bijvoorbeeld worden samengevat door de mediaan van de verdeling, evenals een reeks waarden van de posterior distributie die de 95% meest waarschijnlijke waarden omvat (het 95% waarschijnlijke interval). Deze kunnen worden beschouwd als de tegenhangers van de punt-schatting en het betrouwbaarheidsinterval in een frequentistisch kader. Om het verschil in interpretatie te illustreren, laat het Bayesiaanse raamwerk toe om te zeggen “gezien de geobserveerde gegevens, heeft het effect een 95% kans om binnen dit bereik te vallen”. Het minder eenvoudige alternatief voor de frequentist zou zijn “wanneer herhaaldelijk betrouwbaarheidsintervallen uit deze reeks gegevens worden berekend, is er een 95% kans dat het effect binnen een bepaald bereik valt”. In wezen geven de Bayesiaanse samplingsalgoritmen (met MCMC-technieken) een waarschijnlijkheidsverdeling (de posterior) van een effect dat compatibel is met de waargenomen gegevens. Zo kan een effect worden beschreven door de posterior verdeling te karakteriseren in relatie tot de centraliteit (punt-schattingen), en gaat het over onzekerheid en het bestaan en de betekenis ervan.\nMet andere woorden, als we de ingewikkelde wiskunde achterwege laten, kunnen we zeggen dat:\n\nDe frequentist probeert “het reële effect” in te schatten, bijvoorbeeld, de “echte” waarde van de correlatie tussen x en y. Vandaar dat de modellen van frequentisten een “punt-schatting” opleveren. (d.w.z. één enkele waarde) van de “echte” correlatie (bv. r = 0,42) die wordt geschat op basis van een aantal onduidelijke veronderstellingen (minimaal, aangezien de gegevens willekeurig worden onttrokken van een “ouder”, meestal een normale verdeling).\n\nDe Bayesiaan gaat niet van zoiets uit. De gegevens zijn wat ze zijn. Op basis van deze geobserveerde gegevens (en een eerdere overtuiging over het resultaat) geeft het Bayesiaanse samplingsalgoritme (soms ook wel MCMC sampling genoemd) een waarschijnlijkheidsverdeling (de zogenaamde posterior) van het effect dat compatibel is met de geobserveerde gegevens. Voor de correlatie tussen x en y geeft het een verdeling, die bijvoorbeeld zegt: “het meest waarschijnlijke effect is 0,42, maar deze gegevens zijn ook compatibel met correlaties tussen 0,12 en 0,74”.\n\nOm onze effecten te karakteriseren is geen behoefte aan p-waarden of andere cryptische indices. We beschrijven gewoon de posterior verdeling van het effect. We kunnen bijvoorbeeld de mediaan, de 89% Credible Interval of andere indices rapporteren.\n\nMet andere woorden, als we de wiskunde even achterwege laten, kunnen we zeggen dat:\n\nHoewel het doel van dit pakket is het gebruik van Bayesiaanse statistieken te verdedigen, zijn er serieuze argumenten die de frequentie-indexen ondersteunen (zie bijvoorbeeld hier). Zoals altijd is de wereld niet zwart-wit (p < .001).\n\nNou… hoe werkt het?\n\n\n\n\n\nU kunt bayestestR samen met de hele easystats suite installeren (of alleen bayestestR, omdat de suite installeren bij mij niet werkte) door het volgende uit te voeren: ## A simple example\n\n\nWarning: package 'bayestestR' was built under R version 4.1.3\n\n\nLaten we ook het pakket rstanarm installeren en laden, die het mogelijk maakt om de Bayesiaanse modellen, evenals de bayestestR, te werken.\n\n\nWarning: package 'rstanarm' was built under R version 4.1.3\n\n\nLoading required package: Rcpp\n\n\nWarning: package 'Rcpp' was built under R version 4.1.3\n\n\nThis is rstanarm version 2.21.3\n\n\n- See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n\n\n- Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n\n\n- For execution on a local, multicore CPU with excess RAM we recommend calling\n\n\n  options(mc.cores = parallel::detectCores())\n\n\n\n\n\nLaten we beginnen met een eenvoudige frequentistische lineaire regressie (de lm() functie staat voor lineair model) tussen twee numerieke variabelen, Sepal.Length en Petal.Length uit de beroemde iris-dataset, standaard opgenomen in R.\n\n\n\nCall:\nlm(formula = Sepal.Length ~ Petal.Length, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.24675 -0.29657 -0.01515  0.27676  1.00269 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   4.30660    0.07839   54.94   <2e-16 ***\nPetal.Length  0.40892    0.01889   21.65   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4071 on 148 degrees of freedom\nMultiple R-squared:   0.76, Adjusted R-squared:  0.7583 \nF-statistic: 468.6 on 1 and 148 DF,  p-value: < 2.2e-16\n\n\nDeze analyse laat een significante (wat dat ook moge betekenen) en een positieve (met een coëfficiënt van 0,41) lineaire relatie zien tussen de twee variabelen.\nHet aanpassen en interpreteren van frequentiemodellen is zo eenvoudig dat het duidelijk is dat mensen het gebruiken in plaats van het Bayesiaanse kader… toch?\nNiet meer.\n\n\n\n\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.03 seconds (Warm-up)\nChain 1:                0.044 seconds (Sampling)\nChain 1:                0.074 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.033 seconds (Warm-up)\nChain 2:                0.039 seconds (Sampling)\nChain 2:                0.072 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.031 seconds (Warm-up)\nChain 3:                0.038 seconds (Sampling)\nChain 3:                0.069 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.03 seconds (Warm-up)\nChain 4:                0.04 seconds (Sampling)\nChain 4:                0.07 seconds (Total)\nChain 4: \n\n\nSummary of Posterior Distribution\n\nParameter    | Median |       95% CI |   pd |          ROPE | % in ROPE |  Rhat |     ESS\n-----------------------------------------------------------------------------------------\n(Intercept)  |   4.31 | [4.15, 4.46] | 100% | [-0.08, 0.08] |        0% | 1.000 | 3898.00\nPetal.Length |   0.41 | [0.37, 0.45] | 100% | [-0.08, 0.08] |        0% | 1.000 | 3663.00\n\n\nDat is het! Je hebt een Bayesiaanse versie van het model gedraaid door eenvoudigweg stan_glm() te gebruiken in plaats van lm() en hebt de posterior distributie van de parameters beschreven. De conclusie die we kunnen trekken, voor dit voorbeeld, zijn zeer vergelijkbaar. Het effect (de mediaan van de posterior verdeling van het effect) is ongeveer 0,41, en het kan ook als significant worden beschouwd in de Bayesiaanse zin (meer daarover later).\nDus, klaar om meer te leren?\n\n\n\n\nNu je de beginsectie hebt gelezen, laten we een duik nemen in de subtiliteiten van Bayesiaanse modellering met behulp van R.\n\n\nAls je de benodigde pakketten hebt geïnstalleerd, kun je rstanarm laden (om de modellen te draaien) en ook bayestestR (om bruikbare indices te berekenen) en insight (om toegang te krijgen tot de parameters).\n\n\nWarning: package 'insight' was built under R version 4.1.3\n\n\n\n\n\nWe beginnen met het uitvoeren van een eenvoudige lineaire regressie om het verband tussen Petal.Length (onze voorspeller, of onafhankelijke, variabele) en Sepal.Length (onze respons-, of afhankelijke-variabele) te testen vanuit de irisdataset die standaard is opgenomen in R.\n\n\n\nLaten we beginnen met het draaien van de frequentistische versie van het model, gewoon om een referentiepunt te hebben:\n\n\n\nCall:\nlm(formula = Sepal.Length ~ Petal.Length, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.24675 -0.29657 -0.01515  0.27676  1.00269 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   4.30660    0.07839   54.94   <2e-16 ***\nPetal.Length  0.40892    0.01889   21.65   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4071 on 148 degrees of freedom\nMultiple R-squared:   0.76, Adjusted R-squared:  0.7583 \nF-statistic: 468.6 on 1 and 148 DF,  p-value: < 2.2e-16\n\n\nIn dit model is de lineaire relatie tussen Petal.Length en Sepal.Length positief en significant (beta = 0,41, t(148) = 21,6, p < .001). Dit betekent dat je voor elke toename van Petal.Length (de voorspeller) met één eenheid kunt verwachten dat de Sepal.Length (het antwoord) met 0,41 zal toenemen. Dit effect kan worden gevisualiseerd door de voorspellingswaarden op de x-as en de responswaarden als y te plotten met behulp van het ggplot2 pakket:\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nLaten we nu een Bayesiaanse versie van het model draaien door gebruik te maken van de stan_glm-functie dat in het rstanarmpakket zit:\n\n\n\nJe ziet dat het samplingsalgoritme draait.\n\n\n\nLaten we, als het bovenstaande eenmaal gedaan is, de parameters (d.w.z. de coëfficiënten) van het model extraheren.\n\n\n  (Intercept) Petal.Length\n1    4.359494    0.3961450\n2    4.273491    0.4224131\n3    4.226132    0.4292590\n4    4.314438    0.4130788\n5    4.240116    0.4387224\n6    4.252441    0.4297079\n\n\nZoals we kunnen zien, hebben de parameters de vorm van een lange dataframe met twee kolommen, die overeenkomen met de intercept en het effect van Petal.Length. Deze kolommen bevatten de posterior distributies van deze twee parameters. Eenvoudig gezegd is de posterior distributie een set van verschillende plausibele waarden voor elke parameter.\n\n\nLaten we eerst eens kijken naar de lengtes van de posteriors.\n\n\n[1] 4000\n\n\n\nWaarom zijn dit er 4000, en niet meer of minder?\n\nIn de eerste plaats worden deze waarnemingen (de rijen) meestal aangeduid als posterior ‘draws’ (trekkingen). De achterliggende gedachte is dat het Bayesiaanse samplingsalgoritme (b.v. Monte Carlo Markov Chains - MCMC) zal putten uit de verborgen ware posterior distributie Het is dus door middel van deze ‘posterior draws’ dat we de onderliggende ware posterior distribution kunnen inschatten. Hoe meer trekkingen je hebt, hoe beter je de posterior distriubtion kunt inschatten. Meer trekkingen betekent echter ook een langere rekentijd.\nAls we kijken naar de documentatie (?sampling) voor het rstanarm“sampling”-algoritme dat standaard in het bovenstaande model wordt gebruikt, kunnen we verschillende parameters zien die het aantal posterior draws beïnvloeden. Standaard zijn er 4 ketens (je kunt het zien als aparte sampling runs), die elk 2000 iter (trekkingen, iteraties) aanmaken. Echter, slechts de helft van deze iteraties wordt behouden, aangezien de helft wordt gebruikt voor de opwarming (het convergeren van het algoritme). Het totaal is dus 4 ketens * (2000 iteraties - 1000 warming-up) = 4000 posterior trekkingen. Dat kunnen we aanpassen naar 2 ketens, bijvoorbeeld:\n\n\n\nIn dit geval hebben we, zoals verwacht, 2 ketens * (1000 iteraties - 250 warming-up) = 1500 posterior trekkingen. Maar laten we ons eerste model de standaard instelling aanhouden (omdat het meer trekkingen heeft).\n\n\n\nNu we hebben begrepen waar deze waarden vandaan komen, laten we er eens naar kijken. We zullen beginnen met het visualiseren van de posterieure distributie van de parameter waarin we geïnteresseerd zijn, het effect van Petal.Length.\n\n\n\n\n\nDeze verdeling vertegenwoordigt de waarschijnlijkheid (de y-as) van verschillende effecten (de x-as). De centrale waarden zijn waarschijnlijker dan de extreme waarden. Zoals u ziet varieert deze verdeling van ongeveer 0,35 tot 0,50, waarbij het grootste deel rond 0,41 ligt.\n\nGefeliciteerd! Je hebt zojuist je posterior distribution beschreven.\n\nEn dit is het hart van de Bayesiaanse analyse. We hebben geen p-waarden, t-waarden of vrijheidsgraden nodig: Alles is aanwezig, binnen deze posterior verdeling.\nOnze beschrijving hierboven is consistent met de waarden verkregen uit de frequentistische regressie (die resulteerde in een bèta van 0,41). Dit is geruststellend! Inderdaad, in de meeste gevallen verandert een Bayesiaanse analyse de resultaten niet drastisch of hun interpretatie. Het maakt de resultaten wel beter interpreteerbaar en intuïtief en uiteindelijk gemakkelijker te begrijpen en te beschrijven.\nWe kunnen nu doorgaan en deze posterior verdeling nauwkeurig karakteriseren.\n\n\n\n\nHelaas, het is vaak niet praktisch om de hele posterior verdelingen als grafiek te rapporteren. We moeten een beknopte manier vinden om het samen te vatten. We raden aan om de posterior verdeling te beschrijven op basis van 3 elementen:\n\nEen puntschatting die een samenvatting is van één waarde (vergelijkbaar met de bèta in frequente regressies).\n\nEen credible interval die de bijbehorende onzekerheid weergeeft.\n\nSommige indices van betekenis, die informatie geven over het relatieve belang van dit effect.\n\n\n\nWelke ene waarde kan het beste mijn posterior distributie representeren?\nCentrale indices, zoals het gemiddelde, de mediaan of de modus worden meestal gebruikt als puntschatting - maar wat is het verschil tussen het frequentische en Bayesiaanse raamwerk? Laten we dit beantwoorden door eerst het gemiddelde te inspecteren:\n\n\n[1] 0.4089547\n\n\nDit ligt dicht bij de frequentistische beta. Maar zoals we weten, is het gemiddelde vrij gevoelig voor uitschieters of extremen. Misschien is de mediaan robuuster?\n\n\n[1] 0.4086475\n\n\nNou, dit ligt zeer dicht bij het gemiddelde (en identiek als de waarden worden afgerond). Misschien kunnen we de modus nemen, dat wil zeggen, de piek van de posterior verdeling? In het Bayesiaanse kader wordt deze waarde de Maximum A Posteriori (MAP) genoemd. Laten we daar eens kijken:\n\n\nMAP Estimate: 0.41\n\n\nZe zitten allemaal heel dichtbij elkaar! Laten we deze drie waarden visualiseren op de posterior distributie:\n\n\n\n\n\nNou, al deze waarden geven zeer gelijkaardige resultaten. We zullen de mediaan kiezen, omdat deze waarde een directe betekenis heeft vanuit een probabilistisch perspectief: er is 50% kans dat het werkelijke effect hoger is en 50% kans dat het effect lager is (omdat het de verdeling in twee gelijke delen verdeelt).\n\n\n\nNu we een puntschatting hebben, moeten we de onzekerheid beschrijven. We zouden het bereik kunnen berekenen:\n\n\n[1] 0.3326759 0.4725683\n\n\nMaar heeft het zin om al deze extreme waarden op te nemen? Waarschijnlijk niet. Dus, we zullen een credible interval berekenen. Lang verhaal kort, het lijkt een beetje op een frequentistische confidence interval, maar is makkelijker te interpreteren en gemakkelijker te berekenen - en het is logischer.\nWe zullen dit credible interval berekenen op basis van het Highest Density Interval (HDI). Het geeft ons het bereik dat de 89% meest waarschijnlijke effectwaarden bevat. We zullen 89% CIs gebruiken in plaats van 95% CIs (zoals in het frequentistische kader), omdat het 89%-niveau stabielere resultaten geeft (Kruschke, 2014) en ons herinnert aan de willekeur van dergelijke conventies (McElreath, 2020).\n\n\n89% HDI: [0.38, 0.44]\n\n\nMooi, dus we kunnen concluderen dat het effect 89% kans heeft om binnen het [0,38, 0,44] bereik te vallen. We hebben zojuist de twee belangrijkste stukken informatie berekend om onze effecten te beschrijven.\n\n\n\n\nOp veel wetenschappelijke gebieden is het echter niet voldoende om alleen de effecten te beschrijven. Wetenschappers willen ook weten of dit effect betekenis heeft in praktische of statistische termen. Of, om het met andere woorden te zeggen, of het effect belangrijk is. Wijkt het effect af van 0? Dus hoe berekenen we de significantie van een effect. Hoe kunnen we dit doen?\nWel, in dit specifieke geval is het zeer welsprekend: Alle mogelijke effectwaarden (d.w.z. de hele posterior distributie) zijn positief en meer dan 0,35, wat al een substantieel bewijs is dat het effect niet nul is.\nMaar toch willen we een objectief beslissingscriterium, om te zeggen of het effect ja of nee ‘significant’ is. Een benadering, vergelijkbaar met het frequentistisch kader, zou zijn om te kijken of het Credible Interval een 0 bevat. Als dat niet het geval is, zou dat betekenen dat ons effect ‘significant’ is.\nMaar deze index is toch niet erg fijnmazig? Kunnen we het beter doen? Ja.\n\n\n\nStel je voor dat je geïnteresseerd bent in hoe het gewicht van de kippen varieert, afhankelijk van twee verschillende voedersoorten. Voor dit examen zullen we beginnen met het selecteren van twee voor ons interessante voersoorten uit de chickwts-dataset (zit ook in basis R) (we hebben wel bijzondere interesses): vleesmaaltijden (‘meat meals’) en zonnebloemen (‘sunflowers’).\n\n\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nLaten we nog een Bayesiaanse regressie uitvoeren om het gewicht te voorspellen met de twee voertypesoorten.\n\n\n\n\n\n\n\n\n\n\n\nDit representeert de posterior distributie van het verschil tussen ‘meatmeal’ (‘0’) en ‘sunflowers’(‘1’). Het lijkt erop dat het verschil eerder positief is (de waarden lijken geconcentreerd aan de rechterkant van 0). Het eten van zonnebloemen maakt je dikker (tenminste, als je een kip bent). Maar, door hoeveel?  Laten we de mediaan en de CI berekenen:\n\n\n[1] 52.68758\n\n\n\n\n95% HDI: [3.07, 99.60]\n\n\nHet maakt je met ongeveer 51 gram (de mediaan) dikker. De onzekerheid is echter vrij groot: er is 89% kans dat het verschil tussen de twee voersoorten tussen 14 en 91 ligt.\n\nVerschilt dit effect van 0?\n\n\n\n\nTesten of deze verdeling anders is dan 0 heeft geen zin, omdat 0 een enkele waarde is (en de kans dat een verdeling anders is dan een enkele waarde is oneindig).\nEen manier om significantie te beoordelen kan echter zijn om een gebied rond 0 te definiëren, wat als praktisch equivalent van nul zal worden beschouwd (d.w.z. afwezigheid van, of verwaarloosbaar, effect). Dit wordt de ‘Region of Practical Equivalence’ (ROPE) genoemd en is een manier om de betekenis van de parameters te testen.\nHoe definiëren we dit gebied?\n\nTringgg Tringgg\n\n– U spreekt met het easystatsteam. Hoe kunnen we u helpen?\n– Ja met Prof. Sanders. Ik ben kippenexpert. Ik bel u vanwege mijn expertkennis. Een effect tussen -20 en 20 is verwaarloosbaar. Tot ziens.\nNou, dat komt goed uit. Nu weten we dat we de ROPE kunnen definiëren als het [-20, 20] bereik. Alle effecten binnen dit bereik worden als nihil (te verwaarlozen) beschouwd. We kunnen nu het aandeel van de 89% meest waarschijnlijke waarden (de 89% CI) berekenen die niet nul zijn, d.w.z., die buiten dit bereik liggen.\n\n\n# Proportion of samples inside the ROPE [-20.00, 20.00]:\n\ninside ROPE\n-----------\n4.21 %     \n\n\n5% van de 89% CI kan als nihil worden beschouwd. Is dat veel? Gebaseerd op onze richtlijnen, ja, het is te veel. Op basis van deze specifieke definitie van ROPE concluderen we dat dit effect niet significant is (de kans dat het verwaarloosbaar is, is te groot).\nHoewel, om eerlijk te zijn, heb ik een aantal twijfels over deze Prof. Sanders. Ik vertrouw zijn definitie van ROPE niet echt. Is er een meer objectieve manier om het te definiëren?\nJa. Een betrouwbare manier is bijvoorbeeld het gebruik van een tiende (1/10 = 0,1) van de standaardafwijking (SD) van de responsvariabele, die als een “verwaarloosbare” effectomvang kan worden beschouwd (Cohen, 1988).\n\n\n[1] -6.17469  6.17469\n\n\nLaten we onze ROPE opnieuw definiëren als de regio binnen het [-6.2, 6.2] bereik. Merk op dat dit direct kan worden verkregen met de rope_range functie :)\n\n\n[1] -6.17469  6.17469\n\n\nLaten we nu het percentage in ROPE opnieuw berekenen:\n\n\n# Proportion of samples inside the ROPE [-6.17, 6.17]:\n\ninside ROPE\n-----------\n0.00 %     \n\n\nMet deze redelijke definitie van ROPE stellen we vast dat de 89% van de posterior distributie van het effect niet overlapt met de ROPE. We kunnen dus concluderen dat het effect significant is (in de zin van belangrijk genoeg om op te merken).\n\n\n\nMisschien zijn we niet geïnteresseerd in de vraag of het effect niet te verwaarlozen is. Misschien willen we alleen weten of dit effect positief of negatief is. In dit geval kunnen we eenvoudigweg berekenen welk deel van de posterior distributie positief is, ongeacht de “grootte” van het effect.\n\n\n[1] 98.15\n\n\nWe kunnen concluderen dat het effect positief is met een waarschijnlijkheid van 98%. We noemen deze index de Waarschijnlijkheid van Richting (pd). Het kan in feite gemakkelijker worden berekend met het volgende:\n\n\nProbability of Direction: 0.98\n\n\nInteressant is dat deze index meestal sterk gecorreleerd is met de meest frequente p-waarde. We kunnen de overeenkomstige p-waarde bijna ruwweg afleiden met een eenvoudige transformatie:\n\n\n[1] 0.0436\n\n\nAls we ons model in het frequentistisch kader hebben uitgevoerd, zouden we ongeveer een effect moeten waarnemen met een p-waarde van 0.04. Is dat waar?\n\n\n\n\n\n\nCall:\nlm(formula = weight ~ feed, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-123.909  -25.913   -6.917   32.091  103.091 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     276.91      17.20  16.097 2.74e-13 ***\nfeedsunflower    52.01      23.82   2.184   0.0405 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 57.05 on 21 degrees of freedom\nMultiple R-squared:  0.1851,    Adjusted R-squared:  0.1463 \nF-statistic: 4.769 on 1 and 21 DF,  p-value: 0.04047\n\n\nHet frequentistische model vertelt ons dat het verschil positief en significant (beta = 52, p = 0.04) is.\nAlhoewel we tot een gelijkaardige conclusie kwamen, liet het Bayesiaanse kader ons toe om een meer diepgaand en intuïtief begrip te ontwikkelen van ons effect en van de onzekerheid van de inschatting ervan.\n\n\n\n\nEn toch, ik ben het ermee eens, het was een beetje omslachtig om alle indices eruit te halen en te berekenen. Maar wat als ik je vertel dat we dit allemaal kunnen doen, en meer, met slechts één functie?\n\nZie, beschrijf_posterior!\n\nDeze functie berekent alle genoemde indexen, en kan direct op het model worden uitgevoerd:\n\n\nSampling priors, please wait...\n\n\nWarning: Bayes factors might not be precise.\nFor precise Bayes factors, sampling at least 40,000 posterior samples is recommended.\n\n\nSummary of Posterior Distribution\n\nParameter     | Median |           95% CI |     pd |          ROPE | % in ROPE |  Rhat |     ESS |       BF\n-----------------------------------------------------------------------------------------------------------\n(Intercept)   | 276.27 | [240.98, 311.87] |   100% | [-6.17, 6.17] |        0% | 1.001 | 3274.00 | 1.01e+13\nfeedsunflower |  52.69 | [  3.24,  99.93] | 98.15% | [-6.17, 6.17] |     0.66% | 1.000 | 3389.00 |    0.758\n\n\nTada! Daar hebben we het! De mediaan, de CI, de pd en het ROPE percentage!\nHet begrijpen en beschrijven van posterior distributies is slechts één aspect van Bayesiaanse modellering… Ben je klaar voor meer? \n\n\n\n\nNu het beschrijven en begrijpen van posterior distributies van lineaire regressies voor jou geen geheimen meer heeft, zullen we een stap terug doen en wat eenvoudigere modellen bestuderen: correlaties en t-testen.\nMaar laten we eerst even stilstaan bij het feit dat alle statistische basisprocedures zoals correlaties, t-testen, ANOVA’s of Chisquare-testen ** lineaire regressies** zijn (we raden deze uitstekende demonstratie ten zeerste aan). Op basis van deze eenvoudige modellen introduceren we een complexere index, zoals de Bayes-factor.\n\n\n\n\nLaten we opnieuw beginnen met een frequentistische correlatie tussen twee continue variabelen, de breedte en de lengte van de kelkbladen van sommige bloemen (‘sepals’). De gegevens zijn beschikbaar in R als de iris dataset (dezelfde die we hierboven hebben gebruikt).\nWe zullen een Pearson’s correlatietest berekenen, de resultaten opslaan in een object met de naam resultaat en vervolgens deze resultaten weergeven:\n\n\n\n    Pearson's product-moment correlation\n\ndata:  iris$Sepal.Width and iris$Sepal.Length\nt = -1.4403, df = 148, p-value = 0.1519\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.27269325  0.04351158\nsample estimates:\n       cor \n-0.1175698 \n\n\nZoals je in de output kunt zien, heeft de test die we hebben gedaan eigenlijk twee hypothesen vergeleken: de nul-hypothese (h0; geen correlatie) met de alternatieve hypothese (h1; een niet-nul-correlatie). Op basis van de p-waarde kan de nulhypothese niet worden verworpen: de correlatie tussen de twee variabelen is negatief maar niet significant (r = -.12, p > .05).\n\n\n\nOm een Bayesiaanse correlatietest te berekenen, hebben we het BayesFactor-pakket nodig (u kunt het installeren door install.packages (“BayesFactor”) uit te voeren). We kunnen dan dit pakket laden, de correlatie berekenen met behulp van de correlatieBF() functie en de resultaten op een vergelijkbare manier opslaan.\n\n\nWarning: package 'BayesFactor' was built under R version 4.1.3\n\n\nLoading required package: coda\n\n\nLoading required package: Matrix\n\n\nWarning in .recacheSubclasses(def@className, def, env): undefined subclass\n\"packedMatrix\" of class \"replValueSp\"; definition not updated\n\n\nWarning in .recacheSubclasses(def@className, def, env): undefined subclass\n\"packedMatrix\" of class \"mMatrix\"; definition not updated\n\n\n************\nWelcome to BayesFactor 0.9.12-4.3. If you have questions, please contact Richard Morey (richarddmorey@gmail.com).\n\nType BFManual() to open the manual.\n************\n\n\nLaten we nu eens onze describe_posterior()-functie hierop los:\n\n\nSummary of Posterior Distribution\n\nParameter | Median |        95% CI |     pd |          ROPE | % in ROPE |    BF |         Prior\n-----------------------------------------------------------------------------------------------\nrho       |  -0.11 | [-0.27, 0.04] | 92.05% | [-0.05, 0.05] |    20.13% | 0.509 | Beta (3 +- 3)\n\n\nWe zien hier weer veel dingen, maar de belangrijke indices voor nu zijn de mediaan van de posterior distributie, -.11. Dit komt (weer) dicht in de buurt van de frequentistische correlatie. We zouden, zoals eerder, het credible interval, de pd of het ROPE-percentage kunnen beschrijven, maar we zullen ons hier richten op een andere index die door het Bayesiaanse kader wordt geboden, de Bayes-factor (BF).\n\n\n\nWe zeiden eerder dat een correlatietest eigenlijk twee hypothesen vergelijkt, een nul (afwezigheid van effect) met een alarmerende (aanwezigheid van een effect). De Bayes-factor (BF) laat dezelfde vergelijking toe en bepaalt onder welke van twee modellen de geobserveerde gegevens waarschijnlijker zijn: een model met het effect waarin we geinteresseerd zijn, en een nulmodel zonder het effect daarvan. We kunnen de bayes-factor() gebruiken om de Bayes-factor specifiek te berekenen bij het vergelijken van die modellen:\n\n\nBayes Factors for Model Comparison\n\n    Model         BF\n[2] (rho != 0) 0.509\n\n* Against Denominator: [1] (rho = 0)\n*   Bayes Factor Type: JZS (BayesFactor)\n\n\nWe hebben een BF van 0,51. Wat betekent dat?\nBayes-factoren zijn continue metingen van het relatieve bewijs, waarbij een Bayes-factor groter dan 1 bewijs geeft ten gunste van één van de modellen (vaak de teller genoemd), en een Bayes-factor kleiner dan 1 die bewijs geeft ten gunste van het andere model (de noemer).\n\nJa, je hebt het goed gehoord, bewijs ten gunste van de nul!\n\nDat is een van de redenen waarom het Bayesiaanse kader soms als superieur wordt beschouwd aan het frequentistische kader. Onthoud uit je statistiekenlessen, dat de p waarde alleen gebruikt kan worden om h0 af te wijzen, maar niet om het te accepteren. Met de Bayes-factor kunt je -evidentie meten tegen - en ook ten gunste van - de nul.\nBF’s die het bewijs voor het alternatief tegen de null vertegenwoordigen kunnen worden teruggedraaid met 𝐵𝐹01=1/𝐵𝐹10 (de 01 en 10 komen respectievelijk overeen met h0 tegen h1 en h1 tegen h0) om het bewijs voor de null weer te geven. Dit verbetert de leesbaarheid in gevallen waarin het BF van het alternatief tegen de nul kleiner is dan 1 (d.w.z. ter ondersteuning van de nul).\nIn ons geval, BF = 1/0,51 = 2, geeft aan dat de gegevens 2 keer meer waarschijnlijk zijn onder de null in vergelijking met de alternatieve hypothese. Die weliswaar de voorkeur geeft aan de nul-hypothese, maar slechts als anekdotisch bewijs moet wordt beschouwd.\nWe kunnen dus concluderen dat er anecdotisch bewijs is ten gunste van de hypothese ‘gebrek aan correlatie tussen de twee variabelen’ (mediaan = 0,11, BF = 0,51), wat veel meer informatie geeft dan wat we kunnen doen met de frequentistische statistiek.\nEn dat is nog niet alles!\n\n\n\nIn het algemeen zijn taartgrafieken een absolute ‘no-go’ in datavisualisatie, omdat het waarnemingssysteem van onze hersenen de gepresenteerde informatie op deze manier sterk vervormt. Toch is er één uitzondering: pizzagrafieken.\nHet is een intuïtieve manier om de bewijskracht van BFs te interpreteren als een soort verrassing\nDergelijke “pizzapercelen” kunnen direct worden aangemaakt via het zie visualisatiepakket voor easystats (u kunt het installeren door het uitvoeren van\nDergelijke ‘pizzagrafieken’ kunnen direct worden aangemaakt met het visualisatiepakket voor easystats (u kunt het installeren door install.packages(\"see\")) uit te voeren):\n\n\n\n\n\nDus, na het zien van deze pizza, ben je dan nog verrast door de uitkomst?\n\n\n\n\n“Ik weet dat ik niets weet, en vooral niet als versicolor en virginica verschillen in termen van Sepal.Width”, zei de beroemde Socrates.\n\nTijd om eindelijk een antwoord te geven op deze cruciale vraag!\n\n\n\nBayesiaanse t-testen kunnen worden uitgevoerd op een zeer vergelijkbare manier als correlaties. We zijn met name geïnteresseerd in twee niveaus van de Specie factor, versicolor en virginica. We zullen beginnen met het uit iris uitfilteren van de niet-relevante waarnemingen die overeenkomen met de setosa specie, en we zullen dan de waarnemingen en de distributie van de Sepal.Width variabele visualiseren.\n\n\n\n\n\n\n\n\nHet lijkt er (visueel) op dat virgnica bloemen gemiddeld een iets grotere kelkbladbreedte hebben. Laten we dit verschil statistisch beoordelen met behulp van de ttestBF in het BayesFactor pakket.\n\n\nSummary of Posterior Distribution\n\nParameter  | Median |         95% CI |     pd |          ROPE | % in ROPE |    BF |              Prior\n------------------------------------------------------------------------------------------------------\nDifference |  -0.19 | [-0.31, -0.07] | 99.85% | [-0.03, 0.03] |        0% | 17.72 | Cauchy (0 +- 0.71)\n\n\nOp basis van de indexen kunnen we zeggen dat het verschil tussen virginica en versicolor (van Sepal.Width) een kans heeft van 100% om negatief te zijn [van de pd en het teken van de mediaan] (mediaan = -0,19, 89% CI [-0,29, -0,092]). De gegevens leveren een sterk bewijs tegen de nulhypothese (BF = 18).\nHoud dat in gedachten, want we zullen een andere manier zien om deze vraag te onderzoeken.\n\n\n\nEen hypothese waarvoor men een t-test gebruikt, kan ook getest worden met een binomiaal model (bv. een logistisch model). Het is inderdaad mogelijk om de volgende hypothese te herformuleren, “er is een belangrijk verschil in deze variabele tussen de twee groepen” door “deze variabele in staat te stellen om te discrimineren tussen (of te classificeren in) de twee groepen”. Deze modellen zijn echter veel krachtiger dan een gewone t-test.\nIn het geval van het verschil van Sepal.Width tussen virginica en versicolor wordt de vraag, hoe goed kunnen we de twee soorten classificeren met alleen Sepal.Width.\n\n\n\n\n\n\n\n\n\nEerst prestatie van het model in kaart brengen.\n\n\nWarning: package 'performance' was built under R version 4.1.3\n\n\n# Indices of model performance\n\nELPD    | ELPD_SE |   LOOIC | LOOIC_SE |    WAIC |    R2 |  RMSE | Sigma | Log_loss | Score_log | Score_spherical\n-----------------------------------------------------------------------------------------------------------------\n-66.252 |   3.081 | 132.505 |    6.161 | 132.496 | 0.101 | 0.477 | 1.000 |    0.643 |   -35.484 |           0.014\n\n\nVervolgens de resultaten van enkele indices presenteren.\n\n\nSampling priors, please wait...\n\n\nWarning: Bayes factors might not be precise.\nFor precise Bayes factors, sampling at least 40,000 posterior samples is recommended.\n\n\nSummary of Posterior Distribution\n\nParameter   | Median |          95% CI |   pd |          ROPE | % in ROPE |  Rhat |     ESS |    BF\n---------------------------------------------------------------------------------------------------\n(Intercept) |  -6.21 | [-10.44, -2.29] | 100% | [-0.18, 0.18] |        0% | 1.000 | 2683.00 | 36.57\nSepal.Width |   2.16 | [  0.83,  3.62] | 100% | [-0.18, 0.18] |        0% | 0.999 | 2693.00 | 19.90\n\n\n\n\n\n\nAndrews, M., & Baguley, T. (2013). Prior approval: The growth of bayesian methods in psychology. British Journal of Mathematical and Statistical Psychology, 66(1), 1–7.\nBenjamin, D. J., Berger, J. O., Johannesson, M., Nosek, B. A., Wagenmakers, E.-J., Berk, R., … others. (2018). Redefine statistical significance. Nature Human Behaviour, 2(1), 6.\nChambers, C. D., Feredoes, E., Muthukumaraswamy, S. D., & Etchells, P. (2014). Instead of ’playing the game’ it is time to change the rules: Registered reports at aims neuroscience and beyond. AIMS Neuroscience, 1(1), 4–17.\nEtz, A., & Vandekerckhove, J. (2016). A bayesian perspective on the reproducibility project: Psychology. PloS One, 11(2), e0149794.\nKruschke, J. K. (2010). What to believe: Bayesian methods for data analysis. Trends in Cognitive Sciences, 14(7), 293–300.\nKruschke, J. K., Aguinis, H., & Joo, H. (2012). The time has come: Bayesian methods for data analysis in the organizational sciences. Organizational Research Methods, 15(4), 722–752.\nSzucs, D., & Ioannidis, J. P. (2016). Empirical assessment of published effect sizes and power in the recent cognitive neuroscience and psychology literature. BioRxiv, 071530.\nWagenmakers, E.-J., Marsman, M., Jamil, T., Ly, A., Verhagen, J., Love, J., … others. (2018). Bayesian inference for psychology. Part i: Theoretical advantages and practical ramifications. Psychonomic Bulletin & Review, 25(1), 35–57."
  },
  {
    "objectID": "posts/2019-05-19-website-maken/rmarkdown_websites_tutorialHARRIE.html",
    "href": "posts/2019-05-19-website-maken/rmarkdown_websites_tutorialHARRIE.html",
    "title": "Websites maken in R",
    "section": "",
    "text": "Deze tutorial is van Emily C. Zabor die ik heb bewerkt, vooral ook om te zien of ik mijn eigen website kan maken. Deze tutorial laat je zien hoe je een website maakt met gebruik van R, RMarkdown en GitHub.\nDeze tutorial presenteerde zij voor het eerst op R Gebruikers Groep Bijeenkomst op 23 Januari 2018 op het Memorial Sloan Kettering Cancer Center Department van Epidemiologie and Biostatistiek.\nDeze versie ververste en presenteerde zij op de R Dames NYC Bijeenkomst op 15 Februarie 2018."
  },
  {
    "objectID": "posts/2019-05-19-website-maken/rmarkdown_websites_tutorialHARRIE.html#typen-websites",
    "href": "posts/2019-05-19-website-maken/rmarkdown_websites_tutorialHARRIE.html#typen-websites",
    "title": "Websites maken in R",
    "section": "Typen websites",
    "text": "Typen websites\nDe belangrijkste typen websites die je zou willen maken zijn:\n\nPersoonlijke websites\nWebsites om een pakket te presenteren\nProject websites\nBlogs"
  },
  {
    "objectID": "posts/2019-05-19-website-maken/rmarkdown_websites_tutorialHARRIE.html#de-basis-van-r-markdown-website",
    "href": "posts/2019-05-19-website-maken/rmarkdown_websites_tutorialHARRIE.html#de-basis-van-r-markdown-website",
    "title": "Websites maken in R",
    "section": "De basis van R Markdown website",
    "text": "De basis van R Markdown website\nWat jij minimaal nodig hebt voor een R Markdown website zijn:\n\nindex.Rmd: bevat de inhoud van de homepage van de website\n_site.yml: bevat de metadata voor de website\n\nEen basis voorbeeld voor een _site.yml voor een website met twee pagina’s kan er zo uitzien:\n\n\n\nAnd a basic index.Rmd to create the Home page:\n\n\n\nHier vind je een overzicht van de basis van R Markdown website hier."
  },
  {
    "objectID": "posts/2019-05-19-website-maken/rmarkdown_websites_tutorialHARRIE.html#github",
    "href": "posts/2019-05-19-website-maken/rmarkdown_websites_tutorialHARRIE.html#github",
    "title": "Websites maken in R",
    "section": "GitHub",
    "text": "GitHub\nDeze tutorial left de nadruk op het hosten van websites via GitHub. Hosten van websites via GitHub is gratis.\nAls je nog geen GitHub account hebt, teken dan op via https://github.com/join?source=header-home met username YOUR_GH_NAME. Ik zal naar deze username, YOUR_GH_NAME, als “jouw GitHub username” refereren in deze hele tutorial.\nEr zijn andere mogelijkheden om jouw website gratis te hosten. Een ander populaire keuze is Netlify."
  },
  {
    "objectID": "posts/2019-05-19-website-maken/rmarkdown_websites_tutorialHARRIE.html#persoonlijke-websites",
    "href": "posts/2019-05-19-website-maken/rmarkdown_websites_tutorialHARRIE.html#persoonlijke-websites",
    "title": "Websites maken in R",
    "section": "Persoonlijke websites",
    "text": "Persoonlijke websites\nEen voorbeeld van een homepage van Emily Zobore’s website is:\n\nEr zijn twee belangrijke stappen om een persoonlijke website te maken die op GitHub wordt gehost:\n\nGitHub setup\nLokale setup\n\n\nGitHub setup\n\nCreëer een GitHub repository (“repo”) genaamd YOUR_GH_NAME.github.io, waar YOUR_GH_NAME jouw GitHub username is.\nInitialiseer het met een README\n\nVoor hen die met GitHub weinig ervaring hebben: dit kan het proces van klonen van repository en daarmee het afstemmen met de ‘master branch’ vergemakkelijken.\n\n\n\n\nLokale setup\n\nKloon deze remote repository op een locale directory met dezelfde naam, YOUR_GH_NAME.github.io\nVoeg een R Project toe aan deze directory\nMaak een _site.yml en een index.Rmd file in jouw nieuwe directory\n\n\n\nWaarom heb ik een R Project nodig?\nHet R Project is gemakkelijk omdat RStudio jouw project als een website zal herkennen en zorgt voor de goede gereedschappen die je nodig hebt.\nOpgelet: Nadat je een R Project met de goede files hebt gemaakt, kan het zijn dat je het project moet sluiten en heropenen voordat R het herkent als een website en de goede gereedschappen toont.\n\n\nCreëer inhoud\nPas de _site.yml file aan door de metadata te veranderen en het thema van jouw website. Kijk maar eens naar de Jekyll thema’s hier en speel wat met de verschillende opties. Thema’s zijn makkelijk te veranderen, zelfs nadat je de inhoud hebt toegevoegd.\nBijvoorbeeld de _site.yml voor de persoonlijke website van Emily ziet er zo uit:\n\n\n\nPas aan en creëer .Rmd files met de inhoud van jouw website, die er html-pagina’s voor jouw website van maken als jij ze ‘knit’.\nDe index.Rmd file voor de homepage van Emily’s persoonlijke website ziet er zo uit:\n\n\n\nAls je een keer de inhoud hebt geschreven en de lay-out hebt opgezet, zoek dan de Build tab in RStudio op en selecteer “Build Website”:\n\nNu heeft jouw local directory alle files die nodig zijn om jouw website te maken:\n\n\n\nDe website uitzetten\nBasis benadering:\n\nSelecteer “Upload files” van de hoofpagina pagina van jouw GitHub repository:\n\n\n\nEn sleep eenvoudig of selecteer de files van jouw locale repository:\n\n\nGeavanceerde benadering (aangeraden):\n\ngebruik Git als cliënt of van binnenuit RStudio (een andere goede reden om een R Project! te gebruiken)\n\n\n\nMaar dit is geen Git/GitHub tutorial. Als je meer wilt leren over Git/GitHub, ik raad jou aan dit te doen, dan is dit een goede bron om mee te beginnen: http://happygitwithr.com/\n\n\n\nAangepaste domeinen\nHet standaardadres om jouw wite te hosten is http://YOUR_GH_NAME.github.io, maar je kunt jouw domeinnaam ook aanpassen. Dan zijn er twee stappen te zetten:\n\nIn jouw GitHub repository YOUR_GH_NAME.github.io, ga je naar Settings > GitHub pages. Typ jouw domeinnaam in de box onder Custom domain en sla het op (Save).\n\n\n\nVoeg een CNAME file toe aan jouw GitHub repository YOUR_GH_NAME.github.io.\n\nHet zal als volgt in jouw repository verschijnen:\n\nAnd inside the file you will simply have your domain name:"
  },
  {
    "objectID": "posts/2019-05-19-website-maken/rmarkdown_websites_tutorialHARRIE.html#pakket-websites",
    "href": "posts/2019-05-19-website-maken/rmarkdown_websites_tutorialHARRIE.html#pakket-websites",
    "title": "Websites maken in R",
    "section": "Pakket websites",
    "text": "Pakket websites\nEen voorbeeld hiervan is deze website van Emily’s R-pakket ezfun:\n\nGebruik Hadley Wickham’s goede pakket pkgdown om makkelijk een website van jouw pakket te maken die op GitHub wordt gehost. Details over pkgdown kun je hier vinden de pkgdown website, die ook met inzet van pkgdown is gemaakt.\nDit veronderstelt wel dat je al een R-pakket met een locale directory hebt en een GitHub repository.\nFrom within your package directory run:\n\n\n\n\nDit zal een folder toevoegen met de naam docs binnen de locale directory voor jouw pakket\nUpload/push deze veranderingen in de GitHub repository voor jouw pakket\nIn the GitHub repository voor jouw pakket ga je naar Settings > GitHub pages. Selecteer “master branch/docs folder” als de bron en sla op (Save)\n\n\n\nDe persoonlijke pagina zal worden toegevoegd aan jouw persoonlijke website en aan YOUR_GH_NAME.github.io/repo_name\nDe homepage kun je via README file op jouw repository binnenhalen\nDe referentiepagina van de site omvat alle functies met hun beschrijving\nElke functie klikt door naar de hulppagina ervan,\nEn in bepaalde gevallen ook naar vignettes met goede informatie\n\nEn dan ben je klaar, zo makkelijk als dat."
  },
  {
    "objectID": "posts/2019-05-19-website-maken/rmarkdown_websites_tutorialHARRIE.html#project-websites",
    "href": "posts/2019-05-19-website-maken/rmarkdown_websites_tutorialHARRIE.html#project-websites",
    "title": "Websites maken in R",
    "section": "Project websites",
    "text": "Project websites\nOok als je geen pakket maakt kun je nog wel een repository maken. Emily Zabore heeft bijvoorbeeld een pagina op haar website die linkt naar de repository waarin deze tutorial is opgeslagen.\n\n\nLokale setup\nVanuit de lokale directory van het project waar het jou om te doen is:\n\nCreëer een _site.yml en index.Rmd file in jouw nieuwe directory\nPas deze files met jouw inhoud en layout, net zoals bij persoonlijke websites\n\n\n\nGitHub setup\n\nUpload/push deze nieuwe files in de GitHub repository voor jouw project\nGa naar GitHub pagina’s voor de repository en ga naar Settings > GitHub Pages, waar je de “master branch” folder selecteert en je drukt op Save"
  },
  {
    "objectID": "posts/2019-05-19-website-maken/rmarkdown_websites_tutorialHARRIE.html#blogs",
    "href": "posts/2019-05-19-website-maken/rmarkdown_websites_tutorialHARRIE.html#blogs",
    "title": "Websites maken in R",
    "section": "Blogs",
    "text": "Blogs\nR Markdown websites zijn makkelijk te maken en uit te zetten, maar het wordt lastiger als je het voortdurend moet verversen of veranderingen moet aanbrengen, zoals dat het geval is bij een blog. Gelukkig, het R-pakket blogdown bestaat juist voor dit doel. blogdown is een R pakket dat jou in staat stelt statistische websites te makenthat allows you to create static websites, wat betekent dat de uitgezette versie van de website alleen bestaat uit JavaScript, HTML, CSS en plaatjes. Gelukkig is het blogdown pakket zo opgezet dat je over al die zaken niets af hoeft te weten om toch nog een mooie website te maken voor jouw blog, met de ondersteuning van Hugo.\nVoor de beste referentie van blogdown website, kijk naar dun blogdown boekje.\nEmily Zabore heeft geen persoonlijk blog, maar wel een website/blog gebouwd rond deze bijeenkomsten in New York R-Ladies NYC en dat is hier als voorbeeld toegevoegd.\n\n\nSetup\nDe eerste drie stappen zijn hetzelfde als het maken van een basis R Markdown website:\n\nCreëer een GitHub repository met de naam YOUR_GH_NAME.github.io, waar YOUR_GH_NAME jouw GitHub gebruikersnaam is, geïnistialiseerd met een README file\nKloon deze GitHub repo op een lokale directory met dezelfde naam\nVoeg een R Project aan jouw lokale directory toe\n\nDan beginnen we met blogdown.\n\nInstalleer blogdown en Hugo\n\n\n\n\n\nKies een thema en vind de link naar de thema’s van de GitHub repository. In dit geval zijn de thema’s niet zo makkelijk te wisselen als binnen de basis R Markdown website, dus kies het thema zorgvuldig.\nGenereer een nieuwe site binnen jouw project sessie. De optie theme_example = TRUE zal voor de files van een voorbeeldsite zorgen die je op basis van wat je nodig hebt kunt aanpassen.. “user/repo” refereert naar de GitHub gebruikers naam (user) en de GitHub repository (repo) voor jouw geselecteerde thema.\n\n\n\n\nDit zal alles van de filestructuur van jouw nieuwe blog genereren.\n\nNadat je dit hebt afgerond, moet je sluiten en dan het project weer heropenen. Als je heropent, zal RStudio het project als een website herkennen.\n\n\nHet aanpassen van het beeld\nVeranderingen voer je in de config.toml file door (hetzelfde als de _site.yml die we bij de R Markdown websites tegenkwamen); zo verander je de layout en het beeld van jouw website. De beschikbare kenmerken in de config.toml zullen verschillend zijn afhankelijk van jouw thema en de meeste themavoorbeelden hebben een eigen config.toml die je als template kunt gebruiken.\nAls je een keer de kenmerken van jouw website hebt aangepast, klik dan op RStudio’s “Serve Site” om de site lokaal al te bekijken.\n\n\n\nEen nieuwe blog post schrijven\nEr zijn verschillende manieren om een nieuwe blogpost op jouw site te schrijven, maar het is het makkelijkste om dat via “New Post” in RStudio te doen:\n\nDit opent een pop-up waar je de meta-data voor a nieuwe post kunt plaatsen:\n\nIn aanvulling op Titel, Auteur en Datum van de post, kun je aanvullend ook categorieën creëren, die jouw post in folders organiseren en kun je tags aan de posts toevoegen, waarmee ze te zoeken zijn binnen de inhoud van jouw website. Wees er wel van bewust dat het functioneren van deze kenmerken varieert per thema. Bepaalde blogs kunnen ook in toekomst worden geplaatst.\nZie onderaan dat je ook kunt kiezen tussen een regulier markdown (.md) of R markdown (.Rmd) file. .Rmd files moeten worden gerenderd voordat ze html pagina’s genereren. Dus het is het beste dit te gebruiken waar ook een R code in zit.\nEen file naam en een ‘slug’ worden automatisch genereerd gebaseerd op de andere metadata. De ‘slug’ is een URL-vriendelijke titel van jouw post.\n\n\n\nPresenteren\nEen blogdown site is een beetje lastig te bouwen en te presenteren via GitHub vergeleken met een reguliere R Markdown website en vergeleken met wat hierboven is beschreven.\nProbleem 1: Omdat het een statistische site betreft, de files genereren automatisch on line in een aparte subdirectory onder de naam public binnen jouw lokale directory. Echter, dit veroorzaakt problemen met het hosten (presenteren) van GitHub omdat de files dan in de lokale YOUR_GH_NAME.github.io directory moeten zitten.\nDe oplossing:\n\nHou aparte directories voor de bron files (deze directory bijvoorbeeld “source”, bron noemen) en voor de statische files (de directory YOUR_GH_NAME.github.io) die gegenereerd worden. De “source” folder is waar your R project en config.toml files zich bevinden.\n\n\n\nGebruik in jouw config.toml de optie publishDir = om blogdown te publiceren via de YOUR_GH_NAME.github.io folder. Dat is eenvoudiger dan de standaard manier op jouw lokale locatie.\n\n\nProbleem 2: GitHub gebruikt standaard Jekyll met website inhoud en dat moet ongedaan worden gemaakt omdat blogdown sites met Hugo worden gebouwd.\nOm dit op te lossne moet je een lege file toevoegen met de naam .nojekyll in jouw GitHub repo YOUR_GH_NAME.github.io, voordat je het publiseert."
  },
  {
    "objectID": "posts/2019-05-19-website-maken/rmarkdown_websites_tutorialHARRIE.html#het-pakket-distill",
    "href": "posts/2019-05-19-website-maken/rmarkdown_websites_tutorialHARRIE.html#het-pakket-distill",
    "title": "Websites maken in R",
    "section": "Het pakket Distill",
    "text": "Het pakket Distill\nMijn eigen blog Harrie’s hoekje maak ik met het pakket Distill. Om een blog te maken installeer je Distill. Dan maak je een new blog en gebruik je Distill Blog.\nDan worden er een project gemaakt met een aantal documenten: _site.yml\nindex.rmd\nabout.rmd\n_posts/welcome/welcome.rmd\nVervolgens pas je de *_site.yml* aan op basis van hoe jouw blog eruit moet zien.\nals je een post wilt creëren open je het programma (library(distill)) en je tikt in `create_post(“Hier de naam van de post”). Hier Distill vind je veel meer informatie hierover.Hier [Tensorflow)(https://blogs.rstudio.com/tensorflow/) vind je een ander voorbeeld hoe de blog er dan uit kan zien.\nMet Distill kun je op dezelfde eenvoudige manier een website maken Distill-website. Nadat je het Distill hebt geinstalleerd en geopend (library(Distll)). Krijg je de volgende documenten _site.yml (om de website te configureren)\nindex.rmd (voor de homepage)\nabout.rmd (waar de website over gaat)\nVerder werkt dit hetzelfde."
  },
  {
    "objectID": "posts/2019-05-19-website-maken/rmarkdown_websites_tutorialHARRIE.html#aanvullende-bronnen",
    "href": "posts/2019-05-19-website-maken/rmarkdown_websites_tutorialHARRIE.html#aanvullende-bronnen",
    "title": "Websites maken in R",
    "section": "Aanvullende bronnen",
    "text": "Aanvullende bronnen\nHieronder vind je de aanvullende bronnen en linken waar in deze tutorial naar wordt verwezen:\n\nhttp://rmarkdown.rstudio.com/rmarkdown_websites.html: an overview of R Markdown website basics\nhttp://jekyllthemes.org/: Jekyll themes for use with your R Markdown website\nhttp://happygitwithr.com/: an introduction to Git/GitHub\nhttp://pkgdown.r-lib.org/: Hadley Wickham’s pkgdown website\nhttps://bookdown.org/yihui/blogdown/: Yihui Xie’s blogdown book\nhttps://themes.gohugo.io/: Hugo themes for use with your blogdown website"
  },
  {
    "objectID": "posts/2021-10-03-ontrafelen/ontrafelen.html",
    "href": "posts/2021-10-03-ontrafelen/ontrafelen.html",
    "title": "Opschonen en ontrafelen",
    "section": "",
    "text": "Als je data krijgt is het belangrijk om over enkele technieken te beschikken de data goed te ontrafelen en schoon te maken. Het pakket tidyverse is een standaardpakket waarmee je goed uit de voeten kunt. Ook het pakket janitor hoort thuis in de gereedschapskist van de analyticus. Een aantal codes van dat janitorpakket wilde ik goed leren kennen. Vandaar deze blog. Daarbij bouw ik voort op deze blog hier."
  },
  {
    "objectID": "posts/2021-10-03-ontrafelen/ontrafelen.html#mee-werken",
    "href": "posts/2021-10-03-ontrafelen/ontrafelen.html#mee-werken",
    "title": "Opschonen en ontrafelen",
    "section": "Mee werken",
    "text": "Mee werken\nLaten we een beetje met deze data werken. Eerst geven we de kolommen de naam columns om te voorkomen dat het rommelig wordt. Vervolgens gebruiken we de separate() functie om de kolommen te scheiden. Vervolgens filteren we de gegevens tot Berkshire County, omdat bij nadere inspectie van de gegevens duidelijk wordt dat er een paar items van buiten dit gebied zijn opgenomen. Dan gebruiken we mutate() om verder op te ruimen. str_replace() wordt gebruikt om de ID “598673” te vervangen door “598712,” een ID nummer dat al bestaat in de dataset, om zo een duplicaat ID te maken. Tenslotte wordt een extra kolom genaamd “extra_column” aangemaakt met NAs in elke rij:\n\ncolnames(place_names) = \"columns\"\n\nplace_names = \n  place_names %&gt;% \n  separate(columns, c(\"Feature Name\", \"ID\", \"Class\", \"County\", \"State\", \"Latitude\", \"Longitude\", \"Ele(ft)\", \"Map\", \"BGN Date\", \"Entry Date\"), sep = \"[|]\") %&gt;%\n  filter(County == \"Berkshire\") %&gt;% \n  mutate(\n    ID = str_replace(ID, \"598673\", \"598712\"),\n    extra_column = NA\n  )"
  },
  {
    "objectID": "posts/2021-10-03-ontrafelen/ontrafelen.html#creëer-non_ma_names",
    "href": "posts/2021-10-03-ontrafelen/ontrafelen.html#creëer-non_ma_names",
    "title": "Opschonen en ontrafelen",
    "section": "Creëer non_ma_names",
    "text": "Creëer non_ma_names\nVoor we verder gaan, maken we snel ook een tweede dataset aan. Deze noemen we “non_ma_names”, met data die niet van Berkshire County afkomstig zijn. Nogmaals lezen we het “GNIS Query Result.csv” bestand in en scheiden de kolomnamen. Vervolgens gebruiken we de clean_names() functie uit het janitor pakket, die we in de volgende sectie uitvoerig zullen behandelen. Tenslotte gebruiken we as.numeric() en as.factor() in een mutate stap om onze ele_ft variabele te transformeren naar een numerieke variabele en onze map variabele naar een factor:\n\nnon_ma_names = read.csv(\"data/GNIS Query Result.csv\")\n\ncolnames(non_ma_names) = \"columns\"\n\nnon_ma_names = \n  non_ma_names %&gt;% \n  separate(columns, c(\"Feature Name\", \"ID\", \"Class\", \"County\", \"State\", \"Latitude\", \"Longitude\", \"Ele(ft)\", \"Map\", \"BGN Date\", \"Entry Date\"), sep = \"[|]\") %&gt;% \n  filter(County != \"Berkshire\") %&gt;% \n  clean_names() %&gt;% \n  mutate(\n    ele_ft = as.numeric(ele_ft),\n    map = as.factor(map)\n  )\n\nLaten we nu eens zien wat janitor kan."
  },
  {
    "objectID": "posts/2021-10-03-ontrafelen/ontrafelen.html#gebruik-janitor",
    "href": "posts/2021-10-03-ontrafelen/ontrafelen.html#gebruik-janitor",
    "title": "Opschonen en ontrafelen",
    "section": "Gebruik janitor",
    "text": "Gebruik janitor\n\nrow_to_names()\nJe hebt waarschijnlijk al heel wat databestanden ontvangen, waarschijnlijk in .xlsx-formaat. Bovenaan de spreadsheet staan er dan niet zelden een aantal rijen voordat de eigenlijke gegevens beginnen. Deze rijen kunnen leeg zijn of zijn gevuld met informatie en bedrijfslogo’s. Wanneer je dergelijke gegevens in R laadt, kan de inhoud van deze eerste rijen automatisch jouw kolomkoppen en eerste rijen worden. De functie row_to_names() in het janitor-pakket geeft joun de gelegenheid om aan te geven welke rij in jouw dataframe de eigenlijke kolomnamen bevat en om al het andere dat aan die rij voorafgaat te verwijderen. In onze dataset hadden de kolomnamen al de juiste plaats. Maar laten we deze functie toch eens proberen. We doen alsof de kolomnamen in de derde rij staan. We gebruiken de row_to_names()-functie om een nieuw data frame te maken genaamd “test_names”. De row_to_names() functie neemt de volgende argumenten: de gegevensbron, het rijnummer waar de kolomnamen vandaan moeten komen, of die rij moet worden verwijderd uit de gegevens, en of de rijen erboven moeten worden verwijderd uit de gegevens:\n\ntest_names = row_to_names(place_names, 3, remove_row = TRUE, remove_rows_above = TRUE)\n\n\n\nclean_names()\nclean_names() functie wordt vaak gebruikt als je een nieuwe dataset in R laadt. Als je deze functie nog niet gebruikt, raad ik je sterk aan om deze in jouw workflow op te nemen. Het is niet voor niets de meest populaire functie uit het janitor- pakket - het is uiterst nuttig! Laten we even terugkijken naar onze kolomnamen. Er zijn allerlei hoofdletters en spaties (b.v. “Feature Name”, “BGN Date”) en ook symbolen (“Ele(ft)”). De clean_names() functie zet deze allemaal voor ons om naar kleine letters.\nHet gebruik van clean_names() is eenvoudig en kan als volgt worden uitgevoerd:\n\nplace_names = clean_names(place_names)\n\n#OR\n\nplace_names = \n  place_names %&gt;% \n  clean_names()\n\nZoals je ziet, heeft deze ene functie alle soorten rommelige kolomnamen verwerkt. Alles ziet er nu netjes en opgeruimd uit. Kijk maar eens\n\nhead(place_names)\n\n            feature_name     id     class    county state latitude longitude\n1    A-Z Shopping Center 603538    Locale Berkshire    MA  422755N  0731254W\n2             Abbey Hill 607260    Summit Berkshire    MA  420822N  0730930W\n3             Abbey Lake 617758 Reservoir Berkshire    MA  420818N  0730908W\n4         Abbey Lake Dam 604930       Dam Berkshire    MA  420806N  0730910W\n5            Abbey Swamp 607261     Swamp Berkshire    MA  420822N  0730930W\n6 Abbott Memorial School 598712    School Berkshire    MA  424032N  0730213W\n  ele_ft             map bgn_date  entry_date extra_column\n1   1037 Pittsfield East          27-AUG-2002           NA\n2   1798        Monterey          24-FEB-1974           NA\n3   1463        Monterey          24-FEB-1974           NA\n4   1486        Monterey          27-AUG-2002           NA\n5   1798        Monterey          24-FEB-1974           NA\n6   1696     North Adams          27-AUG-2002           NA\n\n\n\n\nremove_empty()\nDe remove_empty() functie verwijdert, zoals de naam al zegt, kolommen die leeg zijn. We hebben een lege kolom gemaakt in ons “place_names” dataframe tijdens het voorbereiden van onze gegevens, dus we weten dat ten minste één kolom door deze functie zou moeten worden beïnvloed. Laten we het eens uitproberen:\n\nplace_names = \n  place_names %&gt;% \n  remove_empty()\n\nvalue for \"which\" not specified, defaulting to c(\"rows\", \"cols\")\n\n\nZoals je kunt zien is de lege kolom (‘extra_column’) verdwenen en zijn er niet meer 12 maar 11 variabelen over.\nDe bgn_date-kolom lijkt leeg, maar het feit dat deze niet is verwijderd door remove_empty() vertelt ons dat er in ieder geval in één rij gegevens moeten zitten. Scroll maar in de dataset naar beneden en dan zie je het.\n\n\nremove_constant()\nDe remove_constant() functie verwijdert kolommen met dezelfde waarde in alle rijen. Onze dataset heeft er momenteel twee - omdat we de data hebben gefilterd tot Berkshire County, en heel Berkshire County in Massachusetts ligt, is county = “Berkshire” en staat = “MA” voor alle rijen. Deze rijen zijn niet bijzonder nuttig om in de dataset te houden omdat ze geen rij-specifieke informatie geven. We zouden simpelweg select() kunnen gebruiken om deze kolommen te verwijderen, maar het voordeel van remove_constant() is dat deze functie de aanname alle gegevens hetzelfde zijn, dubbel controleert. In feite, door het gebruik van remove_constant() werd ook duidelijk dat 38 van de 1968 items in de ruwe data eigenlijk niet van Berkshire Country waren! Net als remove_empty(), is alle informatie die de remove_constant() functie nodig heeft, de dataset waarop het moet werken:\n\nplace_names = \n  place_names %&gt;% \n  remove_constant()\n\nZoals je kunt zien, zijn de variabelen Berkshire(county) en MA(staat) nu er uit en zijn er nog negen variabelen over.\n\n\ncompare_df_cols()\nOoit geprobeerd om rbind() te gebruiken om twee data frames te stapelen en tegen een onverwachte fout aangelopen? De compare_df_cols() functie vergelijkt direct de kolommen in twee dataframes en is ongelooflijk handig voor het oplossen van dit probleem. Laten we het eens proberen door ons “place_names” data frame te vergelijken met het data frame dat we hebben gemaakt met gegevens buiten Berkshire County, “non_ma_names”:\n\ncompare_df_cols(place_names, non_ma_names)\n\n    column_name place_names non_ma_names\n1      bgn_date   character    character\n2         class   character    character\n3        county        &lt;NA&gt;    character\n4        ele_ft   character      numeric\n5    entry_date   character    character\n6  feature_name   character    character\n7            id   character    character\n8      latitude   character    character\n9     longitude   character    character\n10          map   character       factor\n11        state        &lt;NA&gt;    character\n\n\nDe output is een handige tabel waarin de twee dataframes worden vergeleken. We zien “NA” voor county en state in place_names en “character” voor deze variabelen in non_ma_names. Dit komt omdat we deze kolommen met remove_constant() uit place_names hebben verwijderd, maar nooit iets hebben gedaan met de standaard karaktervariabelen in non_ma_names. We zien ook ele_ft als numeriek en map als een factor variabele in non_ma_names, die we specifiek hebben aangewezen tijdens datavoorbereiding. Als we deze dataframes zouden proberen samen te voegen, zou het nuttig zijn te weten welke kolommen ontbreken en welke kolommen inconsistente types hebben in de dataframes. In dataframes met veel kolommen kan compare_df_cols() de tijd die nodig is om deze vergelijkingen te maken, aanzienlijk verminderen.\n\n\nget_dupes()\nIk heb vaak gewerkt aan projecten met unieke patiënt-ID’s waarvan je niet verwacht dat ze dubbel voorkomen in je dataset. Er zijn tal van andere gevallen waarin je ervoor zou willen zorgen dat een ID-variabele volledig unieke waarden heeft, waaronder onze GNIS-gegevens. Zoals je je zult herinneren, hebben we een dubbele ID aangemaakt toen we onze data voorbereidden. Laten we eens kijken hoe get_dupes() dit detecteert. De functie heeft enkel de naam van ons dataframe nodig en de naam van de kolom die als identifier fungeert:\n\nget_dupes(place_names, id)\n\n      id dupe_count           feature_name  class latitude longitude ele_ft\n1 598712          2 Abbott Memorial School School  424032N  0730213W   1696\n2 598712          2      Abby Lodge School School  422440N  0731503W   1076\n              map bgn_date  entry_date\n1     North Adams          27-AUG-2002\n2 Pittsfield West          27-AUG-2002\n\n\nZoals hieronder getoond, wordt het dataframe gefilterd tot de rijen met dubbele waarden in de kolom ID, zodat eventuele problemen gemakkelijk kunnen worden onderzocht:\n\n\ntabyl()\nDe tabyl() functie is vergelijkbaar met de table() functie van tidyverse. Het is ook compatibel met het knitr pakket, en is erg handig voor data exploratie.Laten we het eerst uitproberen met een enkele variabele. Stel dat we geïnteresseerd zijn in hoeveel scholen er zijn in elk van de steden in Berkshire County. We filteren eerst onze klasse variabele op “School”, en gebruiken dan de tabyl() functie met onze map(locatie) variabele. Tenslotte pijpen we dat in knitr::kable() om de uitvoer in een mooie tabel te formatteren:\n\nplace_names %&gt;% \n  filter(class %in% \"School\") %&gt;% \n  tabyl(map) %&gt;% \n  knitr::kable()\n\n\n\n\nmap\nn\npercent\n\n\n\n\nBash Bish Falls\n2\n0.0143885\n\n\nBecket\n4\n0.0287770\n\n\nCheshire\n3\n0.0215827\n\n\nEast Lee\n3\n0.0215827\n\n\nEgremont\n3\n0.0215827\n\n\nGreat Barrington\n8\n0.0575540\n\n\nHancock\n1\n0.0071942\n\n\nMonterey\n2\n0.0143885\n\n\nNorth Adams\n15\n0.1079137\n\n\nPeru\n1\n0.0071942\n\n\nPittsfield East\n41\n0.2949640\n\n\nPittsfield West\n18\n0.1294964\n\n\nStockbridge\n21\n0.1510791\n\n\nTolland Center\n1\n0.0071942\n\n\nWilliamstown\n10\n0.0719424\n\n\nWindsor\n6\n0.0431655\n\n\n\n\n\nHet uitvoeren van deze zeer eenvoudige code levert de volgende uitvoertabel op:\nWanneer we ons Rmd bestand ‘knitten’, zal de kable() functie de tabel mooi opmaken, zoals hierboven getoond. We krijgen een aantal scholen in elke stad, evenals het percentage van alle scholen in die stad. Het is gemakkelijk om opmerkingen te maken over deze gegevens, zoals dat 29,5% van alle scholen in Pittsfield East zijn, dat 41 scholen telt. Of dat 3 steden zo klein zijn dat ze maar 1 school hebben:\nLaten we nu de kruistabellen van twee variabelen proberen. Laten we eens kijken hoeveel herkenningspunten van elk type aanwezig zijn in elke stad:\n\nplace_names %&gt;% \n  tabyl(map, class) %&gt;% \n knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmap\nAirport\nArch\nBasin\nBay\nBench\nBridge\nBuilding\nCape\nCemetery\nCensus\nChurch\nCivil\nCliff\nCrossing\nDam\nFalls\nFlat\nForest\nGap\nHospital\nIsland\nLake\nLocale\nMilitary\nPark\nPopulated Place\nPost Office\nRange\nRapids\nReserve\nReservoir\nRidge\nSchool\nSpring\nStream\nSummit\nSwamp\nTower\nTrail\nValley\nWoods\n\n\n\n\nAshley Falls\n1\n0\n0\n0\n0\n2\n7\n0\n5\n0\n1\n1\n0\n0\n2\n1\n1\n0\n0\n0\n0\n0\n0\n0\n2\n6\n3\n0\n0\n0\n0\n0\n0\n0\n8\n8\n1\n0\n0\n1\n0\n\n\nBash Bish Falls\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n1\n0\n0\n5\n3\n0\n1\n0\n0\n1\n6\n0\n0\n0\n1\n1\n0\n0\n0\n5\n0\n2\n0\n8\n9\n1\n2\n3\n2\n0\n\n\nBecket\n0\n0\n1\n0\n0\n0\n8\n0\n1\n0\n1\n1\n0\n0\n8\n0\n0\n2\n0\n0\n0\n2\n0\n0\n2\n8\n1\n0\n0\n0\n7\n0\n4\n0\n13\n5\n0\n0\n0\n0\n0\n\n\nBerlin\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n3\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n2\n0\n0\n0\n1\n1\n0\n\n\nBristol\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\nCanaan\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nCheshire\n0\n0\n0\n0\n0\n0\n12\n0\n5\n1\n4\n3\n1\n0\n2\n0\n0\n0\n0\n0\n1\n4\n6\n0\n2\n11\n3\n0\n0\n0\n2\n0\n3\n0\n14\n15\n0\n0\n1\n0\n0\n\n\nChester\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\nCopake\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n\n\nEast Lee\n1\n0\n0\n0\n0\n0\n6\n0\n1\n1\n4\n3\n0\n1\n8\n0\n0\n1\n0\n0\n0\n4\n3\n0\n0\n6\n1\n0\n0\n0\n12\n0\n3\n0\n14\n5\n1\n0\n0\n0\n0\n\n\nEgremont\n1\n0\n0\n0\n0\n0\n8\n0\n3\n0\n1\n2\n1\n0\n3\n0\n0\n1\n0\n0\n0\n4\n1\n0\n3\n4\n1\n0\n0\n0\n5\n0\n3\n0\n6\n10\n0\n1\n0\n0\n0\n\n\nGreat Barrington\n0\n0\n1\n0\n0\n0\n6\n0\n3\n2\n6\n1\n0\n0\n7\n0\n0\n2\n2\n1\n0\n6\n5\n0\n2\n7\n1\n0\n0\n0\n9\n0\n8\n1\n4\n9\n1\n1\n1\n1\n0\n\n\nHancock\n0\n0\n0\n0\n0\n0\n3\n0\n2\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n2\n0\n2\n2\n1\n0\n0\n0\n0\n1\n1\n0\n8\n13\n0\n0\n0\n3\n0\n\n\nMonterey\n0\n0\n0\n0\n0\n0\n5\n0\n2\n0\n3\n2\n0\n0\n11\n0\n0\n0\n0\n0\n0\n3\n4\n0\n1\n5\n2\n1\n0\n0\n13\n0\n2\n0\n6\n13\n6\n0\n0\n1\n0\n\n\nNorth Adams\n0\n1\n0\n0\n2\n0\n19\n0\n9\n1\n7\n4\n1\n0\n7\n1\n0\n1\n1\n1\n0\n6\n4\n0\n14\n11\n1\n1\n0\n0\n8\n0\n15\n0\n22\n6\n1\n3\n1\n0\n0\n\n\nOtis\n0\n0\n0\n1\n0\n0\n4\n1\n2\n0\n0\n1\n0\n0\n9\n1\n0\n0\n0\n0\n2\n10\n4\n0\n0\n6\n2\n0\n0\n0\n11\n1\n0\n0\n8\n7\n0\n0\n0\n0\n0\n\n\nPeru\n0\n0\n0\n0\n0\n0\n5\n0\n2\n0\n0\n2\n0\n0\n4\n1\n0\n1\n0\n0\n0\n3\n1\n0\n4\n6\n0\n0\n0\n2\n4\n0\n1\n0\n15\n7\n0\n0\n0\n0\n0\n\n\nPittsfield East\n1\n0\n0\n0\n0\n0\n29\n0\n6\n0\n37\n1\n0\n0\n7\n0\n0\n0\n1\n2\n0\n2\n19\n0\n16\n21\n59\n0\n0\n0\n16\n1\n41\n0\n14\n9\n0\n2\n0\n0\n0\n\n\nPittsfield West\n1\n0\n0\n0\n0\n0\n24\n1\n7\n0\n19\n2\n1\n0\n2\n1\n0\n1\n0\n1\n0\n5\n17\n0\n9\n9\n1\n1\n0\n0\n3\n1\n18\n0\n25\n14\n1\n7\n3\n2\n0\n\n\nPlainfield\n0\n0\n0\n0\n0\n0\n0\n0\n2\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n8\n1\n0\n0\n0\n0\n0\n\n\nRowe\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n3\n0\n1\n2\n0\n0\n0\n0\n2\n1\n0\n0\n13\n2\n0\n0\n0\n0\n0\n\n\nSouth Sandisfield\n0\n0\n0\n0\n0\n0\n2\n0\n5\n0\n2\n1\n0\n0\n6\n1\n0\n2\n0\n0\n0\n3\n1\n0\n0\n6\n1\n0\n0\n0\n4\n0\n0\n0\n2\n7\n2\n0\n0\n0\n0\n\n\nSouthbury\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nState Line\n0\n0\n0\n0\n0\n0\n0\n0\n4\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n6\n2\n0\n0\n7\n1\n0\n0\n0\n0\n0\n0\n0\n5\n4\n0\n0\n0\n0\n0\n\n\nStockbridge\n0\n0\n0\n0\n0\n0\n35\n0\n7\n2\n6\n2\n0\n1\n7\n0\n0\n0\n1\n1\n0\n10\n7\n0\n7\n18\n8\n0\n0\n0\n6\n0\n21\n0\n20\n10\n1\n4\n0\n1\n1\n\n\nTolland Center\n0\n0\n0\n0\n0\n0\n3\n0\n1\n0\n0\n1\n0\n0\n4\n0\n0\n0\n0\n0\n0\n2\n1\n0\n0\n3\n2\n0\n0\n0\n3\n0\n1\n0\n6\n6\n0\n0\n0\n0\n0\n\n\nWilliamstown\n3\n0\n1\n0\n0\n2\n19\n1\n4\n1\n14\n1\n1\n0\n3\n1\n0\n1\n1\n0\n0\n1\n13\n2\n13\n13\n1\n1\n1\n0\n5\n1\n10\n2\n20\n19\n1\n3\n6\n4\n0\n\n\nWindsor\n2\n0\n0\n0\n0\n0\n10\n0\n4\n0\n4\n2\n0\n0\n1\n1\n0\n2\n0\n2\n0\n0\n2\n0\n5\n6\n2\n0\n0\n4\n1\n0\n6\n0\n12\n9\n0\n1\n0\n0\n0\n\n\nWorthington\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n6\n0\n0\n0\n0\n0\n0\n\n\n\n\n\nEen deel van onze tabel (eenmaal ‘geknit’) is hierboven afgebeeld. Voor elke stad kunnen we duidelijk zien hoeveel van elk oriëntatiepunttype er in de database zitten:\nHoewel eenvoudige tellingen als deze heel nuttig kunnen zijn, geven we misschien meer om kolompercentages. Met andere woorden, hoeveel procent van de items voor elk oriëntatiepunttype zijn er in elke stad? Dit is gemakkelijk te onderzoeken met tabyl() via de adorn_percentages() functie:\n\nplace_names %&gt;% \n  tabyl(map, class) %&gt;% \n  adorn_percentages(\"col\") %&gt;% \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmap\nAirport\nArch\nBasin\nBay\nBench\nBridge\nBuilding\nCape\nCemetery\nCensus\nChurch\nCivil\nCliff\nCrossing\nDam\nFalls\nFlat\nForest\nGap\nHospital\nIsland\nLake\nLocale\nMilitary\nPark\nPopulated Place\nPost Office\nRange\nRapids\nReserve\nReservoir\nRidge\nSchool\nSpring\nStream\nSummit\nSwamp\nTower\nTrail\nValley\nWoods\n\n\n\n\nAshley Falls\n0.0909091\n0\n0.0000000\n0\n0\n0.4\n0.0341463\n0.0000000\n0.0649351\n0.000\n0.0090090\n0.0303030\n0.0\n0.0000000\n0.0204082\n0.0833333\n1\n0.0000\n0.0000000\n0.000\n0.00\n0.0000000\n0.0000000\n0\n0.0235294\n0.03750\n0.0326087\n0.00\n0\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n0.00\n0.0306513\n0.0421053\n0.0625\n0.0000000\n0.0000000\n0.0625\n0\n\n\nBash Bish Falls\n0.0000000\n0\n0.0000000\n0\n0\n0.0\n0.0000000\n0.0000000\n0.0129870\n0.000\n0.0090090\n0.0303030\n0.0\n0.0000000\n0.0510204\n0.2500000\n0\n0.0625\n0.0000000\n0.000\n0.25\n0.0769231\n0.0000000\n0\n0.0000000\n0.00625\n0.0108696\n0.00\n0\n0.0000000\n0.0431034\n0.0000000\n0.0143885\n0.00\n0.0306513\n0.0473684\n0.0625\n0.0833333\n0.1764706\n0.1250\n0\n\n\nBecket\n0.0000000\n0\n0.3333333\n0\n0\n0.0\n0.0390244\n0.0000000\n0.0129870\n0.000\n0.0090090\n0.0303030\n0.0\n0.0000000\n0.0816327\n0.0000000\n0\n0.1250\n0.0000000\n0.000\n0.00\n0.0256410\n0.0000000\n0\n0.0235294\n0.05000\n0.0108696\n0.00\n0\n0.0000000\n0.0603448\n0.0000000\n0.0287770\n0.00\n0.0498084\n0.0263158\n0.0000\n0.0000000\n0.0000000\n0.0000\n0\n\n\nBerlin\n0.0000000\n0\n0.0000000\n0\n0\n0.0\n0.0000000\n0.0000000\n0.0000000\n0.000\n0.0090090\n0.0000000\n0.0\n0.0000000\n0.0000000\n0.0000000\n0\n0.0625\n0.0000000\n0.000\n0.00\n0.0000000\n0.0303030\n0\n0.0117647\n0.00000\n0.0000000\n0.00\n0\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n0.25\n0.0076628\n0.0000000\n0.0000\n0.0000000\n0.0588235\n0.0625\n0\n\n\nBristol\n0.0000000\n0\n0.0000000\n0\n0\n0.0\n0.0000000\n0.0000000\n0.0000000\n0.000\n0.0000000\n0.0000000\n0.0\n0.0000000\n0.0000000\n0.0000000\n0\n0.0000\n0.0000000\n0.000\n0.00\n0.0000000\n0.0000000\n0\n0.0000000\n0.00000\n0.0000000\n0.00\n0\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n0.00\n0.0000000\n0.0000000\n0.0000\n0.0000000\n0.0588235\n0.0000\n0\n\n\nCanaan\n0.0000000\n0\n0.0000000\n0\n0\n0.0\n0.0000000\n0.0000000\n0.0000000\n0.000\n0.0000000\n0.0000000\n0.0\n0.0000000\n0.0000000\n0.0000000\n0\n0.0000\n0.0000000\n0.000\n0.00\n0.0000000\n0.0101010\n0\n0.0000000\n0.00000\n0.0000000\n0.00\n0\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n0.00\n0.0000000\n0.0052632\n0.0000\n0.0000000\n0.0000000\n0.0000\n0\n\n\nCheshire\n0.0000000\n0\n0.0000000\n0\n0\n0.0\n0.0585366\n0.0000000\n0.0649351\n0.125\n0.0360360\n0.0909091\n0.2\n0.0000000\n0.0204082\n0.0000000\n0\n0.0000\n0.0000000\n0.000\n0.25\n0.0512821\n0.0606061\n0\n0.0235294\n0.06875\n0.0326087\n0.00\n0\n0.0000000\n0.0172414\n0.0000000\n0.0215827\n0.00\n0.0536398\n0.0789474\n0.0000\n0.0000000\n0.0588235\n0.0000\n0\n\n\nChester\n0.0000000\n0\n0.0000000\n0\n0\n0.0\n0.0000000\n0.0000000\n0.0000000\n0.000\n0.0000000\n0.0000000\n0.0\n0.0000000\n0.0000000\n0.0000000\n0\n0.0000\n0.0000000\n0.000\n0.00\n0.0000000\n0.0000000\n0\n0.0000000\n0.00000\n0.0000000\n0.00\n0\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n0.00\n0.0038314\n0.0000000\n0.0000\n0.0000000\n0.0000000\n0.0000\n0\n\n\nCopake\n0.0000000\n0\n0.0000000\n0\n0\n0.0\n0.0000000\n0.0000000\n0.0000000\n0.000\n0.0000000\n0.0000000\n0.0\n0.0000000\n0.0000000\n0.0000000\n0\n0.0000\n0.0000000\n0.000\n0.00\n0.0000000\n0.0000000\n0\n0.0000000\n0.00000\n0.0000000\n0.00\n0\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n0.00\n0.0038314\n0.0052632\n0.0000\n0.0000000\n0.0000000\n0.0000\n0\n\n\nEast Lee\n0.0909091\n0\n0.0000000\n0\n0\n0.0\n0.0292683\n0.0000000\n0.0129870\n0.125\n0.0360360\n0.0909091\n0.0\n0.3333333\n0.0816327\n0.0000000\n0\n0.0625\n0.0000000\n0.000\n0.00\n0.0512821\n0.0303030\n0\n0.0000000\n0.03750\n0.0108696\n0.00\n0\n0.0000000\n0.1034483\n0.0000000\n0.0215827\n0.00\n0.0536398\n0.0263158\n0.0625\n0.0000000\n0.0000000\n0.0000\n0\n\n\nEgremont\n0.0909091\n0\n0.0000000\n0\n0\n0.0\n0.0390244\n0.0000000\n0.0389610\n0.000\n0.0090090\n0.0606061\n0.2\n0.0000000\n0.0306122\n0.0000000\n0\n0.0625\n0.0000000\n0.000\n0.00\n0.0512821\n0.0101010\n0\n0.0352941\n0.02500\n0.0108696\n0.00\n0\n0.0000000\n0.0431034\n0.0000000\n0.0215827\n0.00\n0.0229885\n0.0526316\n0.0000\n0.0416667\n0.0000000\n0.0000\n0\n\n\nGreat Barrington\n0.0000000\n0\n0.3333333\n0\n0\n0.0\n0.0292683\n0.0000000\n0.0389610\n0.250\n0.0540541\n0.0303030\n0.0\n0.0000000\n0.0714286\n0.0000000\n0\n0.1250\n0.3333333\n0.125\n0.00\n0.0769231\n0.0505051\n0\n0.0235294\n0.04375\n0.0108696\n0.00\n0\n0.0000000\n0.0775862\n0.0000000\n0.0575540\n0.25\n0.0153257\n0.0473684\n0.0625\n0.0416667\n0.0588235\n0.0625\n0\n\n\nHancock\n0.0000000\n0\n0.0000000\n0\n0\n0.0\n0.0146341\n0.0000000\n0.0259740\n0.000\n0.0000000\n0.0303030\n0.0\n0.0000000\n0.0000000\n0.0000000\n0\n0.0000\n0.0000000\n0.000\n0.00\n0.0128205\n0.0202020\n0\n0.0235294\n0.01250\n0.0108696\n0.00\n0\n0.0000000\n0.0000000\n0.1666667\n0.0071942\n0.00\n0.0306513\n0.0684211\n0.0000\n0.0000000\n0.0000000\n0.1875\n0\n\n\nMonterey\n0.0000000\n0\n0.0000000\n0\n0\n0.0\n0.0243902\n0.0000000\n0.0259740\n0.000\n0.0270270\n0.0606061\n0.0\n0.0000000\n0.1122449\n0.0000000\n0\n0.0000\n0.0000000\n0.000\n0.00\n0.0384615\n0.0404040\n0\n0.0117647\n0.03125\n0.0217391\n0.25\n0\n0.0000000\n0.1120690\n0.0000000\n0.0143885\n0.00\n0.0229885\n0.0684211\n0.3750\n0.0000000\n0.0000000\n0.0625\n0\n\n\nNorth Adams\n0.0000000\n1\n0.0000000\n0\n1\n0.0\n0.0926829\n0.0000000\n0.1168831\n0.125\n0.0630631\n0.1212121\n0.2\n0.0000000\n0.0714286\n0.0833333\n0\n0.0625\n0.1666667\n0.125\n0.00\n0.0769231\n0.0404040\n0\n0.1647059\n0.06875\n0.0108696\n0.25\n0\n0.0000000\n0.0689655\n0.0000000\n0.1079137\n0.00\n0.0842912\n0.0315789\n0.0625\n0.1250000\n0.0588235\n0.0000\n0\n\n\nOtis\n0.0000000\n0\n0.0000000\n1\n0\n0.0\n0.0195122\n0.3333333\n0.0259740\n0.000\n0.0000000\n0.0303030\n0.0\n0.0000000\n0.0918367\n0.0833333\n0\n0.0000\n0.0000000\n0.000\n0.50\n0.1282051\n0.0404040\n0\n0.0000000\n0.03750\n0.0217391\n0.00\n0\n0.0000000\n0.0948276\n0.1666667\n0.0000000\n0.00\n0.0306513\n0.0368421\n0.0000\n0.0000000\n0.0000000\n0.0000\n0\n\n\nPeru\n0.0000000\n0\n0.0000000\n0\n0\n0.0\n0.0243902\n0.0000000\n0.0259740\n0.000\n0.0000000\n0.0606061\n0.0\n0.0000000\n0.0408163\n0.0833333\n0\n0.0625\n0.0000000\n0.000\n0.00\n0.0384615\n0.0101010\n0\n0.0470588\n0.03750\n0.0000000\n0.00\n0\n0.3333333\n0.0344828\n0.0000000\n0.0071942\n0.00\n0.0574713\n0.0368421\n0.0000\n0.0000000\n0.0000000\n0.0000\n0\n\n\nPittsfield East\n0.0909091\n0\n0.0000000\n0\n0\n0.0\n0.1414634\n0.0000000\n0.0779221\n0.000\n0.3333333\n0.0303030\n0.0\n0.0000000\n0.0714286\n0.0000000\n0\n0.0000\n0.1666667\n0.250\n0.00\n0.0256410\n0.1919192\n0\n0.1882353\n0.13125\n0.6413043\n0.00\n0\n0.0000000\n0.1379310\n0.1666667\n0.2949640\n0.00\n0.0536398\n0.0473684\n0.0000\n0.0833333\n0.0000000\n0.0000\n0\n\n\nPittsfield West\n0.0909091\n0\n0.0000000\n0\n0\n0.0\n0.1170732\n0.3333333\n0.0909091\n0.000\n0.1711712\n0.0606061\n0.2\n0.0000000\n0.0204082\n0.0833333\n0\n0.0625\n0.0000000\n0.125\n0.00\n0.0641026\n0.1717172\n0\n0.1058824\n0.05625\n0.0108696\n0.25\n0\n0.0000000\n0.0258621\n0.1666667\n0.1294964\n0.00\n0.0957854\n0.0736842\n0.0625\n0.2916667\n0.1764706\n0.1250\n0\n\n\nPlainfield\n0.0000000\n0\n0.0000000\n0\n0\n0.0\n0.0000000\n0.0000000\n0.0259740\n0.000\n0.0000000\n0.0000000\n0.0\n0.0000000\n0.0000000\n0.0833333\n0\n0.0000\n0.0000000\n0.000\n0.00\n0.0000000\n0.0000000\n0\n0.0000000\n0.00625\n0.0000000\n0.00\n0\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n0.00\n0.0306513\n0.0052632\n0.0000\n0.0000000\n0.0000000\n0.0000\n0\n\n\nRowe\n0.0000000\n0\n0.0000000\n0\n0\n0.2\n0.0000000\n0.0000000\n0.0000000\n0.000\n0.0000000\n0.0000000\n0.0\n0.3333333\n0.0102041\n0.0000000\n0\n0.0000\n0.0000000\n0.000\n0.00\n0.0000000\n0.0303030\n0\n0.0117647\n0.01250\n0.0000000\n0.00\n0\n0.0000000\n0.0172414\n0.1666667\n0.0000000\n0.00\n0.0498084\n0.0105263\n0.0000\n0.0000000\n0.0000000\n0.0000\n0\n\n\nSouth Sandisfield\n0.0000000\n0\n0.0000000\n0\n0\n0.0\n0.0097561\n0.0000000\n0.0649351\n0.000\n0.0180180\n0.0303030\n0.0\n0.0000000\n0.0612245\n0.0833333\n0\n0.1250\n0.0000000\n0.000\n0.00\n0.0384615\n0.0101010\n0\n0.0000000\n0.03750\n0.0108696\n0.00\n0\n0.0000000\n0.0344828\n0.0000000\n0.0000000\n0.00\n0.0076628\n0.0368421\n0.1250\n0.0000000\n0.0000000\n0.0000\n0\n\n\nSouthbury\n0.0000000\n0\n0.0000000\n0\n0\n0.0\n0.0000000\n0.0000000\n0.0000000\n0.000\n0.0000000\n0.0000000\n0.0\n0.0000000\n0.0000000\n0.0000000\n0\n0.0000\n0.0000000\n0.000\n0.00\n0.0000000\n0.0000000\n0\n0.0117647\n0.00000\n0.0000000\n0.00\n0\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n0.00\n0.0000000\n0.0000000\n0.0000\n0.0000000\n0.0000000\n0.0000\n0\n\n\nState Line\n0.0000000\n0\n0.0000000\n0\n0\n0.0\n0.0000000\n0.0000000\n0.0519481\n0.000\n0.0000000\n0.0303030\n0.0\n0.0000000\n0.0102041\n0.0000000\n0\n0.0000\n0.0000000\n0.000\n0.00\n0.0769231\n0.0202020\n0\n0.0000000\n0.04375\n0.0108696\n0.00\n0\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n0.00\n0.0191571\n0.0210526\n0.0000\n0.0000000\n0.0000000\n0.0000\n0\n\n\nStockbridge\n0.0000000\n0\n0.0000000\n0\n0\n0.0\n0.1707317\n0.0000000\n0.0909091\n0.250\n0.0540541\n0.0606061\n0.0\n0.3333333\n0.0714286\n0.0000000\n0\n0.0000\n0.1666667\n0.125\n0.00\n0.1282051\n0.0707071\n0\n0.0823529\n0.11250\n0.0869565\n0.00\n0\n0.0000000\n0.0517241\n0.0000000\n0.1510791\n0.00\n0.0766284\n0.0526316\n0.0625\n0.1666667\n0.0000000\n0.0625\n1\n\n\nTolland Center\n0.0000000\n0\n0.0000000\n0\n0\n0.0\n0.0146341\n0.0000000\n0.0129870\n0.000\n0.0000000\n0.0303030\n0.0\n0.0000000\n0.0408163\n0.0000000\n0\n0.0000\n0.0000000\n0.000\n0.00\n0.0256410\n0.0101010\n0\n0.0000000\n0.01875\n0.0217391\n0.00\n0\n0.0000000\n0.0258621\n0.0000000\n0.0071942\n0.00\n0.0229885\n0.0315789\n0.0000\n0.0000000\n0.0000000\n0.0000\n0\n\n\nWilliamstown\n0.2727273\n0\n0.3333333\n0\n0\n0.4\n0.0926829\n0.3333333\n0.0519481\n0.125\n0.1261261\n0.0303030\n0.2\n0.0000000\n0.0306122\n0.0833333\n0\n0.0625\n0.1666667\n0.000\n0.00\n0.0128205\n0.1313131\n1\n0.1529412\n0.08125\n0.0108696\n0.25\n1\n0.0000000\n0.0431034\n0.1666667\n0.0719424\n0.50\n0.0766284\n0.1000000\n0.0625\n0.1250000\n0.3529412\n0.2500\n0\n\n\nWindsor\n0.1818182\n0\n0.0000000\n0\n0\n0.0\n0.0487805\n0.0000000\n0.0519481\n0.000\n0.0360360\n0.0606061\n0.0\n0.0000000\n0.0102041\n0.0833333\n0\n0.1250\n0.0000000\n0.250\n0.00\n0.0000000\n0.0202020\n0\n0.0588235\n0.03750\n0.0217391\n0.00\n0\n0.6666667\n0.0086207\n0.0000000\n0.0431655\n0.00\n0.0459770\n0.0473684\n0.0000\n0.0416667\n0.0000000\n0.0000\n0\n\n\nWorthington\n0.0909091\n0\n0.0000000\n0\n0\n0.0\n0.0000000\n0.0000000\n0.0129870\n0.000\n0.0000000\n0.0000000\n0.0\n0.0000000\n0.0000000\n0.0000000\n0\n0.0000\n0.0000000\n0.000\n0.00\n0.0000000\n0.0000000\n0\n0.0000000\n0.00625\n0.0000000\n0.00\n0\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n0.00\n0.0229885\n0.0000000\n0.0000\n0.0000000\n0.0000000\n0.0000\n0\n\n\n\n\n\nNu zien we deze kolompercentages in plaats van tellingen, maar de tabel is nogal moeilijk te lezen.\nWe kunnen dit een beetje opschonen met de adorn_pct_formatting() functie, die de gebruiker toestaat het aantal decimalen op te geven dat in de uitvoer moet worden opgenomen. Precisie is niet bijzonder belangrijk voor deze verkennende tabel, dus laten we 0 decimalen gebruiken om deze tabel makkelijker leesbaar te maken:\n\nplace_names %&gt;% \n  tabyl(map, class) %&gt;% \n  adorn_percentages(\"col\") %&gt;% \n  adorn_pct_formatting(digits = 0) %&gt;% \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmap\nAirport\nArch\nBasin\nBay\nBench\nBridge\nBuilding\nCape\nCemetery\nCensus\nChurch\nCivil\nCliff\nCrossing\nDam\nFalls\nFlat\nForest\nGap\nHospital\nIsland\nLake\nLocale\nMilitary\nPark\nPopulated Place\nPost Office\nRange\nRapids\nReserve\nReservoir\nRidge\nSchool\nSpring\nStream\nSummit\nSwamp\nTower\nTrail\nValley\nWoods\n\n\n\n\nAshley Falls\n9%\n0%\n0%\n0%\n0%\n40%\n3%\n0%\n6%\n0%\n1%\n3%\n0%\n0%\n2%\n8%\n100%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n2%\n4%\n3%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n3%\n4%\n6%\n0%\n0%\n6%\n0%\n\n\nBash Bish Falls\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n1%\n0%\n1%\n3%\n0%\n0%\n5%\n25%\n0%\n6%\n0%\n0%\n25%\n8%\n0%\n0%\n0%\n1%\n1%\n0%\n0%\n0%\n4%\n0%\n1%\n0%\n3%\n5%\n6%\n8%\n18%\n12%\n0%\n\n\nBecket\n0%\n0%\n33%\n0%\n0%\n0%\n4%\n0%\n1%\n0%\n1%\n3%\n0%\n0%\n8%\n0%\n0%\n12%\n0%\n0%\n0%\n3%\n0%\n0%\n2%\n5%\n1%\n0%\n0%\n0%\n6%\n0%\n3%\n0%\n5%\n3%\n0%\n0%\n0%\n0%\n0%\n\n\nBerlin\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n1%\n0%\n0%\n0%\n0%\n0%\n0%\n6%\n0%\n0%\n0%\n0%\n3%\n0%\n1%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n25%\n1%\n0%\n0%\n0%\n6%\n6%\n0%\n\n\nBristol\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n6%\n0%\n0%\n\n\nCanaan\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n1%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n1%\n0%\n0%\n0%\n0%\n0%\n\n\nCheshire\n0%\n0%\n0%\n0%\n0%\n0%\n6%\n0%\n6%\n12%\n4%\n9%\n20%\n0%\n2%\n0%\n0%\n0%\n0%\n0%\n25%\n5%\n6%\n0%\n2%\n7%\n3%\n0%\n0%\n0%\n2%\n0%\n2%\n0%\n5%\n8%\n0%\n0%\n6%\n0%\n0%\n\n\nChester\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n\n\nCopake\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n1%\n0%\n0%\n0%\n0%\n0%\n\n\nEast Lee\n9%\n0%\n0%\n0%\n0%\n0%\n3%\n0%\n1%\n12%\n4%\n9%\n0%\n33%\n8%\n0%\n0%\n6%\n0%\n0%\n0%\n5%\n3%\n0%\n0%\n4%\n1%\n0%\n0%\n0%\n10%\n0%\n2%\n0%\n5%\n3%\n6%\n0%\n0%\n0%\n0%\n\n\nEgremont\n9%\n0%\n0%\n0%\n0%\n0%\n4%\n0%\n4%\n0%\n1%\n6%\n20%\n0%\n3%\n0%\n0%\n6%\n0%\n0%\n0%\n5%\n1%\n0%\n4%\n2%\n1%\n0%\n0%\n0%\n4%\n0%\n2%\n0%\n2%\n5%\n0%\n4%\n0%\n0%\n0%\n\n\nGreat Barrington\n0%\n0%\n33%\n0%\n0%\n0%\n3%\n0%\n4%\n25%\n5%\n3%\n0%\n0%\n7%\n0%\n0%\n12%\n33%\n12%\n0%\n8%\n5%\n0%\n2%\n4%\n1%\n0%\n0%\n0%\n8%\n0%\n6%\n25%\n2%\n5%\n6%\n4%\n6%\n6%\n0%\n\n\nHancock\n0%\n0%\n0%\n0%\n0%\n0%\n1%\n0%\n3%\n0%\n0%\n3%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n1%\n2%\n0%\n2%\n1%\n1%\n0%\n0%\n0%\n0%\n17%\n1%\n0%\n3%\n7%\n0%\n0%\n0%\n19%\n0%\n\n\nMonterey\n0%\n0%\n0%\n0%\n0%\n0%\n2%\n0%\n3%\n0%\n3%\n6%\n0%\n0%\n11%\n0%\n0%\n0%\n0%\n0%\n0%\n4%\n4%\n0%\n1%\n3%\n2%\n25%\n0%\n0%\n11%\n0%\n1%\n0%\n2%\n7%\n38%\n0%\n0%\n6%\n0%\n\n\nNorth Adams\n0%\n100%\n0%\n0%\n100%\n0%\n9%\n0%\n12%\n12%\n6%\n12%\n20%\n0%\n7%\n8%\n0%\n6%\n17%\n12%\n0%\n8%\n4%\n0%\n16%\n7%\n1%\n25%\n0%\n0%\n7%\n0%\n11%\n0%\n8%\n3%\n6%\n12%\n6%\n0%\n0%\n\n\nOtis\n0%\n0%\n0%\n100%\n0%\n0%\n2%\n33%\n3%\n0%\n0%\n3%\n0%\n0%\n9%\n8%\n0%\n0%\n0%\n0%\n50%\n13%\n4%\n0%\n0%\n4%\n2%\n0%\n0%\n0%\n9%\n17%\n0%\n0%\n3%\n4%\n0%\n0%\n0%\n0%\n0%\n\n\nPeru\n0%\n0%\n0%\n0%\n0%\n0%\n2%\n0%\n3%\n0%\n0%\n6%\n0%\n0%\n4%\n8%\n0%\n6%\n0%\n0%\n0%\n4%\n1%\n0%\n5%\n4%\n0%\n0%\n0%\n33%\n3%\n0%\n1%\n0%\n6%\n4%\n0%\n0%\n0%\n0%\n0%\n\n\nPittsfield East\n9%\n0%\n0%\n0%\n0%\n0%\n14%\n0%\n8%\n0%\n33%\n3%\n0%\n0%\n7%\n0%\n0%\n0%\n17%\n25%\n0%\n3%\n19%\n0%\n19%\n13%\n64%\n0%\n0%\n0%\n14%\n17%\n29%\n0%\n5%\n5%\n0%\n8%\n0%\n0%\n0%\n\n\nPittsfield West\n9%\n0%\n0%\n0%\n0%\n0%\n12%\n33%\n9%\n0%\n17%\n6%\n20%\n0%\n2%\n8%\n0%\n6%\n0%\n12%\n0%\n6%\n17%\n0%\n11%\n6%\n1%\n25%\n0%\n0%\n3%\n17%\n13%\n0%\n10%\n7%\n6%\n29%\n18%\n12%\n0%\n\n\nPlainfield\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n3%\n0%\n0%\n0%\n0%\n0%\n0%\n8%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n1%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n3%\n1%\n0%\n0%\n0%\n0%\n0%\n\n\nRowe\n0%\n0%\n0%\n0%\n0%\n20%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n33%\n1%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n3%\n0%\n1%\n1%\n0%\n0%\n0%\n0%\n2%\n17%\n0%\n0%\n5%\n1%\n0%\n0%\n0%\n0%\n0%\n\n\nSouth Sandisfield\n0%\n0%\n0%\n0%\n0%\n0%\n1%\n0%\n6%\n0%\n2%\n3%\n0%\n0%\n6%\n8%\n0%\n12%\n0%\n0%\n0%\n4%\n1%\n0%\n0%\n4%\n1%\n0%\n0%\n0%\n3%\n0%\n0%\n0%\n1%\n4%\n12%\n0%\n0%\n0%\n0%\n\n\nSouthbury\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n1%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n\n\nState Line\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n5%\n0%\n0%\n3%\n0%\n0%\n1%\n0%\n0%\n0%\n0%\n0%\n0%\n8%\n2%\n0%\n0%\n4%\n1%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n2%\n2%\n0%\n0%\n0%\n0%\n0%\n\n\nStockbridge\n0%\n0%\n0%\n0%\n0%\n0%\n17%\n0%\n9%\n25%\n5%\n6%\n0%\n33%\n7%\n0%\n0%\n0%\n17%\n12%\n0%\n13%\n7%\n0%\n8%\n11%\n9%\n0%\n0%\n0%\n5%\n0%\n15%\n0%\n8%\n5%\n6%\n17%\n0%\n6%\n100%\n\n\nTolland Center\n0%\n0%\n0%\n0%\n0%\n0%\n1%\n0%\n1%\n0%\n0%\n3%\n0%\n0%\n4%\n0%\n0%\n0%\n0%\n0%\n0%\n3%\n1%\n0%\n0%\n2%\n2%\n0%\n0%\n0%\n3%\n0%\n1%\n0%\n2%\n3%\n0%\n0%\n0%\n0%\n0%\n\n\nWilliamstown\n27%\n0%\n33%\n0%\n0%\n40%\n9%\n33%\n5%\n12%\n13%\n3%\n20%\n0%\n3%\n8%\n0%\n6%\n17%\n0%\n0%\n1%\n13%\n100%\n15%\n8%\n1%\n25%\n100%\n0%\n4%\n17%\n7%\n50%\n8%\n10%\n6%\n12%\n35%\n25%\n0%\n\n\nWindsor\n18%\n0%\n0%\n0%\n0%\n0%\n5%\n0%\n5%\n0%\n4%\n6%\n0%\n0%\n1%\n8%\n0%\n12%\n0%\n25%\n0%\n0%\n2%\n0%\n6%\n4%\n2%\n0%\n0%\n67%\n1%\n0%\n4%\n0%\n5%\n5%\n0%\n4%\n0%\n0%\n0%\n\n\nWorthington\n9%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n1%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n1%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n2%\n0%\n0%\n0%\n0%\n0%\n0%\n\n\n\n\n\nVeel beter! Nu is het veel gemakkelijker om de tabel te lezen en onze kolompercentages te begrijpen:\nHet is net zo eenvoudig om adorn_percentages() te gebruiken om in plaats daarvan naar rij percentages te kijken (in ons geval, het percentage vermeldingen van elke stad dat behoort tot elk oriëntatiepunt type):\n\nplace_names %&gt;% \n  tabyl(map, class) %&gt;% \n  adorn_percentages(\"col\") %&gt;% \n  adorn_pct_formatting(digits = 0) %&gt;% \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmap\nAirport\nArch\nBasin\nBay\nBench\nBridge\nBuilding\nCape\nCemetery\nCensus\nChurch\nCivil\nCliff\nCrossing\nDam\nFalls\nFlat\nForest\nGap\nHospital\nIsland\nLake\nLocale\nMilitary\nPark\nPopulated Place\nPost Office\nRange\nRapids\nReserve\nReservoir\nRidge\nSchool\nSpring\nStream\nSummit\nSwamp\nTower\nTrail\nValley\nWoods\n\n\n\n\nAshley Falls\n9%\n0%\n0%\n0%\n0%\n40%\n3%\n0%\n6%\n0%\n1%\n3%\n0%\n0%\n2%\n8%\n100%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n2%\n4%\n3%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n3%\n4%\n6%\n0%\n0%\n6%\n0%\n\n\nBash Bish Falls\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n1%\n0%\n1%\n3%\n0%\n0%\n5%\n25%\n0%\n6%\n0%\n0%\n25%\n8%\n0%\n0%\n0%\n1%\n1%\n0%\n0%\n0%\n4%\n0%\n1%\n0%\n3%\n5%\n6%\n8%\n18%\n12%\n0%\n\n\nBecket\n0%\n0%\n33%\n0%\n0%\n0%\n4%\n0%\n1%\n0%\n1%\n3%\n0%\n0%\n8%\n0%\n0%\n12%\n0%\n0%\n0%\n3%\n0%\n0%\n2%\n5%\n1%\n0%\n0%\n0%\n6%\n0%\n3%\n0%\n5%\n3%\n0%\n0%\n0%\n0%\n0%\n\n\nBerlin\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n1%\n0%\n0%\n0%\n0%\n0%\n0%\n6%\n0%\n0%\n0%\n0%\n3%\n0%\n1%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n25%\n1%\n0%\n0%\n0%\n6%\n6%\n0%\n\n\nBristol\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n6%\n0%\n0%\n\n\nCanaan\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n1%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n1%\n0%\n0%\n0%\n0%\n0%\n\n\nCheshire\n0%\n0%\n0%\n0%\n0%\n0%\n6%\n0%\n6%\n12%\n4%\n9%\n20%\n0%\n2%\n0%\n0%\n0%\n0%\n0%\n25%\n5%\n6%\n0%\n2%\n7%\n3%\n0%\n0%\n0%\n2%\n0%\n2%\n0%\n5%\n8%\n0%\n0%\n6%\n0%\n0%\n\n\nChester\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n\n\nCopake\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n1%\n0%\n0%\n0%\n0%\n0%\n\n\nEast Lee\n9%\n0%\n0%\n0%\n0%\n0%\n3%\n0%\n1%\n12%\n4%\n9%\n0%\n33%\n8%\n0%\n0%\n6%\n0%\n0%\n0%\n5%\n3%\n0%\n0%\n4%\n1%\n0%\n0%\n0%\n10%\n0%\n2%\n0%\n5%\n3%\n6%\n0%\n0%\n0%\n0%\n\n\nEgremont\n9%\n0%\n0%\n0%\n0%\n0%\n4%\n0%\n4%\n0%\n1%\n6%\n20%\n0%\n3%\n0%\n0%\n6%\n0%\n0%\n0%\n5%\n1%\n0%\n4%\n2%\n1%\n0%\n0%\n0%\n4%\n0%\n2%\n0%\n2%\n5%\n0%\n4%\n0%\n0%\n0%\n\n\nGreat Barrington\n0%\n0%\n33%\n0%\n0%\n0%\n3%\n0%\n4%\n25%\n5%\n3%\n0%\n0%\n7%\n0%\n0%\n12%\n33%\n12%\n0%\n8%\n5%\n0%\n2%\n4%\n1%\n0%\n0%\n0%\n8%\n0%\n6%\n25%\n2%\n5%\n6%\n4%\n6%\n6%\n0%\n\n\nHancock\n0%\n0%\n0%\n0%\n0%\n0%\n1%\n0%\n3%\n0%\n0%\n3%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n1%\n2%\n0%\n2%\n1%\n1%\n0%\n0%\n0%\n0%\n17%\n1%\n0%\n3%\n7%\n0%\n0%\n0%\n19%\n0%\n\n\nMonterey\n0%\n0%\n0%\n0%\n0%\n0%\n2%\n0%\n3%\n0%\n3%\n6%\n0%\n0%\n11%\n0%\n0%\n0%\n0%\n0%\n0%\n4%\n4%\n0%\n1%\n3%\n2%\n25%\n0%\n0%\n11%\n0%\n1%\n0%\n2%\n7%\n38%\n0%\n0%\n6%\n0%\n\n\nNorth Adams\n0%\n100%\n0%\n0%\n100%\n0%\n9%\n0%\n12%\n12%\n6%\n12%\n20%\n0%\n7%\n8%\n0%\n6%\n17%\n12%\n0%\n8%\n4%\n0%\n16%\n7%\n1%\n25%\n0%\n0%\n7%\n0%\n11%\n0%\n8%\n3%\n6%\n12%\n6%\n0%\n0%\n\n\nOtis\n0%\n0%\n0%\n100%\n0%\n0%\n2%\n33%\n3%\n0%\n0%\n3%\n0%\n0%\n9%\n8%\n0%\n0%\n0%\n0%\n50%\n13%\n4%\n0%\n0%\n4%\n2%\n0%\n0%\n0%\n9%\n17%\n0%\n0%\n3%\n4%\n0%\n0%\n0%\n0%\n0%\n\n\nPeru\n0%\n0%\n0%\n0%\n0%\n0%\n2%\n0%\n3%\n0%\n0%\n6%\n0%\n0%\n4%\n8%\n0%\n6%\n0%\n0%\n0%\n4%\n1%\n0%\n5%\n4%\n0%\n0%\n0%\n33%\n3%\n0%\n1%\n0%\n6%\n4%\n0%\n0%\n0%\n0%\n0%\n\n\nPittsfield East\n9%\n0%\n0%\n0%\n0%\n0%\n14%\n0%\n8%\n0%\n33%\n3%\n0%\n0%\n7%\n0%\n0%\n0%\n17%\n25%\n0%\n3%\n19%\n0%\n19%\n13%\n64%\n0%\n0%\n0%\n14%\n17%\n29%\n0%\n5%\n5%\n0%\n8%\n0%\n0%\n0%\n\n\nPittsfield West\n9%\n0%\n0%\n0%\n0%\n0%\n12%\n33%\n9%\n0%\n17%\n6%\n20%\n0%\n2%\n8%\n0%\n6%\n0%\n12%\n0%\n6%\n17%\n0%\n11%\n6%\n1%\n25%\n0%\n0%\n3%\n17%\n13%\n0%\n10%\n7%\n6%\n29%\n18%\n12%\n0%\n\n\nPlainfield\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n3%\n0%\n0%\n0%\n0%\n0%\n0%\n8%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n1%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n3%\n1%\n0%\n0%\n0%\n0%\n0%\n\n\nRowe\n0%\n0%\n0%\n0%\n0%\n20%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n33%\n1%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n3%\n0%\n1%\n1%\n0%\n0%\n0%\n0%\n2%\n17%\n0%\n0%\n5%\n1%\n0%\n0%\n0%\n0%\n0%\n\n\nSouth Sandisfield\n0%\n0%\n0%\n0%\n0%\n0%\n1%\n0%\n6%\n0%\n2%\n3%\n0%\n0%\n6%\n8%\n0%\n12%\n0%\n0%\n0%\n4%\n1%\n0%\n0%\n4%\n1%\n0%\n0%\n0%\n3%\n0%\n0%\n0%\n1%\n4%\n12%\n0%\n0%\n0%\n0%\n\n\nSouthbury\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n1%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n\n\nState Line\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n5%\n0%\n0%\n3%\n0%\n0%\n1%\n0%\n0%\n0%\n0%\n0%\n0%\n8%\n2%\n0%\n0%\n4%\n1%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n2%\n2%\n0%\n0%\n0%\n0%\n0%\n\n\nStockbridge\n0%\n0%\n0%\n0%\n0%\n0%\n17%\n0%\n9%\n25%\n5%\n6%\n0%\n33%\n7%\n0%\n0%\n0%\n17%\n12%\n0%\n13%\n7%\n0%\n8%\n11%\n9%\n0%\n0%\n0%\n5%\n0%\n15%\n0%\n8%\n5%\n6%\n17%\n0%\n6%\n100%\n\n\nTolland Center\n0%\n0%\n0%\n0%\n0%\n0%\n1%\n0%\n1%\n0%\n0%\n3%\n0%\n0%\n4%\n0%\n0%\n0%\n0%\n0%\n0%\n3%\n1%\n0%\n0%\n2%\n2%\n0%\n0%\n0%\n3%\n0%\n1%\n0%\n2%\n3%\n0%\n0%\n0%\n0%\n0%\n\n\nWilliamstown\n27%\n0%\n33%\n0%\n0%\n40%\n9%\n33%\n5%\n12%\n13%\n3%\n20%\n0%\n3%\n8%\n0%\n6%\n17%\n0%\n0%\n1%\n13%\n100%\n15%\n8%\n1%\n25%\n100%\n0%\n4%\n17%\n7%\n50%\n8%\n10%\n6%\n12%\n35%\n25%\n0%\n\n\nWindsor\n18%\n0%\n0%\n0%\n0%\n0%\n5%\n0%\n5%\n0%\n4%\n6%\n0%\n0%\n1%\n8%\n0%\n12%\n0%\n25%\n0%\n0%\n2%\n0%\n6%\n4%\n2%\n0%\n0%\n67%\n1%\n0%\n4%\n0%\n5%\n5%\n0%\n4%\n0%\n0%\n0%\n\n\nWorthington\n9%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n1%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n1%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n0%\n2%\n0%\n0%\n0%\n0%\n0%\n0%"
  },
  {
    "objectID": "posts/2021-10-03-ontrafelen/ontrafelen.html#andere-functies",
    "href": "posts/2021-10-03-ontrafelen/ontrafelen.html#andere-functies",
    "title": "Opschonen en ontrafelen",
    "section": "Andere Functies",
    "text": "Andere Functies\nIn dit blog zijn de functies uit het janitor-pakket beschreven die nuttig zijn voor het dagelijkse werk. Dit is echter geen uitputtende lijst van janitor functies en ik raad aan om de documentatie te raadplegen voor meer informatie over dit pakket.\nEr zijn nog een paar andere functies die op zijn minst de moeite waard zijn om hier te vermelden: excel_numeric_to_date(): Deze functie is ontworpen om veel van Excel’s datum formaten te verwerken en om deze numerieke variabelen om te zetten naar datum variabelen. Het lijkt een grote tijdsbesparing voor diegenen die vaak met gegevens in Excel werken. Als niet frequent gebruiker van Excel, vertrouw ik in plaats daarvan zwaar op het lubridate pakket voor het werken met datum variabelen.\nround_to_fraction(): Met deze functie kun je decimale getallen afronden naar een precieze breuknoemer. Wil je al je waarden afgerond hebben naar het dichtstbijzijnde kwartier, of gebruik je decimalen om minuten in een uur weer te geven? Dan kan de round_to_fraction()-functie jou waarschijnlijk helpen.\ntop_levels(): Deze functie genereert een frequentietabel die een categorische variabele samenbrengt in hoge, middelste en lage niveaus. Veelgebruikte gevallen zijn onder andere het vereenvoudigen van Likert-achtige schalen."
  },
  {
    "objectID": "posts/2021-10-03-ontrafelen/ontrafelen.html#conclusie",
    "href": "posts/2021-10-03-ontrafelen/ontrafelen.html#conclusie",
    "title": "Opschonen en ontrafelen",
    "section": "Conclusie",
    "text": "Conclusie\nHet is op dit punt algemeen bekend dat de meeste dataanalisten en -wetenschappers het grootste deel van hun tijd besteden aan het opschonen en verkennen van gegevens. Daarom is het goed om nieuwe pakketten en functies te ontdekken die deze processen een beetje efficiënter maken.\nOf je het janitor-pakket nu wel of niet eerder hebt gebruikt, ik hoop dat deze blog jouw kennis heeft laten maken met enkele functies die nuttige toevoegingen zullen blijken te zijn aan je datawetenschapsgereedschapskist."
  },
  {
    "objectID": "posts/2022-09-20-wijken-plotten/quarto.html",
    "href": "posts/2022-09-20-wijken-plotten/quarto.html",
    "title": "Werken met Quarto",
    "section": "",
    "text": "10 Jaar geleden werd het pakket RMarkdown geïntroduceerd als onderdeel van het knitr pakket door Yihui Xie van RStudio. Met dat prachtige pakket kun jezelf tekst, analyse en designopdrachten doorvoeren in documenten, rapporten, boerken, blogs en websites, presentaties en interactieve documenten, en dat in allerlei outputformaten. Het is een van die pakketten waar ik afgelopen jaren het meest gebruik van heb gemaakt en waar ik als onderzoeker het meest enthousiast over was (Xie, et al., 2022; Xie et al., 2021). Ikzelf was helemaal niet toe aan iets nieuws op dit gebied want deed voldeed heel goed. Toch verscheen onlangs Quarto dat een vergelijkbaar doel dient als RMarkdown.\nQuarto is een open-source publicatiesysteem waarmee je wetenschappelijke en technische producten kunt maken. Het is gebouwd op Pandoc, het Zwitsers mes dat tekst en code naar documenten, webpagina’s, blog post, boeken en nog veel meer kan omzetten. Quarto is opgenomen in de nieuwste versie van RStudio en je hoeft geen andere pakketten meer te laden. Alles werkt op eenzelfde manier. Het werkt nu niet meer alleen met R, zoals RMarkdown dat wel doet, maar ook met Jupyter van Python en Observable dat ikzelf verder niet ken.Met Quarto wil RStudio een RMarkdown voor iedereen maken.\nHet Quarto systeem kan vaak hetzelfde als je met RMarkdown kunt doen. Om die reden is er niet echt een reden om over te stappen. Er zijn wel enkele makkelijke functies voor het gebruik toegevoegd. Bovendien is het pakket net geïntroduceerd en zullen de mogelijkheden alleen maar sterk toenemen als meer mensen ermee gaan werken (Quarto, 2022).\nIkzelf moest er eerst wat aan wennen en zag amper voordelen van het systeem. Echter, na enige tijd werden de voordelen voor mij wel steeds duidelijker.\nIk wilde mij de afgelopen weken het werken met Quarto wat eigen maken. Ik begon met het maken van enkele eenvoudige documenten. Vervolgens heb ik de mooie presentatie van Maghan Hall over tidyversegenomen en heb deze vertaald en bewerkt (Hall, 2022). Daarna heb ik een eenvoudig boekje gemaakt van een wat ouder rapport dat op mijn voormalig instituut was gemaakt. Toen ik mij deze vaardigheden had eigen gemaakt, heb ik twee blogs genomen die zelf eerder gemaakt had met RMarkdown. De resultaten (output en code) kun je hier onder vinden."
  },
  {
    "objectID": "posts/2022-09-20-wijken-plotten/quarto.html#blogs",
    "href": "posts/2022-09-20-wijken-plotten/quarto.html#blogs",
    "title": "Werken met Quarto",
    "section": "Blogs",
    "text": "Blogs\nEigen blog\nHoe het is gemaakt vind je op github Eigen blog Github\nNSCR blog\nHoe het is gemaakt vind je github NSCR blog Github"
  },
  {
    "objectID": "posts/2022-09-20-wijken-plotten/quarto.html#boeken",
    "href": "posts/2022-09-20-wijken-plotten/quarto.html#boeken",
    "title": "Werken met Quarto",
    "section": "Boeken",
    "text": "Boeken\nQuartoboek"
  },
  {
    "objectID": "posts/2022-09-20-wijken-plotten/quarto.html#presentatie",
    "href": "posts/2022-09-20-wijken-plotten/quarto.html#presentatie",
    "title": "Werken met Quarto",
    "section": "Presentatie",
    "text": "Presentatie\nPresentatie"
  },
  {
    "objectID": "posts/2022-10-07-quarto/wijken.html",
    "href": "posts/2022-10-07-quarto/wijken.html",
    "title": "Wijken plotten",
    "section": "",
    "text": "Het sf pakket\nHet sf pakket is een sleutel tot het eenvoudig maken van kaarten met behulp van ggplot2. De naam van het pakket is afgeleid van simple features, een gestandaardiseerde manier om ruimtelijke vectorgegevens te coderen.\n\n\nread_sf gebruiken om een GeoJSON bestand in te lezen\nDe code hieronder gebruikt sf::read_sf om vector data in te lezen in GeoJSON formaat. Dit GeoJSON bestand is afkomstig van de Chicago Data Portal en bevat de grenzen van 77 geïdentificeerde communities in de stad Chicago, Illinois.\n\nlibrary(sf)\n\nWarning: package 'sf' was built under R version 4.1.3\n\n\nLinking to GEOS 3.9.1, GDAL 3.2.1, PROJ 7.2.1; sf_use_s2() is TRUE\n\nchi_map <- read_sf(\"https://raw.githubusercontent.com/thisisdaryn/data/master/geo/chicago/Comm_Areas.geojson\") \n\nEen blik op het chi_map object laat zien dat het lijkt op een typisch dataframe, misschien met een paar uitzonderingen: er is een begrenzingskader, bbox, wat tekst, proj4string die het gebruikte project aangeeft, en een geometry variabele die lengte- en breedtecoördinaten lijkt te bevatten. (De schermafbeelding hieronder is het resultaat van het uitvoeren van de head functie met chi_map als invoer op mijn eigen machine).\n\nhead(chi_map)\n\nSimple feature collection with 6 features and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -87.7069 ymin: 41.79448 xmax: -87.58001 ymax: 41.99076\nGeodetic CRS:  WGS 84\n# A tibble: 6 x 10\n  community       area  shape_~1 perim~2 area_~3 area_~4 comar~5 comarea shape~6\n  <chr>           <chr> <chr>    <chr>   <chr>   <chr>   <chr>   <chr>   <chr>  \n1 DOUGLAS         0     4600462~ 0       35      35      0       0       31027.~\n2 OAKLAND         0     1691396~ 0       36      36      0       0       19565.~\n3 FULLER PARK     0     1991670~ 0       37      37      0       0       25339.~\n4 GRAND BOULEVARD 0     4849250~ 0       38      38      0       0       28196.~\n5 KENWOOD         0     2907174~ 0       39      39      0       0       23325.~\n6 LINCOLN SQUARE  0     7135232~ 0       4       4       0       0       36624.~\n# ... with 1 more variable: geometry <MULTIPOLYGON [°]>, and abbreviated\n#   variable names 1: shape_area, 2: perimeter, 3: area_num_1, 4: area_numbe,\n#   5: comarea_id, 6: shape_len\n\n\nHier zien we de top van het sf-object ingelezen uit een GeoJSON bestand.\n\n\nEen opmerking over het gebruik van shapefiles\nAls alternatief hadden wij de gegevens kunnen inlezen van een shapefile die de equivalente geospatiale informatie bevat. Shapefiles zijn een industrie-standaard, zeer algemeen bestandsformaat, en het is aannemelijk dat dit het formaat is waarmee de gegevens waarmee je wilt werken het gemakkelijkst gedistribueerd worden.\nDe github repository bevat ook een shapefile directory, Comm_Areas_shp met gelijkwaardige geospatiale informatie die ook verkregen werd via de Chicago Data Portal.\n\n\nGebruik van ggplot2 met geom_sf\nDe sleutel tot het gebruik van ggplot2 om kaarten te maken met sf objecten is dat ze ook dataframes zijn en dus in principe klaar om gebruikt te worden als data voor ggplot2::ggplot.\n\n\nEerste kaart met geom_sf\nWe kunnen een eerste kaart maken door ons kaart-dataframe te gebruiken als data-input voor ggplot2::ggplot en door gebruik te maken van een speciale geometrie, ggplot2::geom_sf:\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\nggplot(data = chi_map) + geom_sf()\n\n\n\n\n\n\nLabels toevoegen met geom_sf_text\nEen andere handige geometrie die ons in staat stelt om vrij eenvoudig informatie aan kaarten toe te voegen is ggplot2::geom_sf_text. In de onderstaande code gebruiken we deze geometrie om het aantal identificatienummers van elk gebied in de Chicago gemeenschap toe te voegen aan de eenvoudige kaart.\n\nggplot(data = chi_map) + \n  geom_sf() + \n  geom_sf_text(aes(label = area_num_1))\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n\n\n\n\n\n\nHet thema van een kaart veranderen\nWe kunnen het thema van een kaart veranderen, net als met elke andere ggplot2 grafiek. Bijvoorbeeld, hier is de vorige kaart met een extra oproep aan ggplot2::theme_bw om een zwart-wit thema te krijgen.\n\nggplot(data = chi_map) + \n  geom_sf() + \n  geom_sf_text(aes(label = area_num_1)) +\n  theme_bw()\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n\n\n\n\n\n\nAndere gegevens verkrijgen\nOm chloropleth-kaarten te maken, moeten we andere informatie verkrijgen die overeenstemt met de geografische gebieden die in de kaart worden afgebakend.\nChicago Open Data heeft een dataset van Publieke Gezondheidsstatistiek voor communities. Dit is een goede databron om te gebruiken om chloropleth kaarten te maken in combinatie met de geospatiale gegevens.\n\nlibrary(readr)\n\nWarning: package 'readr' was built under R version 4.1.3\n\nchi_health <- read_csv(\"https://raw.githubusercontent.com/thisisdaryn/data/master/geo/chicago/Chicago_Health_Statistics.csv\") \n\nRows: 77 Columns: 29\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr  (2): Community Area Name, Gonorrhea in Males\ndbl (27): Community Area, Birth Rate, General Fertility Rate, Low Birth Weig...\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(chi_health)\n\n# A tibble: 6 x 29\n  Community Ar~1 Commu~2 Birth~3 Gener~4 Low B~5 Prena~6 Prete~7 Teen ~8 Assau~9\n           <dbl> <chr>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1              1 Rogers~    16.4    62      11      73      11.2    40.8     7.7\n2              2 West R~    17.3    83.3     8.1    71.1     8.3    29.9     5.8\n3              3 Uptown     13.1    50.5     8.3    77.7    10.3    35.1     5.4\n4              4 Lincol~    17.1    61       8.1    80.5     9.7    38.4     5  \n5              5 North ~    22.4    76.2     9.1    80.4     9.8     8.4     1  \n6              6 Lake V~    13.5    38.7     6.3    79.1     8.1    15.8     1.4\n# ... with 20 more variables: `Breast cancer in females` <dbl>,\n#   `Cancer (All Sites)` <dbl>, `Colorectal Cancer` <dbl>,\n#   `Diabetes-related` <dbl>, `Firearm-related` <dbl>,\n#   `Infant Mortality Rate` <dbl>, `Lung Cancer` <dbl>,\n#   `Prostate Cancer in Males` <dbl>, `Stroke (Cerebrovascular Disease)` <dbl>,\n#   `Childhood Blood Lead Level Screening` <dbl>,\n#   `Childhood Lead Poisoning` <dbl>, `Gonorrhea in Females` <dbl>, ...\n\n\nIn dit bestand zijn de identificaties van communities numerieke variabelen. Om de twee databestanden (chi_map en Chi_health) te kunnen samenvoegen, moet een nieuwe kolom, area_num_1, worden aangemaakt als tekst/karakterdata. (Door dezelfde naam, area_num_1, te gebruiken die reeds in het chi_map data frame staat, wordt het samenvoegen bijzonder gemakkelijk.).\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nchi_health <- chi_health %>% \n  mutate(area_num_1 = as.character(`Community Area`))\n\n\n\nSamenvoegen van een sf-object met een ander dataframe\nVervolgens, gebruik dplyr::left_join om chi_map te verbinden met chi_health:\n\nchi_health_map <- left_join(chi_map, chi_health, by = \"area_num_1\")\n\nDit creëert een enkel dataframe met de geografische grenzen en de waarden voor de gemeten variabelen.\n\n\nEen chloropleth-kaart maken\nOm een chloropleth-kaart te maken met behulp van een van de statistieken in het samengevoegde dataframe, kunnen we de fill aesthetic gebruiken. Hier gebruiken we de kolom “Unemployment” van de samengevoegde gegevens:\n\nchi_health_map_nl<-chi_health_map %>%\n  rename(Werkloosheid=Unemployment) \n\n\nggplot(data = chi_health_map_nl, aes(fill = Werkloosheid)) + \n  geom_sf() + \n  ggtitle(\"Werkloosheidspercentages in de communities van Chicago\")\n\n\n\n\n\n\nDe schaal veranderen\nDe vorige kaart gebruikt de standaard continue schaal voor de vulling esthetiek. Merk op dat we dezelfde kaart kunnen maken als voorheen door scale_fill_continuous te gebruiken:\n\nggplot(data = chi_health_map_nl, aes(fill = Werkloosheid)) +\n  geom_sf() + \n  scale_fill_continuous() + \n  ggtitle(\"Werkloosheidpercentages in communities van Chicago\")\n\n\n\n\n\n\nGebruik van viridis scale\nWe kunnen viridis scale als alternatief gebruiken die ook beschikbaar is voor scale_fill_continuous:\n\nggplot(data = chi_health_map_nl, aes(fill = Werkloosheid)) +\n  geom_sf() + \n  scale_fill_continuous(type = \"viridis\") + \n  ggtitle(\"Werkloosheidsperscentages in communities van Chicago\")\n\n\n\n\n\n\nContinue schaal met de hand instellen\nEen andere optie is met de hand instellen van laag and hoog argumenten voor scale_fill_continuous.\n\nggplot(data = chi_health_map_nl, aes(fill = Werkloosheid)) +\n  geom_sf() + \n  scale_fill_continuous(low = \"ivory\", high = \"brown\") + \n  ggtitle(\"Werkloosheidspercentages in communities van Chicago\")\n\n\n\n\n\n\nEen divergente schaal gebruiken\nMisschien wil je wel een divergente schaal gebruiken:\nOm een divergerende - maar nog steeds continue schaal te krijgen - kun je ggplot2::scale_fill_gradient2 gebruiken. Om deze schaal te gebruiken, stelt je kleuren in voor de argumenten low, high en mid. Het kan zijn dat u ook het midpoint argument moet instellen (dat anders standaard op 0 zou worden gezet).\n\nggplot(data = chi_health_map_nl, aes(fill = Werkloosheid)) +\n  geom_sf() + \n  scale_fill_gradient2(low = \"blue\", mid = \"white\", high = \"red\", midpoint = 20) +\n  ggtitle(\"Werkloosheidspercentages in communities van Chicago\")\n\n\n\n\n\n\nGebruik van een discrete schaal\nTot nu toe hebben we continue schalen gebruikt om de communities van de chloroplethkaarten in te kleuren. Dit is een gevolg van het gebruik van een numerieke variabele als onze fill aes. Als alternatief kunnen we een discrete schaal gebruiken door de fill toe te wijzen aan een categorische variabele.\n\n\nEen nieuwe categorische variabele creëren\nMaak eerst een nieuwe categorische variabele voor Werkloosheid - of een andere variabele in de gegevens van uw keuze. De code gebruikt dplyr::case_when om de variabele Werkloosheid in bereiken te verdelen:\n\nchi_health_map_nl <- chi_health_map_nl %>% \n  mutate(Werkl_cat = case_when(\n    Werkloosheid <= 10 ~ \"0 - 10\",\n    10 < Werkloosheid & Werkloosheid <= 20 ~ \"10+ - 20\",\n    20 < Werkloosheid & Werkloosheid <= 30 ~ \"20+ - 30\",\n    30 < Werkloosheid  ~ \"30+\"))\n\nNow, plot the new map:\n\nggplot(data = chi_health_map_nl) + \n  geom_sf(aes(fill = Werkl_cat))  +\n  labs(fill = \"Werkloosheid (%)\") + \n  ggtitle(\"Werkloosheidspercentages in communities van Chicago\")\n\n\n\n\n\n\nPunten plotten op de kaart\nOm een voorbeeld te geven van het plotten van punten op een kaart, kunnen we gebruik maken van Chicago Restaurant Inspectie Data voor februari 2020. Lees eerst de gegevens in:\n\ninspections <- read_csv(\"https://raw.githubusercontent.com/thisisdaryn/data/master/geo/chicago/Food_Inspections.csv\")\n\nRows: 950 Columns: 17\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (12): DBA Name, AKA Name, Facility Type, Risk, Address, City, State, Ins...\ndbl  (5): Inspection ID, License #, Zip, Latitude, Longitude\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nDeze dataset heeft de volgende variabelen: - Inspection\n- ID,\n- DBA\n- Name,\n- AKA,\n- Name,\n- License #,\n- Facility\n- Type,\n- Risk,\n- Address,\n- City,\n- State,\n- Zip,\n- Inspection Date,\n- Inspection\n- Type,\n- Results,\n- Violations,\n- Latitude,\n- Longitude, and\n- Location.\nNu kunnen we elk geïnspecteerd restaurant in de tijdsperiode op de kaart plaatsen door de x en y aes te koppelen aan de Longitude en Latitude variabelen in het dataframe. (Bovendien, de code hieronder mapt de kleur aes aan de Results variabele).\n\nggplot() + \n  geom_sf(data = chi_map, fill = \"ivory\", colour = \"ivory\") + \n  geom_point(data = inspections, aes(x = Longitude, y = Latitude, colour = Results)) +\n  scale_color_viridis_d()\n\nWarning: Removed 4 rows containing missing values (geom_point).\n\n\n\n\n\n\n\nInteractieve kaarten met plotly\nHet maken van interactieve kaarten met plotly kan ook relatief eenvoudig zijn. In de onderstaande code voeren we de volgende stappen uit:\n\nMaak een nieuwe variabele, desc met daarin de Restaurantnaam, de inspectiedatum en het resultaat van de inspectie aan de hand van de corresponderende variabelen in de gegevens.\n\nMaak een kaart met een uniforme achtergrondkleur met punten voor elke restaurantinspectie\n\nMaak een interactieve grafiek met plotly::ggplotly met de desc variabele als tooltip\n\n\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\ninspections <- inspections %>% mutate(desc = paste(`AKA Name`, `Inspection Date`, Results, sep = \"\\n\"))\n\ninsp_plt <- ggplot() + \n  geom_sf(data = chi_map, fill = \"ivory\", colour = \"ivory\") + \n  geom_point(data = inspections, \n             aes(x = Longitude, y = Latitude, colour = Results, text = desc)) +\n  scale_color_viridis_d()\n\nWarning: Ignoring unknown aesthetics: text\n\nggplotly(insp_plt, tooltip = \"desc\")"
  },
  {
    "objectID": "posts/2022-09-20-wijken-plotten/quarto.html#harries-hoekje",
    "href": "posts/2022-09-20-wijken-plotten/quarto.html#harries-hoekje",
    "title": "Werken met Quarto",
    "section": "Harrie’s Hoekje",
    "text": "Harrie’s Hoekje\nIk blog al wat langer over ontwikkelingen op het terrein van moderne data analyse die mij interesseren. Sinds 2018 schrijf ik het blog Harrie’s Hoekje, waar deze post onderdeel van vormtHier vind je Harrie’s Hoekje. Dat blog is ondertussen qua omvang zo groot geworden dat het werken ermee steeds langdurige wordt. Het lijkt mij goed om een nieuwe versie te starten zodat het makkelijk blijft om ermee te werken. De blogs van de afgelopen twee jaar van Harrie’s Hoekje heb ik naar Quarto overgezet. Binnenkort zal ik de oude RMarkdown versie afsluiten en nieuwe blogs via Quarto publiceren. Het raamwerk staat nu.\nIk heb een eenvoudige opzet binnengehaald en vandaar heb ik mijn eigen blog opgebouwd. Ik heb de post die ik RMardown had geschreven naar Quatro overgehaald. Het draait dan meteen ook, maar ik heb alle syntaxen (mn. de chuncks) waar nodig aangepast en steeds opgeslagen niet als .rmd documenten maar als .qmd documenten. Er waren enkele blogs die mij hierbij hebben geholpen: Bea Milz (2022) en Alber Rapp (2022).\nVia deze link kun je het resultaat zien van Harrie’s Hoekje blog in Quarto\nHoe ik het heb gemaakt vind je op github Eigen blog Github"
  },
  {
    "objectID": "posts/2022-09-20-wijken-plotten/quarto.html#nsc-r-workshops",
    "href": "posts/2022-09-20-wijken-plotten/quarto.html#nsc-r-workshops",
    "title": "Werken met Quarto",
    "section": "NSC-R workshops",
    "text": "NSC-R workshops\nPrecies een jaar geleden heb ik het ontwerp voor de NSC-Rworkshops gemaakt. Dat blog vind je hier NSC-R Workshops.\nVervolgens heb ik ook dit blog als basis genomen en omgezet naar Quarto. Ook dit was een tijdsintensieve klus en ik heb dingen handmatige gedaan die allicht eenvoudiger te doen waren. Echter, door er zo de tijd voor te nemen, kreeg ik het systeem beter onder de knie. Het voorlopige resultaat vind je hier en zal allicht op basis van commentaren nog worden aangepast: NSCR blog\nHoe het is gemaakt vind je github NSCR blog Github"
  },
  {
    "objectID": "posts/2022-09-20-wijken-plotten/quarto.html#referenties",
    "href": "posts/2022-09-20-wijken-plotten/quarto.html#referenties",
    "title": "Werken met Quarto",
    "section": "Referenties",
    "text": "Referenties\nCentinkaya-Rundel, M. (2022) A Quarto tip a day here\nHall, M. (2022). Making Slides in Quarto with reveal.js. here\nMilz, B. (2022). Creating a blog with Quarto in 10 steps. here.\nRapp, A. (2022). The ultimate guide to starting a Quarto blog. here\nQuarto (2022). Website. here\nXie, Y., Allaire, J.J., Grolemund, G. (2022). RMarkdown. The Definitive Guide. CTC Press. Boca Raton: CRC Press here.\nXie, Y., Dervieux, C. , Riederer, E. (2021). RMarkdown Cookboek. Boca Raton: CRC Press."
  },
  {
    "objectID": "posts/2022-10-07-quarto/quarto.html#harries-hoekje",
    "href": "posts/2022-10-07-quarto/quarto.html#harries-hoekje",
    "title": "Werken met Quarto",
    "section": "Harrie’s Hoekje",
    "text": "Harrie’s Hoekje\nIk blog al wat langer over ontwikkelingen op het terrein van moderne data analyse die mij interesseren. Sinds 2018 schrijf ik het blog Harrie’s Hoekje, waar deze post onderdeel van vormtHier vind je Harrie’s Hoekje. Dat blog is ondertussen qua omvang zo groot geworden dat het werken ermee steeds langdurige wordt. Het lijkt mij goed om een nieuwe versie te starten zodat het makkelijk blijft om ermee te werken. De blogs van de afgelopen twee jaar van Harrie’s Hoekje heb ik naar Quarto overgezet. Binnenkort zal ik de oude RMarkdown versie afsluiten en nieuwe blogs via Quarto publiceren. Het raamwerk staat nu.\nIk heb een eenvoudige opzet binnengehaald en vandaar heb ik mijn eigen blog opgebouwd. Ik heb de post die ik RMardown had geschreven naar Quatro overgehaald. Het draait dan meteen ook, maar ik heb alle syntaxen (mn. de chuncks) waar nodig aangepast en steeds opgeslagen niet als .rmd documenten maar als .qmd documenten. Er waren enkele blogs die mij hierbij hebben geholpen: Bea Milz (2022) en Alber Rapp (2022).\nVia deze link kun je het resultaat zien van Harrie’s Hoekje blog in Quarto\nHoe ik het heb gemaakt vind je op github Eigen blog Github"
  },
  {
    "objectID": "posts/2022-10-07-quarto/quarto.html#nsc-r-workshops",
    "href": "posts/2022-10-07-quarto/quarto.html#nsc-r-workshops",
    "title": "Werken met Quarto",
    "section": "NSC-R workshops",
    "text": "NSC-R workshops\nPrecies een jaar geleden heb ik het ontwerp voor de NSC-Rworkshops gemaakt. Dat blog vind je hier NSC-R Workshops.\nVervolgens heb ik ook dit blog als basis genomen en omgezet naar Quarto. Ook dit was een tijdsintensieve klus en ik heb dingen handmatige gedaan die allicht eenvoudiger te doen waren. Echter, door er zo de tijd voor te nemen, kreeg ik het systeem beter onder de knie. Het voorlopige resultaat vind je hier en zal allicht op basis van commentaren nog worden aangepast: NSCR blog\nHoe het is gemaakt vind je github NSCR blog Github"
  },
  {
    "objectID": "posts/2022-10-07-quarto/quarto.html#referenties",
    "href": "posts/2022-10-07-quarto/quarto.html#referenties",
    "title": "Werken met Quarto",
    "section": "Referenties",
    "text": "Referenties\nCentinkaya-Rundel, M. (2022) A Quarto tip a day here\nHall, M. (2022). Making Slides in Quarto with reveal.js. here\nMilz, B. (2022). Creating a blog with Quarto in 10 steps. here.\nRapp, A. (2022). The ultimate guide to starting a Quarto blog. here\nQuarto (2022). Website. here\nXie, Y., Allaire, J.J., Grolemund, G. (2022). RMarkdown. The Definitive Guide. CTC Press. Boca Raton: CRC Press here.\nXie, Y., Dervieux, C. , Riederer, E. (2021). RMarkdown Cookboek. Boca Raton: CRC Press."
  },
  {
    "objectID": "posts/2022-10-07-verhalen-vertellen-met-data/verhalenvertellen.html#inleiding",
    "href": "posts/2022-10-07-verhalen-vertellen-met-data/verhalenvertellen.html#inleiding",
    "title": "Verhalen vertellen met data",
    "section": "Inleiding",
    "text": "Inleiding\nSoms zijn er van die boeken die een goed overzicht geven van de stand van zaken in moderne data analyse en die je jonge mensen zou aanraden om te lezen en te gebruiken. Natuurlijk is het R for Data Science-boek van Wickham en Grolemund [@r4ds] zo’n boek. Irizarry’s Introduction to Data Science [@Introdatas] is ook zo’n boek. Zelf vind ik R for Health Data Science van Harrison and Pius [@RforHealth] een mooi, kort en krachtig leerboek. Omdat er zoveel gebeurt op dit terrein, zijn dit soort boeken al snel niet meer helemaal up-to-date. Niet dat je daar teveel van aan moet trekken; goede boeken blijven ook op dit terrein hun waarde vasthouden. Aan introductieboeken moderne dataanalyse moest ik denken toen ik onlangs Telling Stories With Data van Rohan Alexander [@Tellingstories] las. Dat vind ik een heel goed eigentijds boek over moderne data analyse dat ook in dit rijtje opgenomen kan worden. Als je tweede helft 2022 jonge mensen hierover iets wilt leren, is dit boek heel aantrekkelijk. Rowan Alexander laat je zien wat je kunt doen met data en biedt een goede onderbouwing van hoe je met data en de kennis om kunt gaan. Het is heel uitgebreid en soms allicht net wat te lang omdat Alexander zo op details ingaat.\nHet boek bestaat uit vier delen. In het eerste deel wordt de basis gelegd; je moet verhalen willen vertellen met data. Daarom is het ook zo belangrijk dat je de de componenten van de workflow voor ogen houdt: plannen (bedenk goed waar je heel wilt), simuleren (let bij die voorbereiding op de details), verkrijgen (zorg voor een dataset waar je mee kunt werken), exploreren (beschrijf en modelleer) en delen (communiceer de resultaten die je gevonden hebt en wijs op zwakheden en sterkten van de benadering). In dit deel laat Alexander ook zien hoe R werkt, hoe je met reproduceerbaar onderzoek om moet gaan en ook wat het belang ervan is.\nNadat Alexander het raamwerk gelegd heeft gaat hij, vreemd genoeg, in op communicatie. Ik zou dit verderop in het boek hebben geplaatst, maar in dit deel gaat hij in op schrijven over onderzoek, waar je daarbij op moet letten en hoe data daarin een rol spelen. Dan is er nog statische communicatie en hoe we de lezer door grafieken en tabellen een gevoel geven bij de dataset. Dit deel gaat ook over interactieve communicatie, over websites, interactieve kaarten en apps waarmee de onderzoeker meer en meer zijn verhaal kan doen.\nIn het derde deel acquisitie gaat hij weer terug en vraagt hij zich af Eigen data maar ook census data moeten aan bepaalde eisen voldoen en als een boer land- en tuinbouwproducten teelt, moet een onderzoeker met z’n data omgaan. Waar je aan kunt denken, laat Alexander zien. Soms moeten we we ook data verkrijgen, die op allerlei api’s of andere vormen van dataopslag staan. Weer in andere gevallen moeten we zelf op jacht naar de data, bv. via experimentele onderzoeken.\nHet vierde deel gaat over voorbereiding. Denk daarbij aan het opschonen en prepareren van datasets. Grote delen van de tijd van onderzoekers wordt aan dit deel van het werk besteed en Alexander geeft je hierbij verschillende suggesties en voorbeelden mee. Een groot deel van dit werk wordt natuurlijk met tidyverse en janitor gedaan. Bij voorbereiding gaat het ook om opslaan en deel. Data moeten te vinden, toegankelijk, van verschillende kanten mee te werken en herbruikbaar zijn. Denk hier vanaf het begin over na. Via GitHub kun je niet alleen op een goede manier de gegevens en codes opslaan maar zo kun je ook de analyses en de uiteindelijke resultaten delen. Niet alleen binnen de onderzoeksgroep, ook met de buitenwereld. Dit alles heeft niet alleen met het vraagstuk van veiligheid maar ook met vraagstuk van efficiëntie te maken.\nHet vijfde uitgebreide hoofdstuk gaat over modelleren. Uiteraard gaat hij in op exploratieve data analyse waarmee de variabelen zelf begrijpen, eenvoudige relaties met andere variabelen en wat we aan gegevens missen. Daarna gaat hij verder met hoe je lineaire modellen uitvoert, omgaat met causaliteit in observationele data. Maar hij gaat ook in op ingewikkelde data analyse zoals multilevel analyse en tekstanalyse.\nAlexander sluit af met enkele concluderende opmerkingen die in de richting gaan van dat data analyse zich verder zal als discipline en dat we betere verhalen over de wereld kunnen vertellen. Deze opmerkingen verdienen niet heel veel aandacht nu. Alexander had hier zelf meer aandacht aan moeten besteden. Maar ik wil optimistisch afsluiten omdat het bij elkaar toch een heel leerzaam boek geworden is waarmee je leert hoe je tegenwoordig moderne data-analyse uitvoert, met R en RStudio, met bepaalde pakketten, maar ook GitHub en Quarto dat nog maar kort op de markt is. Het gaat om statistiek en om computerwetenschappen. Centraal staan reproduceerbaarheid, workflows en respect (veiligheid, privacy). Lees het boek, zou ik zeggen en probeer en analyseer met hem mee. Alle codes staan op internet hier.\nAls je een idee wilt krijgen van het boek, hieronder heb ik een deel uit het tweede hoofdstuk gehad. In dat hoofdstuk laat hij aan de hand van enkele voorbeelden zien, wat je in het boek kunt verwachten. Een voorbeeld is onderzoek naar de recente Australische verkiezingen."
  },
  {
    "objectID": "posts/2022-10-07-verhalen-vertellen-met-data/verhalenvertellen.html#australische-verkiezingen",
    "href": "posts/2022-10-07-verhalen-vertellen-met-data/verhalenvertellen.html#australische-verkiezingen",
    "title": "Verhalen vertellen met data",
    "section": "Australische verkiezingen",
    "text": "Australische verkiezingen\nAustralië is een parlementaire democratie met 151 zetels in het Huis van Afgevaardigden, het lagerhuis waaruit de regering wordt gevormd. Er zijn twee grote partijen – “Liberal” en “Labor” – twee kleinere partijen – “Nationals” en “Greens” – en vele kleinere partijen en onafhankelijken. Hieronder maken we een grafiek van het aantal zetels dat elke partij won in de federale verkiezingen van 2022.\n\nPlan\nVoor dit voorbeeld moeten we twee aspecten plannen. Het eerste is hoe de dataset die we nodig hebben eruit zal zien en het tweede is hoe de uiteindelijke grafiek eruit zal zien.\nDe basisvereiste voor de dataset is dat deze in ieder geval de naam van de zetel (in Australië soms “divisie” genoemd) en de partij van de gekozene bevat. Een snelle schets van de dataset die we nodig hebben, zou er dus als volgt uit kunnen zien Figure 1 (a).\n\n\n\n\n\n\n\n(a) Korte schets van een dataset die nuttig zou kunnen zijn voor het analyseren van Australische verkiezingen\n\n\n\n\n\n\n\n(b) Korte schets van een mogelijke grafiek van het aantal door elke partij gewonnen zetels\n\n\n\n\nFigure 1: Schetsen van een dataset en grafiek met betrekking tot de Australische verkiezingen\n\n\nWe moeten ook een plan maken voor de grafiek waarin we geïnteresseerd zijn. Aangezien we het aantal zetels willen weergeven dat elke partij won, is een snelle schets van waar we naar zouden kunnen streven Figure 1 (b).\n\n\nSimuleren\nWe simuleren nu wat gegevens, om onze schetsen wat specifieker te maken.\nOm te beginnen maakt u in RStudio een nieuw Quarto-document (“Bestand” -&gt; “Nieuw bestand” -&gt; “Quarto-document…”). Geef het een titel, zoals “verkenning_australische verkiezingen_2022”, en voeg uw naam toe als auteur. Laat de andere opties als standaard, en klik dan op “Create”. Voor dit voorbeeld zetten we alles in dit ene Quarto-document. U moet het opslaan als “australische_elections.verkiezingen” (“File” -&gt; “Save As…”).\nVerwijder bijna alle standaard inhoud en maak dan onder het titelmateriaal een nieuwe R code chunk (“Code” -&gt; “Insert Chunk”) en voeg preambule documentatie toe die uitlegt:\n\nhet doel van het document;\nde auteur en contactgegevens\nwanneer het bestand is geschreven of voor het laatst is bijgewerkt; en\nrandvoorwaarden waarop het bestand is gebaseerd.\n\n\n#### Preambule ####\n# Doel: Lees de gegevens van de Australische verkiezingen van 2022 in en maak een\n# grafiek van het aantal zetels dat elke partij won.\n# Auteur: Harrie Jonkman\n# E-mail: harriejonkman@xs4all.nl\n# Datum: 6 oktober 2022\n# Voorwaarden: Je moet weten waar je Australische verkiezingsgegevens kunt krijgen.\n\nIn R zijn regels die beginnen met “#” commentaar. Dit betekent dat ze door R niet als code worden uitgevoerd, maar bedoeld zijn om door mensen te worden gelezen. Elke regel van deze preambule moet beginnen met een “\". Maak ook duidelijk dat dit de preambule is door er”####” omheen te zetten. Het resultaat moet er als volgt uitzien Figure 2.\n\n\n\nFigure 2: Screenshot van Rohans australian_elections.qmd na eerste opzet en met een premabel\n\n\nHierna moeten we de werkruimte instellen. Hiervoor moeten alle benodigde pakketten worden geïnstalleerd en geladen. Een pakket hoeft maar één keer per computer geïnstalleerd te worden, maar moet elke keer dat het gebruikt wordt, moet het geladen worden. In dit geval gaan we tidyverse [@Wickham2017], en janitor [@janitor] gebruiken. Ze moeten worden geïnstalleerd omdat dit de eerste keer is dat ze worden gebruikt, en daarna moeten ze elk worden geladen.\nHieronder volgt een voorbeeld van de installatie van de pakketten (er is overdreven veel commentaar toegevoegd om duidelijk te maken wat er gebeurt; in het algemeen is dit niveau van commentaar onnodig). Voer deze code uit door te klikken op de kleine groene pijl bij de R-code chunk (Figure 3). Ikzelf doe dit met het pakket packman waarmee het pakket alleen wordt geïnstalleerd als je het nog niet hebt (Pacman moet wel geïnstalleerd zijn)\n\nlibrary(pacman)\n\np_load(tidyverse, janitor)\n\n\n\n\nFigure 3: Screenshot van Rohans australian_elections.qmd met de groene pijl om de brok uit te voeren\n\n\nNu de pakketten zijn geïnstalleerd, moeten ze worden geladen. Aangezien die installatiestap maar één keer per computer hoeft te gebeuren, kan die code worden uitgecommentarieerd zodat hij niet per ongeluk wordt uitgevoerd.\n\n#### Workspace opzet ####\n# install.packages(\"tidyverse\") # Dit hoef je maar een keer te doen\n# install.packages(\"janitor\") # idem\n\nlibrary(tidyverse) # Een verzameling pakketten voor data-verwerking\nlibrary(janitor) # Om datasets op te schonen\n\nWe kunnen het hele document renderen door op “Renderen” te klikken. Wanneer u dit doet, kan u gevraagd worden om enkele pakketten te installeren. Als dat gebeurt, moet u daarmee instemmen. Dit zal resulteren in een HTML-document.\nVoor een inleiding op de pakketten die zojuist zijn geïnstalleerd, bevat elk pakket een helpbestand met informatie erover en hun functies. Het kan worden geopend door een vraagteken voor de pakketnaam te zetten en dan die code in de console uit te voeren. Bijvoorbeeld ?tidyverse.\nOm onze gegevens te simuleren, moeten we een dataset maken met twee kolommen: “Divisie” en “partij”, en een aantal waarden voor elke kolom. In het geval van “Division” zijn redelijke waarden een naam van een van de 151 Australische divisies. In het geval van “partij” zijn redelijke waarden een van de volgende vijf: “Liberal”, “Labor”, “National”, “Green”, “Other”. Ook deze code kan worden uitgevoerd door te klikken op de kleine groene pijl bij de R-codebrok.\n\nsimulated_data &lt;-\n  tibble(\n    # Gebruik 1 tm 151 voor elke 'divisie\n    \"Division\" = 1:151,\n    # Randomly van een van de vijf opties, met replacement, 151 keer\n    \"Party\" = sample(\n      x = c(\n        \"Liberal\",\n        \"Labor\",\n        \"National\",\n        \"Green\",\n        \"Other\"\n      ),\n      size = 151,\n      replace = TRUE\n    )\n  )\n\nsimulated_data\n\n# A tibble: 151 x 2\n   Division Party   \n      &lt;int&gt; &lt;chr&gt;   \n 1        1 Other   \n 2        2 National\n 3        3 Labor   \n 4        4 Other   \n 5        5 Labor   \n 6        6 Green   \n 7        7 National\n 8        8 Other   \n 9        9 Labor   \n10       10 Other   \n# ... with 141 more rows\n\n\nOp een gegeven moment zal je code niet meer werken en zul je anderen om hulp willen vragen. Maak geen screenshot van een klein stukje code en verwacht niet dat iemand op basis daarvan kan helpen. Dat kunnen ze vrijwel zeker niet. In plaats daarvan moet je ze je hele script geven op een manier die ze kunnen uitvoeren. Over GitHub wordt later in zijn boek geschreven ?@sec-reproducible-workflows, maar voor nu, als je hulp nodig hebt, moet je een GitHub account aanmaken, waarmee je je code kunt delen op een manier die nuttiger is dan het maken van een screenshot. Ga daarvoor naar GitHub (Figure 4 (a)). Nadenken over een geschikte gebruikersnaam is belangrijk omdat dit onderdeel wordt van je professionele profiel. Het zou dus zinvol zijn om een gebruikersnaam te hebben die professioneel is, onafhankelijk van een cursus, en idealiter gerelateerd aan je echte naam. Zoek dan naar een “+” in de rechterbovenhoek, en selecteer dan “New gist”.\n\n\n\n\n\n\n\n(a) GitHub aanmeldscherm\n\n\n\n\n\n\n\n(b) New GitHub Gist\n\n\n\n\n\n\n\n\n\n(c) Creëer een publiek GitHub Gist om code te delen\n\n\n\n\nFigure 4: Een Gist aanmaken om code te delen bij een hulpvraag\n\n\nVanaf hier moet je alle code aan die gist toevoegen, niet alleen het laatste stukje dat een fout geeft. En geef het een betekenisvolle bestandsnaam met “.R” aan het einde, bijvoorbeeld “australische_verkiezingen.R”.\nKlik op “Create public gist”. We kunnen dan de URL naar deze Gist delen met wie we vragen om te helpen, uitleggen wat het probleem is, en wat we proberen te bereiken. Het zal veel gemakkelijker zijn om te helpen, omdat alle code beschikbaar is.\n\n\nVerkrijgen\nNu willen we de feitelijke gegevens hebben. De gegevens die we nodig hebben zijn van de ‘Australian Electoral Commission’ (AEC), het niet-partijgebonden bureau dat de Australische federale verkiezingen organiseert. We kunnen een pagina van hun website doorgeven aan read_csv() van readr [@citereadr]. We hoeven readr niet expliciet te laden omdat het deel uitmaakt van het tidyverse. De &lt;- of “assignment operator” wijst de uitvoer van read_csv() toe aan een object genaamd “raw_elections_data”.\n\n#### Data inlezen ####\nraw_elections_data &lt;-\n  read_csv(\n    file =\n      \"https://results.aec.gov.au/27966/website/Downloads/HouseMembersElectedDownload-27966.csv\",\n    show_col_types = FALSE,\n    skip = 1\n  )\n\n# We hebben de gegevens van de AEC website gelezen. We willen het misschien bewaren\n# voor het geval er iets gebeurt of ze het verplaatsen.\nwrite_csv(\n  x = raw_elections_data,\n  file = \"australian_voting.csv\"\n)\n\nWe kunnen de dataset snel bekijken met head() die de eerste zes rijen toont, en tail() die de laatste zes rijen toont.\n\nhead(raw_elections_data)\n\n# A tibble: 6 x 8\n  DivisionID DivisionNm StateAb CandidateID GivenNm   Surname   PartyNm  PartyAb\n       &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;  \n1        179 Adelaide   SA            36973 Steve     GEORGANAS Austral~ ALP    \n2        197 Aston      VIC           36704 Alan      TUDGE     Liberal  LP     \n3        198 Ballarat   VIC           36409 Catherine KING      Austral~ ALP    \n4        103 Banks      NSW           37018 David     COLEMAN   Liberal  LP     \n5        180 Barker     SA            37083 Tony      PASIN     Liberal  LP     \n6        104 Barton     NSW           36820 Linda     BURNEY    Austral~ ALP    \n\ntail(raw_elections_data)\n\n# A tibble: 6 x 8\n  DivisionID DivisionNm StateAb CandidateID GivenNm    Surname  PartyNm  PartyAb\n       &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;  \n1        152 Wentworth  NSW           37451 Allegra    SPENDER  Indepen~ IND    \n2        153 Werriwa    NSW           36810 Anne Maree STANLEY  Austral~ ALP    \n3        150 Whitlam    NSW           36811 Stephen    JONES    Austral~ ALP    \n4        178 Wide Bay   QLD           37506 Llew       O'BRIEN  Liberal~ LNP    \n5        234 Wills      VIC           36452 Peter      KHALIL   Austral~ ALP    \n6        316 Wright     QLD           37500 Scott      BUCHHOLZ Liberal~ LNP    \n\n\nWe moeten de gegevens opschonen zodat we ze kunnen gebruiken. We proberen ze te laten lijken op de dataset die we in de planningsfase voor ogen hadden. Hoewel het prima is om van het plan af te wijken, moet dit een weloverwogen, beredeneerde beslissing zijn. Na het inlezen van de dataset die we hebben opgeslagen, zullen we eerst de namen van de variabelen aanpassen. Dit doen we met clean_names() uit janitor [@janitor].\n\n#### Basic opschonen ####\nraw_elections_data &lt;-\n  read_csv(\n    file = \"australian_voting.csv\",\n    show_col_types = FALSE\n  )\n\n\n# Maak de namen makkelijker om te typen\ncleaned_elections_data &lt;-\n  clean_names(raw_elections_data)\n\n# Have a look at the first six rows\nhead(cleaned_elections_data)\n\n# A tibble: 6 x 8\n  division_id division_nm state_ab candidate_id given_nm surname party~1 party~2\n        &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  \n1         179 Adelaide    SA              36973 Steve    GEORGA~ Austra~ ALP    \n2         197 Aston       VIC             36704 Alan     TUDGE   Liberal LP     \n3         198 Ballarat    VIC             36409 Catheri~ KING    Austra~ ALP    \n4         103 Banks       NSW             37018 David    COLEMAN Liberal LP     \n5         180 Barker      SA              37083 Tony     PASIN   Liberal LP     \n6         104 Barton      NSW             36820 Linda    BURNEY  Austra~ ALP    \n# ... with abbreviated variable names 1: party_nm, 2: party_ab\n\n\nDe namen zijn sneller te typen omdat RStudio ze automatisch aanvult. Daartoe beginnen we de naam van een kolom te typen en gebruiken dan “tab” om hem aan te vullen.\nEr zijn veel kolommen in de dataset, en wij zijn voornamelijk geïnteresseerd in twee: “division_nm”, en “party_nm”. We kunnen bepaalde interessante kolommen kiezen met select() uit dplyr [@citedplyr] die we geladen hebben als onderdeel van de tidyverse. De “pipe operator”, |&gt;, zet de uitvoer van een regel om in de eerste invoer van de functie op de volgende regel.\n\ncleaned_elections_data &lt;-\n  cleaned_elections_data |&gt;\n  # Select only certain columns\n  select(\n    division_nm,\n    party_nm\n  )\n\n# Even naar de eerste zes rijen kijken\nhead(cleaned_elections_data)\n\n# A tibble: 6 x 2\n  division_nm party_nm              \n  &lt;chr&gt;       &lt;chr&gt;                 \n1 Adelaide    Australian Labor Party\n2 Aston       Liberal               \n3 Ballarat    Australian Labor Party\n4 Banks       Liberal               \n5 Barker      Liberal               \n6 Barton      Australian Labor Party\n\n\nSommige namen van de kolommen zijn nog steeds niet duidelijk omdat ze afgekort zijn. We kunnen de namen van de kolommen in deze dataset bekijken met names(). En we kunnen de namen wijzigen met rename() van dplyr [@citedplyr].\n\nnames(cleaned_elections_data)\n\n[1] \"division_nm\" \"party_nm\"   \n\n\n\ncleaned_elections_data &lt;-\n  cleaned_elections_data |&gt;\n  rename(\n    division = division_nm,\n    elected_party = party_nm\n  )\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 x 2\n  division elected_party         \n  &lt;chr&gt;    &lt;chr&gt;                 \n1 Adelaide Australian Labor Party\n2 Aston    Liberal               \n3 Ballarat Australian Labor Party\n4 Banks    Liberal               \n5 Barker   Liberal               \n6 Barton   Australian Labor Party\n\n\nWe kunnen nu kijken naar de unieke waarden in de kolom “elected_party” met behulp van unique().\n\ncleaned_elections_data$elected_party |&gt;\n  unique()\n\n[1] \"Australian Labor Party\"              \n[2] \"Liberal\"                             \n[3] \"Liberal National Party of Queensland\"\n[4] \"The Greens\"                          \n[5] \"The Nationals\"                       \n[6] \"Independent\"                         \n[7] \"Katter's Australian Party (KAP)\"     \n[8] \"Centre Alliance\"                     \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAangezien hier meer details in staan dan we wilden, kunnen we de partijnamen vereenvoudigen tot wat we gesimuleerd hebben, met behulp van recode() uit dplyr [@citedplyr].\n\ncleaned_elections_data &lt;-\n  cleaned_elections_data |&gt;\n  mutate(\n    elected_party =\n      recode(\n        elected_party,\n        \"Australian Labor Party\" = \"Labor\",\n        \"Liberal National Party of Queensland\" = \"Liberal\",\n        \"The Nationals\" = \"Nationals\",\n        \"The Greens\" = \"Greens\",\n        \"Independent\" = \"Other\",\n        \"Katter's Australian Party (KAP)\" = \"Other\",\n        \"Centre Alliance\" = \"Other\"\n      )\n  )\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 x 2\n  division elected_party\n  &lt;chr&gt;    &lt;chr&gt;        \n1 Adelaide Labor        \n2 Aston    Liberal      \n3 Ballarat Labor        \n4 Banks    Liberal      \n5 Barker   Liberal      \n6 Barton   Labor        \n\n\nOnze gegevens komen nu vrij goed overeen met ons plan (Figure 1 (a)). Voor elke kiesdivisie hebben we de partij van de persoon die deze won.\nNu we de dataset mooi hebben opgeschoond, moeten we hem opslaan, zodat we in de volgende fase met die opgeschoonde dataset kunnen beginnen. We moeten ervoor zorgen dat we het opslaan onder een nieuwe bestandsnaam, zodat we de ruwe gegevens niet vervangen, en zodat het later gemakkelijk is de opgeschoonde dataset te identificeren.\n\nwrite_csv(\n  x = cleaned_elections_data,\n  file = \"cleaned_elections_data.csv\"\n)\n\n\n\nverkennen\nMisschien willen we nu de dataset die we hebben gemaakt, verkennen. Een manier om een dataset beter te begrijpen is het maken van een grafiek. In het bijzonder willen we hier de grafiek bouwen die we gepland hebben in Figure 1 (b).\nEerst lezen we de zojuist gemaakte dataset in.\n\n#### Data inlezen ####\ncleaned_elections_data &lt;-\n  read_csv(\n    file = \"cleaned_elections_data.csv\",\n    show_col_types = FALSE\n  )\n\nWe kunnen snel tellen hoeveel zetels elke partij won met count() van dplyr [@citedplyr].\n\ncleaned_elections_data |&gt;\n  count(elected_party)\n\n# A tibble: 5 x 2\n  elected_party     n\n  &lt;chr&gt;         &lt;int&gt;\n1 Greens            4\n2 Labor            77\n3 Liberal          48\n4 Nationals        10\n5 Other            12\n\n\nOm de grafiek te bouwen waarin we geïnteresseerd zijn, gebruiken we ggplot2 [@citeggplot] dat deel uitmaakt van de tidyverse. Het belangrijkste aspect van dit pakket is dat we grafieken bouwen door lagen toe te voegen met “+”, wat we de “add operator” noemen. In het bijzonder zullen we een staafdiagram maken met geom_bar() uit ggplot2 [@citeggplot]. Australia is a parliamentary democracy with 151 seats in the House of Representatives, which is the house from which government is formed. There are two major parties\n\ncleaned_elections_data |&gt;\n  ggplot(aes(x = elected_party)) + # aes staat voor \"aesthetics\" en\n  # stelt ons in staat de x-as variabele te specificeren\n  geom_bar()\n\n\n\n\nHiermee is bereikt wat we wilden bereiken. Maar we kunnen het er wat mooier laten uitzien door de standaardopties aan te passen en de labels te verbeteren (Figure 5).\n\ncleaned_elections_data |&gt;\n  ggplot(aes(x = elected_party)) +\n  geom_bar() +\n  theme_minimal() + # Maak het thema netter\n  labs(\n    x = \"Partij\",\n    y = \"Aantal zetels\"\n  ) # Maak de labels meer betekenisvol \n\n\n\n\nFigure 5: Aantal gewonnen zetels, per politieke partij, bij de Australische Federale Verkiezingen van 2022\n\n\n\n\n\n\nDelen\nTot nu toe hebben we gegevens gedownload, opgeschoond en een grafiek gemaakt. Normaal gesproken moeten we wat we gedaan hebben uitvoerig toelichten. In dit geval kunnen we een paar alinea’s schrijven over wat we hebben gedaan, waarom we het hebben gedaan en wat we hebben gevonden om onze werkflow af te sluiten. Hieronder volgt een voorbeeld.\n\nAustralië is een parlementaire democratie met 151 zetels in het Huis van Afgevaardigden, het huis waaruit de regering wordt gevormd. Er zijn twee grote partijen—“Liberal” en “Labor”—twee kleinere partijen—“Nationals” en “Greens”—en veel kleine partijtjes. De federale verkiezingen van 2022 vonden plaats op 21 mei en er werden ongeveer 15 miljoen stemmen uitgebracht. Wij waren geïnteresseerd in het aantal zetels dat elke partij won.\nWe hebben de resultaten, per zetel, gedownload van de website van de Australian Electoral Commission. We hebben de dataset opgeschoond en schoongemaakt met behulp van de statistische programmeertaal R [@citeR] inclusief de tidyverse [@tidyverse] en janitor [@janitor]. Vervolgens hebben we een grafiek gemaakt van het aantal zetels dat elke politieke partij won. (Figure 5).\nDe Labor Party won 77 zetels, gevolgd door de Liberal Party met 48 zetels. De kleinere partijen wonnen het volgende aantal zetels: Nationals, 10 zetels en de Greens, 4 zetels. Tenslotte werden er 10 onafhankelijken gekozen en kandidaten van enkele andere partijen.\nDe zetelverdeling is scheef in de richting van de twee grote partijen, wat een weerspiegeling kan zijn van relatief stabiele voorkeuren van de Australische kiezers, of mogelijk van inertie als gevolg van de voordelen van een reeds grote partij, zoals een nationaal netwerk en financiering, of een andere reden. Een beter begrip van de redenen voor deze verdeling is van belang voor toekomstige werkzaamheden. Hoewel de dataset bestaat uit iedereen die heeft gestemd, is het vermeldenswaard dat in Australië sommigen systematisch worden uitgesloten van stemming; en het is voor sommigen veel moeilijker om te stemmen dan voor anderen."
  },
  {
    "objectID": "posts/2022-10-22-hierarchische-logistische-regressie-met-bayes/hierarchische-logistische-regressie-met-bayes.html#inleiding",
    "href": "posts/2022-10-22-hierarchische-logistische-regressie-met-bayes/hierarchische-logistische-regressie-met-bayes.html#inleiding",
    "title": "Hierarchische logistische regressie met Bayes",
    "section": "Inleiding",
    "text": "Inleiding\nOnlangs verscheen een prachtig boek van Alicia A. Johnson, Miles Q. Ot en Mine Dogucu onder de titel Bayes Rules! An Introduction to Applied Bayesian Modeling en het verscheen bij CRC Press (2022). Eerdere versies stonden kon je al via bookdown bekijken (https://www.bayesrulesbook.com/) en vanaf de eerste keer dat ik het zag, was ik hier heel enthousiast over. Ik heb er al enkele keren over geschrevenm in dit blog.\nHet boek bestaat uit vier duidelijke delen. Het eerste deel gaat in op de fundamenten van het Bayesiaanse perspectief. Het leert je denken als een Bayesiaan en het gaat in op die belangrijke Bayesiaanse regel \\(posterior=\\frac{prior.likelihood}{normaliserende constante}\\). Aan de hand van enkele voorbeelden gaan Johnson e.a. in op hoe het in de praktijk werkt. Daarna gaat het in op hoe kennis en data op elkaar inwerken en het laat enkele basisanalyses zien en hoe dat in deze vorm van statistiek werkt (normaal, binair en poisson). Het tweede deel is een meer technisch hoofdstuk en laat je ook onder de moterkap van deze techniek kijken. Het gaat ook op de wetenschappelijke principes van de benadering, waar je hier op moet letten, hoe je hiermee hypothesen kunt testen (niet alleen tov van een nulhypothese, maar hoeveel beter de ene hypothese is ten opzichte van de andere hyposthese) en hoe je hiermee ook kunt voorspellen. De twee volgende delen (Deel drie en vier) zijn praktische delen. Deel drie gaat in op regressieanalyses voor continue variabelen en classificatieanalyses voor binaire variabelen. Het vierde deel ten slotte gaat in op geclusterde datasets en hoe je hierarchische Bayesiaanse regressie en classificatieanalyses uitvoert.\nNatuurlijk, er zijn onderhand al verschillende interessante boeken te krijgen die je laten zien hoe Bayesiaanse denken in de praktijk kan werken. De boeken van Gelman, McElreath, Spiegelhalter en Kruschke verschenen de afgelopen tien/vijftien jaar en leren je dit. Maar Bayes Rules! vind ik op dit moment als introductieboek mogelijk wel het beste.\nUit elk deel heb ik een hoofdstuk genomen en het vertaalt en bewerkt. Dit keer gaat het om hierarchische regressie (dus regressieanalyse met verschillende niveaus, in dit geval indiviudele variabelen en variabelen van de groep waar je toebehoort). De uitkomstmaat is ja/nee (twee mogelijkheden) en daarom is het een logistische uitkomstmaat. Het is een bewerking van het achttiende hoofdstuk van het vierde deel (Non-Normal Hierarchical Regression & Classification).Hoofdstukken zo overzetten is voor mij niet alleen een goede manier om het mij eigen te maken, maar ook een manier om het boek anderen aan te raden. Dus lezen en gebruiken deze Bayes Rules! An Introduction to Applied Bayesian Modeling"
  },
  {
    "objectID": "posts/2022-10-22-hierarchische-logistische-regressie-met-bayes/hierarchische-logistische-regressie-met-bayes.html#hierarchische-logistische-regressie",
    "href": "posts/2022-10-22-hierarchische-logistische-regressie-met-bayes/hierarchische-logistische-regressie-met-bayes.html#hierarchische-logistische-regressie",
    "title": "Hierarchische logistische regressie met Bayes",
    "section": "Hierarchische logistische regressie",
    "text": "Hierarchische logistische regressie\nEerst maar een enkele pakketten laden:\n\n# Laden van pakketten, dit zijn de basispakketten die vaak in het boek worden gebruikt\nlibrary(bayesrules) # dit pakket hoort bij dit boek\nlibrary(tidyverse)  # voor databewerking\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv ggplot2 3.3.6     v purrr   0.3.4\nv tibble  3.1.7     v dplyr   1.0.9\nv tidyr   1.2.0     v stringr 1.4.1\nv readr   2.1.2     v forcats 0.5.1\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'stringr' was built under R version 4.1.3\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(bayesplot)  # voor grafische bewerking van Bayesiaanse technieken\n\nWarning: package 'bayesplot' was built under R version 4.1.3\n\n\nThis is bayesplot version 1.9.0\n\n\n- Online documentation and vignettes at mc-stan.org/bayesplot\n\n\n- bayesplot theme set to bayesplot::theme_default()\n\n\n   * Does _not_ affect other ggplot2 plots\n\n\n   * See ?bayesplot_theme_set for details on theme setting\n\nlibrary(rstanarm)   # voor Bayesiaanse analyse\n\nWarning: package 'rstanarm' was built under R version 4.1.3\n\n\nLoading required package: Rcpp\n\n\nWarning: package 'Rcpp' was built under R version 4.1.3\n\n\nThis is rstanarm version 2.21.3\n\n\n- See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n\n\n- Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n\n\n- For execution on a local, multicore CPU with excess RAM we recommend calling\n\n\n  options(mc.cores = parallel::detectCores())\n\nlibrary(tidybayes)  # opschonen voor Bayesiaanse analyse\n\nWarning: package 'tidybayes' was built under R version 4.1.3\n\nlibrary(broom.mixed)# voor regressieanalyse\n\nWarning: package 'broom.mixed' was built under R version 4.1.3\n\nlibrary(janitor)    # voor opschonen van data\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\n\nBergbeklimmers proberen grote hoogten te beklimmen in de majestueuze Nepalese Himalaya. Dit doen ze vanwege de sensatie van ijle lucht, de uitdaging of het buitenleven. Succes is niet gegarandeerd; slecht weer, defecte uitrusting, verwondingen of gewoon pech zorgen ervoor dat niet alle klimmers hun bestemming bereiken. Dit roept enkele vragen op. Hoe groot is de kans dat een bergbeklimmer de top haalt? Welke factoren kunnen bijdragen aan een hoger succespercentage? Naast het vage gevoel dat een gemiddelde klimmer 50% kans heeft op succes, wegen we dit zwak informatief inzicht af tegen data van klimmers die in het bayesrules pakket zitten. Dit deel van de data is beschikbaar gesteld door ‘The Himalayan Database’ (2020) en verspreid via het #tidytuesday project (R for Data Science 2020b):\n\n# Binnenhalen, herbenoemen & opschonen van data\ndata(climbers_sub)\nclimbers <- climbers_sub %>% \n  select(expedition_id, member_id, success, year, season,\n         age, expedition_role, oxygen_used)\n\nDeze dataset bevat de resultaten van 2076 klimmers vanaf 1978. Slechts 38,87% van hen slaagde erin de top te bereiken:\n\nnrow(climbers)\n\n[1] 2076\n\nclimbers %>% \n  tabyl(success)\n\n success    n   percent\n   FALSE 1269 0.6112717\n    TRUE  807 0.3887283\n\n\nOmdat member_id in essentie een rij van klimmersid is en we maar één observatie per klimmer hebben, is dit geen groepsvariabele. Verder, hoewel het seizoen (seison), rol bij de expeditie (expedition_role) en het gebruik van zuurstof (oxygen_used) categorische variabelen zijn en meerdere malen geobserveerd, zijn dit potentiële voorspellers van van succes (succes), maar geen groepsvariabele. Dan blijft expeditie_id (expedition_id) over - dit is wel een groepsvariabele. De dataset bestaat uit 2076 klimmers verdeeld over 200 verschillende expedities:\n\n# Omvang per expeditie\nclimbers_per_expedition <- climbers %>% \n  group_by(expedition_id) %>% \n  summarize(count = n())\n\n# Aantal expedities\nnrow(climbers_per_expedition)\n\n[1] 200\n\n\nElke expeditie bestaat uit meerdere klimmers. Zo vertrokken onze eerste drie expedities met respectievelijk 5, 6 en 12 klimmers:\n\nclimbers_per_expedition %>% \n  head(3)\n\n# A tibble: 3 x 2\n  expedition_id count\n  <chr>         <int>\n1 AMAD03107         5\n2 AMAD03327         6\n3 AMAD05338        12\n\n\nHet zou fout zijn om deze groepsstructuur te negeren en er anders van uit te gaan dat de individuele klimmers onafhankelijke resultaten boeken. Aangezien elke expeditie als een team werkt, hangt het succes of falen van de ene klimmer in díe expeditie gedeeltelijk af van het succes of falen van anderen in de groep. Bovendien vertrekken alle leden van een expeditie met dezelfde bestemming, met dezelfde leiders en onder dezelfde weersomstandigheden, en zijn dus onderhevig aan dezelfde externe succesfactoren. Het is dus niet alleen juist om rekening te houden met de groepering van de gegevens, maar het kan ook duidelijk maken in welke mate deze factoren variabiliteit veroorzaken in de succespercentages tussen expedities. Meer dan 75 van onze 200 expedities hadden een 0% succesratio - m.a.w. geen enkele klimmer in deze expedities slaagde erin de top te bereiken. Daarentegen hadden bijna 20 expedities een 100% succespercentage. Tussen deze extremen in, is er heel wat variatie in het succespercentage van de expedities.\n\n# Bereken de slagingskans voor elke expeditie\nexpedition_success <- climbers %>% \n  group_by(expedition_id) %>% \n  summarize(success_rate = mean(success))\n\n\n# Plot de slagingskansen over de expedities\nggplot(expedition_success, aes(x = success_rate)) + \n  geom_histogram(color = \"white\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nModel bouw en simulatie\nOm de ‘gegroepeerde’ aard van onze gegevens te weerspiegelen, laat \\(Y_ij\\) aangeven of klimmer \\(i\\) in expeditie\n\\(j\\) succesvol de top van hun piek bereikt:\n\\[\\\nY_ij = \\begin{cases}\n1 Ja \\\\\n0 Nee\\\\\n\\end{cases}\n\\]\\] Er zijn verschillende potentiële voorspellers voor het succes van klimmers in onze dataset. We kijken hier naar slechts twee voorspellers: de leeftijd van de klimmer en of hij extra zuurstof heeft gekregen om gemakkelijker te kunnen ademen op grote hoogte. Als zodanig, definiëren we: \\[ X_ij1=leeftijd van klimmer *i* in expeditie *j*\\]\n\\[X_ij2=of de klimmer in *i* in expeditie *j* zuurstof (oxygen) heeft gekregen\\] Door het aandeel van succes te berekenen bij elke combinatie van leeftijd en zuurstofgebruik, krijgen we een idee van hoe deze factoren gerelateerd zijn aan het klimmerssucces (zij het een wankel idee gezien de kleine steekproefgroottes van sommige combinaties). Kort samengevat lijkt het erop dat het succes van klimmers afneemt met de leeftijd en sterk toeneemt met het gebruik van zuurstof:\n\n# Bereken het slagingspercentage per leeftijd en zuurstofgebruik\ndata_by_age_oxygen <- climbers %>% \n  group_by(age, oxygen_used) %>% \n  summarize(success_rate = mean(success))\n\n`summarise()` has grouped output by 'age'. You can override using the `.groups`\nargument.\n\n# Plot deze relatie\nggplot(data_by_age_oxygen, aes(x = age, y = success_rate, \n                               color = oxygen_used)) + \n  geom_point()\n\n\n\n\nOm een Bayesiaans model van deze relatie op te stellen, erkennen we eerst dat het Bernoulli model redelijk is voor onze binaire responsvariabele \\(Y_ij\\). Stel \\(\\pi_ij\\) de waarschijnlijkheid is dat klimmer\\(i\\) in expeditie\\(j\\) zijn piek succesvol beklimt, d.w.z. dat \\(Y_ij=1\\),\n\\[Y_ij|\\pi_ij \\sim \\Bern{\\pi_ij}\\]\nDit is een complete pooling benadering waarbij een simpel model wordt omgezet in een logistisch regressie model van \\(Y\\) met enkele voorspellers \\(X\\)\n\\[Y_ij|\\beta_0,\\beta_1,beta_2 \\sim^{ind} \\Bernoulli(\\pi_ij) with log(\\frac{\\pi_ij}{1-\\pi_ij})=\\beta_0+\\beta_1X_ij1+\\beta_2X_ij2) \\\\\n\\beta_0c \\sim N(m_0,s_0^2) \\\\\n\\beta_1 \\sim N(m_1, s_1^2) \\\\\n\\beta_2 \\sim N(m_2, s_1^2)\\]\nDit is een goed begin, MAAR het houdt geen rekening met de groepsstructuur van onze data. Overweeg in plaats daarvan het volgende hiërarchische alternatief met onafhankelijke, zwak informatieve priors hieronder afgestemd via stan_glmer() en met een prior model voor \\(beta_0\\) uitgedrukt via het gecentreerde intercept \\(beta_0c\\). Het is immers zinvoller om na te denken over de baseline succesratio bij de typische/gemiddelde klimmer, \\(\\beta_0c\\), dan bij 0-jarige klimmers die geen zuurstof gebruiken, \\(\\beta_0\\)$. Daarom begonnen we onze analyse met de zwakke veronderstelling dat de typische klimmer een kans op succes heeft van 0,5, of met log(kans op succes)=0.\nNet zo goed kunnen we dit logistische regressiemodel met willekeurige intercepts omvormen door de expeditiespecifieke intercepties uit te drukken als aanpassingen op het algemene intercept,\n\\[log(\\frac{\\pi_ij}{1-\\pi_ij})=(\\beta_0+b_0j) +\\beta_1X_ij1 + \\beta_2X_ij2\\]\nmet \\(\\beta_0j|\\sigma_0 \\sim^{ind} N(0,\\sigma_0^2)\\) Laten we eens naar de betekenis van en de veronderstellingen achter de modelparameters kijken:\n\nDe expeditie-specifieke intercepten \\(\\beta_0j\\) beschrijven de onderliggende succespercentages, zoals gemeten door de log(kans op succes), voor elke expeditie\\(j\\). Hiermee wordt erkend dat sommige expedities inherent succesvoller zijn dan andere.\nDe expeditiespecifieke intervallen \\(\\beta_0j\\) worden verondersteld normaal verdeeld te zijn rond een gemiddeld intercept \\(\\beta_0\\) met standaardafwijking \\(\\sigma_0\\). Daarmee beschrijft \\(\\beta_0\\) het typische basissucces over alle expedities, en \\(\\sigma_0\\) de tussen-groep variabiliteit in succespercentages van expeditie tot expeditie.\nBeta_1$ beschrijft het gemiddelde verband tussen succes en leeftijd wanneer gecontroleerd wordt voor zuurstofgebruik. Op dezelfde manier beschrijft \\(beta_2\\) de gemiddelde relatie tussen succes en zuurstofverbruik wanneer gecontroleerd wordt voor leeftijd.\n\nSamengevat maakt ons logistisch regressiemodel met willekeurige intercepten de vereenvoudigende (maar volgens ons redelijke) veronderstelling dat expedities unieke intercepten \\(\\beta_0j\\) kunnen hebben, maar delen de gemeenschappelijke regressieparameters \\(\\beta_1\\) en \\(\\beta_2\\). Anders gezegd, hoewel de onderliggende succespercentages kunnen verschillen van expeditie tot expeditie, zijn jonger zijn of zuurstof gebruiken niet voordeliger in de ene expeditie dan in de andere.\nOm de posterior van het model te simuleren, combineert de stan_glmer() code hieronder het beste van twee werelden: family = binomial geeft aan dat het om een logistisch regressiemodel gaat (à la Hoofdstuk 13) en de (1 | expeditie_id) term in de modelformule incorporeert onze hiërarchische groeperingsstructuur (à la Hoofdstuk 17): Consider the meaning of, and assumptions behind, the model parameters:\n\nDe expeditie-specifieke intercepts \\(\\beta_0j\\) beschrijven de onderliggende succespercentages, zoals gemeten door de log(kans op succes), voor elke expeditie\\(j\\). Hiermee wordt erkend dat sommige expedities inherent succesvoller zijn dan andere.\nDe expeditiespecifieke intervallen \\(\\beta_0j\\) worden verondersteld normaal verdeeld te zijn rond een globaal intercept \\(\\beta_0\\) met standaardafwijking \\(\\sigma_0\\). Daarmee beschrijft \\(\\beta_0\\) het typische basissucces over alle expedities, en \\(\\sigma_0\\) de tussen-groep variabiliteit in succespercentages van expeditie tot expeditie.\nBeta_1$ beschrijft het globale verband tussen succes en leeftijd wanneer gecontroleerd wordt voor zuurstofgebruik. Op dezelfde manier beschrijft \\(beta_2\\) de globale relatie tussen succes en zuurstofverbruik wanneer gecontroleerd wordt voor leeftijd.\n\nSamengevat maakt ons logistisch regressiemodel met willekeurige intercepten de vereenvoudigende (maar volgens ons redelijke) veronderstelling dat expedities unieke intercepten \\(\\beta_0j\\) kunnen hebben, maar gemeenschappelijke regressieparameters \\(\\beta_1\\) en \\(\\beta_2\\) delen. In gewone taal, hoewel de onderliggende succespercentages kunnen verschillen van expeditie tot expeditie, zijn jonger zijn of zuurstof gebruiken niet voordeliger in de ene expeditie dan in de andere.\nOm de posterior van het model te simuleren, combineert de stan_glmer() code, zie hieronder, het beste van twee werelden: family = binomial geeft aan dat het om een logistisch regressiemodel gaat en de (1 | expeditie_id) term in de modelformule incorporeert onze hiërarchische groepstructuur:\n\nclimb_model <- stan_glmer(\n  success ~ age + oxygen_used + (1 | expedition_id), \n  data = climbers, family = binomial,\n  prior_intercept = normal(0, 2.5, autoscale = TRUE),\n  prior = normal(0, 2.5, autoscale = TRUE), \n  prior_covariance = decov(reg = 1, conc = 1, shape = 1, scale = 1),\n  chains = 4, iter = 5000*2, seed = 84735\n)\n\n\nSAMPLING FOR MODEL 'bernoulli' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.002 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 20 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 79.892 seconds (Warm-up)\nChain 1:                73.038 seconds (Sampling)\nChain 1:                152.93 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'bernoulli' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0.001 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 10 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 81.885 seconds (Warm-up)\nChain 2:                76.244 seconds (Sampling)\nChain 2:                158.129 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'bernoulli' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 80.003 seconds (Warm-up)\nChain 3:                74.33 seconds (Sampling)\nChain 3:                154.333 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'bernoulli' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 83.328 seconds (Warm-up)\nChain 4:                551.978 seconds (Sampling)\nChain 4:                635.306 seconds (Total)\nChain 4: \n\n\nJe wordt aangemoedigd deze simulatie te volgen met de uitvoering van de code hierboven en te kijken naar enkele MCMC-diagnoses die hieronder staan. De r:\n\n# Bevestig prior specificaties\nprior_summary(climb_model)\n\nPriors for model 'climb_model' \n------\nIntercept (after predictors centered)\n ~ normal(location = 0, scale = 2.5)\n\nCoefficients\n  Specified prior:\n    ~ normal(location = [0,0], scale = [2.5,2.5])\n  Adjusted prior:\n    ~ normal(location = [0,0], scale = [0.24,5.51])\n\nCovariance\n ~ decov(reg. = 1, conc. = 1, shape = 1, scale = 1)\n------\nSee help('prior_summary.stanreg') for more details\n\n# MCMC diagnostiek (onderstaande diagnostieken werkten niet bij mij)\n#mcmc_trace(climb_model, size = 0.1)\n#mcmc_dens_overlay(climb_model)\n#mcmc_acf(climb_model)\n#neff_ratio(climb_model)\n#rhat(climb_model)\n\nTerwijl deze diagnostiek bevestigt (die doet het hier niet, zie het boek) dat onze MCMC simulatie op het juiste spoor zit, geeft een posterior predictive check hieronder aan dat ons model op het juiste spoor zit. Van elk van de 100 posterior gesimuleerde datasets, stellen we de proportie klimmers vast die succesvol waren met de success_rate() functie. Deze succespercentages variëren van ruwweg 37% tot 41%, in een klein venster rond het werkelijk waargenomen succespercentage van 38.9% in de klimmers data.\n\n# Defineer slagingspercentage functie\nsuccess_rate <- function(x){mean(x == 1)}\n\n# Posterior predictive check\npp_check(climb_model, nreps = 100,\n         plotfun = \"stat\", stat = \"success_rate\") + \n  xlab(\"succes score\")\n\nWarning: 'nreps' is ignored for this PPC\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nPosterior analyse\nIn onze posterior analyse van het succes van bergbeklimmers, concentreren we ons op het geheel. Behalve dat we gerustgesteld zijn door het feit dat we correct rekening houden met de groepsstructuur van onze gegevens, zijn we niet geïnteresseerd in een specifieke expeditie. Hieronder volgen enkele posterior samenvattingen voor onze regressieparameters \\(\\beta_0\\), \\(\\beta_1\\) en \\(\\beta_2\\).\n\ntidy(climb_model, effects = \"fixed\", conf.int = TRUE, conf.level = 0.80)\n\n# A tibble: 3 x 5\n  term            estimate std.error conf.low conf.high\n  <chr>              <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)      -1.41     0.476    -2.04     -0.802 \n2 age              -0.0475   0.00940  -0.0596   -0.0358\n3 oxygen_usedTRUE   5.80     0.485     5.21      6.46  \n\n\nOm te beginnen zien we dat het 80% posterior ‘çredible’ (geloofwaardigheids) interval (CI) voor de age coëfficiënt \\(\\beta_1\\) ruim onder 0 ligt. We hebben dus significant posterior bewijs dat, wanneer we controleren of een klimmer al dan niet zuurstof gebruikt, de kans op succes afneemt met de leeftijd. Meer specifiek, als we de informatie in \\(\\beta_1\\) vertalen van de log(kansen) naar de kans schaal, is er 80% kans dat de kans op een succesvolle beklimming daalt tussen 3,5% en 5,8% voor elk jaar extra leeftijd: \\(e^{-0,0594}, e^{-0,0358}=(0,942, 0,965)\\).\nOp dezelfde manier levert het 80% posterior geloofwaardig interval voor de oxygen_usedTRUE coëfficiënt \\(beta_2\\) significant posterior bewijs dat, wanneer gecontroleerd wordt voor leeftijd, het gebruik van zuurstof de kans op het beklimmen van de top drastisch verhoogt. Er is een kans van 80% dat het gebruik van zuurstof kan overeenkomen met een 182- tot 617-voudige toename van de kans op succes: \\(e^{5.2}}, e^{6.43}=(182,617)\\), Zuurstof alstublieft!\nDoor onze waarnemingen voor \\(\\beta_1\\) en \\(\\beta_2\\) te combineren, wordt het posterior mediaan model voor de relatie tussen de log(kans op succes) van de klimmers en hun leeftijd (\\(X_1\\))en zuurstofgebruik (\\(X_2\\))\n\\[log(\\frac{\\pi}{1-\\pi})=-1.42-0.0474X_1+5.79X_2\\]\nOf, op de schaal van waarschijnlijkheid: \\[\\pi=\\frac{e^{-1.42-0.0474X_1+5.79X_2}}{1+e^{-1.42-0.0474X_1+5.79X_2}}\\]\nDit posterior mediaan model vertegenwoordigt slechts het midden van een bereik van posterior plausibele relaties tussen succes, leeftijd en zuurstofgebruik. Om een idee te krijgen van dit bereik, toont figuur hieronder 100 posterior plausibele alternatieve modellen. Zowel met als zonder zuurstof neemt de kans op succes af met de leeftijd. Bovendien, op elke leeftijd, is de kans op succes dramatisch hoger wanneer klimmers zuurstof gebruiken. Echter, onze zekerheid over deze trends varieert nogal per leeftijd. We hebben veel minder zekerheid over de slaagkans voor oudere klimmers met zuurstof dan voor jongere klimmers met zuurstof, voor wie de slaagkans over het geheel hoog is. Op dezelfde manier, maar minder drastisch, hebben we minder zekerheid over de slaagkans voor jongere klimmers die geen zuurstof gebruiken dan voor oudere klimmers die geen zuurstof gebruiken, voor wie de slaagkans uniform laag is.\n\nclimbers %>%\n  add_fitted_draws(climb_model, n = 100, re_formula = NA) %>%\n  ggplot(aes(x = age, y = success, color = oxygen_used)) +\n    geom_line(aes(y = .value, group = paste(oxygen_used, .draw)), \n              alpha = 0.1) + \n    labs(y = \"waarschijnlijkheid van succes\")\n\nWarning: `fitted_draws` and `add_fitted_draws` are deprecated as their names were confusing.\nUse [add_]epred_draws() to get the expectation of the posterior predictive.\nUse [add_]linpred_draws() to get the distribution of the linear predictor.\nFor example, you used [add_]fitted_draws(..., scale = \"response\"), which\nmeans you most likely want [add_]epred_draws(...).\n\n\n\n\n\n\n\nPosterior classificatie\nStel dat vier klimmers op een nieuwe expeditie gaan. Twee van hen zijn 20 jaar oud en twee zijn 60 jaar. Van beide leeftijdsgroepen is één klimmer van plan zuurstof te gebruiken en de andere niet:\n\n# Nieuwe expeditie\nnew_expedition <- data.frame(\n  age = c(20, 20, 60, 60), oxygen_used = c(FALSE, TRUE, FALSE, TRUE), \n  expedition_id = rep(\"new\", 4))\nnew_expedition\n\n  age oxygen_used expedition_id\n1  20       FALSE           new\n2  20        TRUE           new\n3  60       FALSE           new\n4  60        TRUE           new\n\n\nNatuurlijk willen ze allemaal weten hoe groot de kans is dat ze de top zullen bereiken. Om dit vast te stellen werken we hier met de posterior_predict() snelkoppelingsfunctie om 20.000 posterior voorspellingen (0 of 1) te simuleren voor elk van onze 4 nieuwe klimmers:\n\n# Posterior voorspellingen van binaire uitkomst\nset.seed(84735)\nbinary_prediction <- posterior_predict(climb_model, newdata = new_expedition)\n\n# Eerste drie voorspellingen\nhead(binary_prediction, 3)\n\n     1 2 3 4\n[1,] 0 1 0 0\n[2,] 1 1 0 1\n[3,] 1 1 0 1\n\n\nVoor elke klimmer wordt de kans op succes benaderd door het geobserveerde aandeel van succes onder hun 20.000 posterieure voorspellingen. Aangezien deze kansen de onzekerheid in het basissuccespercentage van de nieuwe expeditie omvatten, zijn ze gematigder dan de algemene trends die we eerder zichtbaar maakten.\n\n# Vat de posterior voorspellingen van Y samen:\ncolMeans(binary_prediction)\n\n      1       2       3       4 \n0.27815 0.80110 0.14630 0.64710 \n\n\nDeze voorspellingen geven meer inzicht in de verbanden tussen leeftijd, zuurstof, en succes. Bijvoorbeeld, onze posterior voorspelling is dat klimmer 1, die 20 jaar oud is en niet van plan is om zuurstof te gebruiken, 27.88% kans heeft om de top te halen. Deze kans is natuurlijk lager dan voor klimmer 2, die ook 20 is maar wel van plan is om zuurstof te gebruiken. Het is hoger dan de posterior voorspelling van succes voor klimmer 3, die ook niet van plan is zuurstof te gebruiken maar wel 60 jaar oud is. Over het algemeen is de voorspelling van succes het hoogst voor klimmer 2, die jonger is en van plan is zuurstof te gebruiken, en het laagst voor klimmer 3, die ouder is en niet van plan is zuurstof te gebruiken.\nPosterior kans voorspellingen kunnen omgezet worden in posterior classificaties van binaire uitkomsten: ja of nee, verwachtingen of de klimmer zal slagen of niet? Als we een eenvoudige cut-off van 0,5 zouden gebruiken om dit te bepalen, dan zouden we klimmers 1 en 3 aanraden niet aan de expeditie deel te nemen (tenminste, niet zonder zuurstof) en klimmers 2 en 4 het groene licht geven. Maar in deze specifieke context moeten we het waarschijnlijk aan de individuele klimmers overlaten om hun eigen resultaten te interpreteren en hun eigen ja-of-nee beslissingen te nemen over het al dan niet voortzetten van hun expeditie. Zo kan een kans op succes van 65,16% voor sommigen de moeite en het risico waard zijn, maar voor anderen niet.\n\n\nModel evaluatie\nOm onze klimanalyse af te ronden, vragen we ons af: Is ons hiërarchisch-logistisch model een goed model? Lang verhaal kort, het antwoord is ja. - Ten eerste, ons model is eerlijk. De gegevens die we hebben gebruikt zijn openbaar en we verwachten niet dat onze analyse een negatief effect zal hebben op individuen of de samenleving. (Nogmaals, saaie antwoorden op de vraag naar eerlijkheid zijn de beste soort.)\n- Ten tweede Posterior Predictive Checque controle toonde aan dat ons model niet al te verkeerd lijkt - onze posterior gesimuleerde succespercentages schommelen rond de waargenomen succespercentages in onze gegevens.\n- Tenslotte, voor de vraag naar posterior classificatie nauwkeurigheid, kunnen we onze posterior classificaties van succes vergelijken met de werkelijke uitkomsten voor de 2076 klimmers in onze dataset. Standaard beginnen we met een kans cut-off van 0.5 - als de kans op succes van een klimmer groter is dan 0.5, voorspellen we dat hij zal slagen. We implementeren en evalueren deze classificatieregel met classification_summary() hieronder.\n\nset.seed(84735)\nclassification_summary(data = climbers, model = climb_model, cutoff = 0.5)\n\n$confusion_matrix\n     y    0   1\n FALSE 1172  97\n  TRUE   77 730\n\n$accuracy_rates\n                          \nsensitivity      0.9045849\nspecificity      0.9235619\noverall_accuracy 0.9161850\n\n\nIn het algemeen voorspelt ons model met deze classificatieregel de resultaten goed voor 91,61% van onze klimmers. Dit ziet er behoorlijk fantastisch uit gezien het feit dat we enkel informatie gebruiken over de leeftijd en het zuurstofverbruik van de klimmers (terwijl er nog andere voorspellers te bedenken zijn (bv. bestemming, seizoen, enz.). Maar gezien de gevolgen van een foute classificatie in deze specifieke context (bv. risico op verwondingen), moeten we voorrang geven aan specificiteit, ons vermogen om te anticiperen wanneer een klimmer niet zou slagen. Om dit te bereiken voorspelde ons model slechts 92.51% van de mislukte beklimmingen correct. Om dit percentage te verhogen, kunnen we de waarschijnlijkheidsgrens in onze classificatieregel aanpassen.\nIn het algemeen kunnen we, om de specificiteit te verhogen, de waarschijnlijkheidsdrempel verhogen, waardoor het moeilijker wordt om “succes” te voorspellen. Na wat trial and error lijkt het erop dat cut-offs van ruwweg 0.65 of hoger een gewenst specificiteitsniveau van 95% zullen bereiken. Deze overschakeling naar 0.65 verlaagt natuurlijk de gevoeligheid van onze posterior classificaties, van 90.46% naar 81.54%, en dus ons vermogen om te detecteren wanneer een klimmer succesvol zal zijn. Wij denken dat de extra voorzichtigheid hier van belang is.\n\nset.seed(84735)\nclassification_summary(data = climbers, model = climb_model, cutoff = 0.65)\n\n$confusion_matrix\n     y    0   1\n FALSE 1213  56\n  TRUE  149 658\n\n$accuracy_rates\n                          \nsensitivity      0.8153656\nspecificity      0.9558708\noverall_accuracy 0.9012524"
  },
  {
    "objectID": "posts/2022-10-22-hierarchische-logistische-regressie-met-bayes/hierarchische-logistische-regressie-met-bayes.html#literatuur",
    "href": "posts/2022-10-22-hierarchische-logistische-regressie-met-bayes/hierarchische-logistische-regressie-met-bayes.html#literatuur",
    "title": "Hierarchische logistische regressie met Bayes",
    "section": "Literatuur",
    "text": "Literatuur\nFast, Shannon, and Thomas Hegland. 2011. “Book Challenges: A Statistical Examination.” Project for Statistics 316-Advanced Statistical Modeling, St. Olaf College.\nLegler, Julie, and Paul Roback. 2021. Beyond Multiple Linear Regression: Applied Generalized Linear Models and Multilevel Models in R. Chapman; Hall/CRC. https://bookdown.org/roback/bookdown-BeyondMLR/. ———. 2020b. “Himalayan Climbing Expeditions.” TidyTuesday Github Repostitory. https://github.com/rfordatascience/tidytuesday/tree/master/data/2020/2020-09-22.\nThe Himalayan Database. 2020. https://www.himalayandatabase.com/. Trinh, Ly, and Pony Ameri. 2016. “AirBnB Price Determinants: A Multilevel Modeling Approach.” Project for Statistics 316-Advanced Statistical Modeling, St. Olaf College."
  },
  {
    "objectID": "posts/2022-11-07-statistisch-denken/Stats21.html",
    "href": "posts/2022-11-07-statistisch-denken/Stats21.html",
    "title": "Statistisch denken in de 21ste eeuw",
    "section": "",
    "text": "Een paar jaar geleden schreef Russell A. Poldrack Statistical Thinking for the 21st Century. Dat vind ik een inspirerend boek. Poldrack wil ons de basisideeën van statistisch denken laten begrijpen. Zijn boek presenteert een systematische manier van denken over het beschrijven van de wereld met behulp van data, hoe we deze kunnen gebruiken bij het nemen van beslissingen en het doen van voorspellingen en dit alles in de context van onzekerheid die er in de wereld bestaat. Met de traditionele statistiekboeken kon hij niet uit de voeten en daarom besloot hij zijn eigen boek te maken. Hij gebruikt aansprekende datasets in zijn boek en moderne manieren om hiermee om te gaan.\nOm je deze nieuwe manier van statistisch denken eigen te maken, moet je ermee werken, je moet het doen."
  },
  {
    "objectID": "posts/2022-11-07-statistisch-denken/Stats21.html#r-en-python",
    "href": "posts/2022-11-07-statistisch-denken/Stats21.html#r-en-python",
    "title": "Statistisch denken in de 21ste eeuw",
    "section": "R en Python",
    "text": "R en Python\nPoldrack presenteert niet alleen zijn ideeën open-source, hij werkt ook met open-source programma’s en wil ons ermee laten werken. Naast zijn open-source boek Statistical Thinking for the 21st Centrury heeft hij ook twee open-source programmaboeken gemaakt die laten zien hoe je met data kunt werken. Allereerst is er An R companion to Statistical Thinking for the 21st Centrury. Dit document richt zich op het gebruik van R (en RStudio) als programmeertaal voor statistiek en data-analyse. Met dit programma heeft hijzelf niet zoveel mee op, maar daarin zitten wel pakketten waar hij graag mee werkt en die niet in andere programma’s zitten. Hijzelf heeft meer op met Python en hij schreef daarom ook de Python Companion to Statistical Thinking in the 21st Century"
  },
  {
    "objectID": "posts/2022-11-07-statistisch-denken/Stats21.html#quarto",
    "href": "posts/2022-11-07-statistisch-denken/Stats21.html#quarto",
    "title": "Statistisch denken in de 21ste eeuw",
    "section": "Quarto",
    "text": "Quarto\nAfgelopen weken heb ik zijn interessante boek nog eens gelezen. Ook nu weer vind ik zijn boek weer zeer aansprekend en uitdagend. Met beide programmaboeken in R en Python ernaast leer je met de goede talen van deze tijd te werken. Voor deze blog heb ik daarom twee hoofdstukken genomen die over waarschijnlijkheid en Bayesiaanse statistiek gaan. Dit leert mij niet alleen om met R te werken maar ook om deze handelingen in Python te doen. Via Quarto kun je met beide programma’s werken. Ik heb hoofdstuk 6 (Waarschijnlijkheid) en hoofdstuk 11 (Bayesiaanse statstiek) genomen en voor dit blog gewerkt. Deze blog schrijf ik in R en Quarto en de bewerking hiervan vind je hieronder1."
  },
  {
    "objectID": "posts/2022-11-07-statistisch-denken/Stats21.html#waarschijnlijkheid",
    "href": "posts/2022-11-07-statistisch-denken/Stats21.html#waarschijnlijkheid",
    "title": "Statistisch denken in de 21ste eeuw",
    "section": "Waarschijnlijkheid",
    "text": "Waarschijnlijkheid\nEerst maar eens enkele R-pakketten openen om mee te werken (dplyr, reshap2, tidyr, ggplot2, knitr, readr, cowplot, janitor) en om de data te kunnen laden (NHANES):\nWaarschijnlijkheidstheorie is de tak van de wiskunde die zich bezighoudt met kans en onzekerheid. Het vormt een belangrijk deel van de basis voor statistiek omdat het ons de wiskundige instrumenten verschaft om onzekere gebeurtenissen te beschrijven. De studie van waarschijnlijkheid is deels ontstaan uit belangstelling voor het begrijpen van kansspelen, zoals kaarten of dobbelen. Deze spelen bieden nuttige voorbeelden van veel statistische concepten omdat bij herhaling van deze spelen de kans op verschillende uitkomsten (meestal) gelijk blijft. Er zijn echter diepgaande vragen over de betekenis van waarschijnlijkheid die we hier niet zullen behandelen (zie Aanbevolen literatuur aan het einde als je meer wilt weten over dit fascinerende onderwerp en zijn geschiedenis)."
  },
  {
    "objectID": "posts/2022-11-07-statistisch-denken/Stats21.html#wat-is-waarschijnlijkheid",
    "href": "posts/2022-11-07-statistisch-denken/Stats21.html#wat-is-waarschijnlijkheid",
    "title": "Statistisch denken in de 21ste eeuw",
    "section": "Wat is waarschijnlijkheid?",
    "text": "Wat is waarschijnlijkheid?\nInformeel denken we bij waarschijnlijkheid meestal aan een getal dat de waarschijnlijkheid beschrijft dat een bepaalde gebeurtenis zich voordoet, variërend van nul (onmogelijk) tot één (zeker). Soms worden waarschijnlijkheden uitgedrukt in percentages, die variëren van nul tot honderd, zoals wanneer het weerbericht voorspelt dat er vandaag twintig procent kans is op regen. In elk geval drukken deze getallen uit hoe waarschijnlijk die bepaalde gebeurtenis is, variërend van absoluut onmogelijk tot absoluut zeker.\nOm de kansrekening te formaliseren, moeten we eerst een paar termen definiëren:\n\nEen experiment is elke activiteit die een resultaat oplevert of observeert. Voorbeelden zijn het opgooien van een munt, het gooien met een zeszijdige dobbelsteen of het uitproberen van een nieuwe route naar het werk om te zien of die sneller is dan de oude route.\nDe steekproefruimte is de verzameling mogelijke uitkomsten voor een experiment. Wij stellen deze voor door ze op te sommen binnen een reeks haakjes. Voor een muntstuk is de steekproefruimte {koppen, munt}. Voor een zeszijdige dobbelsteen is de steekproefruimte elk van de mogelijke getallen die kunnen verschijnen: {1,2,3,4,5,6}. Voor de hoeveelheid tijd die nodig is om aan het werk te gaan, is de steekproefruimte alle mogelijke reële getallen groter dan nul (omdat het geen negatieve hoeveelheid tijd kan kosten om ergens te komen, tenminste nog niet). We zullen niet proberen al die getallen tussen haakjes te zetten.\nEen gebeurtenis is een deelverzameling van de steekproefruimte. In principe kan het een of meer mogelijke uitkomsten in de steekproefruimte zijn, maar we zullen ons hier voornamelijk richten op elementaire gebeurtenissen die bestaan uit precies één mogelijke uitkomst. Dit kan bijvoorbeeld kop zijn bij een enkele muntworp, een 4 gooien bij een dobbelsteenworp of 21 minuten over de nieuwe route naar huis doen.\n\nNu we deze definities hebben, kunnen we de formele kenmerken van een kans schetsen, die voor het eerst werden gedefinieerd door de Russische wiskundige Andrei Kolmogorov. Dit zijn de kenmerken die een waarde moeten hebben om een kans te zijn. Laten we zeggen dat we een steekproefruimte hebben gedefinieerd door N onafhankelijke gebeurtenissen, \\({E_1, E_2, .... , E_N}\\), en \\(X\\) is een willekeurige variabele die aangeeft welke gebeurtenis zich heeft voorgedaan. \\(P(X=E_i)\\) is de kans op gebeurtenis \\(i\\):\n\nDe waarschijnlijkheid kan niet negatief zijn: \\(P(X=E_i) \\ge 0\\).\nDe totale waarschijnlijkheid van alle uitkomsten in de steekproefruimte is 1; dat wil zeggen, als we de waarschijnlijkheid van elke Ei nemen en optellen, moeten ze bij elkaar opgeteld 1 zijn. We kunnen dit uitdrukken met het sommatieteken \\(sum\\):\n\n\\[\n\\sum_{i=1}^N{P(X=E_i)} = P(X=E_1) + P(X=E_2) + ... + P(X=E_N) = 1\n\\]\nDit wordt geïnterpreteerd als: “Neem alle N elementaire gebeurtenissen, die we gelabeld hebben van 1 tot N, en tel hun kansen op. Deze moeten bij elkaar opgeteld één zijn.”\n- De waarschijnlijkheid van een individuele gebeurtenis kan niet groter zijn dan één: \\(P(X=E_i)≤ 1\\). Dit vloeit voort uit het vorige punt; aangezien zij moeten optellen tot één en zij niet negatief kunnen zijn, kan geen enkele kans groter zijn dan één."
  },
  {
    "objectID": "posts/2022-11-07-statistisch-denken/Stats21.html#hoe-stellen-we-waarschijnlijkheden-vast",
    "href": "posts/2022-11-07-statistisch-denken/Stats21.html#hoe-stellen-we-waarschijnlijkheden-vast",
    "title": "Statistisch denken in de 21ste eeuw",
    "section": "Hoe stellen we waarschijnlijkheden vast?",
    "text": "Hoe stellen we waarschijnlijkheden vast?\nNu we weten wat een waarschijnlijkheid is, hoe komen we er eigenlijk achter wat de waarschijnlijkheid is voor een bepaalde gebeurtenis?\n\nPersoonlijke overtuiging\nStel dat ik u vraag wat de kans is dat Bernie Sanders de presidentsverkiezingen van 2016 zou hebben gewonnen als hij de democratische kandidaat was geweest in plaats van Hilary Clinton? We kunnen het experiment niet uitvoeren om de uitkomst te vinden. Maar de meeste mensen met kennis van de Amerikaanse politiek zouden bereid zijn om op zijn minst een gokje te wagen naar de waarschijnlijkheid van deze gebeurtenis. In veel gevallen is persoonlijke kennis en/of mening de enige leidraad die we hebben om de waarschijnlijkheid van een gebeurtenis te bepalen, maar dit is wetenschappelijk gezien niet erg bevredigend.\n\n\nEmpirische frequentie\nEen andere manier om de waarschijnlijkheid van een gebeurtenis te bepalen is het experiment vele malen uit te voeren en te tellen hoe vaak elke gebeurtenis zich voordoet. Uit de relatieve frequentie van de verschillende uitkomsten kunnen we de waarschijnlijkheid van elke uitkomst berekenen. Laten we bijvoorbeeld zeggen dat we de kans op regen in San Francisco willen weten. We moeten eerst het experiment definiëren — laten we zeggen dat we voor elke dag in 2017 de gegevens van de National Weerstation bekijken en bepalen of er regen is gevallen op het weerstation in het centrum van San Francisco. Volgens deze gegevens waren er in 2017 73 regendagen. Om de kans op regen in San Francisco te berekenen, delen we gewoon het aantal regendagen door het aantal getelde dagen (365), wat P(regen in SF in 2017) = 0,2 oplevert.\n\n\n\n\n\naantal regendagen\naantal dagen gemeten\nP(regen)\n\n\n\n\n73\n365\n0.2\n\n\n\n\n\nHoe weten we dat de empirische waarschijnlijkheid ons het juiste getal geeft? Het antwoord op deze vraag komt van de wet van de grote getallen, die aantoont dat de empirische kans de ware kans benadert naarmate de steekproefomvang toeneemt. We kunnen dit zien als een groot aantal keren munten opgooien (simuleren), en kijken naar onze schatting van de kans op kop na elke worp. Ga er voor hier maar van uit dat we voor de computer een manier hebben om een willekeurige uitkomst te genereren voor elke worp.\nHet linkerpaneel van figuur laat zien dat de geschatte kans op koppen convergeert naar de werkelijke waarde van 0,5 naarmate het aantal steekproeven (d.w.z. muntproeven) toeneemt. Merk echter op dat de schattingen zeer ver van de ware waarde kunnen afwijken wanneer de steekproefomvang klein is. Een praktijkvoorbeeld hiervan werd gezien bij de speciale verkiezing voor de Amerikaanse Senaat in Alabama in 2017, waarbij de Republikein Roy Moore het opnam tegen de Democraat Doug Jones.Het rechterpaneel toont het relatieve aantal stemmen dat in de loop van de avond voor elk van de kandidaten werd gemeld, naarmate er meer stembiljetten werden geteld. Aan het begin van de avond waren de stemmenaantallen bijzonder onstabiel, met een grote aanvankelijke voorsprong voor Jones en een lange periode waarin Moore de leiding had, totdat Jones uiteindelijk de leiding nam en de race won.\n\n\n\n\n\nFiguur1. Links: Een demonstratie van de wet van de grote getallen. Een munt werd 30.000 keer opgegooid, en na elke worp werd de kans op kop berekend op basis van het aantal koppen en staarten dat tot dan toe was verzameld. Het duurt ongeveer 15.000 keer voordat de waarschijnlijkheid de werkelijke waarschijnlijkheid van 0,5 bereikt. Rechts: Relatief aandeel van de stemmen in de speciale verkiezing van 12 december 2017 voor de zetel van de Amerikaanse Senaat in Alabama, als functie van het percentage kiesdistricten dat rapporteert. Deze gegevens zijn overgenomen uit: https://www.ajc.com/news/national/alabama-senate-race-live-updates-roy-moore-doug-jones/KPRfkdaweoiXICW3FHjXqI/\n\n\n\n\n Deze twee voorbeelden tonen aan dat, hoewel grote steekproeven uiteindelijk zullen convergeren naar de ware waarschijnlijkheid, de resultaten met kleine steekproeven er ver naast kunnen zitten. Helaas vergeten veel mensen dit en overinterpreteren ze de resultaten van kleine steekproeven. Dit werd de wet van de kleine aantallen genoemd door de psychologen Danny Kahneman en Amos Tversky, die aantoonden dat mensen (zelfs getrainde onderzoekers) zich vaak gedragen alsof de wet van de grote aantallen ook geldt voor kleine steekproeven en te veel geloof hechten aan resultaten die gebaseerd zijn op kleine datasets. In het boek laten we voorbeelden zien van hoe onstabiel statistische resultaten kunnen zijn wanneer zij worden gegenereerd op basis van kleine steekproeven.\n\n\nKlassieke waarschijnlijkheid\nHet is onwaarschijnlijk dat iemand van ons ooit tienduizenden keren een munt heeft opgegooid, maar we zijn toch bereid te geloven dat de kans dat er kop wordt gegooid 0,5 is. Dit weerspiegelt het gebruik van nog een andere benadering om kansen te berekenen, die we klassieke kans noemen. Bij deze benadering berekenen we de kans rechtstreeks op basis van onze kennis van de situatie.\nKlassieke kansberekening is ontstaan uit de studie van kansspelen zoals dobbelen en kaarten. Een beroemd voorbeeld ontstond uit een probleem van een Franse gokker die Chevalier de Méré heette. De Méré speelde twee verschillende dobbelspellen: Bij het eerste gokte hij op de kans op minstens één zes bij vier worpen met een zeszijdige dobbelsteen, terwijl hij bij het tweede gokte op de kans op minstens één dubbele zes bij 24 worpen met twee dobbelstenen. Hij verwachtte bij beide gokken geld te winnen, maar hij ontdekte dat hij bij de eerste gok gemiddeld geld won, maar bij de tweede gok gemiddeld geld verloor. Om dit te begrijpen wendde hij zich tot zijn vriend, de wiskundige Blaise Pascal, die nu erkend wordt als een van de grondleggers van de kansrekening.\nHoe kunnen we deze vraag begrijpen met behulp van de kansrekening? In de klassieke kansrekening gaan we ervan uit dat alle elementaire gebeurtenissen in de steekproefruimte even waarschijnlijk zijn; dat wil zeggen, als je met een dobbelsteen gooit, is de kans dat elk van de mogelijke uitkomsten ({1,2,3,4,5,6}) zich voordoet even groot. (Daarom kunnen we de waarschijnlijkheid van elke individuele uitkomst berekenen als één gedeeld door het aantal mogelijke uitkomsten):\n\\[\nP(uitkomst_i) = \\frac{1}{\\text{aantal mogelijke uitkomsten}}\n\\]\nVoor de zeszijdige dobbelsteen is de kans op elke individuele uitkomst 1/6.\nDit is mooi, maar de Méré was geïnteresseerd in meer complexe gebeurtenissen, zoals wat er gebeurt bij meerdere dobbelsteenworpen. Hoe berekenen we de waarschijnlijkheid van een complexe gebeurtenis (die een vereniging is van afzonderlijke gebeurtenissen), zoals het gooien van een zes bij de eerste of tweede worp? We geven de vereniging van gebeurtenissen wiskundig weer met behulp van het symbool \\(\\Cup\\): als bijvoorbeeld de kans op het gooien van een zes bij de eerste worp \\(P(Uitkomst6_{worp1})\\) is en de kans op het gooien van een zes bij de tweede worp \\(P(Uitkomst6_{worp2})\\), dan wordt de vereniging \\(P(Uitkomst6_{worp1} \\cup Uitkomst6_{worp2})\\) genoemd.\nDe Méré dacht (ten onrechte, zoals we hieronder zullen zien) dat hij gewoon de kansen van de individuele gebeurtenissen kon optellen om de kans op de gecombineerde gebeurtenis te berekenen, wat betekent dat de kans op het gooien van een zes bij de eerste of tweede worp als volgt zou worden berekend:\n\\[\nP(Uitkomst6_{worp1}) = 1/6\n\\]\n\\[\nP(Uitkomst6_{worp2}) = 1/6\n\\]\n\\[\nde Méré's \\ fout:\n\\]\n\\[\nP(Uitkomst6_{worp1} \\cup Uitkomst6_{worp2}) = P(Uitkomst6_{worp1}) + P(Uitkomst6_{worp2}) = 1/6 + 1/6 = 1/3\n\\]\nDe Méré redeneerde op basis van deze onjuiste veronderstelling dat de kans op minstens één zes in vier worpen de som was van de kansen op elke afzonderlijke worp: \\(4*\\frac{1}{6}= \\frac{2}{3}\\). Evenzo redeneerde hij dat aangezien de kans op een dubbele zes bij het gooien van twee dobbelstenen 1/36 is, de kans op minstens één dubbele zes bij 24 worpen met twee dobbelstenen \\(24*\\frac{1}{36}=\\frac{2}{3}\\) zou zijn. Maar terwijl hij consequent geld won met de eerste weddenschap, verloor hij geld met de tweede weddenschap. Wat is er aan de hand?\nOm de fout van de Méré te begrijpen, moeten we enkele regels van de kansrekening introduceren. De eerste is de aftrekregel, die zegt dat de kans dat een gebeurtenis A niet gebeurt, één is min de kans dat de gebeurtenis wel gebeurt:\n\\[\nP(\\neg A) = 1 - P(A)\n\\]\nwaarbij \\(\\neg A\\) betekent “niet A”. Deze regel vloeit rechtstreeks voort uit de axioma’s die we hierboven hebben besproken; omdat A en \\(\\neg A\\) de enige mogelijke uitkomsten zijn, moet hun totale waarschijnlijkheid opgeteld 1 zijn. Bijvoorbeeld, als de kans op het gooien van een één in een enkele worp \\(\\frac{1}{6}\\) is, dan is de kans op het gooien van iets anders dan een één \\(\\frac{5}{6}\\).\nEen tweede regel vertelt ons hoe we de kans op een gecombineerde gebeurtenis moeten berekenen – dat wil zeggen, de kans dat twee gebeurtenissen allebei voorkomen. We noemen dit een intersectie, die wordt aangeduid met het symbool \\(\\cap\\); zo betekent \\(P(A \\cap B)\\) de kans dat zowel A als B zich voordoen. We zullen ons concentreren op een versie van de regel die ons vertelt hoe we deze grootheid moeten berekenen in het speciale geval dat de twee gebeurtenissen onafhankelijk van elkaar zijn; we zullen later leren wat het begrip onafhankelijkheid precies betekent, maar voor nu kunnen we gewoon aannemen dat de twee dobbelsteenworpen onafhankelijke gebeurtenissen zijn. We berekenen de kans op het snijpunt van twee onafhankelijke gebeurtenissen door eenvoudigweg de kansen van de afzonderlijke gebeurtenissen te vermenigvuldigen:\n\\[\nP(A \\cap B) = P(A) * P(B)\\ \\text{als en alleen als A en B onafhankelijk zijn}\n\\]\nDe kans op een zes bij beide worpen is dus \\(\\frac{1}{6}*\\frac{1}{6}=\\frac{1}{36}\\).\nDe derde regel vertelt ons hoe we kansen moeten optellen - en hier zien we de bron van de fout van de Méré. De optelregel vertelt ons dat om de kans te krijgen dat een van twee gebeurtenissen zich voordoet, we de individuele kansen bij elkaar optellen, maar vervolgens de kans dat beide gebeurtenissen zich samen voordoen van elkaar aftrekken:\n\\[\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B)\n\\]\nIn zekere zin voorkomt dit dat we die gevallen tweemaal tellen, en dat is wat de regel onderscheidt van de onjuiste berekening van de Méré. Laten we zeggen dat we de kans op een 6 willen vinden bij een van de twee worpen. Volgens onze regels:\n\\[\nP(Uitkomst6_{worp1} \\cup Uitkomst6_{worp2}) = P(Uitkomst6_{worp1}) + P(Uitkomst6_{worp2}) - P(Uitkomst6_{worp1} \\cap Uitkomst6_{worp2})\n\\]\n\\[\n= \\frac{1}{6} + \\frac{1}{6} - \\frac{1}{36} = \\frac{11}{36}\n\\]\n\n\n\n\n\nFiguur2. Elke cel in deze matrix vertegenwoordigt een uitkomst van twee worpen van een dobbelsteen, waarbij de kolommen staan voor de eerste worp en de rijen voor de tweede worp. De cellen in rood staan voor de cellen met een zes in de eerste of tweede worp; de rest is blauw..\n\n\n\n\nLaten we een grafische voorstelling gebruiken om een andere kijk op deze regel te krijgen. Het figuur hierboven toont een matrix met alle mogelijke combinaties van resultaten bij twee worpen en benadrukt de cellen met een zes bij de eerste of tweede worp. Als je de cellen in het rood optelt, zie je dat er 11 van zulke cellen zijn. Hieruit blijkt waarom de optelregel een ander antwoord geeft dan dat van de Méré; als we simpelweg de kansen voor de twee worpen bij elkaar zouden optellen zoals hij deed, dan zouden we (6,6) bij beide tellen, terwijl het eigenlijk maar één keer geteld zou moeten worden.\n\n\nOplossen van het probleem van de Méré\nBlaise Pascal gebruikte de regels van de waarschijnlijkheid om een oplossing te vinden voor het probleem van de Méré. Ten eerste besefte hij dat het berekenen van de kans op minstens één gebeurtenis uit een combinatie lastig was, terwijl het berekenen van de kans dat iets niet voorkomt bij meerdere gebeurtenissen relatief eenvoudig is – het is gewoon het product van de kansen van de afzonderlijke gebeurtenissen. Dus in plaats van de kans te berekenen op minstens één zes in vier worpen, berekende hij de kans op geen zessen in alle worpen:\n\\[\nP(\\text{geen zessen bij vier keer werpen}) = \\frac{5}{6}*\\frac{5}{6}*\\frac{5}{6}*\\frac{5}{6}=\\bigg(\\frac{5}{6}\\bigg)^4=0.482\n\\]\nHij gebruikte vervolgens het feit dat de kans op geen zessen in vier worpen het complement is van ten minste één zes in vier worpen (ze moeten dus optellen tot één), en gebruikte de aftrekregel om de kans op rente te berekenen:\n\\[\nP(\\text{op z'n minst een zes in vier worpen}) = 1 - \\bigg(\\frac{5}{6}\\bigg)^4=0.517\n\\]\nDe Méré’s gok dat hij minstens één zes zou gooien in vier worpen heeft een waarschijnlijkheid van meer dan 0,5, wat verklaart waarom de Méré gemiddeld geld verdiende met deze inzet.\nMaar hoe zit het met de Méré’s tweede weddenschap? Pascal gebruikte dezelfde truc:\n\\[\nP(\\text{geen dubbele zes in 24 worpen}) = \\bigg(\\frac{35}{36}\\bigg)^{24}=0.509\n\\]\n\\[\nP(\\text{ten minste een dubbele zes in 24 worpen}) = 1 - \\bigg(\\frac{35}{36}\\bigg)^{24}=0.491\n\\]\nDe kans op dit resultaat lag iets onder 0,5, wat aantoont waarom de Méré gemiddeld geld verloor op deze weddenschap."
  },
  {
    "objectID": "posts/2022-11-07-statistisch-denken/Stats21.html#waarschijnlijkheidsdistributies",
    "href": "posts/2022-11-07-statistisch-denken/Stats21.html#waarschijnlijkheidsdistributies",
    "title": "Statistisch denken in de 21ste eeuw",
    "section": "Waarschijnlijkheidsdistributies",
    "text": "Waarschijnlijkheidsdistributies\nEen waarschijnlijkheidsdistributie beschrijft de waarschijnlijkheid van alle mogelijke uitkomsten in een experiment. Bijvoorbeeld, op 20 januari 2018 raakte de basketballer Steph Curry slechts 2 van de 4 vrije worpen in een wedstrijd tegen de Houston Rockets. We weten dat Curry’s totale kans op vrije worpen over het hele seizoen 0,91 was, dus het lijkt vrij onwaarschijnlijk dat hij slechts 50% van zijn vrije worpen in een wedstrijd zou raken, maar hoe onwaarschijnlijk is het precies? We kunnen dit bepalen met behulp van een theoretische kansverdeling; in dit boek zullen we een aantal van deze kansverdelingen tegenkomen, die elk geschikt zijn om verschillende soorten gegevens te beschrijven. In dit geval gebruiken we de binomiale verdeling, die een manier biedt om de kans te berekenen op een bepaald aantal successen uit een aantal proeven met succes of mislukking en niets daartussen (bekend als “Bernoulli proeven”), gegeven een bekende kans op succes op elke proef. Deze verdeling wordt gedefinieerd als:\n\\[\nP(k; n,p) = P(X=k) = \\binom{n}{k} p^k(1-p)^{n-k}\n\\]\nDit verwijst naar de kans op k successen op n proeven als de kans op succes p is. Je bent misschien niet bekend met \\(\\binom{n}{k}\\), die de binomiaalcoëfficiënt wordt genoemd. De binomiaalcoëfficiënt wordt ook wel “n-kiezen” genoemd, omdat hij het aantal verschillende manieren beschrijft waarop men k items kan kiezen uit n totale items. De binomiaalcoëfficiënt wordt als volgt berekend:\n\\[\n\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\n\\] waarbij het uitroepteken (!) verwijst naar het factoriale van het getal:\n\\[\nn! = \\prod_{i=1}^n i = n*(n-1)*...*2*1\n\\]\nDe productoperator \\(\\prod\\) lijkt op de sommator \\(\\sum\\), maar vermenigvuldigt in plaats van telt op. In dit geval worden alle getallen van één tot \\(n\\) met elkaar vermenigvuldigd.\nIn het voorbeeld van Steph Curry’s vrije worpen:\n\\[\nP(2;4,0.91) = \\binom{4}{2} 0.91^2(1-0.91)^{4-2} = 0.040\n\\]\nHieruit blijkt dat gezien Curry’s totale percentage vrije worpen, het zeer onwaarschijnlijk is dat hij slechts 2 van de 4 vrije worpen zou raken. Zo zie je maar dat onwaarschijnlijke dingen echt gebeuren in de echte wereld.\n\nCumulatieve waarschijnlijkheids-distributies\nVaak willen we niet alleen weten hoe waarschijnlijk een bepaalde waarde is, maar ook hoe waarschijnlijk het is om een waarde te vinden die even extreem of meer is dan een bepaalde waarde; dit wordt heel belangrijk als later hypothesetesten worden besproken. Om deze vraag te beantwoorden kunnen we een cumulatieve kansverdeling gebruiken; terwijl een standaard kansverdeling ons de kans op een bepaalde waarde vertelt, vertelt de cumulatieve verdeling ons de kans op een waarde die even groot of groter (of even klein of kleiner) is dan een bepaalde waarde.\n\\[\nP(k\\le2)= P(k=2) + P(k=1) + P(k=0) = 6e^{-5} + .002 + .040 = .043  \n\\]\nIn veel gevallen zou het aantal mogelijke uitkomsten te groot zijn om de cumulatieve kans te berekenen door alle mogelijke waarden op te sommen; gelukkig kan deze direct worden berekend voor elke theoretische kansverdeling. De tabel hieronder toont de cumulatieve kans op elk mogelijk aantal geslaagde vrije worpen in het voorbeeld van hierboven, waaruit we kunnen opmaken dat de kans dat Curry 2 of minder vrije worpen maakt uit 4 pogingen 0,043 is.\n\n\n\nSimpele en cumulatieve waarschijnlijkheidsdistributies voor aantal succesvolle vrije worpen van Steph Curry in 4 pogingen.\n\n\nnumSuccesses\nProbability\nCumulativeProbability\n\n\n\n\n0\n0.000\n0.000\n\n\n1\n0.003\n0.003\n\n\n2\n0.040\n0.043\n\n\n3\n0.271\n0.314\n\n\n4\n0.686\n1.000"
  },
  {
    "objectID": "posts/2022-11-07-statistisch-denken/Stats21.html#conditionele-waarschijnlijkheid",
    "href": "posts/2022-11-07-statistisch-denken/Stats21.html#conditionele-waarschijnlijkheid",
    "title": "Statistisch denken in de 21ste eeuw",
    "section": "Conditionele waarschijnlijkheid",
    "text": "Conditionele waarschijnlijkheid\nTot nu toe hebben wij ons beperkt tot eenvoudige kansen - dat wil zeggen de kans op een enkele gebeurtenis of combinatie van gebeurtenissen. Vaak willen we echter de waarschijnlijkheid van een bepaalde gebeurtenis bepalen, gegeven dat een andere gebeurtenis heeft plaatsgevonden, wat bekend staat als conditionele waarschijnlijkheid.\nLaten we de Amerikaanse presidentsverkiezingen van 2016 als voorbeeld nemen. Er zijn twee eenvoudige waarschijnlijkheden die we kunnen gebruiken om het electoraat te beschrijven. Ten eerste weten we de kans dat een kiezer in de VS aangesloten is bij de Republikeinse partij: \\(p(Republikein) = 0,44\\). We weten ook de kans dat een kiezer zijn stem uitbrengt ten gunste van Donald Trump: \\(p(Trump-stemmer)=0,46\\). Laten we echter zeggen dat we het volgende willen weten: Wat is de kans dat iemand zijn stem op Donald Trump heeft uitgebracht, gegeven dat hij Republikein is?\nOm de voorwaardelijke kans op A gegeven B te berekenen (die we schrijven als \\(P(A|B)\\), “kans op A, gegeven B”), moeten we de gezamenlijke kans kennen (d.w.z. de kans dat zowel A als B voorkomen), evenals de totale kans op B:\n\\[\nP(A|B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\nDat wil zeggen, we willen weten hoe groot de kans is dat beide dingen waar zijn, gegeven dat hetgene waarop de voorwaarde betrekking heeft waar is.\n\n\n\n\n\nFiguur 3. Een grafische voorstelling van de voorwaardelijke waarschijnlijkheid, die laat zien hoe de voorwaardelijke waarschijnlijkheid onze analyse beperkt tot een deelverzameling van de gegevens.\n\n\n\n\nHet kan nuttig zijn dit grafisch voor te stellen. Hierboven zien we een stroomdiagram die laat zien hoe de volledige populatie kiezers uiteenvalt in Republikeinen en Democraten, en hoe de conditionele waarschijnlijkheid (conditionering op partij) de leden van elke partij verder uitsplitst naar hun stemgedrag."
  },
  {
    "objectID": "posts/2022-11-07-statistisch-denken/Stats21.html#vaststellen-van-conditionele-waarschijnlijkheden-van-data",
    "href": "posts/2022-11-07-statistisch-denken/Stats21.html#vaststellen-van-conditionele-waarschijnlijkheden-van-data",
    "title": "Statistisch denken in de 21ste eeuw",
    "section": "Vaststellen van conditionele waarschijnlijkheden van data",
    "text": "Vaststellen van conditionele waarschijnlijkheden van data\nWij kunnen voorwaardelijke kansen ook rechtstreeks uit gegevens berekenen. Stel dat we geïnteresseerd zijn in de volgende vraag: Wat is de kans dat iemand diabetes heeft, als hij niet lichamelijk actief is? – Dat is \\(P(diabetes|inactief)\\). De NHANES-dataset bevat twee variabelen die de twee delen van deze vraag behandelen. De eerste (Diabetes) vraagt of de persoon ooit te horen heeft gekregen dat hij diabetes heeft, de tweede (``PhysActive```) registreert of de persoon deelneemt aan sport, fitness of recreatieve activiteiten van ten minste matige intensiteit. Laten we eerst de eenvoudige kansen berekenen die in de tabel staan. De tabel laat zien dat de kans dat iemand in de NHANES-dataset diabetes heeft .1 is, en dat de kans dat iemand inactief is .45 is.\n\n\n\nSamenvattende data voor diabetes en fysieke activiteit\n\n\nAnswer\nN_diabetes\nP_diabetes\nN_PhysActive\nP_PhysActive\n\n\n\n\nNo\n4893\n0.8989528\n2472\n0.4541613\n\n\nYes\n550\n0.1010472\n2971\n0.5458387\n\n\n\n\n\n\n\n\nGedeelde waarschijnlijkheden voor Diabetes en FysActieve variabelen.\n\n\nDiabetes\nPhysActive\nn\nprob\n\n\n\n\nNo\nNo\n2123\n0.3900423\n\n\nNo\nYes\n2770\n0.5089105\n\n\nYes\nNo\n349\n0.0641191\n\n\nYes\nYes\n201\n0.0369282\n\n\n\n\n\nOm \\(P(diabetes|inactief)\\) te berekenen moeten we ook de gezamenlijke waarschijnlijkheid van diabetes en inactiviteit kennen, naast de eenvoudige waarschijnlijkheid van elk van beide. Deze staan in de tabel. Op basis van deze gezamenlijke kansen kunnen we \\(P(diabetes|inactief)\\) berekenen. Een manier om dit in een computerprogramma te doen is eerst te bepalen of de FysActieve variabele voor elk individu gelijk was aan “Nee”, en dan het gemiddelde van die waarheidswaarden te nemen. Aangezien TRUE/FALSE-waarden door de meeste programmeertalen (waaronder R en Python) worden behandeld als respectievelijk 1/0, kunnen we de waarschijnlijkheid van een eenvoudige gebeurtenis gemakkelijk bepalen door eenvoudigweg het gemiddelde te nemen van een logische variabele die de waarheidswaarde ervan weergeeft. Vervolgens gebruiken we die waarde om de voorwaardelijke kans te berekenen, waarbij we vinden dat de kans dat iemand diabetes heeft, gegeven dat hij lichamelijk inactief is, is 0.141."
  },
  {
    "objectID": "posts/2022-11-07-statistisch-denken/Stats21.html#onafhankelijkheid",
    "href": "posts/2022-11-07-statistisch-denken/Stats21.html#onafhankelijkheid",
    "title": "Statistisch denken in de 21ste eeuw",
    "section": "Onafhankelijkheid",
    "text": "Onafhankelijkheid\nDe term “onafhankelijk” heeft een zeer specifieke betekenis in de statistiek, die enigszins afwijkt van het gangbare gebruik van de term. Statistische onafhankelijkheid tussen twee variabelen betekent dat het kennen van de waarde van de ene variabele niets zegt over de waarde van de andere. Dit kan worden uitgedrukt als:\n\\[\nP(A|B) = P(A)\n\\]\nDat wil zeggen, de kans op A gegeven een bepaalde waarde van B is gewoon hetzelfde als de totale kans op A. Als we het zo bekijken, zien we dat veel gevallen van wat we in de echte wereld “onafhankelijkheid” zouden noemen, eigenlijk niet statistisch onafhankelijk zijn. Zo wil een kleine groep Californiërs momenteel een nieuwe onafhankelijke staat uitroepen, Jefferson genaamd, die een aantal provincies in Noord-Californië en Oregon zou omvatten. Als dit zou gebeuren, dan zou de kans dat een huidige inwoner van Californië nu in de staat Jefferson zou wonen \\(P({Jeffersonian})=0.014\\) zijn, terwijl de kans dat hij een inwoner van Californië zou blijven \\(P({Californian})=0.986\\) zou zijn. De nieuwe staten zouden politiek onafhankelijk kunnen zijn, maar ze zouden niet statistisch onafhankelijk zijn, want als we weten dat iemand Jeffersonian is, dan kunnen we er zeker van zijn dat hij niet Californiër is! Dat wil zeggen, terwijl onafhankelijkheid in het gewone taalgebruik vaak verwijst naar reeksen die elkaar uitsluiten, verwijst statistische onafhankelijkheid naar het geval waarin men niets kan voorspellen over een variabele op basis van de waarde van een andere variabele. Als je bijvoorbeeld iemands haarkleur kent, kun je waarschijnlijk niet zeggen of hij chocolade- of aardbeienijs prefereert.\nLaten we een ander voorbeeld bekijken, aan de hand van de NHANES-gegevens: Zijn lichamelijke en geestelijke gezondheid onafhankelijk van elkaar? NHANES bevat twee relevante vragen: PhysActive, die vraagt of de persoon lichamelijk actief is, en DaysMentHlthBad, die vraagt hoeveel dagen van de laatste 30 de persoon een slechte geestelijke gezondheid had. Laten we iedereen die de afgelopen maand meer dan 7 dagen een slechte geestelijke gezondheid had, beschouwen als iemand met een slechte geestelijke gezondheid. Op basis hiervan kunnen we een nieuwe variabele genaamd badMentalHealth definiëren als een logische variabele die aangeeft of elke persoon meer dan 7 dagen een slechte geestelijke gezondheid had of niet. We kunnen de gegevens eerst samenvatten om te laten zien hoeveel personen in elke combinatie van de twee variabelen vallen (weergegeven in de tabel hieronder en dan delen door het totale aantal waarnemingen om een tabel met verhoudingen te maken.\n\n\n\nSamenvatting van absolute frequentiedata voor mentale gezondheid en fysieke activiteit.\n\n\nPhysActive\nBad Mental Health\nGood Mental Health\nTotal\n\n\n\n\nNo\n629\n2510\n3139\n\n\nYes\n471\n3095\n3566\n\n\nTotal\n1100\n5605\n6705\n\n\n\n\n\n\n\n\nSamenvatting van relatieve frequentiedata voor mentale gezondheid en fysieke activiteit.\n\n\nPhysActive\nBad Mental Health\nGood Mental Health\nTotal\n\n\n\n\nNo\n0.0938106\n0.3743475\n0.4681581\n\n\nYes\n0.0702461\n0.4615958\n0.5318419\n\n\nTotal\n0.1640567\n0.8359433\n1.0000000\n\n\n\n\n\nDit toont ons het aandeel van alle waarnemingen die in elke cel vallen. Wat we hier echter willen weten is de voorwaardelijke kans op een slechte geestelijke gezondheid, afhankelijk van of men lichamelijk actief is of niet. Om dit te berekenen delen we elke groep lichaamsbeweging door het totale aantal waarnemingen, zodat elke rij nu sommeert tot één (weergegeven in Tabel hierboven). Hier zien we de voorwaardelijke kansen op een slechte of goede geestelijke gezondheid voor elke groep lichaamsbeweging (in de bovenste twee rijen) samen met de algemene kans op een goede of slechte geestelijke gezondheid in de derde rij. Om te bepalen of geestelijke gezondheid en lichamelijke activiteit onafhankelijk zijn, zouden we de eenvoudige kans op een slechte geestelijke gezondheid (in de derde rij) vergelijken met de voorwaardelijke kans op een slechte geestelijke gezondheid als men lichamelijk actief is (in de tweede rij).\n\n\n\nSamenvatting van conditionele waarschijnlijkheid voor mentale gezondheid gegeven fysieke activiteit.\n\n\nPhysActive\nBad Mental Health\nGood Mental Health\nTotal\n\n\n\n\nNo\n0.2003823\n0.7996177\n1\n\n\nYes\n0.1320808\n0.8679192\n1\n\n\nTotal\n0.1640567\n0.8359433\n1\n\n\n\n\n\nDe totale kans op slechte geestelijke gezondheid \\(P({ slechte geestelijke gezondheid})\\) is 0.1640567 terwijl de voorwaardelijke kans \\(P({ slechte geestelijke gezondheid|fysiek actief})\\) 0.1320808 is. Het lijkt er dus op dat de voorwaardelijke waarschijnlijkheid iets kleiner is dan de totale waarschijnlijkheid, wat suggereert dat ze niet onafhankelijk zijn, hoewel we dat niet zeker kunnen weten door alleen naar de getallen te kijken, omdat die getallen anders kunnen zijn door willekeurige variabiliteit in onze steekproef. Later in het boek bespreken we statistische hulpmiddelen waarmee we direct kunnen testen of twee variabelen onafhankelijk zijn."
  },
  {
    "objectID": "posts/2022-11-07-statistisch-denken/Stats21.html#omdraaien-van-een-conditionele-waarschijnlijkheid-bayes-regel",
    "href": "posts/2022-11-07-statistisch-denken/Stats21.html#omdraaien-van-een-conditionele-waarschijnlijkheid-bayes-regel",
    "title": "Statistisch denken in de 21ste eeuw",
    "section": "Omdraaien van een conditionele waarschijnlijkheid: Bayes’ regel",
    "text": "Omdraaien van een conditionele waarschijnlijkheid: Bayes’ regel\nIn veel gevallen weten we \\(P(A|B)\\) maar willen we eigenlijk \\(P(B|A)\\) weten. Dit komt vaak voor bij medische screening, waar we \\(P(positieve testuitslag|ziekte)\\) weten, maar wat we willen weten is \\(P(ziekte|positieve testuitslag)\\). Sommige artsen bevelen bijvoorbeeld aan dat mannen boven de 50 jaar een screening ondergaan met een test, die prostaat-specifiek antigeen (PSA) heet, om te screenen op mogelijke prostaatkanker. Voordat een test wordt goedgekeurd voor gebruik in de medische praktijk moet de fabrikant twee aspecten van de werking van de test testen. Ten eerste moeten ze aantonen hoe sensitief de test is - dat wil zeggen, hoe groot de kans is dat de ziekte wordt gevonden als die aanwezig is: \\(\\text{gevoeligheid} = P(\\text{positieve test|ziekte})\\). Ze moeten ook aantonen hoe specifiek de test is, dat wil zeggen hoe groot de kans is dat de test een negatief resultaat geeft als er geen ziekte aanwezig is: \\[\\text{specificiteit} = P(\\text{negatieve test|geen ziekte}\\]. Voor de PSA-test weten we dat de sensitiviteit ongeveer 80% en de specificiteit ongeveer 70% is. Deze geven echter geen antwoord op de vraag die de arts voor een bepaalde patiënt wil beantwoorden: hoe groot is de kans dat hij daadwerkelijk kanker heeft, als de test positief is? Dit vereist dat we de voorwaardelijke waarschijnlijkheid die de gevoeligheid definieert omkeren: in plaats van \\(P(positieve test|ziekte)\\) willen we \\(P(ziekte|positieve test)\\) weten.\nOm een voorwaardelijke kans om te keren, kunnen we de regel van Bayes gebruiken:\n\\[\nP(B|A) = \\frac{P(A|B)*P(B)}{P(A)}\n\\]\nDe regel van Bayes is vrij gemakkelijk af te leiden, op basis van de waarschijnlijkheidsregels die we eerder in het hoofdstuk hebben geleerd (zie de Appendix hiervoor).\nAls we slechts twee uitkomsten hebben, kunnen we de regel van Bayes iets duidelijker uitdrukken door de somregel te gebruiken om \\(P(A)\\) opnieuw te definiëren:\n\\[\nP(A) = P(A|B)*P(B) + P(A|\\neg B)*P(\\neg B)\n\\]\nAls we dit gebruiken, kunnen we Bayes’ regel omdraaien:\n\\[\nP(B|A) = \\frac{P(A|B)*P(B)}{P(A|B)*P(B) + P(A|\\neg B)*P(\\neg B)}\n\\]\nWe kunnen de relevante getallen in deze vergelijking stoppen om de waarschijnlijkheid te bepalen dat iemand met een positieve PSA-uitslag daadwerkelijk kanker heeft – maar om dit te kunnen doen, moeten we ook de algemene waarschijnlijkheid van kanker voor die persoon kennen, die we vaak het basispercentage noemen. Nemen we een man van 60 jaar voor wie de kans op prostaatkanker in de komende 10 jaar \\(P(kanker)=0,058\\) is. Met behulp van de sensitiviteits- en specificiteitswaarden die we hierboven hebben geschetst, kunnen we berekenen hoe groot de kans is dat iemand kanker krijgt bij een positieve test:\n\\[\nP(\\text{kanker|test}) = \\frac{P(\\text{test|kanker})*P(\\text{kanker})}{P(\\text{test|kanker})*P(\\text{kanker}) + P(\\text{test|}\\neg\\text{kanker})*P(\\neg\\text{kanker})}\n\\]\n\\[\n= \\frac{0.8*0.058}{0.8*0.058 +0.3*0.942 } = 0.14\n\\]\nDat is vrij klein – vindt u dat verrassend? Veel mensen wel en in feite is er een aanzienlijke psychologische literatuur die aantoont dat mensen systematisch basiscijfers (d.w.z. algemene prevalentie) verwaarlozen in hun oordeel."
  },
  {
    "objectID": "posts/2022-11-07-statistisch-denken/Stats21.html#leren-van-data",
    "href": "posts/2022-11-07-statistisch-denken/Stats21.html#leren-van-data",
    "title": "Statistisch denken in de 21ste eeuw",
    "section": "Leren van data",
    "text": "Leren van data\nEen andere manier om de regel van Bayes te zien is als een manier om onze overtuigingen bij te werken op basis van gegevens - dat wil zeggen, leren over de wereld met behulp van gegevens. Laten we de regel van Bayes nog eens bekijken:\n\\[\nP(B|A) =  \\frac{P(A|B)*P(B)}{P(A)}\n\\]\nDe verschillende onderdelen van de regel van Bayes hebben specifieke namen, die verband houden met hun rol bij het gebruik van de regel van Bayes om onze overtuigingen bij te werken. We beginnen met een eerste schatting van de waarschijnlijkheid van B (\\(P(B)\\)), die we de prior waarschijnlijkheid noemen. In het PSA-voorbeeld gebruikten we het basispercentage als onze prior, omdat dat onze beste inschatting was van de kans op kanker van de betrokkene voordat we het testresultaat kenden. Vervolgens verzamelen wij gegevens, in ons voorbeeld het testresultaat. De mate waarin de gegevens A overeenkomen met uitkomst B wordt gegeven door \\(P(A|B)\\), die we de waarschijnlijkheid noemen. U kunt dit zien als hoe waarschijnlijk de gegevens zijn, gegeven dat de bepaalde geteste hypothese waar is. In ons voorbeeld was de geteste hypothese of het individu kanker had, en de waarschijnlijkheid was gebaseerd op onze kennis over de sensitiviteit van de test (d.w.z. de kans op een positief testresultaat als er kanker aanwezig is). De noemer (\\(P(A)\\)) wordt de marginale waarschijnlijkheid genoemd, omdat deze de totale waarschijnlijkheid van de gegevens weergeeft, gemiddeld over alle mogelijke waarden van B (in ons voorbeeld ziekte aanwezig en ziekte afwezig). De uitkomst aan de linkerkant (\\(P(B|A)\\)) wordt de posterior genoemd, omdat die na de berekening eruit komt.\nEr is een andere manier om de regel van Bayes te schrijven die dit wat duidelijker maakt:\n\\[\nP(B|A) = \\frac{P(A|B)}{P(A)}*P(B)\n\\]\nHet linkerdeel (\\(\\frac{P(A|B)}{P(A)}\\)) vertelt ons hoeveel meer of minder waarschijnlijk de gegevens A zijn gegeven B, in verhouding tot de algemene (marginale) waarschijnlijkheid van de gegevens, terwijl het rechterdeel (\\(P(B)\\)) ons vertelt hoe waarschijnlijk we dachten dat B was voordat we iets over de gegevens wisten. Dit maakt duidelijker dat de rol van het theorema van Bayes erin bestaat onze voorkennis bij te werken op basis van de mate waarin de gegevens waarschijnlijker zijn gegeven B dan zij globaal zouden zijn. Als de hypothese gezien de gegevens waarschijnlijker is dan zij in het algemeen zou zijn, vergroten wij ons geloof in de hypothese; als zij gezien de gegevens minder waarschijnlijk is, verminderen wij ons geloof."
  },
  {
    "objectID": "posts/2022-11-07-statistisch-denken/Stats21.html#odds-en-odds-ratios",
    "href": "posts/2022-11-07-statistisch-denken/Stats21.html#odds-en-odds-ratios",
    "title": "Statistisch denken in de 21ste eeuw",
    "section": "Odds en odds ratio’s",
    "text": "Odds en odds ratio’s\nHet resultaat in de vorige paragraaf toonde aan dat de kans dat de persoon kanker heeft op basis van een positieve PSA-testuitslag nog steeds vrij laag is, ook al is die meer dan twee keer zo groot als voordat we de testuitslag kenden. Vaak willen we de relatie tussen kansen directer kwantificeren, wat we kunnen doen door ze om te zetten in odds die de relatieve waarschijnlijkheid uitdrukken dat iets wel of niet gebeurt:\n\\[\n\\text{odds van A} = \\frac{P(A)}{P(\\neg A)}\n\\]\nIn ons PSA voorbeeld is dat de kans op kanker (gegeven de positieve test):\n\\[\n\\text{odds van kanker} = \\frac{P(\\text{kanker})}{P(\\neg \\text{kanker})} =\\frac{0.14}{1 - 0.14} = 0.16\n\\]\nDit vertelt ons dat de kans vrij klein is dat je kanker hebt, ook al was de test positief. Ter vergelijking, de kans op een 6 bij een enkele worp met een dobbelsteen is:\n\\[\n\\text{odds van 6} = \\frac{1}{5} = 0.2\n\\]\nTerzijde: dit is een reden waarom veel medische onderzoekers steeds huiveriger zijn geworden voor het gebruik van wijdverbreide screeningtests voor relatief ongebruikelijke aandoeningen; de meeste positieve resultaten zullen vals-positief blijken te zijn, wat leidt tot onnodige vervolgonderzoeken met mogelijke complicaties, om nog maar te zwijgen van de extra stress voor de patiënt.\nWe kunnen kansen ook gebruiken om verschillende waarschijnlijkheden te vergelijken, door een zogenaamde odds ratio te berekenen - en dat is precies hoe het klinkt. Laten we bijvoorbeeld zeggen dat we willen weten hoeveel de positieve test de kans op kanker verhoogt. We kunnen eerst de prior odds berekenen, dat wil zeggen de kansen voordat we wisten dat de persoon positief getest was. Deze worden berekend met behulp van het basispercentage:\n\\[\n\\text{prior odds} = \\frac{P(\\text{kanker})}{P(\\neg \\text{kanker})} =\\frac{0.058}{1 - 0.058} = 0.061\n\\]\nWij kunnen deze dan vergelijken met de posterior odds, die worden berekend met behulp van de posterior waarschijnlijkheid:\n\\[\n\\text{odds ratio} = \\frac{\\text{posterior odds}}{\\text{prior odds}} = \\frac{0.16}{0.061} = 2.62\n\\]\nDit vertelt ons dat de kans om kanker te krijgen 2,62 keer zo groot is bij een positieve testuitslag. Een odds ratio is een voorbeeld van wat we later een effectgrootte zullen noemen, een manier om te kwantificeren hoe relatief groot een bepaald statistisch effect is."
  },
  {
    "objectID": "posts/2022-11-07-statistisch-denken/Stats21.html#wat-betekenen-waarschijnlijkheden",
    "href": "posts/2022-11-07-statistisch-denken/Stats21.html#wat-betekenen-waarschijnlijkheden",
    "title": "Statistisch denken in de 21ste eeuw",
    "section": "Wat betekenen waarschijnlijkheden?",
    "text": "Wat betekenen waarschijnlijkheden?\nHet is misschien een beetje vreemd om te praten over de waarschijnlijkheid dat iemand kanker heeft, afhankelijk van een testresultaat; de persoon heeft immers kanker of niet. Historisch gezien zijn er twee verschillende manieren om kansen te interpreteren. De eerste (bekend als de frequentistische interpretatie) interpreteert kansen in termen van langetermijnfrequenties. In het geval van het opgooien van een muntstuk bijvoorbeeld, zou de kans de relatieve frequentie van koppen op de lange termijn weergeven na een groot aantal opgooien. Hoewel deze interpretatie zinvol kan zijn voor gebeurtenissen die zich vele malen kunnen herhalen, zoals het opgooien van een munt, is zij minder zinvol voor gebeurtenissen die slechts eenmaal zullen plaatsvinden, zoals het leven van een individuele persoon of een bepaalde presidentsverkiezing; en zoals de econoom John Maynard Keynes ooit zei: “Op de lange termijn zijn we allemaal dood”.\nDe andere interpretatie van waarschijnlijkheden (bekend als de Bayesiaanse interpretatie) is als een mate van geloof in een bepaalde stelling. Als ik u zou vragen “Hoe waarschijnlijk is het dat de VS in 2040 naar de maan terugkeert”, kunt u deze vraag beantwoorden op basis van uw kennis en overtuigingen, ook al zijn er geen relevante frequenties om een frequentistische waarschijnlijkheid te berekenen. Een manier waarop we subjectieve kansen vaak formuleren is in termen van iemands bereidheid om een bepaalde gok te aanvaarden. Als u bijvoorbeeld denkt dat de kans dat de VS in 2040 op de maan zal landen 0,1 is (d.w.z. een kans van 9 tegen 1), dan betekent dit dat u bereid zou moeten zijn een gok te aanvaarden die meer dan 9 tegen 1 oplevert als de gebeurtenis plaatsvindt.\nZoals we zullen zien, zijn deze twee verschillende definities van waarschijnlijkheid zeer relevant voor de twee verschillende manieren waarop statistici denken over het testen van statistische hypothesen, die we in latere hoofdstukken zullen tegenkomen."
  },
  {
    "objectID": "posts/2022-11-07-statistisch-denken/Stats21.html#generatieve-modellen",
    "href": "posts/2022-11-07-statistisch-denken/Stats21.html#generatieve-modellen",
    "title": "Statistisch denken in de 21ste eeuw",
    "section": "Generatieve modellen",
    "text": "Generatieve modellen\nStel dat je op straat loopt en een vriend van je loopt voorbij, maar zegt geen gedag. Je zou waarschijnlijk proberen te bepalen waarom dit gebeurde – Heeft hij je niet gezien? Zijn ze boos op je? Ben je plotseling gehuld in een magisch onzichtbaarheidsschild? Een van de basisideeën achter Bayesiaanse statistiek is dat we de details willen afleiden van hoe de gegevens worden gegenereerd, op basis van de gegevens zelf. In dit geval wil je de gegevens (d.w.z. het feit dat je vriend geen gedag heeft gezegd) gebruiken om het proces af te leiden dat de gegevens heeft gegenereerd (d.w.z. of hij je wel of niet heeft gezien, wat hij voor je voelt, enz.)\nHet idee achter een generatief model is dat een latent (ongezien) proces de gegevens genereert die wij waarnemen, meestal met een zekere mate van willekeur in het proces. Wanneer wij een steekproef van gegevens uit een populatie nemen en uit de steekproef een parameter schatten, proberen wij in wezen de waarde te leren van een latente variabele (het populatiegemiddelde) die door bemonstering leidt tot de waargenomen gegevens (het steekproefgemiddelde). Het figuur hieronder toont een schematisch overzicht van dit idee.\n\n\n\n\n\nFiguur 4. Een schema van het idee van een generatief model.\n\n\n\n\nAls we de waarde van de latente variabele kennen, dan is het eenvoudig om te reconstrueren hoe de waargenomen gegevens eruit zouden moeten zien. Stel bijvoorbeeld dat we een munt opgooien waarvan we weten dat hij eerlijk is, zodat we verwachten dat hij in 50% van de gevallen op kop terechtkomt. We kunnen de munt beschrijven met een binomiale verdeling met een waarde van \\(P_{kop}=0,5\\), en dan kunnen we willekeurige steekproeven uit zo’n verdeling genereren om te zien hoe de waargenomen gegevens eruit zouden moeten zien. In het algemeen bevinden we ons echter in de omgekeerde situatie: We kennen de waarde van de latente variabele van belang niet, maar we hebben wel gegevens die we willen gebruiken om die te schatten."
  },
  {
    "objectID": "posts/2022-11-07-statistisch-denken/Stats21.html#bayes-theorema-en-inverse-inferentie",
    "href": "posts/2022-11-07-statistisch-denken/Stats21.html#bayes-theorema-en-inverse-inferentie",
    "title": "Statistisch denken in de 21ste eeuw",
    "section": "Bayes’ theorema en inverse inferentie",
    "text": "Bayes’ theorema en inverse inferentie\nDe Bayesiaanse statistiek heet zo omdat zij gebruik maakt van de stelling van Bayes om uit gegevens conclusies te trekken over het onderliggende proces dat de gegevens heeft voortgebracht. Stel dat we willen weten of een munt eerlijk is. Om dit te testen gooien we de munt 10 keer op en krijgen we 7 koppen. Vóór deze test waren we er vrij zeker van dat de \\(P_{kop}=0,5\\), maar 7 koppen vinden uit 10 opgooien zou ons zeker aan het denken zetten als we geloofden dat \\(P_{kop}=0,5\\). We weten al hoe we met behulp van de binomiale verdeling de voorwaardelijke kans kunnen berekenen dat we 7 of meer koppen op 10 gooien als de munt echt eerlijk is (\\(P(n\\ge7|p_{kop}=0,5)\\)).\n\n\n[1] 0.0546875\n\n\nDe resulterende waarschijnlijkheid is 0.055. Dat is een vrij klein getal, maar dit getal beantwoordt niet echt de vraag die we stellen – het zegt ons iets over de waarschijnlijkheid van 7 of meer koppen bij een bepaalde kans op koppen, terwijl we eigenlijk de werkelijke kans op koppen voor deze specifieke munt willen weten. Dit zou bekend moeten klinken, want het is precies de situatie waarin we ons bevonden met nulhypothesetests, die ons iets zeiden over de waarschijnlijkheid van gegevens in plaats van over de waarschijnlijkheid van hypothesen.\nVergeet niet dat de stelling van Bayes ons het instrument verschaft dat we nodig hebben om een voorwaardelijke kans om te keren:\n\\[\nP(H|D) = \\frac{P(D|H)*P(H)}{P(D)}\n\\]\nWe kunnen deze stelling zien als bestaande uit vier delen:\n\nprior (\\(P(Hypothese)\\)): Onze mate van overtuiging over hypothese H voordat we de Data D hebben gezien\nlikelihood (\\(P(Data|Hypothese)\\)): Hoe waarschijnlijk zijn de waargenomen Data D onder hypothese H?\nmarginale likelihood (\\(P(Data)\\)): Hoe waarschijnlijk zijn de waargenomen gegevens, gecombineerd over alle mogelijke hypothesen?\nposterior (\\(P(Hypothese|Data)\\)): Onze bijgewerkte overtuiging over hypothese H, gegeven de data D\n\nIn het geval van ons voorbeeld van de munt opgooien:\n\nprior (\\(P_{kop}\\)): Onze mate van overtuiging over de waarschijnlijkheid van kop gooien, die \\(P_{kop}=0,5\\) was.\nlikelihood (\\(P\\text{7 of meer koppen uit 10 worpen}|P_{kop}=0.5)\\)): Hoe groot is de kans op 7 of meer koppen uit 10 worpen als \\(P_{kop}=0,5)\\)?\nmarginale waarschijnlijkheid (\\(P\\text{7 of meer koppen uit 10 worpen})\\)): Hoe groot is de kans dat we in het algemeen 7 koppen op 10 munttesten waarnemen?\nposterior (\\(P_{kop}|{7 of meer koppen uit 10 munt worpen})\\)): Onze bijgewerkte overtuiging over \\(P_{kop}\\) gegeven de waargenomen muntworpen.\n\nHier zien we een van de belangrijkste verschillen tussen frequentistische en Bayesiaanse statistiek. Frequentisten geloven niet in het idee van een waarschijnlijkheid van een hypothese (d.w.z. onze mate van geloof in een hypothese) – voor hen is een hypothese waar of niet. Een andere manier om dit te zeggen is dat voor de frequentist de hypothese vaststaat en de gegevens willekeurig zijn. Daarom richt frequentistische inferentie zich op het beschrijven van de waarschijnlijkheid van data gegeven een hypothese (d.w.z. de p-waarde). Bayesianen daarentegen doen graag waarschijnlijkheidsuitspraken over zowel data als hypothesen."
  },
  {
    "objectID": "posts/2022-11-07-statistisch-denken/Stats21.html#een-bayesiaanse-schatting-doen",
    "href": "posts/2022-11-07-statistisch-denken/Stats21.html#een-bayesiaanse-schatting-doen",
    "title": "Statistisch denken in de 21ste eeuw",
    "section": "Een Bayesiaanse schatting doen",
    "text": "Een Bayesiaanse schatting doen\nUiteindelijk willen we Bayesiaanse statistiek gebruiken om beslissingen te nemen over hypothesen, maar voordat we dat doen moeten we de parameters schatten die nodig zijn om de beslissing te nemen. Hier zullen we het proces van Bayesiaanse schatting doorlopen. Laten we een ander screening voorbeeld gebruiken: Veiligheidscontrole op luchthavens. Als je veel vliegt, is het slechts een kwestie van tijd tot een van de willekeurige explosievencontroles positief uitvalt; ik had de bijzonder ongelukkige ervaring dat dit gebeurde kort na 11 september 2001, toen het luchthavenbeveiligingspersoneel bijzonder gespannen was.\n\nDe prior specificeren\nOm het theorema van Bayes te gebruiken, moeten we eerst de prior voor de hypothese specificeren. In dit geval kennen we het reële getal niet, maar we kunnen aannemen dat het vrij klein is. Volgens de FAA waren er in 2017 971.595.898 vliegtuigpassagiers in de VS. Laten we zeggen dat een van die reizigers een explosief in zijn tas had — dat zou een voorafgaande waarschijnlijkheid van 1 op 971 miljoen geven, wat erg klein is! Het beveiligingspersoneel kan in de maanden na de aanslag van 9/11 redelijkerwijs een sterkere waarschijnlijkheid hebben gehad, dus laten we zeggen dat hun subjectieve overtuiging was dat één op de miljoen reizigers een explosief bij zich droeg.\n\n\nWat data verzamelen\nDe gegevens bestaan uit de resultaten van de explosieve screeningtest. Laten we zeggen dat het beveiligingspersoneel de tas 3 keer door zijn testapparaat haalt en het geeft een positieve uitslag op 3 van de 3 tests.\n\n\nDe likelihood berekenen\nWe willen de waarschijnlijkheid van de gegevens berekenen onder de hypothese dat er een explosief in de zak zit. Laten we zeggen dat we weten (van de fabrikant van de machine) dat de sensititiviteit van de test 0.99 is – dat wil zeggen, wanneer een apparaat aanwezig is, zal het 99% van de tijd detecteren. Om de waarschijnlijkheid van onze gegevens te bepalen onder de hypothese dat een apparaat aanwezig is, kunnen we elke test behandelen als een Bernoulli trial (dat is een trial met een uitkomst van waar of onwaar) met een kans op succes van 0.99, die we kunnen modelleren met een binomiale verdeling.\n\n\nBerekenen van de marginale likelihood\nWe moeten ook de totale waarschijnlijkheid van de gegevens kennen – dat wil zeggen 3 positieven vinden uit 3 tests. Het berekenen van de marginale waarschijnlijkheid is vaak een van de moeilijkste aspecten van Bayesiaanse analyse, maar voor ons voorbeeld is het eenvoudig omdat we gebruik kunnen maken van de specifieke vorm van de stelling van Bayes voor een binaire uitkomst die we hebben geïntroduceerd in het vorige deel:\nWe moeten ook de totale waarschijnlijkheid van de gegevens kennen – dat wil zeggen 3 positieven vinden uit 3 tests. Het berekenen van de marginale waarschijnlijkheid is vaak een van de moeilijkste aspecten van Bayesiaanse analyse, maar voor ons voorbeeld is het eenvoudig omdat we gebruik kunnen maken van de specifieke vorm van de stelling van Bayes voor een binaire uitkomst die we eerder hebben geïntroduceerd:\n\\[\nP(E|T) = \\frac{P(T|E)*P(E)}{P(T|E)*P(E) + P(T|\\neg E)*P(\\neg E)}\n\\]\nwaarbij \\(E\\) refereert naar de aanwezigheid van explosieven, en \\(T\\) refereert naar een postief testresultaat.\nDe marginale waarschijnlijkheid is in dit geval een gewogen gemiddelde van de waarschijnlijkheid van de gegevens bij aan- of afwezigheid van het explosief, vermenigvuldigd met de waarschijnlijkheid dat het explosief aanwezig is (d.w.z. de prior). Laten we in dit geval zeggen dat we weten (van de fabrikant) dat de specificiteit van de test 0.99 is, zodat de waarschijnlijkheid van een positief resultaat wanneer er geen explosief is (\\(P(T|neg E)\\)) 0.01 is.\n\n\nDe posterior berekenen\nWe hebben nu alle onderdelen die we nodig hebben om de posterior kans op de aanwezigheid van een explosief te berekenen, gegeven de waargenomen 3 positieve uitkomsten uit 3 testen. Dit resultaat toont ons dat de posterior kans op een explosief in de zak gegeven deze positieve tests (0.492) net onder de 50% ligt, wat nogmaals benadrukt dat het testen op zeldzame gebeurtenissen bijna altijd een groot aantal fout-positieven kan opleveren, zelfs als de specificiteit en sensitiviteit zeer hoog zijn.\nEen belangrijk aspect van Bayesiaanse analyse is dat zij sequentieel kan zijn. Zodra we de posterior van één analyse hebben, kan deze de prior worden voor de volgende analyse!"
  },
  {
    "objectID": "posts/2022-11-07-statistisch-denken/Stats21.html#schatten-van-posterior-distributies",
    "href": "posts/2022-11-07-statistisch-denken/Stats21.html#schatten-van-posterior-distributies",
    "title": "Statistisch denken in de 21ste eeuw",
    "section": "Schatten van posterior distributies",
    "text": "Schatten van posterior distributies\nIn het vorige voorbeeld waren er slechts twee mogelijke uitkomsten - het explosieve is er of het is er niet - en we wilden weten welke uitkomst het meest waarschijnlijk was, gegeven de gegevens. Maar in andere gevallen willen we Bayesiaanse schattingen gebruiken om de numerieke waarde van een parameter te schatten. Laten we zeggen dat we willen weten hoe doeltreffend een nieuw pijnstiller is; om dit te testen kunnen we een groep patiënten het medicijn toedienen en hen dan vragen of hun pijn al dan niet verbeterd is na het innemen van het medicijn. We kunnen Bayesiaanse analyse gebruiken om aan de hand van deze gegevens te schatten voor welk deel van de mensen het geneesmiddel effectief zal zijn.\n\nSpecificeren van de prior\nIn dit geval hebben we geen voorafgaande informatie over de werkzaamheid van het geneesmiddel, dus gebruiken we een uniforme verdeling als onze prior, omdat alle waarden even waarschijnlijk zijn bij een uniforme verdeling. Om het voorbeeld te vereenvoudigen, bekijken we slechts een deelverzameling van 99 mogelijke waarden van werkzaamheid (van .01 tot .99, in stappen van .01). Daarom heeft elke mogelijke waarde een prioriteitswaarschijnlijkheid van 1/99.\n\n\nVerzamelen van wat data\nWe hebben gegevens nodig om het effect van het geneesmiddel te schatten. Laten we zeggen dat we het middel aan 100 personen toedienen en dat we merken dat 64 positief op het middel reageren.\n\n\nBerekenen van de likelihood\nWe kunnen de waarschijnlijkheid van de waargenomen gegevens onder een bepaalde waarde van de effectiviteitsparameter berekenen met behulp van de binomiale dichtheidsfunctie. In het figuur hieronder ziet je de waarschijnlijkheidscurven over het aantal responders voor verschillende waarden van \\(P_{respond}\\). Hieruit blijkt dat onze waargenomen gegevens relatief waarschijnlijker zijn onder de hypothese \\(P_{antwoord}=0,7\\), iets minder waarschijnlijk onder de hypothese \\(P_{antwoord}=0,5\\), en vrij onwaarschijnlijk onder de hypothese \\(P_{antwoord}=0,3\\). Een van de fundamentele ideeën van Bayesiaanse inferentie is dat wij ons geloof in waarden van de parameter die voor ons van belang is, moeten verhogen in verhouding tot hoe waarschijnlijk de gegevens zijn onder die waarden, afgewogen tegen wat wij geloofden over de parameterwaarden voordat wij de gegevens zagen (onze voorkennis).\n\n\n\n\n\nFiguur 5. Waarschijnlijkheid van elk mogelijk aantal responders onder verschillende hypothesen (p(respond)=0,5 (effen), 0,7 (gestippeld), 0,3 (gestreept). Waargenomen waarde weergegeven in de verticale lijn\n\n\n\n\n\n\nBerekenen van de marginale likelihood\nNaast de waarschijnlijkheid van de gegevens onder verschillende hypothesen, moeten we ook de totale waarschijnlijkheid van de gegevens kennen, waarbij alle hypothesen worden gecombineerd (d.w.z. de marginale waarschijnlijkheid). Deze marginale waarschijnlijkheid is vooral belangrijk omdat zij helpt waarborgen dat de posterior waarden ware waarschijnlijkheden zijn. In dit geval maakt ons gebruik van een reeks discrete mogelijke parameterwaarden het gemakkelijk om de marginale waarschijnlijkheid te berekenen, omdat we gewoon de waarschijnlijkheid van elke parameterwaarde onder elke hypothese kunnen berekenen en ze kunnen optellen.\n\n\nBereken de posterior\nWe hebben nu alle onderdelen die we nodig hebben om de posterior kansverdeling over alle mogelijke waarden van \\(p_{respond}\\) te berekenen, zoals hieronder weergegeven.\n\n\n\n\n\nFiguur 6. Posterior kansverdeling voor de waargenomen gegevens uitgezet in ononderbroken lijn tegen uniforme prior-verdeling (stippellijn). De maximale a posteriori (MAP) waarde wordt aangegeven door het diamantensymbool.\n\n\n\n\n\n\nMaximale A Posteriori (MAP) schatting\nGegeven onze gegevens willen we een schatting van \\(p_{reageert}\\) voor onze steekproef. Een manier om dit te doen is het vinden van de waarde van \\(p_{reageert}\\) waarvoor de posterior waarschijnlijkheid het hoogst is, wat we de maximale a posteriori (MAP) schatting noemen. We kunnen dit vinden aan de hand van de gegevens van hierboven — het is de waarde die met een marker bovenaan de verdeling staat. Merk op dat het resultaat (0.64) gewoon het percentage reageerders uit onze steekproef is – dit komt omdat de prior uniform was en dus onze schatting niet beïnvloedde.\n\n\nCredible intervallen (geloofwaardigheidsintervallen)\nVaak willen we niet alleen een enkele schatting voor de posterior weten, maar een interval waarin we er zeker van zijn dat de posterior valt. Het begrip betrouwbaarheidsintervallen wordt gebruikt in de context van frequentistische inferentie en je weet wellicht dat de interpretatie van betrouwbaarheidsintervallen bijzonder ingewikkeld is: Het is een interval dat 95% van de tijd de waarde van de parameter zal bevatten. Wat we eigenlijk willen is een interval waarin we er zeker van zijn dat de ware parameter valt, en de Bayesiaanse statistiek kan ons zo’n interval geven, dat we een credible interval (geloofwaardigheidsinterval) noemen.\nDe interpretatie van dit geloofwaardigheidsinterval ligt veel dichter bij wat we hadden gehoopt van een betrouwbaarheidsinterval (maar niet konden krijgen): Het vertelt ons dat er 95% kans is dat de waarde van \\(p_{reageert}\\) tussen deze twee waarden valt. Belangrijk is dat het in dit geval laat zien dat we een hoge mate van vertrouwen hebben dat \\(p_{reageert} &gt; 0,0\\), wat betekent dat het geneesmiddel een positief effect lijkt te hebben.\nIn sommige gevallen kan het geloofwaardigheidsinterval numeriek worden berekend op basis van een bekende verdeling, maar het is gebruikelijker om een geloofwaardigheidsinterval te genereren door monsters te nemen uit de posterior verdeling en dan kwantielen van de monsters te berekenen. Dit is vooral nuttig wanneer we geen gemakkelijke manier hebben om de posterior distributie numeriek uit te drukken, wat vaak het geval is bij echte Bayesiaanse gegevensanalyse. Eén zo’n methode (rejection sampling) wordt meer in detail uitgelegd in de Appendix aan het eind van dit hoofdstuk.\n\n\nEffecten van verschillende priors\nIn het vorige voorbeeld gebruikten we een vlakke prior, wat betekent dat we geen reden hadden om aan te nemen dat een bepaalde waarde van \\(p_{reageert}\\) meer of minder waarschijnlijk was. Maar stel dat we in plaats daarvan waren uitgegaan van eerdere gegevens: In een eerdere studie hadden onderzoekers 20 mensen getest en vastgesteld dat 10 van hen positief hadden gereageerd. Dit zou ons ertoe gebracht hebben te beginnen met de veronderstelling dat de behandeling bij 50% van de mensen effect heeft. We kunnen dezelfde berekening uitvoeren als hierboven, maar dan met de informatie uit de vorige studie als informatie voor onze prior.\nDe waarschijnlijkheid en de marginale waarschijnlijkheid zijn niet veranderd, alleen de prior. Het effect van de verandering in de prior is dat de posterior dichter bij de massa van de nieuwe prior, die gecentreerd is op 0,5, komt te liggen.\nLaten we nu eens kijken wat er gebeurt als we naar de analyse komen met een nog sterkere prior overtuiging. Stel dat in plaats van 10 responders op 20 mensen te hebben waargenomen, de voorafgaande studie 500 mensen had getest en 250 responders had gevonden. Dit zou ons in principe een veel sterkere prior moeten geven. De prior is veel meer geconcentreerd rond 0,5, en de posterior ligt ook veel dichter bij de prior. Het algemene idee is dat Bayesiaanse inferentie de informatie van de prior en de likelihood combineert, waarbij de relatieve sterkte van elk wordt gewogen.\nDit voorbeeld benadrukt ook het sequentiële karakter van Bayesiaanse analyse – de posterior van de ene analyse kan de prior worden voor de volgende analyse.\nTen slotte is het belangrijk te beseffen dat als de prioriteiten sterk genoeg zijn, ze de gegevens volledig kunnen overweldigen. Stel dat je een absolute prioriteit hebt dat \\(p_{reageert}\\) 0,8 of hoger is, zodat je de prior waarschijnlijkheid van alle andere waarden op nul zet. Wat gebeurt er dan als we de posterior berekenen?\n\n\nWarning: Use of `df$posteriors_absolute` is discouraged. Use\n`posteriors_absolute` instead.\n\n\n\n\n\nFiguur 7. A: Effecten van priors op de posterior verdeling. De oorspronkelijke posterior verdeling op basis van een vlakke prior is blauw gekleurd. De prior gebaseerd op de observatie van 10 responders op 20 personen is uitgezet in de gestippelde zwarte lijn, en de posterior met deze prioriteit is uitgezet in rood. B: Effecten van de sterkte van de prior op de posterior distributie. De blauwe lijn toont de posterior verkregen met de prior op basis van 50 reageerders op 100 personen. De gestippelde zwarte lijn toont de prior gebaseerd op 250 reageerders uit 500 observaties, en de rode lijn toont de posterior gebaseerd op die prioriteit. C: Effecten van de sterkte van de prior op de posterior verdeling. De blauwe lijn toont de posterior verkregen met een absolute prior die stelt dat p(antwoord) 0,8 of groter is. De prior is weergegeven in de gestippelde zwarte lijn.\n\n\n\n\nIn panel C van het figuur zien we dat er nul dichtheid is in de posterior voor elk van de waarden waar de prior op nul is gezet - de gegevens worden overweldigd door de absolute prior."
  },
  {
    "objectID": "posts/2022-11-07-statistisch-denken/Stats21.html#het-kiezen-van-een-prior",
    "href": "posts/2022-11-07-statistisch-denken/Stats21.html#het-kiezen-van-een-prior",
    "title": "Statistisch denken in de 21ste eeuw",
    "section": "Het kiezen van een prior",
    "text": "Het kiezen van een prior\nHet effect van priors op de resulterende conclusies is het meest controversiële aspect van de Bayesiaanse statistiek. Wat is de juiste prior om te gebruiken? Als de keuze van de prior bepalend is voor de resultaten (d.w.z. de posterior), hoe kun je er dan zeker van zijn dat je resultaten betrouwbaar zijn? Dit zijn moeilijke vragen, maar we moeten niet terugkrabbelen omdat we met moeilijke vragen worden geconfronteerd. Zoals we eerder hebben besproken, geven Bayesiaanse analyses ons interpretabele resultaten (geloofwaardige intervallen, enz.). Dit alleen al zou ons moeten inspireren om goed over deze vragen na te denken, zodat we tot redelijke en interpreteerbare resultaten komen.\nEr zijn verschillende manieren om prioriteiten te kiezen, die (zoals we hierboven zagen) de resulterende conclusies kunnen beïnvloeden. Soms hebben we een zeer specifieke priors, zoals in het geval waarin we verwachten dat onze munt 50% van de tijd op kop zal vallen, maar in veel gevallen hebben we niet zo’n sterk uitgangspunt. Oninformatieve prioriteiten proberen de resulterende posterior zo weinig mogelijk te beïnvloeden, zoals we zagen in het voorbeeld van de uniforme prior hierboven. Het is ook gebruikelijk om zwak informatieve priors (of standaardpriors) te gebruiken, die het resultaat slechts in zeer geringe mate beïnvloeden. Als we bijvoorbeeld een binomiale verdeling op basis van één kop van de twee munttoetsen hadden gebruikt, zou de prior rond 0,5 gecentreerd zijn, maar vrij vlak, en de posterior slechts licht beïnvloeden. Het is ook mogelijk priors te gebruiken die gebaseerd zijn op wetenschappelijke literatuur of reeds bestaande gegevens, die wij empirische priors noemen. In het algemeen zullen wij echter vasthouden aan het gebruik van niet-informatieve/zwak informatieve priors, omdat die de minste bezorgdheid wekken over de beïnvloeding van onze resultaten."
  },
  {
    "objectID": "posts/2022-11-07-statistisch-denken/Stats21.html#bayesiaanse-hypothesetests",
    "href": "posts/2022-11-07-statistisch-denken/Stats21.html#bayesiaanse-hypothesetests",
    "title": "Statistisch denken in de 21ste eeuw",
    "section": "Bayesiaanse hypothesetests",
    "text": "Bayesiaanse hypothesetests\nNu we geleerd hebben hoe Bayesiaanse schattingen moeten worden uitgevoerd, gaan we over tot het gebruik van Bayesiaanse methoden voor hypothesetests. Stel dat er twee politici zijn die van mening verschillen over de vraag of het publiek voorstander is van een extra belasting om de nationale parken te steunen. Senator Smith denkt dat slechts 40% van de mensen voor de belasting is, terwijl senator Jones denkt dat 60% van de mensen voor is. Om dit te testen laten zij een peiling uitvoeren, waarin 1000 willekeurig gekozen mensen wordt gevraagd of zij zo’n belasting steunen. De resultaten zijn dat 490 van de mensen in de steekproef voor de belasting zijn. Op basis van deze gegevens zouden we willen weten: Ondersteunen de gegevens de beweringen van de ene senator boven de andere, en met hoeveel? We kunnen dit testen met een concept dat bekend staat als de [Bayes factor] (https://bayesfactor.blogspot.com/2014/02/the-bayesfactor-package-this-blog-is.html), die kwantificeert welke hypothese beter is door te vergelijken hoe goed elke hypothese de waargenomen gegevens voorspelt."
  },
  {
    "objectID": "posts/2022-11-07-statistisch-denken/Stats21.html#bayesiaanse-hypothese-test",
    "href": "posts/2022-11-07-statistisch-denken/Stats21.html#bayesiaanse-hypothese-test",
    "title": "Statistisch denken in de 21ste eeuw",
    "section": "Bayesiaanse hypothese test",
    "text": "Bayesiaanse hypothese test\nNu we geleerd hebben hoe Bayesiaanse schattingen moeten worden uitgevoerd, gaan we over tot het gebruik van Bayesiaanse methoden voor hypothesetoetsing. Stel dat er twee politici zijn die van mening verschillen over de vraag of het publiek voorstander is van een extra belasting om de nationale parken te steunen. Senator Smith denkt dat slechts 40% van de mensen voor de belasting is, terwijl senator Jones denkt dat 60% van de mensen voor is. Om dit te testen laten zij een peiling uitvoeren, waarin 1000 willekeurig gekozen mensen wordt gevraagd of zij zo’n belasting steunen. De resultaten zijn dat 490 van de mensen in de steekproef voor de belasting zijn. Op basis van deze gegevens zouden we willen weten: Ondersteunen de gegevens de beweringen van de ene senator boven de andere, en met hoeveel? We kunnen dit testen met een concept dat bekend staat als de [Bayes factor] (https://bayesfactor.blogspot.com/2014/02/the-bayesfactor-package-this-blog-is.html), die kwantificeert welke hypothese beter is door te vergelijken hoe goed elke hypothese de waargenomen gegevens voorspelt.\n\nBayes factoren\nDe Bayes-factor kenmerkt de relatieve waarschijnlijkheid van de gegevens onder twee verschillende hypothesen. Hij wordt gedefinieerd als:\n\\[\nBF = \\frac{p(data|H_1)}{p(data|H_2)}\n\\]\nvoor twee hypothesen \\(H_1\\) en \\(H_2\\). In het geval van onze twee senatoren weten we hoe we de waarschijnlijkheid van de gegevens onder elke hypothese kunnen berekenen met behulp van de binomiale verdeling; laten we voorlopig aannemen dat onze prior waarschijnlijkheid dat elke senator gelijk heeft (\\(P_{H_1} = P_{H_2} = 0,5\\)). We zetten senator Smith in de teller en senator Jones in de noemer, zodat een waarde groter dan één aangeeft dat er meer bewijs is voor senator Smith, en een waarde kleiner dan één dat er meer bewijs is voor senator Jones. De resulterende Bayes-factor (3325.2567301) geeft een maat voor het bewijs dat de gegevens leveren voor de twee hypothesen - in dit geval vertellen ze ons dat de gegevens Senator Smith meer dan 3000 keer sterker ondersteunen dan Senator Jones.\n\n\nBayes factoren voor statistische hypothesen\nIn het vorige voorbeeld hadden we specifieke voorspellingen van elke senator, waarvan we de waarschijnlijkheid konden kwantificeren met behulp van de binomiale verdeling. Bovendien was onze prior waarschijnlijkheid voor de twee hypothesen gelijk. Bij de analyse van reële gegevens hebben wij echter meestal te maken met onzekerheid over onze parameters, wat de factor Bayes bemoeilijkt, omdat wij de marginale waarschijnlijkheid moeten berekenen (dat wil zeggen een geïntegreerd gemiddelde van de waarschijnlijkheden over alle mogelijke modelparameters, gewogen naar hun prior waarschijnlijkheid). In ruil daarvoor krijgen wij echter de mogelijkheid om de relatieve hoeveelheid bewijsmateriaal ten gunste van de nulhypothesen versus de alternatieve hypothesen te kwantificeren.\nStel dat wij een medisch onderzoeker zijn die een klinische proef uitvoert voor de behandeling van diabetes en wij willen weten of een bepaald geneesmiddel de bloedglucose verlaagt in vergelijking met een placebo. Wij werven een groep vrijwilligers en wijzen hen willekeurig toe aan een van beide groepen en wij meten de verandering in hemoglobine A1C (een marker voor het bloedglucosegehalte) in elke groep gedurende de periode waarin het geneesmiddel of het placebo werd toegediend. Wat we willen weten is: Is er een verschil tussen het medicijn en de placebo?\nLaten we eerst wat gegevens genereren en ze analyseren met behulp van nulhypothesetests. Laten we vervolgens een onafhankelijke t-test uitvoeren, waaruit blijkt dat er een significant verschil is tussen de groepen:\n\n\n\n\n\nFiguur 8. Box plots met gegevens voor medicijn- en placebogroepen.\n\n\n\n\n\n\n\n    Welch Two Sample t-test\n\ndata:  hbchange by group\nt = 2.0813, df = 32.091, p-value = 0.02273\nalternative hypothesis: true difference in means between group 0 and group 1 is greater than 0\n95 percent confidence interval:\n 0.1057051       Inf\nsample estimates:\nmean in group 0 mean in group 1 \n    -0.08248954     -0.65013353 \n\n\nDeze test vertelt ons dat er een significant verschil is tussen de groepen, maar kwantificeert niet hoe sterk het bewijs de nulhypothese versus de alternatieve hypothese ondersteunt. Om dat te meten kunnen we een Bayes-factor berekenen met de functie ttestBF uit het pakket BayesFactor in R:\n\n\nBayes factor analysis\n--------------\n[1] Alt., r=0.707 0&lt;d&lt;Inf    : 3.369297 ±0%\n[2] Alt., r=0.707 !(0&lt;d&lt;Inf) : 0.115034 ±0.01%\n\nAgainst denominator:\n  Null, mu1-mu2 = 0 \n---\nBayes factor type: BFindepSample, JZS\n\n\nWij zijn vooral geïnteresseerd in de Bayes-factor voor een effect groter dan nul. De Bayes-factor hier vertelt ons dat de alternatieve hypothese (d.w.z. dat het verschil groter is dan nul) ongeveer 3 keer waarschijnlijker is dan de nulhypothese (d.w.z. een gemiddeld verschil van precies nul), gegeven de data. Hoewel het effect dus significant is, is de hoeveelheid bewijs die het ons levert ten gunste van de alternatieve hypothese eerder zwak.\n\nEen zijde tests\nIn het algemeen zijn we minder geïnteresseerd in het toetsen tegen de nulhypothese van een specifieke puntwaarde (bv. gemiddeld verschil = 0) dan in het toetsen tegen een directionele nulhypothese (bv. dat het verschil kleiner is dan of gelijk aan nul). We kunnen ook een directionele (of eenzijdige) test uitvoeren met de resultaten van de ttestBF-analyse, aangezien die twee Bayes-factoren oplevert: één voor de alternatieve hypothese dat het gemiddelde verschil groter is dan nul, en één voor de alternatieve hypothese dat het gemiddelde verschil kleiner is dan nul. Als we het relatieve bewijs voor een positief effect willen beoordelen, kunnen we een Bayes-factor berekenen die het relatieve bewijs voor een positief versus een negatief effect vergelijkt door eenvoudig de twee door de functie teruggegeven Bayes-factoren te delen:\n\n\nBayes factor analysis\n--------------\n[1] Alt., r=0.707 0&lt;d&lt;Inf : 29.28958 ±0.01%\n\nAgainst denominator:\n  Alternative, r = 0.707106781186548, mu =/= 0 !(0&lt;d&lt;Inf) \n---\nBayes factor type: BFindepSample, JZS\n\n\nNu zien we dat de Bayes-factor voor een positief effect versus een negatief effect aanzienlijk groter is (bijna 30).\n\n\nInterpreting Bayes Factors\nHoe weten we of een Bayes-factor van 2 of 20 goed of slecht is? Er is een algemene richtlijn voor de interpretatie van Bayes-factoren voorgesteld door [Kass & Rafferty (1995)] (https://www.andrew.cmu.edu/user/kk3n/simplicity/KassRaftery1995.pdf):\n\n\n\nBF\nSterkte van bewijs\n\n\n\n\n1 tot 3\nnauwelijk benoemen waard\n\n\n3 tot 20\npositief\n\n\n20 tot 150\nsterk\n\n\n&gt;150\nheel sterk\n\n\n\nOp basis hiervan is, ook al is het statistische resultaat significant, de hoeveelheid bewijs ten gunste van de alternatieve versus de punt-nulhypothese zo zwak dat het nauwelijks het vermelden waard is, terwijl het bewijs voor de richtingshypothese relatief sterk is.\n\n\n\nBeoordeling van het bewijs voor de nulhypothese\nOmdat de Bayes-factor het bewijs voor twee hypothesen vergelijkt, kunnen we ook beoordelen of er bewijs is ten gunste van de nulhypothese, wat we niet kunnen doen met standaard nulhypothesetests (omdat die uitgaan van de veronderstelling dat de nulhypothese waar is). Dit kan zeer nuttig zijn om te bepalen of een niet-significant resultaat werkelijk sterk bewijs oplevert dat er geen effect is, of in plaats daarvan slechts een algemeen zwak bewijs weergeeft."
  },
  {
    "objectID": "posts/2022-11-07-statistisch-denken/Stats21.html#wat-hebben-we-geleerd",
    "href": "posts/2022-11-07-statistisch-denken/Stats21.html#wat-hebben-we-geleerd",
    "title": "Statistisch denken in de 21ste eeuw",
    "section": "Wat hebben we geleerd",
    "text": "Wat hebben we geleerd\nNa het lezen hiervan ben je in staat om:\n\nDe steekproefruimte voor een geselecteerd willekeurig experiment te beschrijven.\nDe relatieve frequentie en empirische waarschijnlijkheid voor een gegeven reeks gebeurtenissen kunnen berekenen.\nKansen berekenen van afzonderlijke gebeurtenissen, complementaire gebeurtenissen en de unies en intersecties van verzamelingen van gebeurtenissen.\nDe wet van de grote aantallen beschrijven.\nBeschrijf het verschil tussen een kans en een voorwaardelijke kans.\nHet concept van statistische onafhankelijkheid beschrijven.\nDe stelling van Bayes gebruiken om de inverse voorwaardelijke kans te berekenen.\n\nDaarnaast ben je ook in staat om:\n\nDe belangrijkste verschillen tussen Bayesiaanse analyse en nulhypothesetests beschrijven;\nDe stappen in een Bayesiaanse analyse beschrijven en uitvoeren;\nDe effecten van verschillende prioriteiten en de overwegingen bij de keuze van een prioriteit beschrijven;\nHet verschil in interpretatie tussen een betrouwbaarheidsinterval en een Bayesiaans geloofwaardigheidsinterval beschrijven."
  },
  {
    "objectID": "posts/2022-11-07-statistisch-denken/Stats21.html#suggesties-om-te-lezen",
    "href": "posts/2022-11-07-statistisch-denken/Stats21.html#suggesties-om-te-lezen",
    "title": "Statistisch denken in de 21ste eeuw",
    "section": "Suggesties om te lezen",
    "text": "Suggesties om te lezen\n\nThe Drunkard’s Walk: How Randomness Rules Our Lives, van Leonard Mlodinow\nTen Great Ideas about Chance, van Persi Diaconis en Brian Skyrms\nThe Theory That Would Not Die: How Bayes’ Rule Cracked the Enigma Code, Hunted Down Russian Submarines, and Emerged Triumphant from Two Centuries of Controversy, van Sharon Bertsch McGrayne\nDoing Bayesian Data Analysis: A Tutorial Introduction with R, van John K. Kruschke"
  },
  {
    "objectID": "posts/2022-11-13-visualisaties/crimebynumbersHarrie.html#een-eenvoudige-kaart",
    "href": "posts/2022-11-13-visualisaties/crimebynumbersHarrie.html#een-eenvoudige-kaart",
    "title": "Visualisaties",
    "section": "Een eenvoudige kaart",
    "text": "Een eenvoudige kaart\nOm deze kaarten te maken gebruiken we het pakket ggmap.\n\n# Harrie: alleen installeren als je het pakket nog niet hebt\n#| eval: false\n#install.packages(\"ggmap\")\n\n\nlibrary(ggmap)\n\nLoading required package: ggplot2\n\n\nGoogle's Terms of Service: https://cloud.google.com/maps-platform/terms/.\n\n\nPlease cite ggmap if you use it! See citation(\"ggmap\") for details.\n\n\nWe beginnen met het maken van de achtergrond van onze kaart, die San Francisco laat zien. Dat doen we met de get_map() functie van ggmap, die een kaartachtergrond krijgt van een aantal bronnen. We stellen de bron in op “stamen”, omdat Google ons niet langer toestaat een kaart te krijgen zonder een account aan te maken. De eerste parameter in get_map() zijn simpelweg coördinaten voor de bounding box van San Francisco om er zeker van te zijn dat we een kaart van de juiste plek krijgen. Een bounding box bestaat uit vier coördinaten die samen een rechthoek vormen en wordt gebruikt om te bepalen waar in de wereld de kaart wordt getoond.\nEen gemakkelijke manier om de vier coördinaten voor een bounding box te vinden is naar de site Bounding Box. Deze site heeft een kaart van de wereld en een box op het scherm. Verplaats de doos naar het gebied waarvan u de kaart wilt hebben. Misschien moet u de grootte van de doos aanpassen om het gebied dat u wilt te bedekken. Verander dan in het gedeelte “Kopiëren en plakken” de dropdown box in “CSV”. In de sectie rechts hiervan staan de vier getallen die de bounding box vormen. Je kunt die nummers kopiëren in get_map().\n\n\n\n\n\n\nsf_map &lt;- ggmap(get_map(c(-122.530392,37.698887,-122.351177,37.812996), \n                            source = \"stamen\"))\n\nSource : http://tile.stamen.com/terrain/12/653/1582.png\n\n\nSource : http://tile.stamen.com/terrain/12/654/1582.png\n\n\nSource : http://tile.stamen.com/terrain/12/655/1582.png\n\n\nSource : http://tile.stamen.com/terrain/12/653/1583.png\n\n\nSource : http://tile.stamen.com/terrain/12/654/1583.png\n\n\nSource : http://tile.stamen.com/terrain/12/655/1583.png\n\n\nSource : http://tile.stamen.com/terrain/12/653/1584.png\n\n\nSource : http://tile.stamen.com/terrain/12/654/1584.png\n\n\nSource : http://tile.stamen.com/terrain/12/655/1584.png\n\nsf_map\n\n\n\n\nOmdat we de kaartuitvoer hebben opgeslagen in sf_map kunnen we deze kaartachtergrond hergebruiken voor alle kaarten die we maken. Dit bespaart ons tijd, omdat we niet elke keer hoeven te wachten om de kaart te downloaden. Laten we de zelfmoorden uit onze dataset plotten. Net als bij een scatterplot gebruiken we de geom_point() functie uit het ggplot2 pakket en zetten we onze lengte- en breedtegraad variabelen op respectievelijk de x- en y-as. Wanneer we ggmap laden, wordt ook automatisch ggplot2 geladen, omdat dat pakket nodig is om ggmap te laten werken, zodat we zelf geen library(ggplot2) hoeven te doen.\n\nsf_map +\n  geom_point(aes(x = X, y = Y),\n             data  = suicide)\n\nWarning: Removed 1 rows containing missing values (geom_point).\n\n\n\n\n\nAls we de stippen willen kleuren, kunnen we color = gebruiken en dan een kleur kiezen. Laten we het proberen met “forestgreen.”\n\nsf_map +\n  geom_point(aes(x = X, y = Y),\n             data  = suicide,\n             color = \"forestgreen\")\n\nWarning: Removed 1 rows containing missing values (geom_point).\n\n\n\n\n\nNet als bij andere grafieken kunnen we de grootte van de punt veranderen met size =.\nKleiner:\n\nsf_map +\n  geom_point(aes(x = X, y = Y),\n             data  = suicide,\n             color = \"forestgreen\",\n             size  = 0.5)\n\nWarning: Removed 1 rows containing missing values (geom_point).\n\n\n\n\n\nGroter:\n\nsf_map +\n  geom_point(aes(x = X, y = Y),\n             data  = suicide,\n             color = \"forestgreen\",\n             size  = 2)\n\nWarning: Removed 1 rows containing missing values (geom_point).\n\n\n\n\n\nVoor kaarten als deze - met één punt per gebeurtenis - is het moeilijk te zien of er gebeurtenissen plaatsvinden op dezelfde, of bijna dezelfde, plaats, aangezien elk punt ononderbroken groen is. We willen de punten semi-transparant maken, zodat als er meerdere zelfmoorden plaatsvinden op dezelfde plaats, dat punt donkerder wordt gearceerd dan wanneer er slechts één zelfmoord plaatsvond. Daarvoor gebruiken we de parameter alpha = die een waarde tussen 0 en 1 aanneemt. Hoe lager de waarde, hoe transparanter de stip.\n\nsf_map +\n  geom_point(aes(x = X, y = Y),\n             data  = suicide,\n             color = \"forestgreen\",\n             size  = 2,\n             alpha = 0.5)\n\nWarning: Removed 1 rows containing missing values (geom_point).\n\n\n\n\n\nDeze kaart is nuttig omdat ze ons toelaat gemakkelijk te zien waar elke zelfmoord in San Francisco plaatsvond tussen 2003 en 2017. Er zijn echter enkele beperkingen. Zo toont deze kaart alle zelfmoorden in één kaart, wat betekent dat trends in de tijd verloren gaan."
  },
  {
    "objectID": "posts/2022-11-13-visualisaties/crimebynumbersHarrie.html#wat-zijn-kaarten-eigenlijk",
    "href": "posts/2022-11-13-visualisaties/crimebynumbersHarrie.html#wat-zijn-kaarten-eigenlijk",
    "title": "Visualisaties",
    "section": "Wat zijn kaarten eigenlijk?",
    "text": "Wat zijn kaarten eigenlijk?\nLaten we even stilstaan bij wat een kaart eigenlijk is. Ik heb de volgende eenvoudige scatterplot gemaakt van onze gegevens met één stip per zelfmoord (minus die zonder coördinaten). Vergelijk dit met de vorige kaart en je zult zien dat ze hetzelfde zijn, behalve dat de kaart een nuttige achtergrond heeft, terwijl de plot een lege achtergrond heeft. Dat is alles wat statische kaarten zijn (in het hoofdstuk interactieve kaarten zullen we leren over interactieve kaarten), spreidingskaarten van coördinaten op een achtergrond van een kaart. Eigenlijk zijn het spreidingskaarten met context. En die context is nuttig; we kunnen de kaart interpreteren om te zien dat er bijvoorbeeld veel zelfmoorden zijn in het noordoosten van San Francisco, maar niet zoveel elders. Precies hetzelfde patroon is aanwezig in de scatterplot, maar zonder de mogelijkheid om te vertellen “waar” een stip is.\n\nplot(suicide$X, suicide$Y, col = \"forestgreen\")"
  },
  {
    "objectID": "posts/2022-11-13-visualisaties/crimebynumbersHarrie.html#een-hotspotkaart-maken",
    "href": "posts/2022-11-13-visualisaties/crimebynumbersHarrie.html#een-hotspotkaart-maken",
    "title": "Visualisaties",
    "section": "Een hotspotkaart maken",
    "text": "Een hotspotkaart maken\nNu kunnen we beginnen met het maken van hotspotkaarten, die helpen om gebieden met clusters van gebeurtenissen weer te geven. We doen dit met hexagonale bins, een efficiënte manier om clusters van gebeurtenissen op een kaart weer te geven. Onze syntax is vergelijkbaar met de kaart hierboven, maar nu willen we de functie stat_binhex() gebruiken in plaats van geom_point(). Het begint hetzelfde als voorheen met aes(x = X, y = Y) (of hoe de kolommen lengtegraad en breedtegraad ook heten in jouw gegevens), evenals data = suicide buiten de aes() parameter.\nEr zijn twee nieuwe dingen die we nodig hebben om de hotspotkaart te maken. Ten eerste voegen we de parameter bins = number_of_bins toe, waarbij “number_of_bins” een getal is dat we kiezen. bins zegt in wezen hoe groot of klein we elk cluster van gebeurtenissen willen hebben. Een kleinere waarde voor bins zegt dat we meer gebeurtenissen geclusterd willen hebben, waardoor grotere bins ontstaan. Een grotere waarde voor bins laat elke bin kleiner zijn op de kaart en minder gebeurtenissen bevatten. Dit zal duidelijker worden met voorbeelden.\nHet tweede is het toevoegen van de functie coord_cartesian(), die ggplot() vertelt dat we een ruimtelijke analyse gaan uitvoeren bij het maken van de bins. We hoeven hier geen parameters aan toe te voegen.\nOm stat_binhex() te gebruiken, moeten we er ook voor zorgen dat het pakket hexbin geïnstalleerd is. stat_binhex() zal de benodigde functie van hexbin intern aanroepen zodat we library(hexbin) niet hoeven te draaien.\n\n# Wel installeren als je dat nog niet gedaan hebt\n#| echo: false\n# install.packages(\"hexbin\")\n\nLaten we beginnen met 60 bins en dan een ander aantal bins proberen om te zien hoe dat de kaart verandert.\n\n\nCoordinate system already present. Adding new coordinate system, which will replace the existing one.\n\n\nWarning: Removed 1 rows containing non-finite values (stat_binhex).\n\n\n\n\n\n\n\nWarning: Removed 1 rows containing non-finite values (stat_binhex).\n\n\n\n\n\nUit deze kaart blijkt dat de meeste gebieden in de stad geen zelfmoorden hadden en dat de gebieden met de meeste zelfmoorden in het centrum van San Francisco liggen.\nWat gebeurt er als we het aantal bins verlagen tot 30?\n\n\nCoordinate system already present. Adding new coordinate system, which will replace the existing one.\n\n\nWarning: Removed 1 rows containing non-finite values (stat_binhex).\n\n\n\n\n\n\n\nWarning: Removed 1 rows containing non-finite values (stat_binhex).\n\n\n\n\n\nElke bin is veel groter en bestrijkt bijna heel San Francisco. Wees voorzichtig met kaarten als deze! Deze kaart is zo breed dat het lijkt alsof zelfmoorden overal in de stad voorkomen. We weten uit de kaart waarop elke zelfmoord als een stip wordt weergegeven dat er minder dan 1300 zelfmoorden zijn; dit is dus niet waar. Kaarten als deze maken het gemakkelijk om de lezer te misleiden, inclusief jezelf!\nHoe zit het met het kijken naar 100 bins?\n\n\nCoordinate system already present. Adding new coordinate system, which will replace the existing one.\n\n\nWarning: Removed 1 rows containing non-finite values (stat_binhex).\n\n\n\n\n\n\n\nWarning: Removed 1 rows containing non-finite values (stat_binhex).\n\n\n\n\n\nNu is elke bin erg klein en een veel kleiner gebied in San Francisco heeft een zelfmoord gehad. Dus wat is het juiste aantal bins om te gebruiken? Hier is geen universeel antwoord op te geven- je moet beslissen wat het doel is van de gegevens die je gebruikt. Dit levert ernstige problemen op voor - al dan niet opzettelijke - manipulatie van de gegevens, aangezien de kaart zo gemakkelijk kan worden gewijzigd zonder dat de gegevens zelf veranderen.\n\nKleuren\nOm de kleuren van de bin te veranderen kunnen we de parameter scale_fill_gradient() gebruiken. Deze accepteert een kleur voor “laag”, wanneer de gebeurtenissen zeldzaam zijn, en “hoog” voor de bakken met frequente gebeurtenissen. We gebruiken kleuren uit ColorBrewer, en selecteren het geel-roodachtige thema (“3-class YlOrRd”) uit de Multi-hue sectie van het “sequentiële” gegevensgedeelte van de pagina.\n\nsf_map +\n  stat_binhex(aes(x = X, y = Y),\n              bins  = 60,\n              data = suicide) +\n  coord_cartesian() +\n  scale_fill_gradient(low = \"#ffeda0\",\n                      high = \"#f03b20\")\n\n\n\nWarning: Removed 1 rows containing non-finite values (stat_binhex).\n\n\n\n\n\nStandaard labelt het de legende als “count”. Aangezien we weten dat het hier gaat om tellingen van zelfmoorden, laten we dat als zodanig labelen.\n\nsf_map +\n  stat_binhex(aes(x = X, y = Y),\n              bins  = 60,\n              data = suicide) +\n  coord_cartesian() +\n  scale_fill_gradient('Zelfmoorden',\n                      low = \"#ffeda0\",\n                      high = \"#f03b20\")\n\n\n\nWarning: Removed 1 rows containing non-finite values (stat_binhex)."
  },
  {
    "objectID": "posts/2022-11-13-visualisaties/crimebynumbersHarrie.html#ruimtelijke-verbindingen",
    "href": "posts/2022-11-13-visualisaties/crimebynumbersHarrie.html#ruimtelijke-verbindingen",
    "title": "Visualisaties",
    "section": "Ruimtelijke verbindingen",
    "text": "Ruimtelijke verbindingen\nWat we met deze buurten willen doen is uitzoeken in welke buurt elke zelfmoord plaatsvond en het aantal zelfmoorden per buurt optellen. Zodra we dat gedaan hebben, kunnen we een kaart maken op buurtniveau en het aantal zelfmoorden per buurt meten. Een ruimtelijke verbinding (spatial join) lijkt sterk op gewone verbindingen waarbij we twee datasets samenvoegen op basis van gemeenschappelijke variabelen (zoals de naam van de staat of de unieke ID-code van een persoon). In dit geval voegt het samen op basis van een gedeeld geografisch kenmerk, als twee lijnen elkaar snijden of (zoals we hier zullen doen) als een punt binnen een bepaald geografisch gebied ligt.\nOp dit moment staan onze suïcide gegevens in een data.frame met wat informatie over elke zelfmoord en de lengte- en breedtegraad van de zelfmoord in aparte kolommen. We willen dit data.frame omzetten in een ruimtelijk object zodat we kunnen vinden in welke buurt elke zelfmoord plaatsvond. We kunnen het omzetten in een ruimtelijk object met behulp van de st_as_sf() functie van sf. Onze invoer is eerst onze data, suïcide. Dan zetten we in de coords parameter een vector van de kolomnamen zodat de functie weet welke kolommen de lengte- en breedtegraad zijn zodat hij die kolommen kan converteren naar een “geometrie” kolom zoals we eerder zagen in sf_neighborhoods. We stellen de CRS in op de WGS84-standaard die we eerder zagen, maar we passen hem aan aan de CRS van de buurtgegevens.\n\nsuicide &lt;- st_as_sf(suicide, \n                    coords = c(\"X\", \"Y\"),\n                    crs = \"+proj=longlat +ellps=WGS84 +no_defs\")\n\nWe willen onze zelfmoordgegevens in dezelfde projectie als de buurtgegevens, dus moeten we st_transform() gebruiken om de projectie te veranderen. Aangezien we willen dat de CRS dezelfde is als in sf_neighborhoods, kunnen we deze instellen met st_crs(sf_neighborhoods) om de juiste CRS te gebruiken.\n\nsuicide &lt;- st_transform(suicide, \n                        crs = st_crs(sf_neighborhoods))\n\nNu kunnen we er naar kijken met head() om te zien of het is geprojecteerd.\n\nhead(suicide)\n\nSimple feature collection with 6 features and 12 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -122.4893 ymin: 37.72218 xmax: -122.3964 ymax: 37.79414\nGeodetic CRS:  WGS84(DD)\n  IncidntNum Category                           Descript DayOfWeek       Date\n1  180318931  SUICIDE ATTEMPTED SUICIDE BY STRANGULATION    Monday 04/30/2018\n2  180315501  SUICIDE       ATTEMPTED SUICIDE BY JUMPING  Saturday 04/28/2018\n3  180295674  SUICIDE              SUICIDE BY LACERATION  Saturday 04/21/2018\n4  180263659  SUICIDE                            SUICIDE   Tuesday 04/10/2018\n5  180235523  SUICIDE     ATTEMPTED SUICIDE BY INGESTION    Friday 03/30/2018\n6  180236515  SUICIDE            SUICIDE BY ASPHYXIATION  Thursday 03/29/2018\n      Time PdDistrict Resolution                 Address\n1 06:30:00    TARAVAL       NONE     0 Block of BRUCE AV\n2 17:54:00   NORTHERN       NONE   700 Block of HAYES ST\n3 12:20:00   RICHMOND       NONE   3700 Block of CLAY ST\n4 05:13:00    CENTRAL       NONE     0 Block of DRUMM ST\n5 09:15:00    TARAVAL       NONE 0 Block of FAIRFIELD WY\n6 17:30:00   RICHMOND       NONE    300 Block of 29TH AV\n                                        Location         PdId year\n1  POINT (-122.45168059935614 37.72218061554315) 1.803189e+13 2018\n2  POINT (-122.42876060987851 37.77620120112792) 1.803155e+13 2018\n3   POINT (-122.45462091999406 37.7881754224736) 1.802957e+13 2018\n4  POINT (-122.39642194376758 37.79414474237039) 1.802637e+13 2018\n5  POINT (-122.46324153155875 37.72679184368551) 1.802355e+13 2018\n6 POINT (-122.48929119750689 37.782735835121265) 1.802365e+13 2018\n                    geometry\n1 POINT (-122.4517 37.72218)\n2  POINT (-122.4288 37.7762)\n3 POINT (-122.4546 37.78818)\n4 POINT (-122.3964 37.79414)\n5 POINT (-122.4632 37.72679)\n6 POINT (-122.4893 37.78274)\n\n\nWe zien dat het nu een “simple feature collection” is met de juiste projectie. En we zien dat er een nieuwe kolom “geometrie” is, net als in sf_neighborhoods. Het gegevenstype in “geometrie” is PUNT, aangezien onze gegevens slechts een enkele locatie zijn in plaats van een veelhoek zoals in de buurtgegevens.\nAangezien we zowel de buurt- als de zelfmoordengegevens hebben, maken we snel een kaart om de gegevens te zien.\n\nplot(sf_neighborhoods$geometry)\nplot(suicide$geometry, add = TRUE, col = \"red\")\n\n\n\n\nOnze volgende stap is het combineren van deze twee datasets om uit te zoeken hoeveel zelfmoorden er in elke buurt plaatsvonden. Dit wordt een proces in meerdere stappen, dus laten we het eerst plannen. Onze zelfmoordgegevens zijn één rij voor elke zelfmoord; onze buurtgegevens zijn één rij voor elke buurt. Aangezien ons doel is om de gegevens op buurtniveau in kaart te brengen, moeten we de buurt berekenen waar elke zelfmoord plaatsvond en vervolgens aggregeren tot buurtniveau om een telling van de zelfmoorden per buurt te krijgen. Dan moeten we dat combineren met de oorspronkelijke buurtgegevens en kunnen we die in kaart brengen.\n\nZoek uit in welke buurt elke zelfmoord plaatsvond.\nVoeg de zelfmoordgegevens samen tot we een rij per buurt krijgen en een kolom met het aantal zelfmoorden in die buurt.\nCombineer met de buurtgegevens\nMaak een kaart.\n\nWe beginnen met het vinden van de buurt waar elke zelfmoord plaatsvond met behulp van de functie st_join(), een functie in sf. Dit zorgt voor een ruimtelijke verbinding en vindt de polygoon (buurt in ons geval) waarin elk punt zich bevindt. Omdat we de gegevens gaan aggregeren, noemen we de uitvoer van deze functie suicide_agg. De volgorde in de () is belangrijk! Voor onze aggregatie willen we de uitvoer op zelfmoordniveau, dus beginnen we met de suïcide gegevens. In de volgende stap zullen we zien waarom dit van belang is.\n\nsuicide_agg &lt;- st_join(suicide, sf_neighborhoods)\n\nLaten we naar de eerste zes rijen kijken.\n\nhead(suicide_agg)\n\nSimple feature collection with 6 features and 13 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -122.4893 ymin: 37.72218 xmax: -122.3964 ymax: 37.79414\nGeodetic CRS:  WGS84(DD)\n  IncidntNum Category                           Descript DayOfWeek       Date\n1  180318931  SUICIDE ATTEMPTED SUICIDE BY STRANGULATION    Monday 04/30/2018\n2  180315501  SUICIDE       ATTEMPTED SUICIDE BY JUMPING  Saturday 04/28/2018\n3  180295674  SUICIDE              SUICIDE BY LACERATION  Saturday 04/21/2018\n4  180263659  SUICIDE                            SUICIDE   Tuesday 04/10/2018\n5  180235523  SUICIDE     ATTEMPTED SUICIDE BY INGESTION    Friday 03/30/2018\n6  180236515  SUICIDE            SUICIDE BY ASPHYXIATION  Thursday 03/29/2018\n      Time PdDistrict Resolution                 Address\n1 06:30:00    TARAVAL       NONE     0 Block of BRUCE AV\n2 17:54:00   NORTHERN       NONE   700 Block of HAYES ST\n3 12:20:00   RICHMOND       NONE   3700 Block of CLAY ST\n4 05:13:00    CENTRAL       NONE     0 Block of DRUMM ST\n5 09:15:00    TARAVAL       NONE 0 Block of FAIRFIELD WY\n6 17:30:00   RICHMOND       NONE    300 Block of 29TH AV\n                                        Location         PdId year\n1  POINT (-122.45168059935614 37.72218061554315) 1.803189e+13 2018\n2  POINT (-122.42876060987851 37.77620120112792) 1.803155e+13 2018\n3   POINT (-122.45462091999406 37.7881754224736) 1.802957e+13 2018\n4  POINT (-122.39642194376758 37.79414474237039) 1.802637e+13 2018\n5  POINT (-122.46324153155875 37.72679184368551) 1.802355e+13 2018\n6 POINT (-122.48929119750689 37.782735835121265) 1.802365e+13 2018\n                           nhood                   geometry\n1     Oceanview/Merced/Ingleside POINT (-122.4517 37.72218)\n2                   Hayes Valley  POINT (-122.4288 37.7762)\n3               Presidio Heights POINT (-122.4546 37.78818)\n4 Financial District/South Beach POINT (-122.3964 37.79414)\n5             West of Twin Peaks POINT (-122.4632 37.72679)\n6                 Outer Richmond POINT (-122.4893 37.78274)\n\n\nEr is nu de kolom nhood van de buurtgegevens, die zegt in welke buurt de zelfmoord plaatsvond. Nu kunnen we aggregeren tot op buurtniveau met de group_by() en summarize() functies uit het dplyr pakket.\nWe hebben eigenlijk geen variabele met het aantal zelfmoorden, dus die moeten we maken. We kunnen het gewoon number_suicides noemen en het de waarde 1 geven omdat elke rij slechts één zelfmoord is.\n\nsuicide_agg$number_suicides &lt;- 1\n\nNu kunnen we de gegevens samenvoegen en de resultaten weer toewijzen aan suicide_agg.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nsuicide_agg &lt;- suicide_agg %&gt;% \n  group_by(nhood) %&gt;% \n  summarize(number_suicides = sum(number_suicides))\n\nLaten we een samenvatting bekijken van de number_suicides variabele die we hebben gemaakt.\n\nsummary(suicide_agg$number_suicides)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00   13.50   23.50   32.30   37.25  141.00 \n\n\nHet minimum is één zelfmoord per buurt, 32 gemiddeld, en 141 in de buurt met de meeste zelfmoorden. Dus wat maken we van deze gegevens? Nou, er zijn enkele gegeven die problemen veroorzaken. Laten we eens nadenken over de minimumwaarde. Had elke buurt in de stad minstens één zelfmoord? Nee. Kijk eens naar het aantal rijen in deze gegevens, in gedachten houdend dat er één rij per buurt zou moeten zijn.\n\nnrow(suicide_agg)\n\n[1] 40\n\n\nEn laten we het eens vergelijken met de sf_neighborhoods data.\n\nnrow(sf_neighborhoods)\n\n[1] 41\n\n\nIn de gegevens over zelfmoorden ontbreken 2 buurten (een van de 40 waarden ontbreekt en is NA, geen echte buurt). Dat komt omdat als daar geen zelfmoorden plaatsvonden, er nooit een overeenkomstige rij in de gegevens zou zijn, zodat die buurt niet zou voorkomen in de zelfmoordgegevens. Dat zal hier geen groot probleem zijn, maar is iets om in gedachten te houden als dit een echt onderzoeksproject zou zijn.\nDe gegevens zijn klaar om samen te voegen met de sf_neighborhoods gegevens. We introduceren een nieuwe functie die het samenvoegen van gegevens eenvoudig maakt. Deze functie komt ook uit het pakket dplyr.\nleft_join(data1, data2)\nDeze functie voegt deze gegevens samen en behoudt alle rijen van de linker gegevens en elke kolom van beide gegevenssets. Hij combineert de gegevens op basis van overeenkomende kolommen (overeenkomend betekent dezelfde kolomnaam) in beide gegevenssets. Aangezien in onze gegevenssets de kolom nhood in beide bestaat, worden de gegevens op basis van die kolom samengevoegd.\nEr zijn twee andere functies die vergelijkbaar zijn, maar verschillen op basis van welke rijen ze bewaren.\n\nleft_join() - Alle rijen uit linkse gegevens en alle kolommen uit linkse en rechtse gegevens\nright_join() - Alle rijen van de gegevens rechts en alle kolommen van de gegevens links en rechts\nfull_join() - Alle rijen en alle kolommen van linkse en rechtse gegevens\n\nWe zouden ook de merge() functie kunnen gebruiken, die in R is ingebouwd, maar die functie is langzamer dan de dplyr functies en vereist dat we handmatig de overeenkomende kolommen instellen.\nWe willen alle rijen in sf_neighborhoods (alle buurten behouden), dus we kunnen left_join(sf_neighborhoods, suicide_agg) gebruiken. Laten we de resultaten toewijzen aan een nieuwe gegevensverzameling genaamd sf_neighborhoods_suicide.\nWe hebben de ruimtelijke gegevens voor “suicide_agg” niet meer nodig, en het zal problemen veroorzaken met onze join als we die houden, dus laten we de kolom “geometrie” uit die gegevens verwijderen. We kunnen dit doen door de kolom de waarde NULL te geven.\n\nsuicide_agg$geometry &lt;- NULL\n\nNu kunnen we het koppelen.\n\nsf_neighborhoods_suicide &lt;- left_join(sf_neighborhoods, suicide_agg)\n\nJoining, by = \"nhood\"\n\n\nAls we opnieuw naar summary() kijken voor number_suicides zien we dat er nu twee rijen met NA’s zijn. Dit zijn de buurten waar geen zelfmoorden plaatsvonden en die dus niet voorkomen in de suicide_agg gegevens.\n\nsummary(sf_neighborhoods_suicide$number_suicides)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   1.00   15.00   24.00   33.08   38.50  141.00       2 \n\n\nWe moeten deze waarden omzetten in 0. We gebruiken de functie is.na() om voorwaardelijk alle rijen te vinden met een NA-waarde in de kolom number_suicides en gebruiken de notatie met vierkante haken om de waarde in 0 te veranderen.\n\nsf_neighborhoods_suicide$number_suicides[\n  is.na(sf_neighborhoods_suicide$number_suicides)] &lt;- 0\n\nAls we het opnieuw controleren, zien we dat het minimum nu 0 is en dat het gemiddelde aantal zelfmoorden iets afneemt tot ongeveer 31,5 per buurt.\n\nsummary(sf_neighborhoods_suicide$number_suicides)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00   12.00   23.00   31.46   36.00  141.00"
  },
  {
    "objectID": "posts/2022-11-13-visualisaties/crimebynumbersHarrie.html#choropleth-kaarten-maken",
    "href": "posts/2022-11-13-visualisaties/crimebynumbersHarrie.html#choropleth-kaarten-maken",
    "title": "Visualisaties",
    "section": "Choropleth kaarten maken",
    "text": "Choropleth kaarten maken\nTot slot zijn we klaar om enkele chloropleth-kaarten te maken.\nVoor deze kaarten maken we weer gebruik van ggplot2 en zorg ervoor dat het pakket is geladen.\n\nlibrary(ggplot2)\n\nHet voordeel van ggplot2 is dat je langzaam grafieken of kaarten kunt bouwen en de grafiek bij elke stap kunt verbeteren. Zo heb je de functies als geom_line() voor lijngrafieken en geom_point() voor scatter plots. Voor het in kaart brengen van onze polygonen zullen we geom_sf() gebruiken, die weet hoe hij ruimtelijke gegevens moet verwerken.\nZoals gewoonlijk beginnen we met ggplot(), waarbij we eerst onze gegevens invoeren. Dan gebruiken we binnen aes (de esthetiek van de grafiek/kaart) een nieuwe parameter fill. In fill zetten we de kolom number-suicides en het zal de polygonen (buurten) kleuren op basis van de waarden in die kolom. Dan kunnen we de geom_sf() toevoegen.\n\nggplot(sf_neighborhoods_suicide, aes(fill = number_suicides)) +\n  geom_sf() \n\n\n\n\nWe hebben nu een choropleth kaart gemaakt met het aantal zelfmoorden per buurt in San Francisco! Volgens de legenda hebben lichtblauwe buurten de meeste zelfmoorden en donkerblauwe buurten de minste (of helemaal geen). Normaal zouden we het omgekeerde willen, met donkere gebieden die een grotere hoeveelheid van wat de kaart laat zien betekenen.\nWe kunnen scale_fill_gradient() gebruiken om de kleuren in te stellen zoals we willen. We voeren een kleur in voor een lage waarde en een kleur voor een hoge waarde en de kaart wordt met die kleuren verduisterd.\n\nggplot(sf_neighborhoods_suicide, \n       aes(fill = number_suicides)) +\n  geom_sf() +\n  scale_fill_gradient(low = \"white\",\n                      high = \"red\") \n\n\n\n\nDit geeft een veel betere kaart en toont duidelijk de gebieden waar zelfmoorden het meest voorkomen en waar geen zelfmoorden plaatsvonden.\nOm deze kaart leesbaarder te maken en er beter uit te laten zien, voegen we een titel toe aan de kaart en de legenda.\n\nggplot(sf_neighborhoods_suicide, \n       aes(fill = number_suicides)) +\n  geom_sf() +\n  scale_fill_gradient(low = \"white\",\n                      high = \"red\") +\n  labs(fill = \"# van zelfmoorden\",\n       title = \"Zelfmoorden in San Francisco, per buurt\",\n       subtitle = \"2003 - 2017\") \n\n\n\n\nAangezien de coördinaten niets toevoegen aan de kaart, halen we ze weg.\n\nggplot(sf_neighborhoods_suicide,\n       aes(fill = number_suicides)) +\n  geom_sf() +\n  scale_fill_gradient(low = \"white\",\n                      high = \"red\") +\n  labs(fill = \"# van zelfmoorden\",\n       title = \"Zelfmoorden in San Francisco, per buurt\",\n       subtitle = \"2003 - 2017\") +\n  theme(axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks = element_blank())\n\n\n\n\nDus wat moeten we meenemen van deze kaart? Er zijn meer zelfmoorden in de binnenstad dan elders in de stad. Betekent dit dat mensen daar eerder zelfmoord plegen dan elders? Niet noodzakelijkerwijs. Een grote fout die mensen maken bij het maken van een choropleth kaart (of eigenlijk elk type kaart) is het per ongeluk maken van een bevolkingskaart. De donkerder gearceerde delen van onze kaart zijn ook de plaatsen waar veel mensen wonen. Dus als er meer mensen zijn, is het redelijk dat er meer zelfmoorden zijn (of misdaden, enz.). Wat we eigenlijk zouden willen doen, is een percentage maken per bevolkingsaantal (meestal per 100.000, hoewel dit veronderstelt dat het risico voor elke persoon in de stad gelijk is, wat niet echt correct is) om te controleren voor bevolkingsverschillen.\nWe gebruiken deze gegevens straks om interactieve choropleth-kaarten te maken, dus laten we ze opslaan.\n\nsave(sf_neighborhoods_suicide, file = \"data/sf_neighborhoods_suicide.rda\")"
  },
  {
    "objectID": "posts/2022-11-13-visualisaties/crimebynumbersHarrie.html#waarom-zijn-interactieve-grafieken-belangrijk",
    "href": "posts/2022-11-13-visualisaties/crimebynumbersHarrie.html#waarom-zijn-interactieve-grafieken-belangrijk",
    "title": "Visualisaties",
    "section": "Waarom zijn interactieve grafieken belangrijk?",
    "text": "Waarom zijn interactieve grafieken belangrijk?\n\nUw gegevens begrijpen\nBelangrijk is het om jouw data goed te begrijpen, dat is cruciaal voor goed onderzoek. Het maken van interactieve kaarten is een zeer nuttige manier om je gegevens beter te begrijpen, omdat je meteen geografische patronen kunt zien en snel naar kenmerken van die incidenten kunt kijken om ze te begrijpen.\nStraks maken we een kaart van elke marihuana apotheek in San Francisco waarop je op de apotheek kunt klikken om er informatie over te zien. Als we een cluster van apotheken zien, kunnen we op elke apotheek klikken om te zien of ze op elkaar lijken - bijvoorbeeld of ze eigendom zijn van dezelfde persoon. Hoewel het mogelijk is om deze patronen te vinden door alleen naar de gegevens te kijken, is het gemakkelijker om een geografisch patroon te zien en meteen informatie over elk incident te bekijken.\n\n\nPolitieafdelingen gebruiken ze\nInteractieve kaarten zijn populair bij grote politieafdelingen, zoals Philadelphia en New York City. Ze maken het mogelijk geografische patronen in de gegevens gemakkelijk te begrijpen en, wat belangrijk is, ze maken die toegang mogelijk voor mensen die niet over de technische vaardigheden beschikken om met de gegevens zelf te werken. Het leren van interactieve kaarten kan je helpen bij een toekomstige baan."
  },
  {
    "objectID": "posts/2022-11-13-visualisaties/crimebynumbersHarrie.html#de-interactieve-kaart-maken",
    "href": "posts/2022-11-13-visualisaties/crimebynumbersHarrie.html#de-interactieve-kaart-maken",
    "title": "Visualisaties",
    "section": "De interactieve kaart maken",
    "text": "De interactieve kaart maken\nZoals gewoonlijk bekijken we de bovenste 6 rijen van de gegevens.\n\nhead(marijuana)\n\n   License_Number                License_Type   Business_Owner\n1 C10-0000614-LIC Cannabis - Retailer License     Terry Muller\n2 C10-0000586-LIC Cannabis - Retailer License    Jeremy Goodin\n3 C10-0000587-LIC Cannabis - Retailer License     Justin Jarin\n4 C10-0000539-LIC Cannabis - Retailer License Ondyn Herschelle\n5 C10-0000522-LIC Cannabis - Retailer License      Ryan Hudson\n6 C10-0000523-LIC Cannabis - Retailer License      Ryan Hudson\n         Business_Structure                         Premise_Address Status\n1 Limited Liability Company  2165 IRVING ST san francisco, CA 94122 Active\n2               Corporation 122 10TH ST SAN FRANCISCO, CA 941032605 Active\n3               Corporation   843 Howard ST SAN FRANCISCO, CA 94103 Active\n4               Corporation    70 SECOND ST SAN FRANCISCO, CA 94105 Active\n5 Limited Liability Company   527 Howard ST San Francisco, CA 94105 Active\n6 Limited Liability Company 2414 Lombard ST San Francisco, CA 94123 Active\n  Issue_Date Expiration_Date                Activities Adult-Use/Medicinal\n1  9/13/2019       9/12/2020 N/A for this license type                BOTH\n2  8/26/2019       8/25/2020 N/A for this license type                BOTH\n3  8/26/2019       8/25/2020 N/A for this license type                BOTH\n4   8/5/2019        8/4/2020 N/A for this license type                BOTH\n5  7/29/2019       7/28/2020 N/A for this license type                BOTH\n6  7/29/2019       7/28/2020 N/A for this license type                BOTH\n       lat      long\n1 37.76318 -122.4811\n2 37.77480 -122.4157\n3 37.78228 -122.4035\n4 37.78823 -122.4004\n5 37.78783 -122.3965\n6 37.79944 -122.4414\n\n\nDeze gegevens bevatten informatie over het type vergunning, wie de eigenaar is en waar de apotheek zich bevindt (als adres en als coördinaten). We gaan een kaart maken met alle apotheken in de stad en zorgen ervoor dat als je op een stip klikt, er een popup verschijnt met informatie over die apotheek.\nWe gebruiken het pakket leaflet voor onze interactieve kaart. leaflet produceert kaarten die lijken op Google Maps met cirkels (of elk icoontje dat we kiezen) voor elke waarde die we aan de kaart toevoegen. Je kunt inzoomen, rondscrollen, en bij elk incident een context geven die niet beschikbaar is op een statische kaart.\n\n# wel installeren als je het nog niet hebt\n# install.packages(\"leaflet\")\n\n\nlibrary(leaflet)\n\nOm een leaflet map te maken moeten we de functie leaflet() uitvoeren en een tile aan de map toevoegen. We kunnen gewoon de standaard tile gebruiken die geen invoer nodig heeft. Als je geïnteresseerd bent in andere tiles, zie dan deze website.\nWij zullen een ‘standaardtile’ van Open Street Maps gebruiken. Deze geeft straatnamen en markeert belangrijke kenmerken zoals parken en grote winkels, wat een nuttige context biedt voor het bekijken van de gegevens.\n\nleaflet() %&gt;% \n  addTiles()\n\n\n\n\n\n\nWanneer je de bovenstaande code uitvoert, toont het een wereldkaart (verschillende keren gekopieerd). Zoom daarop in, en het toont relevante kenmerken van waar je ook kijkt.\nLet op de %&gt;% tussen de leaflet() functie en de addTiles() functie. leaflet is een van de pakketten in R waarin we pipes kunnen gebruiken.\nOm de punten aan de grafiek toe te voegen gebruiken we de functie addMarkers(), die twee parameters heeft, lng en lat. Voor beide parameters zetten we de kolom waarin respectievelijk de lengtegraad en de breedtegraad staan.\n\nleaflet() %&gt;% \n  addTiles() %&gt;%\n  addMarkers(lng = marijuana$long, \n             lat = marijuana$lat)\n\n\n\n\n\n\nHet voegt nu een pictogram toe dat aangeeft waar elke apotheek in onze dataset zich bevindt. U kunt inzoomen en rondscrollen om meer te zien over waar de apotheken zich bevinden. Er zijn slechts enkele tientallen locaties in de gegevens, dus de popups die elkaar een beetje overlappen hebben geen invloed op onze kaart. Als we er meer hadden - zoals misdaadgegevens met miljoenen overtredingen - zou het erg moeilijk leesbaar worden. Om de pictogrammen in cirkels te veranderen kunnen we de functie addMarkers() veranderen in addCircleMarkers(), waarbij de rest van de code hetzelfde blijft.\n\nleaflet() %&gt;% \n  addTiles() %&gt;%\n  addCircleMarkers(lng = marijuana$long, \n                   lat = marijuana$lat)\n\n\n\n\n\n\nDit maakt van het icoontje cirkels, die minder ruimte innemen. Om de grootte van onze icoontjes aan te passen gebruiken we de radius parameter in addMarkers() of addCircleMarkers(). Hoe groter de straal, hoe groter de icoontjes.\n\nleaflet() %&gt;% \n  addTiles() %&gt;%\n  addCircleMarkers(lng = marijuana$long, \n                   lat = marijuana$lat,\n                   radius = 5)\n\n\n\n\n\n\nDoor de radius optie op 5 te zetten wordt de grootte van het icoontje een stuk kleiner. In je eigen kaarten zul je met deze optie moeten rommelen om het eruit te laten zien zoals je wilt. Laten we verder gaan met het toevoegen van informatie over elk icoontje wanneer erop geklikt wordt."
  },
  {
    "objectID": "posts/2022-11-13-visualisaties/crimebynumbersHarrie.html#popup-informatie-toevoegen",
    "href": "posts/2022-11-13-visualisaties/crimebynumbersHarrie.html#popup-informatie-toevoegen",
    "title": "Visualisaties",
    "section": "Popup informatie toevoegen",
    "text": "Popup informatie toevoegen\nMet de parameter popup in de addMarkers() of addCircleMarkers() functies kun je een karakterwaarde invoeren (als het nog geen karakterwaarde is, wordt het geconverteerd naar een karakterwaarde) die als popup wordt getoond als je op het icoontje klikt. Laten we hier eenvoudig beginnen met het invoeren van de kolom bedrijfseigenaar in onze gegevens en het vervolgens opbouwen tot een meer gecompliceerde popup.\n\nleaflet() %&gt;% \n  addTiles() %&gt;%\n  addCircleMarkers(lng = marijuana$long, \n                   lat = marijuana$lat,\n                   radius = 5,\n                   popup = marijuana$Business_Owner)\n\n\n\n\n\n\nProbeer rond te klikken en je zult zien dat de eigenaar van de apotheek waarop je hebt geklikt boven de stip verschijnt. Meestal willen we een titel hebben die aangeeft wat de waarde in de popup betekent. We kunnen dit doen door de functie paste() te gebruiken om tekst die de waarde uitlegt te combineren met de waarde zelf. Laten we de woorden “Bedrijfseigenaar:” toevoegen voor de kolom Bedrijfseigenaar.\n\nleaflet() %&gt;% \n  addTiles() %&gt;%\n  addCircleMarkers(lng = marijuana$long, \n                   lat = marijuana$lat,\n                   radius = 5,\n                   popup = paste(\"Eigenaar:\",\n                                 marijuana$Business_Owner))\n\n\n\n\n\n\nWe hebben niet al te veel informatie in de gegevens, maar laten we het adres en het licentienummer toevoegen aan de popup door ze toe te voegen aan de paste() functie die we gebruiken.\n\nleaflet() %&gt;% \n  addTiles() %&gt;%\n  addCircleMarkers(lng = marijuana$long, \n                   lat = marijuana$lat,\n                   radius = 5,\n                   popup = paste(\"Eigenaar:\",\n                                 marijuana$Business_Owner,\n                                 \"Adres:\",\n                                 marijuana$Premise_Address,\n                                 \"Licentie:\", \n                                 marijuana$License_Number))\n\n\n\n\n\n\nAls alles op een lijn gezet wordt, is het moeilijk om te lezen. &lt;br&gt; is HTML taal voor regelonderbreking.\n\nleaflet() %&gt;% \n  addTiles() %&gt;%\n  addCircleMarkers(lng = marijuana$long, \n                   lat = marijuana$lat,\n                   radius = 5,\n                   popup = paste(\"Eigenaar:\",\n                                 marijuana$Business_Owner,\n                                 \"&lt;br&gt;\",\n                                 \"Adres:\", \n                                 marijuana$Premise_Address,\n                                 \"&lt;br&gt;\",\n                                 \"Licentie:\",\n                                 marijuana$License_Number))"
  },
  {
    "objectID": "posts/2022-11-13-visualisaties/crimebynumbersHarrie.html#omgaan-met-te-veel-markers",
    "href": "posts/2022-11-13-visualisaties/crimebynumbersHarrie.html#omgaan-met-te-veel-markers",
    "title": "Visualisaties",
    "section": "Omgaan met te veel markers",
    "text": "Omgaan met te veel markers\nIn ons geval met slechts 33 rijen gegevens lost het veranderen van de markers in cirkels ons zichtbaarheidsprobleem op. In gevallen met veel meer rijen gegevens werkt dit niet altijd. Een oplossing hiervoor is om de gegevens te clusteren in groepen waarbij de punten alleen zichtbaar zijn als u inzoomt.\nAls we de code clusterOptions = markerClusterOptions() toevoegen aan onze addCircleMarkers() zal het voor ons clusteren.\n\nleaflet() %&gt;% \n  addTiles() %&gt;%\n  addCircleMarkers(lng = marijuana$long, \n                   lat = marijuana$lat,\n                   radius = 5,\n                   popup = paste(\"Eigenaar:\", \n                                 marijuana$Business_Owner,\n                                 \"&lt;br&gt;\",\n                                 \"Adres:\",\n                                 marijuana$Premise_Address,\n                                 \"&lt;br&gt;\",\n                                 \"Licentie:\",\n                                 marijuana$License_Number),\n                   clusterOptions = markerClusterOptions())\n\n\n\n\n\n\nDicht bij elkaar gelegen locaties zijn gegroepeerd in tamelijk willekeurige groepen. We kunnen zien hoe groot elke groepering is door onze cursor over de cirkel te bewegen. Klik op een cirkel of zoom in, en het toont kleinere groepen op lagere aggregatieniveaus. Blijf klikken of inzoomen, en uiteindelijk wordt elke locatie als een eigen cirkel weergegeven.\nDeze methode is zeer nuttig voor het verwerken van grote hoeveelheden gegevens, omdat zo wordt vermeden dat de kaart met te veel pictogrammen tegelijk wordt overladen. Een nadeel is echter dat de clusters willekeurig worden gecreëerd, waardoor belangrijke context, zoals de buurt, verloren kan gaan."
  },
  {
    "objectID": "posts/2022-11-13-visualisaties/crimebynumbersHarrie.html#interactieve-choropleth-kaarten",
    "href": "posts/2022-11-13-visualisaties/crimebynumbersHarrie.html#interactieve-choropleth-kaarten",
    "title": "Visualisaties",
    "section": "Interactieve choropleth kaarten",
    "text": "Interactieve choropleth kaarten\nHierboven hebben we gewerkt met choropleth-kaarten, dat zijn kaarten met gearceerde regio’s, zoals staten die gekleurd zijn door welke politieke partij ze gewonnen hebben bij een verkiezing. Nu gaan we interactieve choropletkaarten maken waarbij je op een gearceerde regio kunt klikken en informatie over die regio te zien krijgt. We maken dezelfde kaart als voorheen - buurten gearceerd door het aantal zelfmoorden.\nWe laden de gegevens over zelfmoorden in San Francisco per buurt die we eerder hebben gemaakt. We moeten ze ook projecteren op de standaard lengte- en breedteprojectie, anders werkt onze kaart niet goed.\n\nlibrary(sf)\nload(\"data/sf_neighborhoods_suicide.rda\") \nsf_neighborhoods_suicide &lt;- st_transform(sf_neighborhoods_suicide, \n                                         \"+proj=longlat +datum=WGS84\")\n\nWe beginnen de leaflet map op dezelfde manier als voorheen, maar gebruiken de functie addPolygons(), en onze invoer is hier de geometriekolom van sf_neighborhoods_suicide.\n\nleaflet() %&gt;% \n  addTiles() %&gt;%\n  addPolygons(data = sf_neighborhoods_suicide$geometry)\n\n\n\n\n\n\nHet maakte een kaart met dikke blauwe lijnen die elke buurt aangeven. Laten we het uiterlijk van de grafiek een beetje veranderen voordat we een popup maken of de wijken arceren De parameter color in addPolygons() verandert de kleur van de lijnen - laten we hem veranderen in zwart. De lijnen zijn ook erg dik, waardoor ze in elkaar overlopen en de buurten moeilijk te zien zijn. We kunnen de weight parameter veranderen om de grootte van deze lijnen te veranderen - kleinere waarden zijn dunnere lijnen. Laten we proberen dit op 1 te zetten.\n\nleaflet() %&gt;% \n  addTiles() %&gt;%\n  addPolygons(data = sf_neighborhoods_suicide$geometry,\n              color = \"black\",\n              weight = 1)\n\n\n\n\n\n\nDat ziet er beter uit en we kunnen elke buurt nu duidelijk onderscheiden.\nZoals we eerder deden, kunnen we de popup-tekst direct toevoegen aan de functie die de geografische vormen maakt, in dit geval addPolygons(). Laten we de kolomwaarde nhood toevoegen - de naam van die buurt - en het aantal zelfmoorden in die buurt. Zoals eerder, wanneer we op een buurt klikken verschijnt een popup met de uitvoer die we hebben opgegeven.\n\nleaflet() %&gt;% \n  addTiles() %&gt;%\n  addPolygons(data = sf_neighborhoods_suicide$geometry,\n              col = \"black\",\n              weight = 1,\n              popup = paste0(\"Buurt: \", \n                             sf_neighborhoods_suicide$nhood,\n                             \"&lt;br&gt;\",\n                             \"Aantal zelfmoorden: \",\n                             sf_neighborhoods_suicide$number_suicides))\n\n\n\n\n\n\nVoor dit soort kaarten willen we meestal elke polygoon een kleur geven om aan te geven hoe vaak de gebeurtenis in de polygoon heeft plaatsgevonden. We gebruiken de functie colorNumeric(), die veel werk wegneemt van het inkleuren van de kaart. Deze functie neemt twee ingangen, eerst een kleurenpalet, dat we kunnen halen van de site Color Brewer. Laten we de vierde balk in de Sequential pagina gebruiken, die licht oranje tot rood is. Als je kijkt in de sectie met elke HEX-waarde staat er dat het palet “3-class OrRd” is. De “3-klasse” betekent gewoon dat we 3 kleuren hebben geselecteerd, de “OrRd” is het deel dat we willen. Dat vertelt colorNumeric() om het palet te maken met deze kleuren. De tweede parameter is de kolom voor onze numerieke variabele, number_suicides. We slaan de uitvoer van colorNumeric(\"OrRd\", sf_neighborhoods_suicide$number_suicides) op als een nieuw object, dat we voor het gemak pal noemen omdat het een kleurenpalet is. Vervolgens stellen we in addPolygons() de parameter fillColor in op pal(sf_neighborhoods_suicide$number_suicides), waardoor deze functie op de kolom wordt uitgevoerd. Wat dit echt doet is bepalen welke kleur elke buurt moet krijgen op basis van de waarde in de number_suicides kolom.\n\npal &lt;- colorNumeric(\"OrRd\", sf_neighborhoods_suicide$number_suicides)\nleaflet() %&gt;% \n  addTiles() %&gt;%\n  addPolygons(data = sf_neighborhoods_suicide$geometry,\n              col = \"black\",\n              weight = 1,\n              popup = paste0(\"Buurt: \",\n                             sf_neighborhoods_suicide$nhood,\n                             \"&lt;br&gt;\",\n                             \"Aantal zelfmoorden: \",\n                             sf_neighborhoods_suicide$number_suicides),\n              fillColor = pal(sf_neighborhoods_suicide$number_suicides))\n\n\n\n\n\n\nAangezien de buurten transparant zijn, is het moeilijk te onderscheiden welke kleur wordt getoond. We kunnen elke buurt een effen kleur geven door de parameter fillOpacity in addPolygons() op 1 te zetten.\n\nleaflet() %&gt;% \n  addTiles() %&gt;%\n  addPolygons(data = sf_neighborhoods_suicide$geometry,\n              col = \"black\",\n              weight = 1,\n              popup = paste0(\"Buurt: \",\n                             sf_neighborhoods_suicide$nhood,\n                             \"&lt;br&gt;\",\n                             \"Aantal zelfmoorden: \",\n                             sf_neighborhoods_suicide$number_suicides),\n              fillColor = pal(sf_neighborhoods_suicide$number_suicides),\n              fillOpacity = 1)\n\n\n\n\n\n\nOm een legenda toe te voegen gebruiken we de functie addLegend(), die drie parameters nodig heeft. pal vraagt welk kleurenpalet we gebruiken - we willen dat het precies hetzelfde is als waarmee we de buurten kleuren, dus gebruiken we het pal object dat we gemaakt hebben. De values parameter wordt gebruikt voor welke kolom onze numerieke waarden komen, in ons geval de number_suicides kolom dus die voeren we in. Tenslotte bepaalt opacity hoe transparant de legenda zal zijn. Aangezien elke buurt is ingesteld om helemaal niet transparant te zijn, zetten we dit ook op 1 om consistent te zijn.\n\nleaflet() %&gt;% \n  addTiles() %&gt;%\n  addPolygons(data = sf_neighborhoods_suicide$geometry,\n              col = \"black\",\n              weight = 1,\n              popup = paste0(\"Buurt: \", \n                             sf_neighborhoods_suicide$nhood,\n                             \"&lt;br&gt;\",\n                             \"Aantal zelfmoorden: \",\n                             sf_neighborhoods_suicide$number_suicides),\n              fillColor = pal(sf_neighborhoods_suicide$number_suicides),\n              fillOpacity = 1) %&gt;%\n  addLegend(pal = pal, \n            values = sf_neighborhoods_suicide$number_suicides,\n            opacity = 1)\n\n\n\n\n\n\nTenslotte kunnen we een titel aan de legende toevoegen met de title parameter binnen addLegend().\n\nleaflet() %&gt;% \n  addTiles() %&gt;%\n  addPolygons(data = sf_neighborhoods_suicide$geometry,\n              col = \"black\",\n              weight = 1,\n              popup = paste0(\"Buurt: \",\n                             sf_neighborhoods_suicide$nhood,\n                             \"&lt;br&gt;\",\n                             \"Aantal zelfmoorden: \",\n                             sf_neighborhoods_suicide$number_suicides),\n              fillColor = pal(sf_neighborhoods_suicide$number_suicides),\n              fillOpacity = 1) %&gt;%\n  addLegend(pal = pal, \n            values = sf_neighborhoods_suicide$number_suicides,\n            opacity = 1,\n            title = \"Suicides\") %&gt;%\n addProviderTiles(providers$CartoDB.Positron)"
  },
  {
    "objectID": "posts/2022-12-08-expertsschattingen/expertsschattingen.html#inleiding",
    "href": "posts/2022-12-08-expertsschattingen/expertsschattingen.html#inleiding",
    "title": "Werken met moderne schattingstechnieken",
    "section": "Inleiding",
    "text": "Inleiding\nEen belangrijk kenmerk van Bayesiaanse analyse is het incorporeren van kennis teneinde de kwaliteit van conclusies en voorspellingen te verbeteren. Dat is zeker belangrijk als er geen of beperkte data beschikbaar zijn. Kennis vooraf is er veelal voordat je met een onderzoek begint. Er zijn eerder studies gedaan of je kunt de kennis van experts gebruiken om deze priors te definiëren. In ieder geval is het belangrijk de kennis zichtbaar en expliciet te maken in je onderzoek zodat anderen het kunnen beoordelen en bekritiseren.\nIn deze blog laat ik zien hoe we de kennis van experts kunnen gebruiken om schattingen te maken. Hier wordt gebruik gemaakt van moderne schattingstechnieken en met name de SHELF-methode (Cooke, 1991; Cooke & Goossens, 1999; European Food Safety Authority, 2014; Gosling, 2018; Morris et al., 2014; O’Hagan et al., 2006; Zondervan-Zwijnenburg et al., 2017). Experts weten veel over een bepaald onderwerp maar hebben vaak moeite om goede schattingen te maken. De SHELF-methode helpt ons hier goed mee om te gaan en geeft praktische en statistische ondersteuning in het schattingsproces. Dat proces staat onder begeleiding van een facilitator die hierin is getraind. Zelf heb ik vlak voor het uitbreken van de coronaepidemie aan zo’n training van Tony O’Hagan meegedaan.\nHieronder beschrijf ik het schattingsproces van het SHeffield ELicitation Framework in het kort, dat in een aantal publicaties goed is omschreven (O’Hagan, 2019; O’Hagan et al., 2006; Oakley & O’Hagan, 2016). De data die in dit voorbeeld gebruikt worden sluiten aan bij het vierde hoofdstuk van het Bayesrules! boek\nEerst maar eens een aantal pakketten openen:\n\nlibrary(SHELF)\nlibrary(bayesrules)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.2      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\n\nDe data gaan over de Bechdel-test, die iets zegt over de representatie van vrouwen in films. In het stripverhaal The Rule van Alison Bechdel uit 1985 stelt een personage dat ze een film alleen zien als die aan de volgende drie regels voldoet (Bechdel 1986):\n\nin de film moeten minstens twee vrouwen voorkomen;\ndeze twee vrouwen praten met elkaar; en\nze praten over iets anders dan een man.\n\nDeze criteria vormen de Bechdel-test. Als je denkt aan films in de periode 1980-heden, welk percentage daarvan voldoet volgens jou aan de Bechdel-test? Is dat eerder 10%, 50%, 80% of 100%?\nStel dat je nu precies dat percentage wilt vaststellen. Precieze studies hierover zijn er vooralsnog niet en het enige dat je kunt doen is gebruik maken van de kennis van experts. Voor het schatten hiervan nodig je vier experts uit.\nDe elicitatiemethode om het percentage vast te stellen wordt in twee rondes met in totaal acht stappen uitgewerkt. Het resultaat kan al het een resultaat van een onderzoek zijn. In dit blog wordt het resultaat van zo’n expertschatting ook nog eens gekoppeld aan een klein onderzoek. De prior van de experts en de resultaten van dat onderzoek worden gecombineerd tot eindresultaat."
  },
  {
    "objectID": "posts/2022-12-08-expertsschattingen/expertsschattingen.html#eerste-ronde-van-het-elicitatieproces",
    "href": "posts/2022-12-08-expertsschattingen/expertsschattingen.html#eerste-ronde-van-het-elicitatieproces",
    "title": "Werken met moderne schattingstechnieken",
    "section": "Eerste ronde van het Elicitatieproces",
    "text": "Eerste ronde van het Elicitatieproces\n\nDefiniëren van het problemem: Om het percentage van de Bechdel-test vast te stellen gaan we met een groep van vier experts in gesprek. Voor de uitvoering van deze procedure is allereerst informatie verzameld over vrouwen in de film, hun rol daarin en aanvullende informatie. De drie regels waaraan de test moet voldoen is zo helder mogelijk gedefinieerd voor de experts,\nSelecteren van experts: Vervolgens is er een groep experts geselecteerd. Voor experts hebben we gezocht naar een goede mengeling uit filmwetenschap en -industrie. We hebben mensen betrokken die een goed begrip hebben van het probleem en ook openstaan voor andere meningen. Uiteindelijk is er gekozen voor vier experts: een algemene filmwetenschapster (A), een wetenschapster op het terrein van vrouwenstudies en film (B) en iemand uit de filmindustrie (C) en een cultuurwetenschapper (D).\nEvidence dossier: Voor de uitvoering van de procedure is een evidence dossier gemaakt, dat belangrijke informatie omvat waarmee experts hun mening over het percentage dat voldoet aan de Bechdel-test kunnen onderbouwen. Dat dossier is gevuld met informatie over het doel van het onderzoeksproject, met informatie over het eerder uitgevoerde filmonderzoek, praktijkonderzoek en enkele relevante meta-analyses die zijn uitgevoerd. Dit dossier is toegestuurd nadat experts waren uitgenodigd en hadden aangegeven dat ze willen deelnemen. Dit evidence-dossier is ook nog een keer doorgenomen toen de experts samenkwamen in de tweede ronde.\nTrainen van experts: De experts kregen een overzicht van het schattingsproces en het gebruik van subjectieve waarschijnlijkheid en waarschijnlijkheidsdistributies. De informatie over de procedure (eerst een mediaan (p50) en de andere twee tertielscores (p25 en p75) is in een kort document gezet, dat is toegestuurd. Het is verwerkt in een korte power-point presentatie bij de tweede ronde waarin de experts samenkwamen.\nSchatten van individuele priors: De vier individuele experts hebben in een eerste ronde individueel hun schattingen meegedeeld in een aangeleverd format. De experts ontvingen een eenvoudig gereedschap om de invloed van de werkzame elementen te schatten. Hier hebben we de tertielmanier toegepast: iedere deelnemer ontving 20 muntjes die ze moesten verdelen over een schattingsformulier van 0-100 met tientallen. Waar ligt de mediaan (p50). Dan de linkerhelft van de muntjes waar ligt hier de p25 grens/ Vervolgens de andere muntjes, waar ligt hier de p75-grens van de resterende muntjes? Schattingen van individuele priors zijn vervolgens door de facilitator zichtbaar gemaakt (experts wisten in deze fase niet wat anderen hebben geschat). De resultaten van de individuele schattingen zijn daarvoor verwerkt in het programma R met het pakket SHELF (Oakley& O’Hagan, 2019)."
  },
  {
    "objectID": "posts/2022-12-08-expertsschattingen/expertsschattingen.html#resultaten-eerste-ronde-in-r-bewerken",
    "href": "posts/2022-12-08-expertsschattingen/expertsschattingen.html#resultaten-eerste-ronde-in-r-bewerken",
    "title": "Werken met moderne schattingstechnieken",
    "section": "Resultaten eerste ronde in R bewerken",
    "text": "Resultaten eerste ronde in R bewerken\nDe resultaten van de vier experts zijn verwerkt met het pakket SHELF in R. Als je de volgende handeling uitvoert verschijnt het invoerprogramma.\n\nlibrary(SHELF)\nelicitMultiple\n\nJe kunt ook de shiny-app openen.\nDe scores zie je hieronder. \nJe ziet de resultaten van de vier experts.\n- Expert A zegt dat hij hierover niets durft te zeggen. De twintig muntjes heeft hij gelijk verdeeld over het formulier (twee muntjes per tiental). De mediaan ligt op 50 en de drie tertielen zijn gelijk verdeeld.\n- Volgens Expert B voldoet het merendeel aan de test en zij legt de mediaan op 70 (tien muntjes ter linke en tien muntjes ter rechtezijde). Het 25ste percentiel ligt op 50 en het 75ste percentiel ligt op goed. Zij is de enige optimistische expert in dit gezelschap.\n- Expert C legt de 50grens op 40, de 25grens op 30 en 75grens op 50. De resultaten liggen bij hem redelijk dicht bij elkaar en dat betekent dat hij redelijk zeker is van z’n zaak.\n- Expert D legt de 50grens ook op 40, en wat dat betreft zijn C en D het met elkaar eens. Alleen deze expert is net iets minder zeker over deze zaak. Zij legt de 25grens op 10 en de 75grens op 70.\nDe resultaten van de individuele schattingen zijn vervolgens door de facilitator in het distributierapport ingevoerd en dat rapport vind je hier\nDe resultaten van de individuele schattingen kun je ook op een volgende manier uitschrijven.\n\nExpert A P(X ≤ 25) = 0.25, P(X ≤ 50) = 0.5, P(X ≤ 75) = 0.75,\nExpert B P(X ≤ 50) = 0.25, P(X ≤ 70) = 0.5, P(X ≤ 90) = 0.75,\nExpert C P(X ≤ 30) = 0.25, P(X ≤ 40) = 0.5, P(X ≤ 50) = 0.75,\nexpert D P(X ≤ 10) = 0.25, P(X ≤ 40) = 0.5, P(X ≤ 70) = 0.75,\n\nDeze resultaten kun je in R zichtbaar maken als vier distributies. Zo voer je het in:\n\nv &lt;- matrix(c(25, 50, 75, 50, 70, 90, 30, 40, 50, 10, 40, 70), nrow = 3, ncol = 4)\n\np &lt;- c(0.25, 0.5, 0.75)\n\n\nmyfit &lt;- fitdist(vals = v, probs = p, lower = 0, upper = 100)\n\nDit zijn dan de Beta’s van de vier experts.\n\nmyfit$Beta\n\n            shape1    shape2\nexpert.A 1.0000000 1.0000000\nexpert.B 1.5561883 0.7754301\nexpert.C 4.5012666 6.6327143\nexpert.D 0.5371061 0.7338722\n\n\nEn dit zijn de distributies, inclusief het totaal resultaat (de resultaten van de vier experts samengebracht). Je ziet hier weer Expert A die hier niets over durft te zeggen, de optimistische Expert B, met de negatievere Expert C die redelijk zeker is over de zaak en de negatieve Expert D die meer onzeker is.\n\nplotfit(myfit, lp = TRUE)\n\n\n\n\nDe gegevens van de eerste ronde kunnen in dit SHELF-document worden geplaatst. hier"
  },
  {
    "objectID": "posts/2022-12-08-expertsschattingen/expertsschattingen.html#tweede-ronde-van-het-elicitatieproces",
    "href": "posts/2022-12-08-expertsschattingen/expertsschattingen.html#tweede-ronde-van-het-elicitatieproces",
    "title": "Werken met moderne schattingstechnieken",
    "section": "Tweede ronde van het Elicitatieproces",
    "text": "Tweede ronde van het Elicitatieproces\nBovenstaande kan de faciltator doen op basis van de individuele schattingen. In de tweede ronde worden de experts samengebracht. In die ronde worden de individuele resulaten zichtbaar gemaakt aan de experts, met elkaar besproken en geprobeerd op basis van een discussie tot een onderbouwd totaalresultaat te komen. In deze gezamenlijke tweede ronde worden de laatste drie stappen in het elicitatieproces gezet.\n\nBediscussiëren van indiviuele resultaten\nIn een tweede ronde worden de resultaten van individuele schattingen gedeeld en besproken, die ondertussen door de facilitator bewerkt en gevisualiseerd zijn, zoals hierboven besproken. De tertielen, het rapport en de distributies worden gepresenteerd en toegelicht door de facilitator. De experts lichten vervolgens de rationale van hun eigen overtuiging nog eens toe (waarom ze de uitkomst zo inschatten en hoe zeker ze zijn over hun eigen schatting). De overeenkomsten en verschillen worden zichtbaar gemaakt en de experts spreken met elkaar hierover door.\nOvereenkomen op consensus prior\nDaarna identificeerden de experts vervolgens in deze tweede ronde (de eerste bijeenkomst met elkaar) ook het gedeelde percentage van de Bechdel-test. Hier wordt met elkaar als het ware een ‘consensus prior’ van de experts vastgesteld en vormen ze zo met elkaar het standpunt van de Rationeel Onafhankelijke Observator (Rational Impartial Observator). Op deze manier definiëren ze een voorstel (een schatting) waar ze het met elkaar over eens of, allicht beter geformuleerd, zoveel mogelijk over eens zijn. Deze totaalschatting is gebaseerd op de discussies die ondertussen met elkaar zijn gevoerd en laat de uitkomst zien waarin alle experts zich onder leiding van de facilitator kunnen vinden.\n\nStel dat in de discussie en lang beraad uiteindelijk wordt vastgesteld dat de P50-grens op 40 ligt, p25-grens op 25 en p75-grens op 55 ligt. Dit zetten we nogmaals in R.\n\nv &lt;- matrix(c(25, 40, 55), nrow = 3, ncol = 1)\n\np &lt;- c(0.25, 0.5, 0.75)\n\nmyfit &lt;- fitdist(vals = v, probs = p, lower = 0, upper = 100)\n\nDe beta’s van deze totaalscore is dan:\n\nmyfit$Beta\n\n    shape1   shape2\n1 2.062969 2.982134\n\n\nLaat \\(pi\\), een willekeurige waarde tussen 0 en 1, staan voor het onbekende aandeel van recente films die de Bechdel-test doorstaan. Vier experts hebben voor de volgende totaalscore gekozen:\n\nplot_beta(2.06, 2.98) + ggtitle(\"Beta(2.06,2.98),Totaalscore experts\")\n\n\n\n\n\nDocumentatie: Op basis van deze twee rondes hebben is een verslag in twee delen geschreven van het gehele identificatie en schattingsproces. De resultaten worden hieronder samengevat.\n\nDe gegevens van de tweede ronde kunnen in dit SHELF-document worden geplaatst. hier"
  },
  {
    "objectID": "posts/2022-12-08-expertsschattingen/expertsschattingen.html#de-posterior-van-de-experts-als-prior-gebruiken",
    "href": "posts/2022-12-08-expertsschattingen/expertsschattingen.html#de-posterior-van-de-experts-als-prior-gebruiken",
    "title": "Werken met moderne schattingstechnieken",
    "section": "De posterior van de experts als prior gebruiken",
    "text": "De posterior van de experts als prior gebruiken\nNu we dit weten van de experts besluiten we vervolgens een willekeurige steekproef van \\(n=20\\) recente films te bekijken met behulp van gegevens die verzameld zijn over de Bechdel-test. Het pakket bayesrules bevat een gedeeltelijke versie van deze dataset, genaamd bechdel. Een volledige versie wordt geleverd door het pakket fivethirtyeight R (Kim, Ismay en Chunn 2020). Samen met de titel en het jaar van elke film in deze dataset, registreert de binaire variabele of de film de Bechdel-test heeft doorstaan of niet:\n\n# Importeer data\ndata(bechdel, package = \"bayesrules\")\n\n# Haal ert een sample uit van 20 films\nset.seed(84735)\nbechdel_20 &lt;- bechdel %&gt;% \n  sample_n(20)\n\nbechdel_20 %&gt;% \n  head(3)\n\n# A tibble: 3 × 3\n   year title      binary\n  &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt; \n1  2005 King Kong  FAIL  \n2  1983 Flashdance PASS  \n3  2013 The Purge  FAIL  \n\n\nVan de 20 films in deze steekproef slaagden er slechts 9 (45%) voor de test:\n\nbechdel_20 %&gt;% \n  tabyl(binary) %&gt;% \n  adorn_totals(\"row\")\n\n binary  n percent\n   FAIL 11    0.55\n   PASS  9    0.45\n  Total 20    1.00\n\n\nDus 9 van \\(n=20\\) geselecteerde films voldoet aan de Bechdel test. De prior van de experts was \\(2.06,2.98\\) en de posterior wordt dan \\(11.06,23.98\\) omdat je aan \\(\\alpha=2.06\\) 9 en aan \\(\\beta=2.98\\) 11 toevoegt. Oftewel:\n\n\n\nExpert\nPrior\nPosterior\n\n\n\n\nTotaal\nBeta(2.06,2.98)\nBeta(11.06,13.98)\n\n\n\nWe kunnen het ook in een figuur zetten.\n\nplot_beta_binomial(alpha=2,06, beta=2,98, y=9, n=20) +ggtitle(\"Totaal\")\n\n\n\n\nDit wordt dan de nieuwe posterior, die een combinatie is van de prior zoals vastgesteld door de experts en de data die we verzameld hebben. Dat figuur ziet er dan zo uit:\n\nplot_beta(11.06, 13.98) + ggtitle(\"Beta(11.06,13.98),Totaalscore experts en data\")"
  },
  {
    "objectID": "posts/2022-12-08-expertsschattingen/expertsschattingen.html#conclusie",
    "href": "posts/2022-12-08-expertsschattingen/expertsschattingen.html#conclusie",
    "title": "Werken met moderne schattingstechnieken",
    "section": "Conclusie",
    "text": "Conclusie\nAan de hand van een simpel voorbeeld hebben we laten zien hoe het schattingsproces van een bepaalde uitkomst door experts en onder begeleiding van een facilitator verloopt. Wanneer het moeilijk is om een bepaalde uitkomst te schatten, is dit een goede werkwijze. De uitkomst die dat oplevert kan op zich goed gebruikt worden als schatting. Het is ook mogelijk om deze schatting als prior te gebruiken en te koppelen aan een bepaalde dataset. De combinatie van kennis van experts en inzichten via een verzamelde dataset scherpt onze kennis vervolgens aan. We hebben laten zien hoe dit kan worden uitgevoerd in R."
  },
  {
    "objectID": "posts/2022-12-08-expertsschattingen/expertsschattingen.html#literatuur",
    "href": "posts/2022-12-08-expertsschattingen/expertsschattingen.html#literatuur",
    "title": "Werken met moderne schattingstechnieken",
    "section": "Literatuur",
    "text": "Literatuur\nBurgman, M.A. (2015). Trusting Judgements: How to Get the Best Out of Experts, Cambridge, UK: Cambridge University Press\nCooke, R. M. (1991). Experts in Uncertainty: Opinion and Subjective Probability in Science. New York, NY: Oxford University Press.\nCooke, R. M., and Goossens, L. (1999). Procedures Guide for Structured Expert Judgment. Brussels: Commission of the European Communities.\nEuropean Food Safety Authority (2014). Guidance on Expert Knowledge Elicitation in Food and Feed Safety Risk Assessment. EFSA Journal 2014, 12(6), 3734.\nGoossens, L., Cooke, R., Hale, A., and Rodić-Wiersma, L. (2008). Fifteen years of expert judgement at TUDelft. Safety Sci. 46, 234–244. doi: 10.1016/j.ssci.2007.03.002\nGosling, J.P. (2018). SHELF: The Sheffield Elicitation Framework, in: Elicitation: The Science and Art of Structuring Judgements, eds. L.C.Dias, A. Morton and J. Quigley. Springer, 61-93.\nHanea, A.M., McBride, M.F., Brugman, M.A., and Wintle, B.C. (2018). Classical Meets Modern in the IDEA Protocol for Structured Expert Judgement. Journal of Risk Research, 4, 417-433.\nMikkola,P., Martin, O.A.,Chandramouli, S., Hartmann, M.,Pla, O.A., Thomas, O., Pesonen, H., Corander,J., Vehtari, A., Kaski, S., Bürkner,P.C. & Klami, A. Prior knowledge elicitation: The past, present, and future. arXiv:2112.01380v1 [stat.ME] 1 Dec 2021. https://arxiv.org/pdf/2112.01380.pdf\nMorris, D. E., Oakley, J. E., and Crowe, J. A. (2014). A web-based tool for eliciting probability distributions from experts. Environ. Model. Softw. 52, 1–4. doi: 10.1016/j.envsoft.2013.10.010\nOakley, J. E., and O’Hagan, A. (2016). SHELF: the Sheffield Elicitation Framework (Version 3.0) [Computer Software and Documentation]. School of Mathematics and Statistics, University of Sheffield.\nO’Hagan, A. (2019). Expert Knowledge Elicitation: Subjective but Scientific. The American Statistician, 73, 68-81.\nO’Hagan, A., Buck, C. E., Daneshkhah, A., Eiser, J. R., Garthwaite, P. H., Jenkinson, D. J., et al. (2006). Uncertain Judgements: Eliciting Experts’ Probabilities. Chichester: John Wiley & Sons.\nZondervan-Zwijnenburg, M., Schoot-Hueek, W. van de, Lek, K., Hoytink, H., and Schoot, R. van de (2017). Application and Evaluation of an Expert Judgment Elicitation Procedure for Correlations. Frontiers in Psychology, 31, doi: 10.3389/fpsyg.2017.00090"
  },
  {
    "objectID": "posts/2022-12-13-bayes-logistisch-hierarchisch/Bayeshierarchlog.html#inleiding",
    "href": "posts/2022-12-13-bayes-logistisch-hierarchisch/Bayeshierarchlog.html#inleiding",
    "title": "Bayesiaanse hierarchische logistische regressie",
    "section": "Inleiding",
    "text": "Inleiding\nOnlangs verscheen een prachtig boek van Alicia A. Johnson, Miles Q. Ot en Mine Dogucu onder de titel Bayes Rules! An Introduction to Applied Bayesian Modeling en het verscheen bij CRC Press (2022). Eerdere versies stonden kon je al via bookdown bekijken hier en vanaf de eerste keer dat ik het zag, was ik hier heel enthousiast over. Het boek heb ik direct besteld en met groot enthousiasme gelezen.\nHet boek bestaat uit vier duidelijke delen. Het eerste deel gaat in op de fundamenten van het Bayesiaanse perspectief. Het leert je denken als een Bayesiaan en het gaat in op die belangrijke Bayesiaanse regel \\(posterior=\\frac{prior.likelihood}{normaliserende constante}\\). Aan de hand van enkele voorbeelden gaan Johnson e.a. in op hoe het in de praktijk werkt. Daarna gaat het in op hoe kennis en data op elkaar inwerken en het laat enkele basisanalyses zien en hoe dat in deze vorm van statistiek werkt (normaal, binair en poisson). Het tweede deel is een meer technisch hoofdstuk en laat je ook onder de moterkap van deze techniek kijken. Het gaat ook op de wetenschappelijke principes van de benadering, waar je hier op moet letten, hoe je hiermee hypothesen kunt testen (niet alleen tov van een nulhypothese, maar hoeveel beter de ene hypothese is ten opzichte van de andere hyposthese) en hoe je hiermee ook kunt voorspellen. De twee volgende delen (Deel drie en vier) zijn praktische delen. Deel drie gaat in op regressieanalyses voor continue variabelen en classificatieanalyses voor binaire variabelen. Het vierde deel ten slotte gaat in op geclusterde datasets en hoe je hierarchische Bayesiaanse regressie en classificatieanalyses uitvoert.\nNatuurlijk, er zijn onderhand al verschillende interessante boeken te krijgen die je laten zien hoe Bayesiaanse denken in de praktijk kan werken. De boeken van Gelman, McElreath, Spiegelhalter en Kruschke verschenen de afgelopen tien/vijftien jaar en leren je dit. Maar Bayes Rules! vind ik op dit moment als introductieboek mogelijk wel het beste.\nOver het boek heb ik al eerder blogs geschreven. Hieronder zie je een bewerking van een deel van het achttiende hoofdstuk van het vierde deel (Non-Normal Hierarchical Regression & Classification). Dit zo overzetten is voor mij niet alleen een goede manier om het mij eigen te maken, maar ook een manier om het boek anderen aan te raden. Dus lezen en gebruiken deze Bayes Rules! An Introduction to Applied Bayesian Modeling. Dat is wat ik graag idereen aanraad."
  },
  {
    "objectID": "posts/2022-12-13-bayes-logistisch-hierarchisch/Bayeshierarchlog.html#hierarchische-logistische-regressie",
    "href": "posts/2022-12-13-bayes-logistisch-hierarchisch/Bayeshierarchlog.html#hierarchische-logistische-regressie",
    "title": "Bayesiaanse hierarchische logistische regressie",
    "section": "Hierarchische logistische regressie",
    "text": "Hierarchische logistische regressie\nEerst maar een enkele pakketten laden:\n\n# Laden van pakketten\nlibrary(bayesrules)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.2      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(bayesplot)\n\nThis is bayesplot version 1.9.0\n- Online documentation and vignettes at mc-stan.org/bayesplot\n- bayesplot theme set to bayesplot::theme_default()\n   * Does _not_ affect other ggplot2 plots\n   * See ?bayesplot_theme_set for details on theme setting\n\nlibrary(rstanarm)\n\nLoading required package: Rcpp\nThis is rstanarm version 2.21.3\n- See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n- Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n- For execution on a local, multicore CPU with excess RAM we recommend calling\n  options(mc.cores = parallel::detectCores())\n\nlibrary(tidybayes)\nlibrary(broom.mixed)\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\n\nBergbeklimmers proberen grote hoogten te beklimmen in de majestueuze Nepalese Himalaya. Dit doen ze vanwege de sensatie van ijle lucht, de uitdaging of het buitenleven. Succes is niet gegarandeerd; slecht weer, defecte uitrusting, verwondingen of gewoon pech zorgen ervoor dat niet alle klimmers hun bestemming bereiken. Dit roept enkele vragen op. Hoe groot is de kans dat een bergbeklimmer de top wel haalt? Welke factoren kunnen bijdragen aan een hoger succespercentage? Naast het vage gevoel dat een gemiddelde klimmer 50% kans op succes heeft, wegen we dit zwak informatief inzicht af tegen data van klimmers die in het bayesrules pakket zitten. Dit deel van de data is beschikbaar gesteld door ‘The Himalayan Database’ (2020) en verspreid via het #tidytuesday project (R for Data Science 2020b):\n\n# Binnenhalen, herbenoemen & opschonen van data\ndata(climbers_sub)\nclimbers &lt;- climbers_sub %&gt;% \n  select(expedition_id, member_id, success, year, season,\n         age, expedition_role, oxygen_used)\n\nDe dataset climbers bevat de resultaten van 2076 klimmers vanaf 1978 tot en met 2019. Slechts 38,87% van hen slaagde erin de top te bereiken:\n\nnrow(climbers)\n\n[1] 2076\n\nclimbers %&gt;% \n  tabyl(success)\n\n success    n   percent\n   FALSE 1269 0.6112717\n    TRUE  807 0.3887283\n\n\nOmdat member_id in essentie een rij van klimmersid is en we maar één observatie per klimmer hebben, is dit geen groepsvariabele. Verder, het seizoen (seison), rol bij de expeditie (expedition_role) en het gebruik van zuurstof (oxygen_used) zijn categorische variabelen die meerdere malen zijn geobserveerd. Het zijn potentiële voorspellers van succes (succes), maar ook geen groepsvariabele. Dan blijft expeditie_id (expedition_id) over - dit is wel een groepsvariabele. De dataset beslaat 200 verschillende expedities:\n\n# Omvang per expeditie\nclimbers_per_expedition &lt;- climbers %&gt;% \n  group_by(expedition_id) %&gt;% \n  summarize(count = n())\n\n# Aantal expedities\nnrow(climbers_per_expedition)\n\n[1] 200\n\n\nElke expeditie bestaat uit meerdere klimmers. Zo vertrokken onze eerste drie expedities met respectievelijk 5, 6 en 12 klimmers:\n\nclimbers_per_expedition %&gt;% \n  head(3)\n\n# A tibble: 3 × 2\n  expedition_id count\n  &lt;chr&gt;         &lt;int&gt;\n1 AMAD03107         5\n2 AMAD03327         6\n3 AMAD05338        12\n\n\nHet zou fout zijn om deze groepsstructuur te negeren en er anders van uit te gaan dat de individuele klimmers onafhankelijke resultaten boeken. Aangezien elke expeditie als een team werkt, hangt het succes of falen van de ene klimmer in díe expeditie gedeeltelijk af van het succes of falen van anderen in de groep. Bovendien vertrekken alle leden van een expeditie met dezelfde bestemming, met dezelfde leiders en onder dezelfde weersomstandigheden, en zijn dus onderhevig aan dezelfde externe succesfactoren. Het is dus niet alleen juist om rekening te houden met de groepering van de gegevens, maar het kan ook duidelijk maken in welke mate deze factoren variabiliteit veroorzaken in de succespercentages tussen expedities. Meer dan 75 van onze 200 expedities hadden een 0% succesratio - m.a.w. geen enkele klimmer in deze expedities slaagde erin de top te bereiken. Daarentegen hadden bijna 20 expedities een 100% succespercentage. Tussen deze extremen in, is er heel wat variatie in het succespercentage van de expedities.\n\n# Bereken de slagingskans voor elke expeditie\nexpedition_success &lt;- climbers %&gt;% \n  group_by(expedition_id) %&gt;% \n  summarize(success_rate = mean(success))\n\n\n# Plot de slagingskansen over de expedities\nggplot(expedition_success, aes(x = success_rate)) + \n  geom_histogram(color = \"white\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nModel bouw en simulatie\nOm de ‘gegroepeerde’ aard van onze gegevens te weerspiegelen, geeft \\(Y_ij\\) aan of klimmer \\(i\\) in expeditie\n\\(j\\) succesvol de top van hun piek bereikt:\n\\[\\\nY_ij = \\begin{cases}\n1 Ja \\\\\n0 Nee\\\\\n\\end{cases}\n\\]\\] Er zijn verschillende potentiële voorspellers voor het succes van klimmers in onze dataset. We kijken hier naar slechts twee voorspellers: de leeftijd van de klimmer en of hij extra zuurstof heeft gekregen om gemakkelijker te kunnen ademen op grote hoogte. Als zodanig, definiëren we: \\[ X_ij1=leeftijd van klimmer *i* in expeditie *j*\\]\n\\[X_ij2=of de klimmer in *i* in expeditie *j* zuurstof (oxygen) heeft gekregen\\] Door het aandeel van succes te berekenen bij elke combinatie van leeftijd en zuurstofgebruik, krijgen we een idee van hoe deze factoren gerelateerd zijn aan het klimmerssucces (zij het dat het een wankel idee is gezien de kleine steekproefgroottes van sommige combinaties). Kort samengevat lijkt het erop dat het succes van klimmers afneemt met de leeftijd en sterk toeneemt met het gebruik van zuurstof:\n\n# Bereken het slagingspercentage per leeftijd en zuurstofgebruik\ndata_by_age_oxygen &lt;- climbers %&gt;% \n  group_by(age, oxygen_used) %&gt;% \n  summarize(success_rate = mean(success))\n\n`summarise()` has grouped output by 'age'. You can override using the `.groups`\nargument.\n\n# Plot deze relatie\nggplot(data_by_age_oxygen, aes(x = age, y = success_rate, \n                               color = oxygen_used)) + \n  geom_point()\n\n\n\n\nOm een Bayesiaans model van deze relatie op te stellen, erkennen we eerst dat het Bernoulli model redelijk is voor onze binaire responsvariabele \\(Y_ij\\). Stel \\(\\pi_ij\\) de waarschijnlijkheid is dat klimmer_\\(i\\) in expeditie_\\(j\\) zijn piek succesvol beklimt, d.w.z. dat \\(Y_ij=1\\),\n\\[Y_ij|\\pi_ij \\sim \\Bern{\\pi_ij}\\] Dit is een complete pooling benadering waarbij een simpel model wordt omgezet in een logistisch regressiemodel van \\(Y\\) met enkele voorspellers \\(X\\)\n$$Y_ij|_0,_1,beta_2 ^{ind} (_ij) with log()=_0+_1X_ij1+_2X_ij2) \\\n_0c N(m_0,s_0^2) \\ _1 N(m_1, s_1^2) \\ _2 N(m_2, s_1^2)$$\nDit is een goed begin, MAAR het houdt geen rekening met de groepsstructuur van onze data. Overweeg in plaats daarvan het volgende hiërarchische alternatief met onafhankelijke, zwak informatieve priors hieronder afgestemd via stan_glmer() en met een prior model voor \\(beta_0\\) uitgedrukt via het gecentreerde intercept \\(beta_0c\\). Het is immers zinvoller om na te denken over de baseline succesratio bij de typische gemiddelde klimmer, \\(\\beta_0c\\), dan bij 0-jarige klimmers die geen zuurstof gebruiken, \\(\\beta_0\\)$. Daarom begonnen we onze analyse met de zwakke veronderstelling dat de typische klimmer een kans op succes heeft van 0,5, of met log(kans op succes)=0.\nNet zo goed kunnen we dit logistische regressiemodel met willekeurige intercepts omvormen door de expeditiespecifieke intercepties uit te drukken als aanpassingen op het algemene intercept,\n\\[log(\\frac{\\pi_ij}{1-\\pi_ij})=(\\beta_0+b_0j) +\\beta_1X_ij1 + \\beta_2X_ij2\\] met \\(\\beta_0j|\\sigma_0 \\sim^{ind} N(0,\\sigma_0^2)\\) Laten we eens naar de betekenis van en de veronderstellingen achter de modelparameters kijken:\n\nDe expeditie-specifieke intercepten \\(\\beta_0j\\) beschrijven de onderliggende succespercentages, zoals gemeten door de log(kans op succes), voor elke expeditie\\(j\\). Hiermee wordt erkend dat sommige expedities inherent succesvoller zijn dan andere.\nDe expeditiespecifieke intervallen \\(\\beta_0j\\) worden verondersteld normaal verdeeld te zijn rond een gemiddeld intercept \\(\\beta_0\\) met standaardafwijking \\(\\sigma_0\\). Daarmee beschrijft \\(\\beta_0\\) het typische basissucces over alle expedities, en \\(\\sigma_0\\) de tussen-groep variabiliteit in succespercentages van expeditie tot expeditie.\n\\(\\beta_1\\) beschrijft het gemiddelde verband tussen succes en leeftijd wanneer gecontroleerd wordt voor zuurstofgebruik. Op dezelfde manier beschrijft \\(\\beta_2\\) de gemiddelde relatie tussen succes en zuurstofverbruik wanneer gecontroleerd wordt voor leeftijd.\n\nSamengevat maakt ons logistisch regressiemodel met willekeurige intercepten de vereenvoudigende (maar volgens ons redelijke) veronderstelling dat expedities unieke intercepten \\(\\beta_0j\\) kunnen hebben, maar delen de gemeenschappelijke regressieparameters \\(\\beta_1\\) en \\(\\beta_2\\). Anders gezegd, hoewel de onderliggende succespercentages kunnen verschillen van expeditie tot expeditie, zijn jonger zijn of zuurstof gebruiken niet voordeliger in de ene expeditie dan in de andere.\nOm de posterior van het model te simuleren, combineert de stan_glmer() code hieronder het beste van twee werelden: family = binomial geeft aan dat het om een logistisch regressiemodel gaat en de (1 | expeditie_id) term in de modelformule incorporeert onze hiërarchische groeperingsstructuur:\nHoudt de betekenis en assumpties erachter in je hoofd, want dat zijn de modelparameters:\n\nclimb_model &lt;- stan_glmer(\n  success ~ age + oxygen_used + (1 | expedition_id), \n  data = climbers, family = binomial,\n  prior_intercept = normal(0, 2.5, autoscale = TRUE),\n  prior = normal(0, 2.5, autoscale = TRUE), \n  prior_covariance = decov(reg = 1, conc = 1, shape = 1, scale = 1),\n  chains = 4, iter = 5000*2, seed = 84735\n)\n\n\nSAMPLING FOR MODEL 'bernoulli' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000584 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 5.84 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 45.2975 seconds (Warm-up)\nChain 1:                41.2669 seconds (Sampling)\nChain 1:                86.5644 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'bernoulli' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0.000261 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 2.61 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 44.3967 seconds (Warm-up)\nChain 2:                41.2545 seconds (Sampling)\nChain 2:                85.6512 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'bernoulli' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0.000271 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 2.71 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 45.5909 seconds (Warm-up)\nChain 3:                41.4828 seconds (Sampling)\nChain 3:                87.0737 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'bernoulli' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0.000246 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 2.46 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 45.0229 seconds (Warm-up)\nChain 4:                41.6041 seconds (Sampling)\nChain 4:                86.627 seconds (Total)\nChain 4: \n\n\nBekijk de simulatie eens goed en kijk ook eens naar enkele MCMC-diagnoses die hieronder staan. Dat doe je zo.\n\n# Bevestig prior specificaties\nprior_summary(climb_model)\n\n# MCMC diagnostiek\nmcmc_trace(climb_model, size = 0.1)\n\n\n\nmcmc_dens_overlay(climb_model)\n\n\n\nmcmc_acf(climb_model)\n\n\n\nneff_ratio(climb_model)\nrhat(climb_model)\n\nTerwijl deze diagnostiek bevestigt dat onze MCMC simulatie op het juiste spoor zit, geeft ook de posterior predictive check van hieronder aan dat ons model op het juiste spoor zit. Van elk van de 100 posterior gesimuleerde datasets, stellen we de proportie klimmers vast die succesvol waren met de success_rate() functie. Deze succespercentages variëren van ruwweg 37% tot 41%, in een klein venster rond het werkelijk waargenomen succespercentage van 38.9% in de klimmers data.\n\n# Defineer slagingspercentage functie\nsuccess_rate &lt;- function(x){mean(x == 1)}\n\n# Posterior predictive check\npp_check(climb_model, nreps = 100,\n         plotfun = \"stat\", stat = \"success_rate\") + \n  xlab(\"succes score\")\n\nWarning: 'nreps' is ignored for this PPC\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nPosterior analyse\nIn onze posterior analyse van het succes van bergbeklimmers, concentreren we ons op het geheel. Behalve dat we gerustgesteld zijn door het feit dat we correct rekening houden met de groepsstructuur van onze gegevens, zijn we niet geïnteresseerd in een specifieke expeditie. Hieronder volgen enkele posterior samenvattingen voor onze regressieparameters \\(\\beta_0\\), \\(\\beta_1\\) en \\(\\beta_2\\).\n\ntidy(climb_model, effects = \"fixed\", conf.int = TRUE, conf.level = 0.80)\n\n# A tibble: 3 × 5\n  term            estimate std.error conf.low conf.high\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)      -1.41     0.480    -2.05     -0.813 \n2 age              -0.0475   0.00929  -0.0595   -0.0359\n3 oxygen_usedTRUE   5.80     0.485     5.21      6.44  \n\n\nOm te beginnen zien we dat het 80% posterior ‘çredible’ (geloofwaardigheids) interval (CI) voor de age coëfficiënt \\(\\beta_1\\) ruim onder 0 ligt. We hebben dus significant posterior bewijs dat, wanneer we controleren of een klimmer al dan niet zuurstof gebruikt, de kans op succes afneemt met de leeftijd. Meer specifiek, als we de informatie in \\(\\beta_1\\) vertalen van de log(kansen) naar de kans schaal, is er 80% kans dat de kans op een succesvolle beklimming daalt tussen 3,5% en 5,8% voor elk jaar extra leeftijd: \\(e^{-0,0594}, e^{-0,0358}=(0,942, 0,965)\\).\nOp dezelfde manier levert het 80% posterior geloofwaardig interval voor de oxygen_usedTRUE coëfficiënt \\(beta_2\\) significant posterior bewijs dat, wanneer gecontroleerd wordt voor leeftijd, het gebruik van zuurstof de kans op het beklimmen van de top drastisch verhoogt. Er is een kans van 80% dat het gebruik van zuurstof kan overeenkomen met een 182- tot 617-voudige toename van de kans op succes: \\(e^{5.2}}, e^{6.44}=(182,617)\\), Zuurstof alstublieft!\nDoor onze waarnemingen voor \\(\\beta_1\\) en \\(\\beta_2\\) te combineren, wordt het posterior mediaan model voor de relatie tussen de log(kans op succes) van de klimmers en hun leeftijd (\\(X_1\\))en zuurstofgebruik (\\(X_2\\)) \\[log(\\frac{\\pi}{1-\\pi})=-1.42-0.0474X_1+5.80X_2\\]\nOf, op de schaal van waarschijnlijkheid: \\[\\pi=\\frac{e^{-1.42-0.0474X_1+5.80X_2}}{1+e^{-1.42-0.0474X_1+5.80X_2}}\\] Dit posterior mediaan model vertegenwoordigt slechts het midden van een bereik van posterior plausibele relaties tussen succes, leeftijd en zuurstofgebruik. Om een idee te krijgen van dit bereik, toont figuur hieronder 100 posterior plausibele alternatieve modellen. Zowel met als zonder zuurstof neemt de kans op succes af met de leeftijd. Bovendien, op elke leeftijd, is de kans op succes dramatisch hoger wanneer klimmers zuurstof gebruiken. Echter, onze zekerheid over deze trends varieert nogal per leeftijd. We hebben veel minder zekerheid over de slaagkans voor oudere klimmers met zuurstof dan voor jongere klimmers met zuurstof, voor wie de slaagkans over het geheel hoog is. Op dezelfde manier, maar minder drastisch, hebben we minder zekerheid over de slaagkans voor jongere klimmers die geen zuurstof gebruiken dan voor oudere klimmers die geen zuurstof gebruiken, voor wie de slaagkans uniform laag is.\n\nclimbers %&gt;%\n  add_fitted_draws(climb_model, n = 100, re_formula = NA) %&gt;%\n  ggplot(aes(x = age, y = success, color = oxygen_used)) +\n    geom_line(aes(y = .value, group = paste(oxygen_used, .draw)), \n              alpha = 0.1) + \n    labs(y = \"waarschijnlijkheid van succes\")\n\nWarning: `fitted_draws` and `add_fitted_draws` are deprecated as their names were confusing.\nUse [add_]epred_draws() to get the expectation of the posterior predictive.\nUse [add_]linpred_draws() to get the distribution of the linear predictor.\nFor example, you used [add_]fitted_draws(..., scale = \"response\"), which\nmeans you most likely want [add_]epred_draws(...).\n\n\n\n\n\n\n\nPosterior classificatie\nStel dat vier klimmers op een nieuwe expeditie gaan. Twee van hen zijn 20 jaar oud en twee zijn 60 jaar. Van beide leeftijdsgroepen is één klimmer van plan zuurstof te gebruiken en de andere niet:\n\n# Nieuwe expeditie\nnew_expedition &lt;- data.frame(\n  age = c(20, 20, 60, 60), oxygen_used = c(FALSE, TRUE, FALSE, TRUE), \n  expedition_id = rep(\"new\", 4))\nnew_expedition\n\n  age oxygen_used expedition_id\n1  20       FALSE           new\n2  20        TRUE           new\n3  60       FALSE           new\n4  60        TRUE           new\n\n\nNatuurlijk willen ze allemaal weten hoe groot de kans is dat ze de top zullen bereiken. Om dit vast te stellen werken we hier met de posterior_predict() snelkoppelingsfunctie om 20.000 posterior voorspellingen (0 of 1) te simuleren voor elk van onze 4 nieuwe klimmers:\n\n# Posterior voorspellingen van binaire uitkomst\nset.seed(84735)\nbinary_prediction &lt;- posterior_predict(climb_model, newdata = new_expedition)\n\n# Eerste drie voorspellingen\nhead(binary_prediction, 3)\n\n     1 2 3 4\n[1,] 0 0 0 1\n[2,] 0 1 0 1\n[3,] 0 1 0 0\n\n\nVoor elke klimmer wordt de kans op succes benaderd door het geobserveerde aandeel van succes onder hun 20.000 posterieure voorspellingen. Aangezien deze kansen de onzekerheid in het basissuccespercentage van de nieuwe expeditie omvatten, zijn ze gematigder dan de algemene trends die we eerder zichtbaar maakten.\n\n# Vat de posterior voorspellingen van Y samen:\ncolMeans(binary_prediction)\n\n      1       2       3       4 \n0.28045 0.80460 0.14825 0.65085 \n\n\nDeze voorspellingen geven meer inzicht in de verbanden tussen leeftijd, zuurstof, en succes. Bijvoorbeeld, onze posterior voorspelling is dat klimmer 1, die 20 jaar oud is en niet van plan is om zuurstof te gebruiken, 27.88% kans heeft om de top te halen. Deze kans is natuurlijk lager dan voor klimmer 2, die ook 20 is maar wel van plan is om zuurstof te gebruiken. Het is hoger dan de posterior voorspelling van succes voor klimmer 3, die ook niet van plan is zuurstof te gebruiken maar wel 60 jaar oud is. Over het algemeen is de voorspelling van succes het hoogst voor klimmer 2, die jonger is en van plan is zuurstof te gebruiken, en het laagst voor klimmer 3, die ouder is en niet van plan is zuurstof te gebruiken.\nPosterior kans voorspellingen kunnen omgezet worden in posterior classificaties van binaire uitkomsten: ja of nee, verwachtingen of de klimmer zal slagen of niet? Als we een eenvoudige cut-off van 0,5 zouden gebruiken om dit te bepalen, dan zouden we klimmers 1 en 3 aanraden niet aan de expeditie deel te nemen (tenminste, niet zonder zuurstof) en klimmers 2 en 4 het groene licht geven. Maar in deze specifieke context moeten we het waarschijnlijk aan de individuele klimmers overlaten om hun eigen resultaten te interpreteren en hun eigen ja-of-nee beslissingen te nemen over het al dan niet voortzetten van hun expeditie. Zo kan een kans op succes van 65,16% voor sommigen de moeite en het risico waard zijn, maar voor anderen niet.\n\n\nModel evaluatie\nOm onze klimanalyse af te ronden, vragen we ons af: Is ons hiërarchisch-logistisch model een goed model? Lang verhaal kort, het antwoord is ja. - Ten eerste, ons model is eerlijk. De gegevens die we hebben gebruikt zijn openbaar en we verwachten niet dat onze analyse een negatief effect zal hebben op individuen of de samenleving. (Nogmaals, saaie antwoorden op de vraag naar eerlijkheid zijn de beste soort.)\n- Ten tweede Posterior Predictive Checque controle toonde aan dat ons model niet al te verkeerd lijkt - onze posterior gesimuleerde succespercentages schommelen rond de waargenomen succespercentages in onze gegevens.\n- Tenslotte, voor de vraag naar posterior classificatie nauwkeurigheid, kunnen we onze posterior classificaties van succes vergelijken met de werkelijke uitkomsten voor de 2076 klimmers in onze dataset. Standaard beginnen we met een kans cut-off van 0.5 - als de kans op succes van een klimmer groter is dan 0.5, voorspellen we dat hij zal slagen. We implementeren en evalueren deze classificatieregel met classification_summary() hieronder.\n\nset.seed(84735)\nclassification_summary(data = climbers, model = climb_model, cutoff = 0.5)\n\n$confusion_matrix\n     y    0   1\n FALSE 1172  97\n  TRUE   77 730\n\n$accuracy_rates\n                          \nsensitivity      0.9045849\nspecificity      0.9235619\noverall_accuracy 0.9161850\n\n\nIn het algemeen voorspelt ons model met deze classificatieregel de resultaten goed voor 91,61% van onze klimmers. Dit ziet er behoorlijk fantastisch uit gezien het feit dat we enkel informatie gebruiken over de leeftijd en het zuurstofverbruik van de klimmers (terwijl er nog andere voorspellers te bedenken zijn (bv. bestemming, seizoen, enz.). Maar gezien de gevolgen van een foute classificatie in deze specifieke context (bv. risico op verwondingen), moeten we voorrang geven aan specificiteit, ons vermogen om te anticiperen wanneer een klimmer niet zou slagen. Om dit te bereiken voorspelde ons model slechts 92.51% van de mislukte beklimmingen correct. Om dit percentage te verhogen, kunnen we de waarschijnlijkheidsgrens in onze classificatieregel aanpassen.\nIn het algemeen kunnen we, om de specificiteit te verhogen, de waarschijnlijkheidsdrempel verhogen, waardoor het moeilijker wordt om “succes” te voorspellen. Na wat trial and error lijkt het erop dat cut-offs van ruwweg 0.65 of hoger een gewenst specificiteitsniveau van 95% zullen bereiken. Deze overschakeling naar 0.65 verlaagt natuurlijk de gevoeligheid van onze posterior classificaties, van 90.46% naar 81.54%, en dus ons vermogen om te detecteren wanneer een klimmer succesvol zal zijn. Wij denken dat de extra voorzichtigheid hier van belang is.\n\nset.seed(84735)\nclassification_summary(data = climbers, model = climb_model, cutoff = 0.65)\n\n$confusion_matrix\n     y    0   1\n FALSE 1213  56\n  TRUE  148 659\n\n$accuracy_rates\n                          \nsensitivity      0.8166047\nspecificity      0.9558708\noverall_accuracy 0.9017341"
  },
  {
    "objectID": "posts/2022-12-13-bayes-logistisch-hierarchisch/Bayeshierarchlog.html#literatuur",
    "href": "posts/2022-12-13-bayes-logistisch-hierarchisch/Bayeshierarchlog.html#literatuur",
    "title": "Bayesiaanse hierarchische logistische regressie",
    "section": "Literatuur",
    "text": "Literatuur\nFast, Shannon, and Thomas Hegland. 2011. “Book Challenges: A Statistical Examination.” Project for Statistics 316-Advanced Statistical Modeling, St. Olaf College.\nLegler, Julie, and Paul Roback. 2021. Beyond Multiple Linear Regression: Applied Generalized Linear Models and Multilevel Models in R. Chapman; Hall/CRC. https://bookdown.org/roback/bookdown-BeyondMLR/. ———. 2020b. “Himalayan Climbing Expeditions.” TidyTuesday Github Repostitory. https://github.com/rfordatascience/tidytuesday/tree/master/data/2020/2020-09-22.\nThe Himalayan Database. 2020. https://www.himalayandatabase.com/. Trinh, Ly, and Pony Ameri. 2016. “AirBnB Price Determinants: A Multilevel Modeling Approach.” Project for Statistics 316-Advanced Statistical Modeling, St. Olaf College."
  },
  {
    "objectID": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html",
    "href": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html",
    "title": "Werken met Tidymodels, een suite voor machine learning",
    "section": "",
    "text": "Tidymodels is een nieuwe versie van Max Kuhns pakket CARET en kan voor verschillende machine learning taken worden gebruikt. Het is sterk geïnspireerd door Tidyverse. Ook Tidymodels is een suite van verschillende pakketten, van voorbereiding tot en met evaluatie en dat het mogelijk maakt om het uitvoeren van analyses steeds op een vergelijkbare manier uit te voeren. Over dit pakket is een zeer duidelijke website gemaakt met uitleg website. Tegelijkertijd is er het boek. Hieronder zie je een bewerkte versie van de vijf handleidingen die op de website zijn te vinden."
  },
  {
    "objectID": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html#de-dataset",
    "href": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html#de-dataset",
    "title": "Werken met Tidymodels, een suite voor machine learning",
    "section": "De dataset",
    "text": "De dataset\nIn deze handleiding wordt met data van zeeëgels gewerkt. Hier vind je het artikel over voedingsregimes dat hieronder wordt uitgewerkt\nEerst maar eens die data inlezen.\n\nurchins &lt;-\n# Data werden verzameld voor de handleiding\n# zie https://www.flutterbys.com.au/stats/tut/tut7.5a.html\nread_csv(\"https://tidymodels.org/start/models/urchins.csv\") %&gt;%\n# Verander de namen om ze iets minder uitgebreid te laten zijn\nsetNames(c(\"food_regime\", \"initial_volume\", \"width\")) %&gt;%\n# Factoren zijn handig bij modeleren, daarom een kolumn omgezet\nmutate(food_regime = factor(food_regime, levels = c(\"Initial\", \"Low\", \"High\")))\n\nRows: 72 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): TREAT\ndbl (2): IV, SUTW\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLaten we de data vervolgens eens bekijken, met 72 rijen (zeeëgels) en drie variabelen.\n\nglimpse(urchins)\n\nRows: 72\nColumns: 3\n$ food_regime    &lt;fct&gt; Initial, Initial, Initial, Initial, Initial, Initial, I…\n$ initial_volume &lt;dbl&gt; 3.5, 5.0, 8.0, 10.0, 13.0, 13.0, 15.0, 15.0, 16.0, 17.0…\n$ width          &lt;dbl&gt; 0.010, 0.020, 0.061, 0.051, 0.041, 0.061, 0.041, 0.071,…\n\n\nHet is goed om de data dan ook eens te visualiseren.\n\n ggplot(urchins,\n               aes(x = initial_volume,\n                   y = width,\n                   group = food_regime,\n                   col = food_regime)) +\n                geom_point() +\n                geom_smooth(method = lm, se = FALSE) +\n                scale_color_viridis_d(option = \"plasma\", end = .7)\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html#bouwen-en-fitten-van-een-model",
    "href": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html#bouwen-en-fitten-van-een-model",
    "title": "Werken met Tidymodels, een suite voor machine learning",
    "section": "Bouwen en fitten van een model",
    "text": "Bouwen en fitten van een model\nEen standaard twee-weg analyse van variantie (ANOVA) is zinvol voor deze dataset omdat deze zowel een continue als een categorische voorspeller bevat.\nIn dit geval draaien we een lineaire regressie.\n\nlm_mod &lt;-\n        linear_reg() %&gt;%\n        set_engine(\"lm\")\n##  Train/fit/schatten van model \nlm_fit &lt;-\n                lm_mod %&gt;%\n                fit(width ~ initial_volume * food_regime, data = urchins)\n## tidy uitprinten\ntidy(lm_fit)\n\n# A tibble: 6 × 5\n  term                            estimate std.error statistic  p.value\n  &lt;chr&gt;                              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)                     0.0331    0.00962      3.44  0.00100 \n2 initial_volume                  0.00155   0.000398     3.91  0.000222\n3 food_regimeLow                  0.0198    0.0130       1.52  0.133   \n4 food_regimeHigh                 0.0214    0.0145       1.47  0.145   \n5 initial_volume:food_regimeLow  -0.00126   0.000510    -2.47  0.0162  \n6 initial_volume:food_regimeHigh  0.000525  0.000702     0.748 0.457   \n\n## resultaten plotten\n        tidy(lm_fit) %&gt;%\n                dwplot(dot_args = list(size = 2, color = \"black\"),\n                       whisker_args = list(color = \"black\"),\n                       vline = geom_vline(xintercept = 0, colour = \"grey50\", linetype = 2))"
  },
  {
    "objectID": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html#een-model-gebruiken-om-te-voorspellen",
    "href": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html#een-model-gebruiken-om-te-voorspellen",
    "title": "Werken met Tidymodels, een suite voor machine learning",
    "section": "Een model gebruiken om te voorspellen",
    "text": "Een model gebruiken om te voorspellen\nHet model hebben we gedefinieerd. Stel dat we vervolgens een voorspelling willen maken voor egels met een volume van 20ml. Zet deze punten erin.\n\n new_points &lt;- expand.grid(initial_volume = 20,\n                          food_regime = c(\"Initial\", \"Low\", \"High\"))\n\nFit dan het model met deze nieuwe datapunten.\n\nmean_pred &lt;- predict(lm_fit, new_data = new_points)\nmean_pred\n\n# A tibble: 3 × 1\n   .pred\n   &lt;dbl&gt;\n1 0.0642\n2 0.0588\n3 0.0961\n\n\nLaat dan ook de betrouwbaarheidsintervallen hiervoor zien.\n\nconf_int_pred &lt;- predict(lm_fit,\n                                 new_data = new_points,\n                                 type = \"conf_int\")\n\nCombineer de informatie.\n\nplot_data &lt;-\n                new_points %&gt;%\n                bind_cols(mean_pred) %&gt;%\n                bind_cols(conf_int_pred)\n\nEn plot dan de resultaten.\n\n ggplot(plot_data, aes(x = food_regime)) +\n                geom_point(aes(y = .pred)) +\n                geom_errorbar(aes(ymin = .pred_lower,\n                                  ymax = .pred_upper),\n                              width = .2) +\n                labs(y = \"urchin size\")"
  },
  {
    "objectID": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html#model-met-een-andere-engine.",
    "href": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html#model-met-een-andere-engine.",
    "title": "Werken met Tidymodels, een suite voor machine learning",
    "section": "Model met een andere engine.",
    "text": "Model met een andere engine.\nLaten we nu niet lineaire regressie op een standaardmanier uitvoeren. Stel dat we het nu Bayesiaans willen doen. In dat geval moet je eerst de prior-distributie vastzetten.\n\n prior_dist &lt;- rstanarm::student_t(df = 1)\n        set.seed(123)\n\nDan definiëren we het model opnieuw.\n\n bayes_mod &lt;-\n                linear_reg() %&gt;%\n                set_engine(\"stan\",\n                           prior_intercept = prior_dist,\n                           prior = prior_dist)\n\nVervolgens trainen we het nieuwe model.\n\n bayes_fit &lt;-\n                bayes_mod %&gt;%\n                fit(width ~ initial_volume * food_regime, data = urchins)\n\nPrint de gegevens van het model vervolgens uit.\n\n  print(bayes_fit, digits = 5)\n\nparsnip model object\n\nstan_glm\n family:       gaussian [identity]\n formula:      width ~ initial_volume * food_regime\n observations: 72\n predictors:   6\n------\n                               Median   MAD_SD  \n(Intercept)                     0.03338  0.00947\ninitial_volume                  0.00155  0.00039\nfood_regimeLow                  0.01936  0.01348\nfood_regimeHigh                 0.02073  0.01395\ninitial_volume:food_regimeLow  -0.00125  0.00052\ninitial_volume:food_regimeHigh  0.00055  0.00069\n\nAuxiliary parameter(s):\n      Median  MAD_SD \nsigma 0.02143 0.00180\n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg"
  },
  {
    "objectID": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html#de-data",
    "href": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html#de-data",
    "title": "Werken met Tidymodels, een suite voor machine learning",
    "section": "De data",
    "text": "De data\nHet gaat hier om New York City vluchtdata.\n\n# set seed om ervoor te zorgen dat herhalingen zelfde resultaten geven ----\nset.seed(123)\n## Laden van data ----\ndata(flights)\n## Bekijken van data ----\nskimr::skim(flights)\n\n\nData summary\n\n\nName\nflights\n\n\nNumber of rows\n336776\n\n\nNumber of columns\n19\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nnumeric\n14\n\n\nPOSIXct\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ncarrier\n0\n1.00\n2\n2\n0\n16\n0\n\n\ntailnum\n2512\n0.99\n5\n6\n0\n4043\n0\n\n\norigin\n0\n1.00\n3\n3\n0\n3\n0\n\n\ndest\n0\n1.00\n3\n3\n0\n105\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear\n0\n1.00\n2013.00\n0.00\n2013\n2013\n2013\n2013\n2013\n▁▁▇▁▁\n\n\nmonth\n0\n1.00\n6.55\n3.41\n1\n4\n7\n10\n12\n▇▆▆▆▇\n\n\nday\n0\n1.00\n15.71\n8.77\n1\n8\n16\n23\n31\n▇▇▇▇▆\n\n\ndep_time\n8255\n0.98\n1349.11\n488.28\n1\n907\n1401\n1744\n2400\n▁▇▆▇▃\n\n\nsched_dep_time\n0\n1.00\n1344.25\n467.34\n106\n906\n1359\n1729\n2359\n▁▇▇▇▃\n\n\ndep_delay\n8255\n0.98\n12.64\n40.21\n-43\n-5\n-2\n11\n1301\n▇▁▁▁▁\n\n\narr_time\n8713\n0.97\n1502.05\n533.26\n1\n1104\n1535\n1940\n2400\n▁▃▇▇▇\n\n\nsched_arr_time\n0\n1.00\n1536.38\n497.46\n1\n1124\n1556\n1945\n2359\n▁▃▇▇▇\n\n\narr_delay\n9430\n0.97\n6.90\n44.63\n-86\n-17\n-5\n14\n1272\n▇▁▁▁▁\n\n\nflight\n0\n1.00\n1971.92\n1632.47\n1\n553\n1496\n3465\n8500\n▇▃▃▁▁\n\n\nair_time\n9430\n0.97\n150.69\n93.69\n20\n82\n129\n192\n695\n▇▂▂▁▁\n\n\ndistance\n0\n1.00\n1039.91\n733.23\n17\n502\n872\n1389\n4983\n▇▃▂▁▁\n\n\nhour\n0\n1.00\n13.18\n4.66\n1\n9\n13\n17\n23\n▁▇▇▇▅\n\n\nminute\n0\n1.00\n26.23\n19.30\n0\n8\n29\n44\n59\n▇▃▆▃▅\n\n\n\nVariable type: POSIXct\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\ntime_hour\n0\n1\n2013-01-01 05:00:00\n2013-12-31 23:00:00\n2013-07-03 10:00:00\n6936\n\n\n\n\n\nLaten we enkele veranderingen in de dataset aanbrengen.\n\nflight_data &lt;-\n                flights %&gt;%\n                mutate(\n# Converteer de 'arrival delay'-variabele in een factorvariabele\n                  arr_delay = ifelse(arr_delay &gt;= 30, \"late\", \"on_time\"),\n                  arr_delay = factor(arr_delay),\n# We zullen de datum en niet de tijd gebruiken\n                  date = as.Date(time_hour)\n                ) %&gt;%\n# Includeer ook de weersdata\ninner_join(weather, by = c(\"origin\", \"time_hour\")) %&gt;%\n# We gebruiken alleen specifieke kolommen\nselect(dep_time, flight, origin, dest, air_time, distance,\n                       carrier, date, arr_delay, time_hour) %&gt;%\n# Missende data halen we eruit\nna.omit() %&gt;%\n# Voor het draaien van modellen, is het beter om kwalitatieve data te hebbebn\n# zet deze om in factoren (ipv karakter strings)\nmutate_if(is.character, as.factor)\n\nWe zien dat 16% meer dan een half uur vertraging heeft.\n\nflight_data %&gt;% \n  count(arr_delay) %&gt;% \n  mutate(prop = n/sum(n))\n\n# A tibble: 2 × 3\n  arr_delay      n  prop\n  &lt;fct&gt;      &lt;int&gt; &lt;dbl&gt;\n1 late       52540 0.161\n2 on_time   273279 0.839\n\n\nLaten we de veranderingen eens bekijken. We zien bv dat de variabele arr-delay een factor variabele geworden is. Dat is voor het trainen van een logistisch regressiemodel van belang. flight is een numerieke variabele en time-hour is een dttm variabele. Die gebruiken we niet in de training maar wel als eventuele controlevariabelen.\n\n glimpse(flight_data)\n\nRows: 325,819\nColumns: 10\n$ dep_time  &lt;int&gt; 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 558, 558, …\n$ flight    &lt;int&gt; 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301, 49, 71…\n$ origin    &lt;fct&gt; EWR, LGA, JFK, JFK, LGA, EWR, EWR, LGA, JFK, LGA, JFK, JFK, …\n$ dest      &lt;fct&gt; IAH, IAH, MIA, BQN, ATL, ORD, FLL, IAD, MCO, ORD, PBI, TPA, …\n$ air_time  &lt;dbl&gt; 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149, 158, 3…\n$ distance  &lt;dbl&gt; 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 733, 1028,…\n$ carrier   &lt;fct&gt; UA, UA, AA, B6, DL, UA, B6, EV, B6, AA, B6, B6, UA, UA, AA, …\n$ date      &lt;date&gt; 2013-01-01, 2013-01-01, 2013-01-01, 2013-01-01, 2013-01-01,…\n$ arr_delay &lt;fct&gt; on_time, on_time, late, on_time, on_time, on_time, on_time, …\n$ time_hour &lt;dttm&gt; 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-01 05:00:…\n\n\nEr zijn 104 vluchtbestemmingen en 16 verschillende maatschappijen.\n\nflight_data %&gt;% \n  skimr::skim(dest, carrier) \n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n325819\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ndest\n0\n1\nFALSE\n104\nATL: 16771, ORD: 16507, LAX: 15942, BOS: 14948\n\n\ncarrier\n0\n1\nFALSE\n16\nUA: 57489, B6: 53715, EV: 50868, DL: 47465"
  },
  {
    "objectID": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html#data-splitten",
    "href": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html#data-splitten",
    "title": "Werken met Tidymodels, een suite voor machine learning",
    "section": "Data splitten",
    "text": "Data splitten\nVervolgens splitsenen we de dataset in training- en testdata. We splitten het en dan maken we twee datasets.\n\nset.seed(555)\n## Splitsen ----\ndata_split &lt;- initial_split(flight_data, prop = 3/4)\n## Training & Testing ----\ntrain_data &lt;- training(data_split)\ntest_data  &lt;- testing(data_split)\n\nDefinieer het model en geef twee variabelen een nieuwe rol (ID). Deze kun je later gebruiken om te zien als iets niet helemaal goed gegaan is bij het voorspellen. Laat uiteindelijk zien hoe de dataset eruit ziet.\n\nflights_rec &lt;-\n        recipe(arr_delay ~ ., data = train_data)\n        \nflights_rec &lt;-\n                recipe(arr_delay ~ ., data = train_data) %&gt;%\n                update_role(flight, time_hour, new_role = \"ID\")\n\nsummary(flights_rec)\n\n# A tibble: 10 × 4\n   variable  type    role      source  \n   &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;   \n 1 dep_time  numeric predictor original\n 2 flight    numeric ID        original\n 3 origin    nominal predictor original\n 4 dest      nominal predictor original\n 5 air_time  numeric predictor original\n 6 distance  numeric predictor original\n 7 carrier   nominal predictor original\n 8 date      date    predictor original\n 9 time_hour date    ID        original\n10 arr_delay nominal outcome   original\n\n\nWe voegen nog enkele handelingen toe met recipe. Je kunt verschillende zaken tegelijk uitvoeren mbt verschillende variabelen.\n\nflights_rec &lt;-\n        recipe(arr_delay ~ ., data = train_data) %&gt;%\n        update_role(flight, time_hour, new_role = \"ID\") %&gt;%\n        step_date(date, features = c(\"dow\", \"month\")) %&gt;%\n        step_holiday(date, holidays = timeDate::listHolidays(\"US\")) %&gt;%\n        step_rm(date) %&gt;%\n        step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n        step_zv(all_predictors())"
  },
  {
    "objectID": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html#model-fitten",
    "href": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html#model-fitten",
    "title": "Werken met Tidymodels, een suite voor machine learning",
    "section": "Model fitten",
    "text": "Model fitten\nWe specificeren het model als logistische regressie met de glm als engine en specificeren de workflow.\n\nlr_mod &lt;-\n          logistic_reg() %&gt;%\n          set_engine(\"glm\")\n## Specificeren workflow ----\nflights_wflow &lt;-\n          workflow() %&gt;%\n          add_model(lr_mod) %&gt;%\n          add_recipe(flights_rec)\nflights_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_date()\n• step_holiday()\n• step_rm()\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\nVervolgens fitten we het model en kijken naar de resultaten.\n\n flights_fit &lt;-\n                flights_wflow %&gt;%\n                fit(data = train_data)\n## Halen resultaten eruit\n flights_fit %&gt;%\n                pull_workflow_fit() %&gt;%\n                tidy()\n\nWarning: `pull_workflow_fit()` was deprecated in workflows 0.2.3.\nℹ Please use `extract_fit_parsnip()` instead.\n\n\n# A tibble: 158 × 5\n   term                         estimate std.error statistic  p.value\n   &lt;chr&gt;                           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)                   5.25    2.72           1.93 5.40e- 2\n 2 dep_time                     -0.00167 0.0000141   -118.   0       \n 3 air_time                     -0.0438  0.000561     -78.0  0       \n 4 distance                      0.00615 0.00150        4.10 4.09e- 5\n 5 date_USChristmasDay           1.14    0.171          6.65 2.86e-11\n 6 date_USColumbusDay            0.627   0.169          3.72 2.03e- 4\n 7 date_USCPulaskisBirthday      0.702   0.133          5.29 1.25e- 7\n 8 date_USDecorationMemorialDay  0.363   0.117          3.11 1.86e- 3\n 9 date_USElectionDay            0.695   0.177          3.92 8.87e- 5\n10 date_USGoodFriday             1.15    0.156          7.39 1.45e-13\n# … with 148 more rows"
  },
  {
    "objectID": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html#voorspellen",
    "href": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html#voorspellen",
    "title": "Werken met Tidymodels, een suite voor machine learning",
    "section": "Voorspellen",
    "text": "Voorspellen\nWe gebruiken de getrainde werkflow om te voorspellen. Nu zetten we deze werkflow van de trainingsset in op de testdata.\n\npredict(flights_fit, test_data)\n\n# A tibble: 81,455 × 1\n   .pred_class\n   &lt;fct&gt;      \n 1 on_time    \n 2 on_time    \n 3 on_time    \n 4 on_time    \n 5 on_time    \n 6 on_time    \n 7 on_time    \n 8 on_time    \n 9 on_time    \n10 on_time    \n# … with 81,445 more rows\n\n\nLaat het voorspellen van de testdata zien en geef de waarschijnlijkheid terug.\n\nflights_pred &lt;-\n                predict(flights_fit, test_data, type = \"prob\") %&gt;%\n                bind_cols(test_data %&gt;% select(arr_delay, time_hour, flight))\n        flights_pred\n\n# A tibble: 81,455 × 5\n   .pred_late .pred_on_time arr_delay time_hour           flight\n        &lt;dbl&gt;         &lt;dbl&gt; &lt;fct&gt;     &lt;dttm&gt;               &lt;int&gt;\n 1     0.0183         0.982 on_time   2013-01-01 06:00:00    461\n 2     0.0426         0.957 on_time   2013-01-01 06:00:00   5708\n 3     0.0413         0.959 on_time   2013-01-01 06:00:00     71\n 4     0.0253         0.975 on_time   2013-01-01 06:00:00    194\n 5     0.0306         0.969 on_time   2013-01-01 06:00:00   1743\n 6     0.0236         0.976 on_time   2013-01-01 06:00:00   1077\n 7     0.0119         0.988 on_time   2013-01-01 06:00:00    709\n 8     0.137          0.863 on_time   2013-01-01 06:00:00    245\n 9     0.0526         0.947 on_time   2013-01-01 06:00:00   4599\n10     0.0246         0.975 on_time   2013-01-01 06:00:00   1019\n# … with 81,445 more rows\n\n\nPlot deze gegevens met name via yardstick-pakket\n\n flights_pred %&gt;%\n                roc_curve(truth = arr_delay, .pred_late) %&gt;%\n                autoplot()\n\n\n\n\nHoe groot is nu de AREA onder de curve? 76,1%, redelijk.\n\n flights_pred %&gt;%\n                roc_auc(truth = arr_delay, .pred_late)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.761"
  },
  {
    "objectID": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html#data-splitsen",
    "href": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html#data-splitsen",
    "title": "Werken met Tidymodels, een suite voor machine learning",
    "section": "Data splitsen",
    "text": "Data splitsen\nDe functie rsample::initial_split() neemt de oorspronkelijke gegevens en slaat de informatie op over hoe de delen moeten worden gemaakt. In de oorspronkelijke analyse maakten de auteurs hun eigen trainings-/testset en die informatie staat in de kolom “case”. Om te demonstreren hoe we een splitsing maken, verwijderen we deze kolom voordat we onze eigen splitsing maken.\n\nset.seed(123)\ncell_split &lt;- rsample::initial_split(cells %&gt;% select(-case),\n                            strata = class)\n\nHier hebben we het strata-argument gebruikt, dat een gestratificeerde splitsing uitvoert. Dit zorgt ervoor dat, ondanks de onevenwichtigheid die we in onze klassenvariabele hebben opgemerkt, onze trainings- en testdatasets ongeveer dezelfde proporties slecht gesegmenteerde en goed gesegmenteerde cellen behouden als in de oorspronkelijke gegevens. Na de initiële splitsing leveren de functies training() en test() de eigenlijke datasets op.\n\ncell_train &lt;- training(cell_split)\ncell_test  &lt;- testing(cell_split)\n\nnrow(cell_train)\n\n[1] 1514\n\nnrow(cell_train)/nrow(cells)\n\n[1] 0.7498762\n\n# trainingset proporties volgens class\ncell_train %&gt;% \n  count(class) %&gt;% \n  mutate(prop = n/sum(n))\n\n# A tibble: 2 × 3\n  class     n  prop\n  &lt;fct&gt; &lt;int&gt; &lt;dbl&gt;\n1 PS      975 0.644\n2 WS      539 0.356\n\n# testset proporties volgens class\ncell_test %&gt;% \n  count(class) %&gt;% \n  mutate(prop = n/sum(n))\n\n# A tibble: 2 × 3\n  class     n  prop\n  &lt;fct&gt; &lt;int&gt; &lt;dbl&gt;\n1 PS      325 0.644\n2 WS      180 0.356\n\n\nHet meeste modelleerwerk wordt op de trainingset uitgevoerd."
  },
  {
    "objectID": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html#modelleren",
    "href": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html#modelleren",
    "title": "Werken met Tidymodels, een suite voor machine learning",
    "section": "Modelleren",
    "text": "Modelleren\nEen van de voordelen van een random forest model is dat het zeer onderhoudsarm is; het vereist zeer weinig voorbewerking van de gegevens en de standaardparameters geven doorgaans redelijke resultaten. Om die reden zullen we geen recept maken voor de celgegevens en gaan meteen aan de slag.\n\nrf_mod &lt;-\n        rand_forest(trees = 1000) %&gt;%\n        set_engine(\"ranger\") %&gt;%\n        set_mode(\"classification\")\n\nDit nieuwe object rf_fit is het model dat we hebben getraind op de trainingsgegevensverzameling\n\nset.seed(234)\nrf_fit &lt;-\n        rf_mod %&gt;%\n        fit(class ~ ., data = cell_train)"
  },
  {
    "objectID": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html#schatten-van-de-performance",
    "href": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html#schatten-van-de-performance",
    "title": "Werken met Tidymodels, een suite voor machine learning",
    "section": "Schatten van de performance",
    "text": "Schatten van de performance\nPrestaties kunnen worden gemeten aan de hand van de algemene classificatienauwkeurigheid en de Receiver Operating Characteristic (ROC) curve. Het yardstick-pakket heeft functies voor het berekenen van beide maten, genaamd roc_auc() en accuracy(). Gebruik hiervoor niet de trainingsset. Je moet de trainingsset opnieuw bewerken om betrouwbare schattingen te krijgen.\nDaarom modelleren we het opnieuw maar nu met resample.\n\n set.seed(345)\n        folds &lt;- vfold_cv(cell_train, v = 10)\n        folds\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits             id    \n   &lt;list&gt;             &lt;chr&gt; \n 1 &lt;split [1362/152]&gt; Fold01\n 2 &lt;split [1362/152]&gt; Fold02\n 3 &lt;split [1362/152]&gt; Fold03\n 4 &lt;split [1362/152]&gt; Fold04\n 5 &lt;split [1363/151]&gt; Fold05\n 6 &lt;split [1363/151]&gt; Fold06\n 7 &lt;split [1363/151]&gt; Fold07\n 8 &lt;split [1363/151]&gt; Fold08\n 9 &lt;split [1363/151]&gt; Fold09\n10 &lt;split [1363/151]&gt; Fold10\n\n        rf_wf &lt;-\n                workflow() %&gt;%\n                add_model(rf_mod) %&gt;%\n                add_formula(class ~ .)\n\nDe kolom .metrics bevat de prestatiestatistieken die uit de 10 beoordelingssets zijn gemaakt. Deze kunnen handmatig worden ontnomen, maar het tune-pakket bevat een aantal eenvoudige functies die deze gegevens kunnen extraheren:\n\n  set.seed(456)\n        rf_fit_rs &lt;-\n                rf_wf %&gt;%\n                fit_resamples(folds)\n## Om de metrieken te krijgen ----\n        collect_metrics(rf_fit_rs)\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.832    10 0.00952 Preprocessor1_Model1\n2 roc_auc  binary     0.904    10 0.00610 Preprocessor1_Model1"
  },
  {
    "objectID": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html#conclusie",
    "href": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html#conclusie",
    "title": "Werken met Tidymodels, een suite voor machine learning",
    "section": "Conclusie",
    "text": "Conclusie\nDenk aan de waarden die we nu hebben voor nauwkeurigheid en AUC. Deze prestatiecijfers zijn nu realistischer (d.w.z. lager) dan onze eerste poging om prestatiecijfers te berekenen in de handleiding hierboven.\n\nrf_testing_pred &lt;-                      # originele slechte idee\n        predict(rf_fit, cell_test) %&gt;%\n        bind_cols(predict(rf_fit, cell_test, type = \"prob\")) %&gt;%\n        bind_cols(cell_test %&gt;% select(class))\nrf_testing_pred %&gt;%                   # testset voorspellingen\n        roc_auc(truth = class, .pred_PS)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.891\n\nrf_testing_pred %&gt;%                   # test set voorspellingen\n        accuracy(truth = class, .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.816"
  },
  {
    "objectID": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html#voorspellen-van-beeldsegmentatie-maar-nu-beter",
    "href": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html#voorspellen-van-beeldsegmentatie-maar-nu-beter",
    "title": "Werken met Tidymodels, een suite voor machine learning",
    "section": "Voorspellen van beeldsegmentatie maar nu beter",
    "text": "Voorspellen van beeldsegmentatie maar nu beter\nRandom forest-modellen is een methode om bomen te schatten en die presteren doorgaans goed met standaard hyperparameters. De nauwkeurigheid van sommige andere soortgelijke modellen kan echter gevoelig zijn voor de waarden van de hyperparameters. In dit artikel zullen we een beslisboommodel (decision tree model) trainen.\n\nset.seed(123)\ncell_split &lt;- initial_split(cells %&gt;% select(-case),\n                            strata = class)\ncell_train &lt;- training(cell_split)\ncell_test  &lt;- testing(cell_split)"
  },
  {
    "objectID": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html#laatste-fit",
    "href": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html#laatste-fit",
    "title": "Werken met Tidymodels, een suite voor machine learning",
    "section": "Laatste fit",
    "text": "Laatste fit\nTot slot passen we dit definitieve model toe op de opleidingsgegevens en gebruiken we onze testgegevens om de modelprestatie te schatten die we verwachten te zien met nieuwe gegevens. Wij kunnen de functie last_fit() gebruiken voor ons definitieve model; deze functie past het definitieve model toe op de volledige reeks opleidingsgegevens en evalueert het definitieve model op de testgegevens.\n\nfinal_tree &lt;-\n            final_wf %&gt;%\n            fit(data = cell_train)\n\nfinal_tree\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nclass ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 1514 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n   1) root 1514 539 PS (0.64398943 0.35601057)  \n     2) total_inten_ch_2&lt; 41732.5 642  33 PS (0.94859813 0.05140187)  \n       4) shape_p_2_a_ch_1&gt;=1.251801 631  27 PS (0.95721078 0.04278922)  \n         8) avg_inten_ch_2&lt; 125.8919 525  12 PS (0.97714286 0.02285714) *\n         9) avg_inten_ch_2&gt;=125.8919 106  15 PS (0.85849057 0.14150943)  \n          18) var_inten_ch_4&gt;=39.85951 82   6 PS (0.92682927 0.07317073) *\n          19) var_inten_ch_4&lt; 39.85951 24   9 PS (0.62500000 0.37500000)  \n            38) inten_cooc_asm_ch_4&gt;=0.2197672 12   0 PS (1.00000000 0.00000000) *\n            39) inten_cooc_asm_ch_4&lt; 0.2197672 12   3 WS (0.25000000 0.75000000) *\n       5) shape_p_2_a_ch_1&lt; 1.251801 11   5 WS (0.45454545 0.54545455) *\n     3) total_inten_ch_2&gt;=41732.5 872 366 WS (0.41972477 0.58027523)  \n       6) fiber_width_ch_1&lt; 11.37318 406 160 PS (0.60591133 0.39408867)  \n        12) avg_inten_ch_1&lt; 145.4883 293  85 PS (0.70989761 0.29010239)  \n          24) fiber_width_ch_1&lt; 7.878131 68   5 PS (0.92647059 0.07352941) *\n          25) fiber_width_ch_1&gt;=7.878131 225  80 PS (0.64444444 0.35555556)  \n            50) total_inten_ch_1&lt; 12969.5 74  15 PS (0.79729730 0.20270270)  \n             100) inten_cooc_asm_ch_4&lt; 0.06289989 34   2 PS (0.94117647 0.05882353) *\n             101) inten_cooc_asm_ch_4&gt;=0.06289989 40  13 PS (0.67500000 0.32500000)  \n               202) neighbor_min_dist_ch_1&gt;=32.71331 9   0 PS (1.00000000 0.00000000) *\n               203) neighbor_min_dist_ch_1&lt; 32.71331 31  13 PS (0.58064516 0.41935484)  \n                 406) skew_inten_ch_4&gt;=1.060929 16   3 PS (0.81250000 0.18750000) *\n                 407) skew_inten_ch_4&lt; 1.060929 15   5 WS (0.33333333 0.66666667) *\n            51) total_inten_ch_1&gt;=12969.5 151  65 PS (0.56953642 0.43046358)  \n             102) kurt_inten_ch_1&gt;=-0.3447192 110  37 PS (0.66363636 0.33636364)  \n               204) diff_inten_density_ch_4&gt;=112.6034 35   5 PS (0.85714286 0.14285714) *\n               205) diff_inten_density_ch_4&lt; 112.6034 75  32 PS (0.57333333 0.42666667)  \n                 410) inten_cooc_contrast_ch_4&lt; 3.122366 11   0 PS (1.00000000 0.00000000) *\n                 411) inten_cooc_contrast_ch_4&gt;=3.122366 64  32 PS (0.50000000 0.50000000)  \n                   822) fiber_align_2_ch_4&gt;=1.591445 11   1 PS (0.90909091 0.09090909) *\n                   823) fiber_align_2_ch_4&lt; 1.591445 53  22 WS (0.41509434 0.58490566)  \n                    1646) neighbor_avg_dist_ch_1&lt; 217.8143 21   7 PS (0.66666667 0.33333333)  \n                      3292) eq_ellipse_lwr_ch_1&gt;=1.942086 14   2 PS (0.85714286 0.14285714) *\n                      3293) eq_ellipse_lwr_ch_1&lt; 1.942086 7   2 WS (0.28571429 0.71428571) *\n                    1647) neighbor_avg_dist_ch_1&gt;=217.8143 32   8 WS (0.25000000 0.75000000) *\n             103) kurt_inten_ch_1&lt; -0.3447192 41  13 WS (0.31707317 0.68292683)  \n               206) shape_bfr_ch_1&gt;=0.635439 12   5 PS (0.58333333 0.41666667) *\n               207) shape_bfr_ch_1&lt; 0.635439 29   6 WS (0.20689655 0.79310345)  \n                 414) shape_bfr_ch_1&lt; 0.5196834 7   3 PS (0.57142857 0.42857143) *\n                 415) shape_bfr_ch_1&gt;=0.5196834 22   2 WS (0.09090909 0.90909091) *\n        13) avg_inten_ch_1&gt;=145.4883 113  38 WS (0.33628319 0.66371681)  \n          26) total_inten_ch_3&gt;=57919.5 33  10 PS (0.69696970 0.30303030)  \n            52) spot_fiber_count_ch_3&lt; 2.5 24   4 PS (0.83333333 0.16666667)  \n             104) kurt_inten_ch_1&gt;=-0.335807 17   0 PS (1.00000000 0.00000000) *\n             105) kurt_inten_ch_1&lt; -0.335807 7   3 WS (0.42857143 0.57142857) *\n            53) spot_fiber_count_ch_3&gt;=2.5 9   3 WS (0.33333333 0.66666667) *\n\n...\nand 40 more lines.\n\n## variabele belang\nlibrary(vip)\nfinal_tree %&gt;%\n                pull_workflow_fit() %&gt;%\n                vip(geom = \"point\")\n\n\n\n\nTot slot passen we dit definitieve model toe op de trainingsgegevens en gebruiken we onze testgegevens om de modelprestatie te schatten die we verwachten te zien met nieuwe gegevens.\nWij kunnen de functie last_fit() gebruiken voor ons definitieve model; deze functie past het definitieve model toe op de volledige reeks trainingsgegevens en evalueert het definitieve model op de testgegevens.\n\nfinal_fit &lt;-\n          final_wf %&gt;%\n          last_fit(cell_split)\n## verzamel de metrieken\nfinal_fit %&gt;%\n          collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.758 Preprocessor1_Model1\n2 roc_auc  binary         0.839 Preprocessor1_Model1\n\n\nToon nog even de ROC-curve.\n\n final_fit %&gt;%\n                collect_predictions() %&gt;%\n                roc_curve(class, .pred_PS) %&gt;%\n                autoplot()\n\n\n\n\nDe prestatiecijfers van de testset geven aan dat we tijdens onze tuneprocedure niet te veel hebben aangepast.\nHet object final_fit bevat een definitieve, passende workflow die je kunt gebruiken voor voorspellingen op nieuwe gegevens of om de resultaten verder te begrijpen. Je kunt dit object uitpakken met een van de helpfuncties extract_.\n\nfinal_tree &lt;- extract_workflow(final_fit)\nfinal_tree\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nclass ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 1514 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n   1) root 1514 539 PS (0.64398943 0.35601057)  \n     2) total_inten_ch_2&lt; 41732.5 642  33 PS (0.94859813 0.05140187)  \n       4) shape_p_2_a_ch_1&gt;=1.251801 631  27 PS (0.95721078 0.04278922)  \n         8) avg_inten_ch_2&lt; 125.8919 525  12 PS (0.97714286 0.02285714) *\n         9) avg_inten_ch_2&gt;=125.8919 106  15 PS (0.85849057 0.14150943)  \n          18) var_inten_ch_4&gt;=39.85951 82   6 PS (0.92682927 0.07317073) *\n          19) var_inten_ch_4&lt; 39.85951 24   9 PS (0.62500000 0.37500000)  \n            38) inten_cooc_asm_ch_4&gt;=0.2197672 12   0 PS (1.00000000 0.00000000) *\n            39) inten_cooc_asm_ch_4&lt; 0.2197672 12   3 WS (0.25000000 0.75000000) *\n       5) shape_p_2_a_ch_1&lt; 1.251801 11   5 WS (0.45454545 0.54545455) *\n     3) total_inten_ch_2&gt;=41732.5 872 366 WS (0.41972477 0.58027523)  \n       6) fiber_width_ch_1&lt; 11.37318 406 160 PS (0.60591133 0.39408867)  \n        12) avg_inten_ch_1&lt; 145.4883 293  85 PS (0.70989761 0.29010239)  \n          24) fiber_width_ch_1&lt; 7.878131 68   5 PS (0.92647059 0.07352941) *\n          25) fiber_width_ch_1&gt;=7.878131 225  80 PS (0.64444444 0.35555556)  \n            50) total_inten_ch_1&lt; 12969.5 74  15 PS (0.79729730 0.20270270)  \n             100) inten_cooc_asm_ch_4&lt; 0.06289989 34   2 PS (0.94117647 0.05882353) *\n             101) inten_cooc_asm_ch_4&gt;=0.06289989 40  13 PS (0.67500000 0.32500000)  \n               202) neighbor_min_dist_ch_1&gt;=32.71331 9   0 PS (1.00000000 0.00000000) *\n               203) neighbor_min_dist_ch_1&lt; 32.71331 31  13 PS (0.58064516 0.41935484)  \n                 406) skew_inten_ch_4&gt;=1.060929 16   3 PS (0.81250000 0.18750000) *\n                 407) skew_inten_ch_4&lt; 1.060929 15   5 WS (0.33333333 0.66666667) *\n            51) total_inten_ch_1&gt;=12969.5 151  65 PS (0.56953642 0.43046358)  \n             102) kurt_inten_ch_1&gt;=-0.3447192 110  37 PS (0.66363636 0.33636364)  \n               204) diff_inten_density_ch_4&gt;=112.6034 35   5 PS (0.85714286 0.14285714) *\n               205) diff_inten_density_ch_4&lt; 112.6034 75  32 PS (0.57333333 0.42666667)  \n                 410) inten_cooc_contrast_ch_4&lt; 3.122366 11   0 PS (1.00000000 0.00000000) *\n                 411) inten_cooc_contrast_ch_4&gt;=3.122366 64  32 PS (0.50000000 0.50000000)  \n                   822) fiber_align_2_ch_4&gt;=1.591445 11   1 PS (0.90909091 0.09090909) *\n                   823) fiber_align_2_ch_4&lt; 1.591445 53  22 WS (0.41509434 0.58490566)  \n                    1646) neighbor_avg_dist_ch_1&lt; 217.8143 21   7 PS (0.66666667 0.33333333)  \n                      3292) eq_ellipse_lwr_ch_1&gt;=1.942086 14   2 PS (0.85714286 0.14285714) *\n                      3293) eq_ellipse_lwr_ch_1&lt; 1.942086 7   2 WS (0.28571429 0.71428571) *\n                    1647) neighbor_avg_dist_ch_1&gt;=217.8143 32   8 WS (0.25000000 0.75000000) *\n             103) kurt_inten_ch_1&lt; -0.3447192 41  13 WS (0.31707317 0.68292683)  \n               206) shape_bfr_ch_1&gt;=0.635439 12   5 PS (0.58333333 0.41666667) *\n               207) shape_bfr_ch_1&lt; 0.635439 29   6 WS (0.20689655 0.79310345)  \n                 414) shape_bfr_ch_1&lt; 0.5196834 7   3 PS (0.57142857 0.42857143) *\n                 415) shape_bfr_ch_1&gt;=0.5196834 22   2 WS (0.09090909 0.90909091) *\n        13) avg_inten_ch_1&gt;=145.4883 113  38 WS (0.33628319 0.66371681)  \n          26) total_inten_ch_3&gt;=57919.5 33  10 PS (0.69696970 0.30303030)  \n            52) spot_fiber_count_ch_3&lt; 2.5 24   4 PS (0.83333333 0.16666667)  \n             104) kurt_inten_ch_1&gt;=-0.335807 17   0 PS (1.00000000 0.00000000) *\n             105) kurt_inten_ch_1&lt; -0.335807 7   3 WS (0.42857143 0.57142857) *\n            53) spot_fiber_count_ch_3&gt;=2.5 9   3 WS (0.33333333 0.66666667) *\n\n...\nand 40 more lines.\n\n\nMisschien willen we ook begrijpen welke variabelen belangrijk zijn in dit uiteindelijke model. Wij kunnen het vip-pakket gebruiken om het belang van variabelen te schatten op basis van de structuur van het model.\n\nlibrary(vip)\n\nfinal_tree %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip()\n\n\n\n\nDit zijn de geautomatiseerde beeldanalysemetingen die het belangrijkst zijn voor de voorspelling van de segmentatiekwaliteit.\nWe laten het aan de lezer over om te onderzoeken of zij een andere beslisboom-hyperparameter willen afstemmen. Daarvoor kun je de referentiedocumenten raadplegen,of de functie args() gebruiken om te zien welke parsnip-objectargumenten beschikbaar zijn:\n\nargs(decision_tree)\n\nfunction (mode = \"unknown\", engine = \"rpart\", cost_complexity = NULL, \n    tree_depth = NULL, min_n = NULL) \nNULL"
  },
  {
    "objectID": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html#data",
    "href": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html#data",
    "title": "Werken met Tidymodels, een suite voor machine learning",
    "section": "Data",
    "text": "Data\nEerst de data binnenhalen, iets aanpassen en bekijken.\n\n##  Inlezen\nhotels &lt;-\n                read_csv('https://tidymodels.org/start/case-study/hotels.csv') %&gt;%\n                mutate_if(is.character, as.factor)\n\nRows: 50000 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (11): hotel, children, meal, country, market_segment, distribution_chan...\ndbl  (11): lead_time, stays_in_weekend_nights, stays_in_week_nights, adults,...\ndate  (1): arrival_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndim(hotels)\n\n[1] 50000    23\n\n\nAllicht alle variabelen nog eens goed bekijken.\n\n glimpse(hotels)\n\nRows: 50,000\nColumns: 23\n$ hotel                          &lt;fct&gt; City_Hotel, City_Hotel, Resort_Hotel, R…\n$ lead_time                      &lt;dbl&gt; 217, 2, 95, 143, 136, 67, 47, 56, 80, 6…\n$ stays_in_weekend_nights        &lt;dbl&gt; 1, 0, 2, 2, 1, 2, 0, 0, 0, 2, 1, 0, 1, …\n$ stays_in_week_nights           &lt;dbl&gt; 3, 1, 5, 6, 4, 2, 2, 3, 4, 2, 2, 1, 2, …\n$ adults                         &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2, …\n$ children                       &lt;fct&gt; none, none, none, none, none, none, chi…\n$ meal                           &lt;fct&gt; BB, BB, BB, HB, HB, SC, BB, BB, BB, BB,…\n$ country                        &lt;fct&gt; DEU, PRT, GBR, ROU, PRT, GBR, ESP, ESP,…\n$ market_segment                 &lt;fct&gt; Offline_TA/TO, Direct, Online_TA, Onlin…\n$ distribution_channel           &lt;fct&gt; TA/TO, Direct, TA/TO, TA/TO, Direct, TA…\n$ is_repeated_guest              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ previous_cancellations         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ previous_bookings_not_canceled &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ reserved_room_type             &lt;fct&gt; A, D, A, A, F, A, C, B, D, A, A, D, A, …\n$ assigned_room_type             &lt;fct&gt; A, K, A, A, F, A, C, A, D, A, D, D, A, …\n$ booking_changes                &lt;dbl&gt; 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ deposit_type                   &lt;fct&gt; No_Deposit, No_Deposit, No_Deposit, No_…\n$ days_in_waiting_list           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ customer_type                  &lt;fct&gt; Transient-Party, Transient, Transient, …\n$ average_daily_rate             &lt;dbl&gt; 80.75, 170.00, 8.00, 81.00, 157.60, 49.…\n$ required_car_parking_spaces    &lt;fct&gt; none, none, none, none, none, none, non…\n$ total_of_special_requests      &lt;dbl&gt; 1, 3, 2, 1, 4, 1, 1, 1, 1, 1, 0, 1, 0, …\n$ arrival_date                   &lt;date&gt; 2016-09-01, 2017-08-25, 2016-11-19, 20…\n\n\nDe uitkomst variabele is children, een factorvariabele met twee niveaus (wel of geen kinderen. 8,1% van de gasten heeft kinderen bij zich tijdens de hotelovernachtingen.\n\nhotels %&gt;%\n                count(children) %&gt;%\n                mutate(prop = n/sum(n))\n\n# A tibble: 2 × 3\n  children     n   prop\n  &lt;fct&gt;    &lt;int&gt;  &lt;dbl&gt;\n1 children  4038 0.0808\n2 none     45962 0.919"
  },
  {
    "objectID": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html#splitsen-van-data",
    "href": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html#splitsen-van-data",
    "title": "Werken met Tidymodels, een suite voor machine learning",
    "section": "Splitsen van data",
    "text": "Splitsen van data\nWe reserveren 25% van de data voor de test-data. De variabele children is behoorlijk uit balans, dus we stratificeren de dataset op deze variabele als we deze opsplitsen.\n\nset.seed(123)\n        splits      &lt;- initial_split(hotels, strata = children)\n        hotel_other &lt;- training(splits)\n        hotel_test  &lt;- testing(splits)\n\nZo ziet de trainingsset er nu uit qua children variabele.\n\nhotel_other %&gt;%\n                count(children) %&gt;%\n                mutate(prop = n/sum(n))\n\n# A tibble: 2 × 3\n  children     n   prop\n  &lt;fct&gt;    &lt;int&gt;  &lt;dbl&gt;\n1 children  3027 0.0807\n2 none     34473 0.919 \n\n\nZo ziet de testtest eruit op dezelfde variabele, vergelijkbaar:\n\nhotel_test  %&gt;%\n                count(children) %&gt;%\n                mutate(prop = n/sum(n))\n\n# A tibble: 2 × 3\n  children     n   prop\n  &lt;fct&gt;    &lt;int&gt;  &lt;dbl&gt;\n1 children  1011 0.0809\n2 none     11489 0.919 \n\n\nVan de trainingsset maken we ook nog een aparte validitatie set. De\n\n\n\nOpzet\n\n\nZo ziet het er dan uit.\nWe gebruiken de functie validation_split() om 20% van de hotel_other verblijven toe te wijzen aan de validatieset en 30.000 verblijven aan de trainingset. Dit betekent dat de prestatiecijfers van ons model worden berekend op een enkele set van 7.500 hotelovernachtingen. Dat is vrij groot, dus de hoeveelheid gegevens zou voldoende precisie moeten opleveren om een betrouwbare indicator te zijn voor hoe goed elk model de uitkomst voorspelt met een enkele iteratie van resampling.\n\n set.seed(234)\n        val_set &lt;- validation_split(hotel_other,\n                                    strata = children,\n                                    prop = 0.80)\n\nOok dit hebben we gestratificeerd op de uitkomstvariabele children."
  },
  {
    "objectID": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html#eerste-model",
    "href": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html#eerste-model",
    "title": "Werken met Tidymodels, een suite voor machine learning",
    "section": "Eerste model",
    "text": "Eerste model\nHier wordt, en ik gebruik toch maar even de Engelse woorden, een ‘penalized logistic regression’ model gebruikt via glmnet. De penalty=tune(),mixture = 1 haalt irrelevante predictoren weg.\n\n  lr_mod &lt;-\n                logistic_reg(penalty = tune(), mixture = 1) %&gt;%\n                set_engine(\"glmnet\")\n\nVia het pakket recipe dat in tidymodels zit kun je enkele aanvullende voorbereidende handelingen verrichten. Zoals:\n- step_date() creëert voorspellers voor het jaar, de maand en de dag van de week.\n- step_holiday() genereert een reeks indicatorvariabelen voor specifieke feestdagen. Hoewel we niet weten waar deze twee hotels zich bevinden, weten we wel dat de landen van herkomst voor de meeste verblijven in Europa liggen.\n- step_rm() verwijdert variabelen; hier gebruiken we het om de oorspronkelijke datumvariabele te verwijderen omdat we die niet langer in het model willen.\nBovendien moeten alle categorische voorspellers (bv. distribution-channel, hotel, …) worden omgezet naar dummy-variabelen en moeten alle numerieke voorspellers worden gecentreerd en geschaald.\n- step_dummy() zet tekens of factoren (d.w.z. nominale variabelen) om in een of meer numerieke binaire modeltermen voor de niveaus van de oorspronkelijke gegevens.\n- step_zv() verwijdert indicatorvariabelen die slechts één unieke waarde bevatten (bv. allemaal nullen). Dit is belangrijk omdat voor gestrafte modellen de voorspellers moeten worden gecentreerd en geschaald.\n- step_normalize() centreert en schaalt numerieke variabelen.\n\nAls we al deze stappen samenvoegen tot een recept voor ons gekozen model (’penalized logistic regression`), hebben we:\n\nholidays &lt;- c(\"AllSouls\", \"AshWednesday\", \"ChristmasEve\", \"Easter\",\n                      \"ChristmasDay\", \"GoodFriday\", \"NewYearsDay\", \"PalmSunday\")\n\n\n lr_recipe &lt;-\n                recipe(children ~ ., data = hotel_other) %&gt;%\n                step_date(arrival_date) %&gt;%\n                step_holiday(arrival_date, holidays = holidays) %&gt;%\n                step_rm(arrival_date) %&gt;%\n                step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n                step_zv(all_predictors()) %&gt;%\n                step_normalize(all_predictors())\n\nLaten we nu alles (‘model en ’recipe’) in een workflow plaatsen.\n\n lr_workflow &lt;-\n                workflow() %&gt;%\n                add_model(lr_mod) %&gt;%\n                add_recipe(lr_recipe)\n\nWelke penalties moeten we gebruiken? Omdat we slechts een hyperparameter hoeven af te stellen, gebruiken we een grid met 30 verschillende waarden in een kolom.\n\nlr_reg_grid &lt;- tibble(penalty = 10^seq(-4, -1, length.out = 30))\n        lr_reg_grid %&gt;% top_n(-5) # lowest penalty values\n\nSelecting by penalty\n\n\n# A tibble: 5 × 1\n   penalty\n     &lt;dbl&gt;\n1 0.0001  \n2 0.000127\n3 0.000161\n4 0.000204\n5 0.000259\n\n        lr_reg_grid %&gt;% top_n(5)  # highest penalty values\n\nSelecting by penalty\n\n\n# A tibble: 5 × 1\n  penalty\n    &lt;dbl&gt;\n1  0.0386\n2  0.0489\n3  0.0621\n4  0.0788\n5  0.1   \n\n        ## 4.5 Train & Tune ----\n        lr_res &lt;-\n                lr_workflow %&gt;%\n                tune_grid(val_set,\n                          grid = lr_reg_grid,\n                          control = control_grid(save_pred = TRUE),\n                          metrics = metric_set(roc_auc))\n\nHet is makkelijk om de validatieset metrieken te visualiseren door het gebied onder de ROC-curve uit te zetten tegen de reeks van waarden:\n\n lr_plot &lt;-\n                lr_res %&gt;%\n                collect_metrics() %&gt;%\n                ggplot(aes(x = penalty, y = mean)) +\n                geom_point() +\n                geom_line() +\n                ylab(\"Gebied onder de ROC Curve\") +\n                scale_x_log10(labels = scales::label_number())\n\n        lr_plot\n\n\n\n\nDe prestaties van ons model lijken overall het beste te doen bij de kleinere strafwaarden. Als we alleen uitgaan van de roc_auc-metriek zouden we meerdere opties voor de “beste” waarde van deze hyperparameter kunnen vinden:\n\ntop_models &lt;-\n  lr_res %&gt;% \n  show_best(\"roc_auc\", n = 15) %&gt;% \n  arrange(penalty) \ntop_models\n\n# A tibble: 15 × 7\n    penalty .metric .estimator  mean     n std_err .config              \n      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1 0.000127 roc_auc binary     0.872     1      NA Preprocessor1_Model02\n 2 0.000161 roc_auc binary     0.872     1      NA Preprocessor1_Model03\n 3 0.000204 roc_auc binary     0.873     1      NA Preprocessor1_Model04\n 4 0.000259 roc_auc binary     0.873     1      NA Preprocessor1_Model05\n 5 0.000329 roc_auc binary     0.874     1      NA Preprocessor1_Model06\n 6 0.000418 roc_auc binary     0.874     1      NA Preprocessor1_Model07\n 7 0.000530 roc_auc binary     0.875     1      NA Preprocessor1_Model08\n 8 0.000672 roc_auc binary     0.875     1      NA Preprocessor1_Model09\n 9 0.000853 roc_auc binary     0.876     1      NA Preprocessor1_Model10\n10 0.00108  roc_auc binary     0.876     1      NA Preprocessor1_Model11\n11 0.00137  roc_auc binary     0.876     1      NA Preprocessor1_Model12\n12 0.00174  roc_auc binary     0.876     1      NA Preprocessor1_Model13\n13 0.00221  roc_auc binary     0.876     1      NA Preprocessor1_Model14\n14 0.00281  roc_auc binary     0.875     1      NA Preprocessor1_Model15\n15 0.00356  roc_auc binary     0.873     1      NA Preprocessor1_Model16\n\n\nAls we select_best() zouden gebruiken, zou dit kandidaat-model 11 opleveren met een penalty-waarde van 0,00137. Kandidaat-model 12 met een strafwaarde van 0,00174 heeft in feite dezelfde prestaties als het numeriek beste model, maar kan meer voorspellers elimineren. Laten we deze nemen.\n\nlr_best &lt;- \n  lr_res %&gt;% \n  collect_metrics() %&gt;% \n  arrange(penalty) %&gt;% \n  slice(12)\nlr_best\n\n# A tibble: 1 × 7\n  penalty .metric .estimator  mean     n std_err .config              \n    &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1 0.00137 roc_auc binary     0.876     1      NA Preprocessor1_Model12\n\n\nLaten we deze visualiseren:\n\nlr_auc &lt;- \n  lr_res %&gt;% \n  collect_predictions(parameters = lr_best) %&gt;% \n  roc_curve(children, .pred_children) %&gt;% \n  mutate(model = \"Logistic Regression\")\n\nautoplot(lr_auc)\n\n\n\n\nHet prestatieniveau van dit logistische regressiemodel is goed, maar niet baanbrekend. Misschien is de lineaire aard van de voorspellingsvergelijking te beperkend voor deze dataset. Als volgende stap zouden we een sterk niet-lineair model kunnen overwegen dat wordt gegenereerd met behulp van een ‘vertakte’-methode."
  },
  {
    "objectID": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html#vertakte-methode",
    "href": "posts/2023-01-11-machine-learning-with-tidymodels/TutorialALL.html#vertakte-methode",
    "title": "Werken met Tidymodels, een suite voor machine learning",
    "section": "‘Vertakte’-methode",
    "text": "‘Vertakte’-methode\nEen effectieve en onderhoudsarme modelleringstechniek is een random forest. Vertakte modellen vereisen zeer weinig voorbewerking en kunnen vele soorten voorspellers aan (continu, categorisch, enz.).\nBouw het model zo dat het de trainingstijd reduceert. Het tune-pakket kan parallelle verwerking voor u doen en staat gebruikers toe om meerdere processors of aparte machines te gebruiken om modellen te fitten. Zo detecteer je de processoren:\n\n cores &lt;- parallel::detectCores()\n        cores\n\n[1] 4\n\n\nVervolgens het model bouwen.\n\n rf_mod &lt;-\n                rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %&gt;%\n                # tune() is voor later\n                set_engine(\"ranger\", num.threads = cores) %&gt;%\n                set_mode(\"classification\")\n\nOpgelet: Geen processoren vaststellen behalve voor random forest\nIn tegenstelling tot de `penalized logistic regression’ modellen zoals hierboven gebruikt, vraagt het ‘random forest model’ geen dummies of genormaliseerde voorspellers.\n\nrf_recipe &lt;-\n                recipe(children ~ ., data = hotel_other) %&gt;%\n                step_date(arrival_date) %&gt;%\n                step_holiday(arrival_date) %&gt;%\n                step_rm(arrival_date)\n\nCreëer vervolgens de workflow.\n\n rf_workflow &lt;-\n                workflow() %&gt;%\n                add_model(rf_mod) %&gt;%\n                add_recipe(rf_recipe)\n\nTrain en stel het model af. Laat zien wat er moet worden afgesteld.\n\n rf_mod %&gt;%\n                parameters()\n\nWarning: `parameters.model_spec()` was deprecated in tune 0.1.6.9003.\nℹ Please use `hardhat::extract_parameter_set_dials()` instead.\n\n\nCollection of 2 parameters for tuning\n\n identifier  type    object\n       mtry  mtry nparam[?]\n      min_n min_n nparam[+]\n\nModel parameters needing finalization:\n   # Randomly Selected Predictors ('mtry')\n\nSee `?dials::finalize` or `?dials::update.parameters` for more information.\n\n\nLaat zien wel ruimte je hebt.\n\nset.seed(345)\n        rf_res &lt;-\n                rf_workflow %&gt;%\n                tune_grid(val_set,\n                          grid = 25,\n                          control = control_grid(save_pred = TRUE),\n                          metrics = metric_set(roc_auc))\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\nLaat zien wat de beste keuze is.\n\nrf_res %&gt;%\n                show_best(metric = \"roc_auc\")\n\n# A tibble: 5 × 8\n   mtry min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1     8     7 roc_auc binary     0.926     1      NA Preprocessor1_Model13\n2    12     7 roc_auc binary     0.926     1      NA Preprocessor1_Model01\n3    13     4 roc_auc binary     0.925     1      NA Preprocessor1_Model05\n4     9    12 roc_auc binary     0.924     1      NA Preprocessor1_Model19\n5     6    18 roc_auc binary     0.924     1      NA Preprocessor1_Model24\n\n\nHet bereik van de y-as geeft echter aan dat het model zeer robuust is voor de keuze van deze parameterwaarden — op één na zijn alle ROC AUC-waarden groter dan 0,90.\n\nautoplot(rf_res)\n\n\n\n\nSelecteer de beste.\n\n rf_best &lt;-\n                rf_res %&gt;%\n                select_best(metric = \"roc_auc\")\n        rf_best\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1     8     7 Preprocessor1_Model13\n\n\nStel het model af op de beste voorspelling.\n\nrf_auc &lt;-\n                rf_res %&gt;%\n                collect_predictions(parameters = rf_best) %&gt;%\n                roc_curve(children, .pred_children) %&gt;%\n                mutate(model = \"Random Forest\")\n\nPlot vervolgens het beste model.\n\nbind_rows(rf_auc, lr_auc) %&gt;%\n                ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) +\n                geom_path(lwd = 1.5, alpha = 0.8) +\n                geom_abline(lty = 3) +\n                coord_equal() +\n                scale_color_viridis_d(option = \"plasma\", end = .6)\n\n\n\n\nDe laatste fit.\nBouw het model opnieuw op en neem de beste hyperparameter waarde voor ons ‘random forest model’. Definieer ook een nieuw argument: importance = \"impurity\"\nHet laatste model ziet er dan zo uit.\n\nlast_rf_mod &lt;-\n                rand_forest(mtry = 8, min_n = 7, trees = 1000) %&gt;%\n                set_engine(\"ranger\", num.threads = cores, importance = \"impurity\") %&gt;%\n                set_mode(\"classification\")\n        ## Laatste werkflow\n        last_rf_workflow &lt;-\n                rf_workflow %&gt;%\n                update_model(last_rf_mod)\n\nDe laatste fit dan nu.\n\n set.seed(345)\n        last_rf_fit &lt;-\n                last_rf_workflow %&gt;%\n                last_fit(splits)\n\nEvalueer het model.\n\nlast_rf_fit %&gt;%\n                collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.946 Preprocessor1_Model1\n2 roc_auc  binary         0.923 Preprocessor1_Model1\n\n        ## 6.5 review variable importance ----\n        last_rf_fit %&gt;%\n                pluck(\".workflow\", 1) %&gt;%\n                pull_workflow_fit() %&gt;%\n                vip(num_features = 20)\n\n\n\n\nLaatste roc, zelfde voor de validatie set. Goede voorspeller op de nieuwe data.\n\nlast_rf_fit %&gt;%\n                collect_predictions() %&gt;%\n                roc_curve(children, .pred_children) %&gt;%\n                autoplot()"
  },
  {
    "objectID": "posts/2023-02-13-quarto-presentatie-toelichting/Quatropresentatie.html",
    "href": "posts/2023-02-13-quarto-presentatie-toelichting/Quatropresentatie.html",
    "title": "Begin van mijn Quartoreis, een opzet voor een Quarto presentatie",
    "section": "",
    "text": "Afgelopen weken heb ik wat met Quarto geëxperimenteerd. Zo heb ik met Quarto een boek gemaakt en daarover zal ik nog een andere blog schrijven. Ook heb ik met Quarto wat presentaties gemaakt. Meghan Hall heeft mij hier veel geleerd. Zij heeft een presentatie in Quarto gezet hier en de repository daarvan vind je hier. Hoe ze deze aantrekkelijke presentatie heeft opgemaakt, beschrijft ze in haar blog.\nHIER VIND JE DIE PRESENTATIE\nDe presentatie heb ik vertaald en in andere kleuren gezet hier. De blogopmaak heb ik zelf wat aangepast. De aangepaste presentatie en blog vind je op mijn github-site hier. Hartelijke dank Meghan Hall voor jouw duidelijke uitleg, veel van geleerd."
  },
  {
    "objectID": "posts/2023-02-13-quarto-presentatie-toelichting/Quatropresentatie.html#begin-van-mijn-quarto-reis",
    "href": "posts/2023-02-13-quarto-presentatie-toelichting/Quatropresentatie.html#begin-van-mijn-quarto-reis",
    "title": "Begin van mijn Quartoreis, een opzet voor een Quarto presentatie",
    "section": "Begin van mijn Quarto reis",
    "text": "Begin van mijn Quarto reis\nQuarto is een nieuw open-source technisch publicatiesysteem van RStudio. Het lijkt in veel opzichten op R Markdown (dat niet verdwijnt, maak je geen zorgen!), behalve dat het geen R vereist, meer talen ondersteunt en de functionaliteit van veel RMarkdown pakketten combineert (b.v. blogdown, bookdown).\nMeghan, maar ikzelf ook, is een toegewijde RMarkdown gebruiker, maar ze wil beginnen met het verkennen van de nieuwe Quarto functies. Haar eerste project was slides voor een workshop—Ze zou normaal de RMarkdown-gebaseerde xaringan gebruiken voor HTML presentaties, maar besloot om het Quarto alternatief uit te proberen, dat reveal.js gebruikt.\n\nDe slides die ze met Quarto maakte zijn hier en de code vind je op GitHub hier.\n\nVeel van de functionaliteit is gelijkaardig aan xaringan of andere gelijkaardige RMarkdown HTML formaten, maar er is veel nieuwe syntax om te leren (en ook veel nieuwe functies!), dus heeft ze deze post samengesteld om een paar dingen te verzamelen die ze geleerd heeft terwijl ze voor de eerste keer een Quarto-presentatie maakte."
  },
  {
    "objectID": "posts/2023-02-13-quarto-presentatie-toelichting/Quatropresentatie.html#quarto-bronnen",
    "href": "posts/2023-02-13-quarto-presentatie-toelichting/Quatropresentatie.html#quarto-bronnen",
    "title": "Begin van mijn Quartoreis, een opzet voor een Quarto presentatie",
    "section": "Quarto bronnen",
    "text": "Quarto bronnen\nHaar favoriete introductie op het publicatiesysteem zelf is Alison Hill’s samenvattendepost. Mine Çetinkaya-Rundel heeft een doorlopende blog met een nieuwe Quartotip elke dag."
  },
  {
    "objectID": "posts/2023-02-13-quarto-presentatie-toelichting/Quatropresentatie.html#emojis",
    "href": "posts/2023-02-13-quarto-presentatie-toelichting/Quatropresentatie.html#emojis",
    "title": "Begin van mijn Quartoreis, een opzet voor een Quarto presentatie",
    "section": "Emojis",
    "text": "Emojis\nEerst maar eens—hoe gebruik je emoji’s in Quarto?? Gelukkig was dat niet zo moeilijk. Door de volgende regel aan de YAML toe te voegen:\n---\nfrom: markdown+emoji\n---\nJe kunt emoji’s toevoegen door te schrijven :wave: om :wave: te krijgen. De hele lijst met emoji codes vind je hier. (Dit is een goede lijst met welke YAML opties er beschikbaar zijn.)"
  },
  {
    "objectID": "posts/2023-02-13-quarto-presentatie-toelichting/Quatropresentatie.html#aangepaste-elementen",
    "href": "posts/2023-02-13-quarto-presentatie-toelichting/Quatropresentatie.html#aangepaste-elementen",
    "title": "Begin van mijn Quartoreis, een opzet voor een Quarto presentatie",
    "section": "Aangepaste elementen",
    "text": "Aangepaste elementen\nEr zijn ingebouwde presentatiethema’s in Quarto, en net als bij xaringan heb je de mogelijkheid om een van de meegeleverde thema’s aan te passen of om een eigen thema te bouwen. Om dit te doen, maak je een custom.scss bestand aan (hier veranderd in customNSCR.scss) en verwijs je ernaar in jouw YAML:\n---\nformat: \n  revealjs:\n    theme: [moon, customNSCR.scss]\n---\nDe oorspronkelijke custom.scss file die Meghan gebruikte voor haar slides vind je hier en naast enkele “gewone” aanpassingen zoals het specificeren van achtergrondkleur, letterkleur, linkkleur, enz., waren er een paar specifieke aspecten die ik wilde veranderen of toevoegen.\n\nUiterlijk van het codeblok\nStandaard hebben codeblokken in Quarto dezelfde achtergrondkleur als de dia’s zelf, met een lichtere rand (alle standaard elementen staan hier). Ik wilde een lichtere achtergrond voor mijn codeblok, met een donkere rand, dus heb ik de volgende veranderingen aangebracht in mijn .scss bestand:\n$code-block-bg: #ded9ca;\n$code-block-border-color: #000000;\n\n\nFooter text\nDe standaard voettekst in Quarto reveal.js-slides heeft tekst gecentreerd onderaan de pagina, maar Meghan wilde dat haar voettekst kleinere tekst had die rechts uitgelijnd was. Dit leek een simpele aanpassing, maar het sleutelen aan de .footer class in de .scss bestand werkte niet en ze kwam er maar niet achter waarom.\nLater, nadat ze het had opgegeven :expressionless:, was ze aan het bladeren door de reveal.js GitHub issues (nog een techniek die ze aanbeveelt om te leren!) en stuitte toevallig op een die toevallig de oplossing voor haar footerprobleem bevatte!\nHet standaard footer.css bestand is onderdeel van een reveal.js plugin die na elk thema of aangepaste bestanden laadt. En zo leerde ze over de !important eigenschap! Door die eigenschap aan haar elementen toe te voegen, kon ze eindelijk de voettekst bewerken. Zoals JJ in dat nummer schreef, is dit “buiten de lijntjes kleuren”.\n.reveal .footer {\n  font-size: 0.35em !important;\n  text-align: right !important;\n}\n\n\nSectie koppen\nDe dia’s die ze ontwikkeld heeft, hebben verschillende secties. Ze wilde iets ontwerpen om in de rechter bovenhoek te plaatsen dat de huidige sectie zou aangeven. Om dit te doen, maakte ze een nieuwe klasse in mijn .scss bestand genaamd .sectionhead.\n\nZe wou dat ze een meer specifieke aanbeveling had voor het leren van CSS/SCSS (deel het alsjeblieft als je een favoriete bron hebt!; daar ben ikzelf, Harrie, ook naar op zoek trouwens). Alles wat ze heeft geleerd heeft gevonden door ofwel te googlen of door het bekijken van de bestanden van andermans werk dat ze aantrekkelijk vindt. Uitzoeken hoe ze deze .sectionhead moest maken, was zoeken op hoe maak ik een tekstvak in CSS en veel spelen met de verschillende elementen totdat ze iets had dat haar aansprak.\n.sectionhead {\n  font-size: 1em;\n  text-align: center;\n  color: $presentation-heading-color;\n  font-family: $presentation-heading-font;\n  background-color: $body-bg;\n  margin: 1px;\n  padding: 2px 2px 2px 2px;\n  width: 120px;\n  border-left: 10px solid;\n}\n\n\nElementen vinden om aan te passen\nDit is erg specifiek en willekeurig, maar er is een functie in Quarto slides die tekst vervaagt bij een klik. Het lijkt ~50% te vervagen, maar ze wilde dat het nog meer vervaagde. Maar waar moet je beginnen om uit te zoeken hoe je dat kunt aanpassen?\nOm dit in de slides aan te geven, gebruikt ze .fragment.semi-fade-out. Dus ze zocht de Quarto repo voor die bepaalde string en vond het hier here, in de hoofd .scss file voor reveal. Die file verschafte de standaardsyntax, die ze in haar custom.scss file kon plakken en zo veranderde de ondoorzichtigheid.\n.reveal .slides section .fragment.semi-fade-out {\n    opacity: 1;\n    visibility: inherit;\n\n    &.visible {\n        opacity: 0.25;\n        visibility: inherit;\n    }\n}"
  },
  {
    "objectID": "posts/2023-02-13-quarto-presentatie-toelichting/Quatropresentatie.html#absolute-positie",
    "href": "posts/2023-02-13-quarto-presentatie-toelichting/Quatropresentatie.html#absolute-positie",
    "title": "Begin van mijn Quartoreis, een opzet voor een Quarto presentatie",
    "section": "Absolute positie",
    "text": "Absolute positie\nEen van de handigste functies van Quarto reveal.js slides is absolute positie(https://quarto.org/docs/presentations/revealjs/advanced.html#absolute-position, waarmee je elementen specifiek op een slide kunt plaatsen.\n\nZe heeft dit gebruikt om afbeeldingen te plaatsen, natuurlijk, maar ook om screenshots en code met tekst te annoteren (zoals in het voorbeeld hierboven), haar .sectionhead op elke dia te plaatsen en, in gevallen zoals de dia hieronder, een kleine outline toe te voegen voor nadruk.\n\nDeze elementen kunnen ook als fragmenten worden toegevoegd, d.w.z., om te verschijnen bij een klik. De volgende code plaatst deze twee boxen (die ik heb gemaakt met een nieuwe .blackbox klasse in mijn .scss bestand) op de dia. De . . . bovenaan deze code geeft aan dat ze verschijnen bij een klik na de rest van de diacode. Je zou ook een . . . tussen deze twee codegedeeltes plaatsen om de vakjes te laten verschijnen bij aparte kliks.\n. . .\n\n::: {.absolute top=\"42%\" left=\"4%\" width=\"150\"}\n::: {.blackbox}\n:::\n:::\n\n::: {.absolute top=\"42%\" left=\"69%\" width=\"185\"}\n::: {.blackbox}\n:::\n:::"
  },
  {
    "objectID": "posts/2023-02-13-quarto-presentatie-toelichting/Quatropresentatie.html#code-aanpassingen",
    "href": "posts/2023-02-13-quarto-presentatie-toelichting/Quatropresentatie.html#code-aanpassingen",
    "title": "Begin van mijn Quartoreis, een opzet voor een Quarto presentatie",
    "section": "Code aanpassingen",
    "text": "Code aanpassingen\nCodeblokken in reveal.js laten veel aanpassingen toe en dit zijn de twee die ik het nuttigst vond.\n\nCodevouwen\n\nVoor sommige voorbeelden tijdens het lesgeven, wil ze de code beschikbaar hebben, maar niet noodzakelijk alles op het scherm, wat onhandelbaar en/of afleidend zou kunnen zijn. De code-fold optie is hier heel geschikt voor, omdat de code alleen “on demand” beschikbaar is, en je kunt zelfs aangeven welke tekst er naast de pijl getoond moet worden. De code hieronder toont de code-fold en code-summary opties in gebruik.\nOpmerking terzijde: ik vind het prettig dat de chunkopties in de chunk zelf staan, voorafgegaan door #|, in plaats van in de {r} haakjes–het is veel leesbaarder.\n#| code-fold: true\n#| code-summary: \"expand for full code\"\n#| fig-align: \"center\"\nfac_enr %&gt;% \n  filter(!is.na(avg_enr)) %&gt;% \n  ggplot(aes(x = year, y = avg_enr, group = rank, color = rank)) +\n  geom_line() +\n  geom_point() +\n  scale_color_brewer(type = \"qual\", palette = \"Dark2\") +\n  labs(x = NULL, y = \"Average enrollment\",\n       title = \"Average undergraduate enrollment per rank over time\") +\n  theme_linedraw() +\n  theme(panel.grid.major.x = element_blank(),\n        axis.ticks = element_blank(),\n        legend.title = element_blank(),\n        legend.background = element_rect(fill = NA),\n        legend.key = element_rect(fill = NA),\n        legend.position = c(0.85, 0.82))\n\n\nIncrementele code markering\n\nCode highlighting is op zich geen nieuwe functie (je kon zeker al coderegels op laten lichten in xaringan), maar hier is het heel gemakkelijk om door code te “stappen” door verschillende regels te benadrukken bij een klik. Dit is super handig om de aandacht te vestigen tijdens het lesgeven en om code regel voor regel uit te leggen.\nDe volgende optie zou beginnen met het oplichten van regel 1 en 2 van het codeblok, dan regel 3 bij een klik, dan regel 4 bij een volgende klik.\n::: {.cell}\n\n:::"
  },
  {
    "objectID": "posts/2023-02-13-quarto-presentatie-toelichting/Quatropresentatie.html#tekst-stijlen",
    "href": "posts/2023-02-13-quarto-presentatie-toelichting/Quatropresentatie.html#tekst-stijlen",
    "title": "Begin van mijn Quartoreis, een opzet voor een Quarto presentatie",
    "section": "Tekst stijlen",
    "text": "Tekst stijlen\nUitzoeken hoe je tekst kunt stijlen was een beetje een leercurve voor haar—niet omdat het moeilijk is, het is gewoon anders dan xaringan en ze kon het niet goed uitgelegd vinden in de docs (hoewel de docs zo uitgebreid zijn, dat ik het waarschijnlijk gewoon ergens gemist heb). De code voor de demo presentatie was nuttig om wat stylingvoorbeelden op te pikken. Haar beste tip is om het werk van anderen te volgen en als je iets ziet dat je bevalt, zoek dan het GitHub bestand en zoek uit hoe zij het gedaan hebben! (zo heb ik dat zelf ook bij jou gedaan, Maghan)\n\nIn-line\nAls je wilt red words, type je [red words]{style=\"color:#cc0000\"}.\n\n\nGrotere stukken\nAls je in plaats daarvan de stijl van een groter stuk tekst wilt aanpassen, geef dan de sectie aan met ::: (die aan het eind moet worden herhaald) en gebruik dezelfde {style} syntaxis.\n::: {style=\"font-size: 1.5em; text-align: center\"}\ntext\n\ntext\n\ntext\n:::\n\n\nLogo\nIkzelf heb er nog een logo aan toegevoegd, dat links bovenin geplaatst worden. Ik wilde het alleen groter afgedrukt krijgen en dat lukte mij niet. Als ik gevonden heb hoe dat werkt, zal ik dat nog aanpassen."
  },
  {
    "objectID": "posts/2023-02-20-quarto-book/schrijf_publiseer_boek.html",
    "href": "posts/2023-02-20-quarto-book/schrijf_publiseer_boek.html",
    "title": "Maken en publiceren van een boek",
    "section": "",
    "text": "Met Quarto ben je in staat om inhoud en code naar verschillende wetenschappelijke producten om te zetten. Dat kunnen artikelen, rapporten, blogs, websites of b.v. dashboards zijn. Met Quarto kun je ook boeken maken in verschillende vormen (html, pdf, word of epub). In dit blog laat ik jullie zien hoe je een html-boek kunt maken. Om meer over Quarto te leren verwijs ik je naar de Quarto-website."
  },
  {
    "objectID": "posts/2023-02-20-quarto-book/schrijf_publiseer_boek.html#tt_book",
    "href": "posts/2023-02-20-quarto-book/schrijf_publiseer_boek.html#tt_book",
    "title": "Maken en publiceren van een boek",
    "section": "TT_book",
    "text": "TT_book\nDit is overigens het boek dat ik met Quarto heb gemaakt TT-book, het eindresultaat van deze tutorial. Al het materiaal (teksten, plaatjes, ondersteunend materiaal) vind je op Github."
  },
  {
    "objectID": "posts/2023-02-20-quarto-book/schrijf_publiseer_boek.html#achtergrond-van-het-boek",
    "href": "posts/2023-02-20-quarto-book/schrijf_publiseer_boek.html#achtergrond-van-het-boek",
    "title": "Maken en publiceren van een boek",
    "section": "Achtergrond van het boek",
    "text": "Achtergrond van het boek\nDe NSC-R werkgroep organiseert regelmatig bijeenkomsten om medewerkers en betrokkenen te informeren over ontwikkelingen op het terrein van moderne data-analyse met inzet van het programma R. Het werk van deze werkgroep vind je op NSC-R Workshops.\nOnderdeel van deze workshops zijn NSC-R Tidy Tuesday-bijeenkomsten. Iemand leidt mensen door een bepaald analysescript en laat zo zien wat en hoe je de analyse met R kunt uitvoeren. In de periode januari 2022-januari 2023 zijn verschillende van deze bijeenkomsten georganiseerd. In enkele stappen heb ik deze workshopteksten naar een boek omgezet en dit boek heb ik als product via internet gepubliceerd. Een aantal stappen zijn essentieel:\n\nAllereerst heb ik de verschillende scripts genomen en in Quarto omgezet (Tekstschrijven met Quarto).\n\nVervolgens heb ik het boekformat van Quarto binnengehaald en dat als uitgangspunt voor dit boek genomen. Vervolgens heb ik van de workshopteksten hoofdstukken gemaakt, ik heb er een inleiding en samenvatting aan toegevoegd en ik heb de referenties eraan toegevoegd (Creëren van een boek).\nBelangrijk is dat je het project op GitHub zet (Werken vanaf GitHub).\nAls je het project als repository of GitHub heb staan, kun je het product vervolgens via Netlify publiceren (Publiceren).\nDaarna heb ik de opmaak nog wat aangepast (Opmaak).\n\nHieronder ga ik wat uitgebreider op de vijf stappen in."
  },
  {
    "objectID": "posts/2023-02-20-quarto-book/schrijf_publiseer_boek.html#tekstschrijven-met-quarto",
    "href": "posts/2023-02-20-quarto-book/schrijf_publiseer_boek.html#tekstschrijven-met-quarto",
    "title": "Maken en publiceren van een boek",
    "section": "1. Tekstschrijven met Quarto",
    "text": "1. Tekstschrijven met Quarto\nQuarto is de nieuwe versie van R Markdown, het populaire pakket van R om wetenschappelijk te kunnen schrijven. R Markdown is er nu tien/twaalf jaar en blijft bestaan (ook ik zal het blijven gebruiken). Echter, Quarto heeft enkele kenmerken dat deze innovatie interessant maakt. Een van de voordelen is b.v. dat je er niet alleen in RStudio mee kunt werken maar ook met Python of VS Code. In deze tutorial wordt gebruik gemaakt van RStudio. In de nieuwe versies van RStudio is Quarto al opgenomen (dus dat hoef je niet apart te installeren). Als je met R Markdown werkt moet je steeds een ander pakket binnenhalen (voor het maken van een blog weer een ander pakket dan voor een boek). Bij Quarto hoef je dat niet steeds te doen. Nog een ander voordeel, ook het werken met Quarto zelf heeft enkele voordelen (bv dat je snel kunt wisselen tussen resultaat (Visual) en code (Soure) bv).\n\nQuarto-documenten sla je op als .qmd-documenten.Verder werkt het veelal het zelfde als .rmd-documenten. Ook dit systeem werkt met Markdown tekst. De code chunks schrijf je wel net wat anders.Je begint {r} en de opdracht zet je eronder met #| zols hieronder.\n#| warning: false\n#| echo: true\n#| label: fig-agecat\n#| fig-cap: \"Age categories and their numbers\"\n#| cap-location: margin\nUitgebreide informatie vind je over Schrijven en coderen op de website van Quarto. Je kunt ook de teksten van het boek nemen en zien hoe ik dit heb gedaan."
  },
  {
    "objectID": "posts/2023-02-20-quarto-book/schrijf_publiseer_boek.html#creëren-van-een-boek",
    "href": "posts/2023-02-20-quarto-book/schrijf_publiseer_boek.html#creëren-van-een-boek",
    "title": "Maken en publiceren van een boek",
    "section": "2. Creëren van een boek",
    "text": "2. Creëren van een boek\nNet als andere wetenschappelijke producten (zoals artikelen en rapporten) kun je ook boek in verschillende formaten creëren: html, pdf, word of epub. In deze tutorial heb ik een boek in html-formaat gemaakt.Crëer eerst binnen RStudio een nieuw project en kies voor New Directory.\n\n\n\nNieuw project\n\n\nVervolgens kies je voor Quarto Book.\n En dit boek geef je dan een naam, in ons geval TT_book\n\n\n\nTT_book\n\n\nAls je dit project opent, zie je dat er verschillende bestanden zijn binnengehaald.\n Als je dit project vervolgens opent en het document index.qmdrendert, zal het bestand -book worden toegevoegd.\n\n\n\n_book\n\n\nAls je dan _book/index.html opent, zie je het resultaat.\nDit project heb ik als uitgangspunt genomen en vervolgens heb ik de workshopsbestanden, de inleiding, de samenvatting en de referenties hieraan toegevoegd. Dat moet je steeds stap voor stap doen. Maar hier zie je hoe de projectmap er uiteindelijk uitziet. Belangrijk is wel dat je het yaml-document goed aanpast en dat je daarin omschrijft wat er aan hoofdstukken in het boek moet worden opgenomen.\nZo ziet dan het hele boek eruit, met een intro, tien hoofdstukken en een samenvatting. Over andere zaken zal ik straks nog wat bij de opmaak opmerken.\n Belangrijk is wel dat je het yaml-document aanpast aan de opzet die jij wilt maken.Vergelijk zelf het yaml-document dat je krijgt als je Quarto book opent en de yaml voor dit TT_book. Alle hoofdstukken moeten in het yaml-document zijn omschreven. Hier zie je het yaml document. Bij de opmaak zeg ik hier nog wat over. Hier zie een deel van yaml-opzet. Let voor op de hoofdstukken. Bij de opmaak kom ik nog op dit document terug.\n\n\n\nyaml-deel"
  },
  {
    "objectID": "posts/2023-02-20-quarto-book/schrijf_publiseer_boek.html#vanuit-github-werken",
    "href": "posts/2023-02-20-quarto-book/schrijf_publiseer_boek.html#vanuit-github-werken",
    "title": "Maken en publiceren van een boek",
    "section": "3. Vanuit GitHub werken",
    "text": "3. Vanuit GitHub werken\nOok voor deze klus is het belangrijk dat je vanaf GitHub werkt. Dat is het versie controle programma waarover ik al eens vaker heb geschreven.Zie bijvoorbeeld hier. Het is belangrijk om stap voor stap te kunnen werken, om alles goed bij te houden (versie-controle), eventueel met anderen in het project te kunnen samenwerken, maar ook om het eindproduct te kunnen communiceren.\nHet is natuurlijk goed als je vanaf het begin het project in GitHub hebt opgeslagen. In dat geval begin je met het openen van een nieuwe repository om GitHub, koppel je het met jouw lokale projectmap en voeg je er nieuwe bestanden aan toe. Je zorgt steeds dat de nieuwste versie op GitHub staat. Je moet dan wel een account daar hebben.\nHet kan ook zijn dat je het project al op jouw lokale computer hebt staan en het vervolgens op GitHub moet plaatsen. In dat geval kun je ook het pakket usethis gebruiken. [Usethis](https://usethis.r-lib.org/. Dat pakket moet je dan wel hebben geinstalleerd en geöpend in de projectmap van het boek.\nStart met het commando\nusethis::use_git()\nHij vraagt dan of je commit your changes toestaat en herstart dan RStudio en accepteer dit.\nGeef dan het volgende commando om een Github repository te creëren om alles op GitHub op te slaan.\nusethis::use_github()\nNogmaals, alles moet op GitHub staan voordat je het boek kunt publiceren."
  },
  {
    "objectID": "posts/2023-02-20-quarto-book/schrijf_publiseer_boek.html#publiceren",
    "href": "posts/2023-02-20-quarto-book/schrijf_publiseer_boek.html#publiceren",
    "title": "Maken en publiceren van een boek",
    "section": "4. Publiceren",
    "text": "4. Publiceren\nVoor het publiceren van het boek heb ik het programma Netlifygebruikt. Ook hier moet je een account hebben (wel gratis net als GitHub). Vervolgens moet je ervoor zorgen dat ingelogd bent.\nZeg dan dat je een nieuwe site wilt openen (je moet het boek in dit geval als nieuwe site zien).\n.\nBij het inloggen kun je het beste jouw GitHub account gebruiken. Verbind het met jouw GitHub account\n\n\n\nVia GitHub\n\n\nKies uit jouw repositories in dit geval de TT-repository (de projectmap waarin alles zit)\n\n\n\nTT-bookrepository\n\n\nKies jouw settings uit. In dit geval is -book de basisrepository en is _book/ de directory. Klik op deploy (publiceren).\n\n\n\nkies setting\n\n\nVervolgens kun je ook de naam veranderen (van de onbegrijpelijke Netlifynaam naar jouw herkenbare TT_book naam).\n\n\n\nNaam veranderen\n\n\nDe naam is dus veranderd in TT_book en uiteindelijk is dit het adres geworden dat je van Netlify krijgt: https://ttbook.netlify.app en dit is het resultaan\nTT_book"
  },
  {
    "objectID": "posts/2023-02-20-quarto-book/schrijf_publiseer_boek.html#opmaak",
    "href": "posts/2023-02-20-quarto-book/schrijf_publiseer_boek.html#opmaak",
    "title": "Maken en publiceren van een boek",
    "section": "5. Opmaak",
    "text": "5. Opmaak\n\nReferenties\nHeel kort iets over referenties. Zelf werk ik met het Zotero om referenties in op te slaan. Als je dat gedaan hebt, moet je dit bestand vervolgens als bib-bestand opslaan. Als je in de map van TT_book kijkt, vind je het referencestt.bib bestand. Daarin zitten alle referenties van de workshops. In de tekst zelf zet je dan bv @bernasco_nsc-r_2022 tussen haakjes ([]) als je wilt verwijzen naar een bijeenkomst die Bernasco in 2022 heeft gegeven. In de _yml schrijf je dan dat de bibliografie te vinden is in het bestand referencestt.bib.\nReferenties kunnen op verschillende manieren beschreven worden. Op internet kun je verschillende stijlen vinden citation styles. Hier is gekozen voor de apa stijl (apa.csl). Dit kun je als document binnenhalen en daar kun je in jouw yml-bestand naar verwijzen (csl:apa.csl).\n\n\n\nReferenties\n\n\n\n\nStijl\nDe stijl van het product kun je op verschillende manieren defiëren. In Quarto zelf zitten er verschillende thema’s waar je voor kunt kiezen. In mijn geval heb ik voor cosmo gekozen. Dat vermeld je dan in het yml-bestand. Je kunt in datzelfde bestand ook lettergroottes, -types, achtergrondkleuren of andere stijlaspecten definiëren. Wat je ook kunt doen is een apart css-bestand toevoegen waarin je de stijl van, in dit geval boek, definieert. Ook over dit onderwerp zou veel meer te vertellen zijn, maar dat laat ik voor nu even zitten."
  },
  {
    "objectID": "posts/2023-02-20-quarto-book/schrijf_publiseer_boek.html#tot-slot",
    "href": "posts/2023-02-20-quarto-book/schrijf_publiseer_boek.html#tot-slot",
    "title": "Maken en publiceren van een boek",
    "section": "Tot slot",
    "text": "Tot slot\nHet maken van een boek lijkt heel erg op het maken van een blog of een website. Als je het een kunt, wordt het andere product makkelijker om te maken. Werk vanuit een project met verschillende mappen; zorg ervoor dat je altijd met GitHub werkt en doe alles stap voor stap. Begin met een klein project en werk het daarna uit. Om te publiceren heb ik tot nu toe met Netlify en GitHub Pages gewerkt. Quarto heeft ook een eigen systeem om te publiceren (Quarto Pub) en dan is er nog Posit Connect en zijn er nog allerlei andere diensten waarmee je jouw producten onder de aandacht kunt brengen. Later zal ik nog eens een blog schrijven over deze mogelijkheden."
  },
  {
    "objectID": "posts/2023-02-20-quarto-book/Quatropresentatie.html",
    "href": "posts/2023-02-20-quarto-book/Quatropresentatie.html",
    "title": "Begin van mijn Quartoreis, een opzet voor een Quarto presentatie",
    "section": "",
    "text": "Afgelopen weken heb ik wat met Quarto geëxperimenteerd. Zo heb ik met Quarto een boek gemaakt en daarover zal ik nog een andere blog schrijven. Ook heb ik met Quarto wat presentaties gemaakt. Meghan Hall heeft mij hier veel geleerd. Zij heeft een presentatie in Quarto gezet hier en de repository daarvan vind je hier. Hoe ze deze aantrekkelijke presentatie heeft opgemaakt, beschrijft ze in haar blog.\nHIER VIND JE DIE PRESENTATIE\nDe presentatie heb ik vertaald en in andere kleuren gezet hier. De blogopmaak heb ik zelf wat aangepast. De aangepaste presentatie en blog vind je op mijn github-site hier. Hartelijke dank Meghan Hall voor jouw duidelijke uitleg, veel van geleerd."
  },
  {
    "objectID": "posts/2023-02-20-quarto-book/Quatropresentatie.html#begin-van-mijn-quarto-reis",
    "href": "posts/2023-02-20-quarto-book/Quatropresentatie.html#begin-van-mijn-quarto-reis",
    "title": "Begin van mijn Quartoreis, een opzet voor een Quarto presentatie",
    "section": "Begin van mijn Quarto reis",
    "text": "Begin van mijn Quarto reis\nQuarto is een nieuw open-source technisch publicatiesysteem van RStudio. Het lijkt in veel opzichten op R Markdown (dat niet verdwijnt, maak je geen zorgen!), behalve dat het geen R vereist, meer talen ondersteunt en de functionaliteit van veel RMarkdown pakketten combineert (b.v. blogdown, bookdown).\nMeghan, maar ikzelf ook, is een toegewijde RMarkdown gebruiker, maar ze wil beginnen met het verkennen van de nieuwe Quarto functies. Haar eerste project was slides voor een workshop—Ze zou normaal de RMarkdown-gebaseerde xaringan gebruiken voor HTML presentaties, maar besloot om het Quarto alternatief uit te proberen, dat reveal.js gebruikt.\n\nDe slides die ze met Quarto maakte zijn hier en de code vind je op GitHub hier.\n\nVeel van de functionaliteit is gelijkaardig aan xaringan of andere gelijkaardige RMarkdown HTML formaten, maar er is veel nieuwe syntax om te leren (en ook veel nieuwe functies!), dus heeft ze deze post samengesteld om een paar dingen te verzamelen die ze geleerd heeft terwijl ze voor de eerste keer een Quarto-presentatie maakte."
  },
  {
    "objectID": "posts/2023-02-20-quarto-book/Quatropresentatie.html#quarto-bronnen",
    "href": "posts/2023-02-20-quarto-book/Quatropresentatie.html#quarto-bronnen",
    "title": "Begin van mijn Quartoreis, een opzet voor een Quarto presentatie",
    "section": "Quarto bronnen",
    "text": "Quarto bronnen\nHaar favoriete introductie op het publicatiesysteem zelf is Alison Hill’s samenvattendepost. Mine Çetinkaya-Rundel heeft een doorlopende blog met een nieuwe Quartotip elke dag."
  },
  {
    "objectID": "posts/2023-02-20-quarto-book/Quatropresentatie.html#emojis",
    "href": "posts/2023-02-20-quarto-book/Quatropresentatie.html#emojis",
    "title": "Begin van mijn Quartoreis, een opzet voor een Quarto presentatie",
    "section": "Emojis",
    "text": "Emojis\nEerst maar eens—hoe gebruik je emoji’s in Quarto?? Gelukkig was dat niet zo moeilijk. Door de volgende regel aan de YAML toe te voegen:\n---\nfrom: markdown+emoji\n---\nJe kunt emoji’s toevoegen door te schrijven :wave: om :wave: te krijgen. De hele lijst met emoji codes vind je hier. (Dit is een goede lijst met welke YAML opties er beschikbaar zijn.)"
  },
  {
    "objectID": "posts/2023-02-20-quarto-book/Quatropresentatie.html#aangepaste-elementen",
    "href": "posts/2023-02-20-quarto-book/Quatropresentatie.html#aangepaste-elementen",
    "title": "Begin van mijn Quartoreis, een opzet voor een Quarto presentatie",
    "section": "Aangepaste elementen",
    "text": "Aangepaste elementen\nEr zijn ingebouwde presentatiethema’s in Quarto, en net als bij xaringan heb je de mogelijkheid om een van de meegeleverde thema’s aan te passen of om een eigen thema te bouwen. Om dit te doen, maak je een custom.scss bestand aan (hier veranderd in customNSCR.scss) en verwijs je ernaar in jouw YAML:\n---\nformat: \n  revealjs:\n    theme: [moon, customNSCR.scss]\n---\nDe oorspronkelijke custom.scss file die Meghan gebruikte voor haar slides vind je hier en naast enkele “gewone” aanpassingen zoals het specificeren van achtergrondkleur, letterkleur, linkkleur, enz., waren er een paar specifieke aspecten die ik wilde veranderen of toevoegen.\n\nUiterlijk van het codeblok\nStandaard hebben codeblokken in Quarto dezelfde achtergrondkleur als de dia’s zelf, met een lichtere rand (alle standaard elementen staan hier). Ik wilde een lichtere achtergrond voor mijn codeblok, met een donkere rand, dus heb ik de volgende veranderingen aangebracht in mijn .scss bestand:\n$code-block-bg: #ded9ca;\n$code-block-border-color: #000000;\n\n\nFooter text\nDe standaard voettekst in Quarto reveal.js-slides heeft tekst gecentreerd onderaan de pagina, maar Meghan wilde dat haar voettekst kleinere tekst had die rechts uitgelijnd was. Dit leek een simpele aanpassing, maar het sleutelen aan de .footer class in de .scss bestand werkte niet en ze kwam er maar niet achter waarom.\nLater, nadat ze het had opgegeven :expressionless:, was ze aan het bladeren door de reveal.js GitHub issues (nog een techniek die ze aanbeveelt om te leren!) en stuitte toevallig op een die toevallig de oplossing voor haar footerprobleem bevatte!\nHet standaard footer.css bestand is onderdeel van een reveal.js plugin die na elk thema of aangepaste bestanden laadt. En zo leerde ze over de !important eigenschap! Door die eigenschap aan haar elementen toe te voegen, kon ze eindelijk de voettekst bewerken. Zoals JJ in dat nummer schreef, is dit “buiten de lijntjes kleuren”.\n.reveal .footer {\n  font-size: 0.35em !important;\n  text-align: right !important;\n}\n\n\nSectie koppen\nDe dia’s die ze ontwikkeld heeft, hebben verschillende secties. Ze wilde iets ontwerpen om in de rechter bovenhoek te plaatsen dat de huidige sectie zou aangeven. Om dit te doen, maakte ze een nieuwe klasse in mijn .scss bestand genaamd .sectionhead.\n\nZe wou dat ze een meer specifieke aanbeveling had voor het leren van CSS/SCSS (deel het alsjeblieft als je een favoriete bron hebt!; daar ben ikzelf, Harrie, ook naar op zoek trouwens). Alles wat ze heeft geleerd heeft gevonden door ofwel te googlen of door het bekijken van de bestanden van andermans werk dat ze aantrekkelijk vindt. Uitzoeken hoe ze deze .sectionhead moest maken, was zoeken op hoe maak ik een tekstvak in CSS en veel spelen met de verschillende elementen totdat ze iets had dat haar aansprak.\n.sectionhead {\n  font-size: 1em;\n  text-align: center;\n  color: $presentation-heading-color;\n  font-family: $presentation-heading-font;\n  background-color: $body-bg;\n  margin: 1px;\n  padding: 2px 2px 2px 2px;\n  width: 120px;\n  border-left: 10px solid;\n}\n\n\nElementen vinden om aan te passen\nDit is erg specifiek en willekeurig, maar er is een functie in Quarto slides die tekst vervaagt bij een klik. Het lijkt ~50% te vervagen, maar ze wilde dat het nog meer vervaagde. Maar waar moet je beginnen om uit te zoeken hoe je dat kunt aanpassen?\nOm dit in de slides aan te geven, gebruikt ze .fragment.semi-fade-out. Dus ze zocht de Quarto repo voor die bepaalde string en vond het hier here, in de hoofd .scss file voor reveal. Die file verschafte de standaardsyntax, die ze in haar custom.scss file kon plakken en zo veranderde de ondoorzichtigheid.\n.reveal .slides section .fragment.semi-fade-out {\n    opacity: 1;\n    visibility: inherit;\n\n    &.visible {\n        opacity: 0.25;\n        visibility: inherit;\n    }\n}"
  },
  {
    "objectID": "posts/2023-02-20-quarto-book/Quatropresentatie.html#absolute-positie",
    "href": "posts/2023-02-20-quarto-book/Quatropresentatie.html#absolute-positie",
    "title": "Begin van mijn Quartoreis, een opzet voor een Quarto presentatie",
    "section": "Absolute positie",
    "text": "Absolute positie\nEen van de handigste functies van Quarto reveal.js slides is absolute positie(https://quarto.org/docs/presentations/revealjs/advanced.html#absolute-position, waarmee je elementen specifiek op een slide kunt plaatsen.\n\nZe heeft dit gebruikt om afbeeldingen te plaatsen, natuurlijk, maar ook om screenshots en code met tekst te annoteren (zoals in het voorbeeld hierboven), haar .sectionhead op elke dia te plaatsen en, in gevallen zoals de dia hieronder, een kleine outline toe te voegen voor nadruk.\n\nDeze elementen kunnen ook als fragmenten worden toegevoegd, d.w.z., om te verschijnen bij een klik. De volgende code plaatst deze twee boxen (die ik heb gemaakt met een nieuwe .blackbox klasse in mijn .scss bestand) op de dia. De . . . bovenaan deze code geeft aan dat ze verschijnen bij een klik na de rest van de diacode. Je zou ook een . . . tussen deze twee codegedeeltes plaatsen om de vakjes te laten verschijnen bij aparte kliks.\n. . .\n\n::: {.absolute top=\"42%\" left=\"4%\" width=\"150\"}\n::: {.blackbox}\n:::\n:::\n\n::: {.absolute top=\"42%\" left=\"69%\" width=\"185\"}\n::: {.blackbox}\n:::\n:::"
  },
  {
    "objectID": "posts/2023-02-20-quarto-book/Quatropresentatie.html#code-aanpassingen",
    "href": "posts/2023-02-20-quarto-book/Quatropresentatie.html#code-aanpassingen",
    "title": "Begin van mijn Quartoreis, een opzet voor een Quarto presentatie",
    "section": "Code aanpassingen",
    "text": "Code aanpassingen\nCodeblokken in reveal.js laten veel aanpassingen toe en dit zijn de twee die ik het nuttigst vond.\n\nCodevouwen\n\nVoor sommige voorbeelden tijdens het lesgeven, wil ze de code beschikbaar hebben, maar niet noodzakelijk alles op het scherm, wat onhandelbaar en/of afleidend zou kunnen zijn. De code-fold optie is hier heel geschikt voor, omdat de code alleen “on demand” beschikbaar is, en je kunt zelfs aangeven welke tekst er naast de pijl getoond moet worden. De code hieronder toont de code-fold en code-summary opties in gebruik.\nOpmerking terzijde: ik vind het prettig dat de chunkopties in de chunk zelf staan, voorafgegaan door #|, in plaats van in de {r} haakjes–het is veel leesbaarder.\n#| code-fold: true\n#| code-summary: \"expand for full code\"\n#| fig-align: \"center\"\nfac_enr %>% \n  filter(!is.na(avg_enr)) %>% \n  ggplot(aes(x = year, y = avg_enr, group = rank, color = rank)) +\n  geom_line() +\n  geom_point() +\n  scale_color_brewer(type = \"qual\", palette = \"Dark2\") +\n  labs(x = NULL, y = \"Average enrollment\",\n       title = \"Average undergraduate enrollment per rank over time\") +\n  theme_linedraw() +\n  theme(panel.grid.major.x = element_blank(),\n        axis.ticks = element_blank(),\n        legend.title = element_blank(),\n        legend.background = element_rect(fill = NA),\n        legend.key = element_rect(fill = NA),\n        legend.position = c(0.85, 0.82))\n\n\nIncrementele code markering\n\nCode highlighting is op zich geen nieuwe functie (je kon zeker al coderegels op laten lichten in xaringan), maar hier is het heel gemakkelijk om door code te “stappen” door verschillende regels te benadrukken bij een klik. Dit is super handig om de aandacht te vestigen tijdens het lesgeven en om code regel voor regel uit te leggen.\nDe volgende optie zou beginnen met het oplichten van regel 1 en 2 van het codeblok, dan regel 3 bij een klik, dan regel 4 bij een volgende klik.\n::: {.cell}\n\n:::"
  },
  {
    "objectID": "posts/2023-02-20-quarto-book/Quatropresentatie.html#tekst-stijlen",
    "href": "posts/2023-02-20-quarto-book/Quatropresentatie.html#tekst-stijlen",
    "title": "Begin van mijn Quartoreis, een opzet voor een Quarto presentatie",
    "section": "Tekst stijlen",
    "text": "Tekst stijlen\nUitzoeken hoe je tekst kunt stijlen was een beetje een leercurve voor haar—niet omdat het moeilijk is, het is gewoon anders dan xaringan en ze kon het niet goed uitgelegd vinden in de docs (hoewel de docs zo uitgebreid zijn, dat ik het waarschijnlijk gewoon ergens gemist heb). De code voor de demo presentatie was nuttig om wat stylingvoorbeelden op te pikken. Haar beste tip is om het werk van anderen te volgen en als je iets ziet dat je bevalt, zoek dan het GitHub bestand en zoek uit hoe zij het gedaan hebben! (zo heb ik dat zelf ook bij jou gedaan, Maghan)\n\nIn-line\nAls je wilt red words, type je [red words]{style=\"color:#cc0000\"}.\n\n\nGrotere stukken\nAls je in plaats daarvan de stijl van een groter stuk tekst wilt aanpassen, geef dan de sectie aan met ::: (die aan het eind moet worden herhaald) en gebruik dezelfde {style} syntaxis.\n::: {style=\"font-size: 1.5em; text-align: center\"}\ntext\n\ntext\n\ntext\n:::\n\n\nLogo\nIkzelf heb er nog een logo aan toegevoegd, dat links bovenin geplaatst worden. Ik wilde het alleen groter afgedrukt krijgen en dat lukte mij niet. Als ik gevonden heb hoe dat werkt, zal ik dat nog aanpassen."
  },
  {
    "objectID": "posts/2023-03-18-quarto-blog/schrijf_publiseer_blog.html",
    "href": "posts/2023-03-18-quarto-blog/schrijf_publiseer_blog.html",
    "title": "Maken en publiceren van een blog",
    "section": "",
    "text": "Met Quarto ben je in staat om inhoud en code naar verschillende wetenschappelijke producten om te zetten. Dat kunnen artikelen, rapporten, boeken, websites of b.v. dashboards zijn. Met Quarto kun je ook een blog maken. Hier laat ik jullie zien hoe je zo’n blog kunt maken. Om meer over Quarto te leren verwijs ik je naar de uitgebreide Quarto-website. Ondertussen heb ik verschillende blogs/websites en boeken met Quarto gemaakt, waaronder: - Mijn eigen website\n\nHarrie’s Hoekje\nNSCR-workshops\nDemocratie en Onderwijs\nQuarto Boek]"
  },
  {
    "objectID": "posts/2023-03-18-quarto-blog/schrijf_publiseer_blog.html#tt_blog",
    "href": "posts/2023-03-18-quarto-blog/schrijf_publiseer_blog.html#tt_blog",
    "title": "Maken en publiceren van een blog",
    "section": "TT_blog",
    "text": "TT_blog\nDit is overigens het blog dat ik met Quarto heb gemaakt TT-blog, het eindresultaat van deze tutorial. Al het materiaal (teksten, plaatjes, ondersteunend materiaal) vind je op Github. Op 4 april geef ik er een NSC-R workshop over. Die presentatie zal hier komen te staan."
  },
  {
    "objectID": "posts/2023-03-18-quarto-blog/schrijf_publiseer_blog.html#achtergrond-van-tt_blog",
    "href": "posts/2023-03-18-quarto-blog/schrijf_publiseer_blog.html#achtergrond-van-tt_blog",
    "title": "Maken en publiceren van een blog",
    "section": "Achtergrond van TT_blog",
    "text": "Achtergrond van TT_blog\nDe NSC-R werkgroep organiseert regelmatig bijeenkomsten om medewerkers en betrokkenen te informeren over ontwikkelingen op het terrein van moderne data-analyse met inzet van het programma R. Het werk van deze werkgroep vind je op NSC-R Workshops.\nOnderdeel van deze workshops zijn NSC-R Tidy Tuesday-bijeenkomsten. Iemand leidt mensen door een bepaald analysescript en laat zo zien wat en hoe je de analyse met R kunt uitvoeren. In de periode januari 2022-januari 2023 zijn er tien verschillende van deze bijeenkomsten georganiseerd. In enkele stappen heb ik elk van deze workshopteksten naar tien posts van dit blog omgezet en ik heb er een about-pagina aan toegevoegd met achtergrondinformatie over de workshops. Het blog heb ik als product via internet gepubliceerd. Een aantal stappen zijn essentieel:\n\nAllereerst heb ik op GitHub een repository geopend (Repository op GitHub).\nVervolgens heb ik het blogformat van Quarto binnengehaald en dat als uitgangspunt voor dit blog genomen. Vervolgens heb ik van de diverse workshopteksten posts gemaakt, ik heb er een about-pagina mert achtergrondinformatie aan toegevoegd (Open Quartoblog).\nAl deze documenten heb ik vervolgens binnen de repository van GitHub geplaatst (Werken vanaf GitHub).\nDaarna heb ik de opmaak nog wat aangepast (Opmaak).\nAls je het project als repository op GitHub hebt staan, kun je het product vervolgens via Netlify publiceren (Publiceren).\n\nHieronder ga ik wat uitgebreider op de vijf stappen in."
  },
  {
    "objectID": "posts/2023-03-18-quarto-blog/schrijf_publiseer_blog.html#repository-op-github",
    "href": "posts/2023-03-18-quarto-blog/schrijf_publiseer_blog.html#repository-op-github",
    "title": "Maken en publiceren van een blog",
    "section": "1. Repository op GitHub",
    "text": "1. Repository op GitHub\nVaak begint een project met het openen van een repository op GitHub. Probeer zo veel mogelijk vanuit GitHub te werken. Jouw laatste versies zijn dan veilig opgeborgen en in principe kun je zo mogelijk makkelijk en goed met anderen samenwerken.\n ## 2. Open Quartoblog\nQuarto is de nieuwe versie van R Markdown, het populaire pakket van R om wetenschappelijk te kunnen schrijven. R Markdown is er nu tien/twaalf jaar en blijft bestaan (ook ik zal het blijven gebruiken). Echter, Quarto heeft enkele kenmerken dat deze innovatie interessant maakt. Een van de voordelen is b.v. dat je er niet alleen in RStudio mee kunt werken maar ook met Python of VS Code. In deze tutorial wordt gebruik gemaakt van RStudio. In de nieuwe versies van RStudio is Quarto al opgenomen (dus dat hoef je niet apart te installeren). Als je met R Markdown werkt moet je steeds een ander pakket binnenhalen (voor het maken van een blog weer een ander pakket dan voor een boek). Bij Quarto hoef je dat niet steeds te doen. Nog een ander voordeel, ook het werken met Quarto zelf heeft enkele voordelen (bv dat je snel kunt wisselen tussen resultaat (Visual) en code (Soure) bv).\n\nQuarto-documenten sla je op als .qmd-documenten.Verder werkt het veelal het zelfde als .rmd-documenten. Ook dit systeem werkt met Markdown tekst. De code chunks schrijf je wel net wat anders.Je begint {r} en de opdracht zet je eronder met #| zols hieronder.\n#| warning: false\n#| echo: true\n#| label: fig-agecat\n#| fig-cap: \"Age categories and their numbers\"\n#| cap-location: margin\nUitgebreide informatie vind je over Schrijven en coderen op de website van Quarto. Je kunt ook de teksten van het boek nemen en zien hoe ik dit heb gedaan.\nNet als andere wetenschappelijke producten (zoals artikelen, rapporten, boeken) kun je er ook een blog mee maken. Voor deze tutorial heb ik een eenvoudig blog gemaakt. Crëeer eerst binnen RStudio een nieuw project en kies voor New Directory.\n\n\n\nNieuw project\n\n\nVervolgens kies je voor Quarto Blog.\n En dit blog geef je dan een naam, in ons geval TT_blog\n\n\n\nTT_blog\n\n\nAls je dit project opent, zie je dat er verschillende bestanden zijn binnengehaald.\n\n\n\nBestanden\n\n\nAls je dit project vervolgens opent en het document index.qmdrendert, zal het bestand _site worden toegevoegd.\n\n\n\n_book\n\n\nAls je dan _site/index.html opent, zie je het resultaat.\nDit project heb ik als uitgangspunt genomen en vervolgens heb ik de tien workshopsbestanden toegevoegd en achtergrondinformatie gegeven over het project in de about-file. Dat moet je steeds stap voor stap doen. Maar hier zie je hoe de projectmap er uiteindelijk uitziet.\nZo ziet dan dan de structuur van het blog eruit. Over andere zaken zal ik straks nog wat bij de opmaak opmerken"
  },
  {
    "objectID": "posts/2023-03-18-quarto-blog/schrijf_publiseer_blog.html#vanuit-github-werken",
    "href": "posts/2023-03-18-quarto-blog/schrijf_publiseer_blog.html#vanuit-github-werken",
    "title": "Maken en publiceren van een blog",
    "section": "3. Vanuit GitHub werken",
    "text": "3. Vanuit GitHub werken\nOok voor deze klus is het belangrijk dat je vanaf GitHub werkt. Dat is het versie controle programma waarover ik al eens vaker heb geschreven.Zie bijvoorbeeld hier. Het is belangrijk om stap voor stap te kunnen werken, om alles goed bij te houden (versie-controle), eventueel met anderen in het project te kunnen samenwerken, maar ook om het eindproduct te kunnen communiceren.\nHet is natuurlijk goed als je vanaf het begin het project in GitHub hebt opgeslagen. In dat geval begin je met het openen van een nieuwe repository om GitHub, koppel je het met jouw lokale projectmap en voeg je er nieuwe bestanden aan toe. Je zorgt steeds dat de nieuwste versie op GitHub staat. Je moet dan wel een account daar hebben.\nHet kan ook zijn dat je het project al op jouw lokale computer hebt staan en het vervolgens op GitHub moet plaatsen. In dat geval kun je ook het pakket usethis gebruiken. [Usethis](https://usethis.r-lib.org/. Dat pakket moet je dan wel hebben geinstalleerd en geöpend in de projectmap van het boek.\nStart met het commando\nusethis::use_git()\nHij vraagt dan of je commit your changes toestaat en herstart dan RStudio en accepteer dit.\nGeef dan het volgende commando om een Github repository te creëren om alles op GitHub op te slaan.\nusethis::use_github()\nIn mijn geval heb ik trouwens alle nieuwe bestanden met de hand in de GitHub repository geplaatst. Hoe je het ook doet, alles moet op in ieder geval op GitHub staan voordat je het blog kunt publiceren."
  },
  {
    "objectID": "posts/2023-03-18-quarto-blog/schrijf_publiseer_blog.html#opmaak",
    "href": "posts/2023-03-18-quarto-blog/schrijf_publiseer_blog.html#opmaak",
    "title": "Maken en publiceren van een blog",
    "section": "4. Opmaak",
    "text": "4. Opmaak\nDe stijl van het product kun je op verschillende manieren aanpassen. In Quarto zelf zitten er verschillende thema’s waar je voor kunt kiezen. In mijn geval heb ik voor cosmo gekozen. Dat vermeld je dan in het yml-bestand. Je kunt in datzelfde bestand ook lettergroottes, -types, achtergrondkleuren of andere stijlaspecten definiëren. Wat je ook kunt doen is een apart css-bestand toevoegen waarin je de stijl van, in dit geval boek, definieert. Ook over dit onderwerp zou veel meer te vertellen zijn, maar dat laat ik voor nu even zitten.\n\nCosmoDarklySketchy\n\n\n\n\n\n\n\n\n\n\n\n\nVoor de about-site heb je ook verschillende keuze\n\nJollaTrestlesSolanaMarqueeBroadside\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJe kunt de stijl ook nog ‘met de hand’ aanpassen.\n 5. Publiceren\nVoor het publiceren van het blog heb ik het programma Netlifygebruikt. Ook hier moet je een account hebben (wel gratis net als GitHub). Vervolgens moet je ervoor zorgen dat ingelogd bent.\nZeg dan dat je een nieuwe site wilt openen.Bij het inloggen kun je het beste jouw GitHub account gebruiken. Verbind het met jouw GitHub account en kies jouw repository (in ons geval Quartoblog) die je wilt publiceren.\n\n\n\nVia GitHub\n\n\nKies jouw settings uit. In dit geval is _site de basisrepository en is _site/ de directory. Klik op deploy (publiceren).\n\n\n\nkies setting\n\n\nVervolgens kun je ook de naam veranderen (van de onbegrijpelijke Netlifynaam naar jouw herkenbare tt_blog naam).\n\n\n\nNaam veranderen\n\n\nEn\n\n\n\nNaam Veranderen2\n\n\nDe naam is dus veranderd in tt_blog en uiteindelijk is dit het adres geworden dat je van Netlify krijgt: https://tt-blog.netlify.app en dit is het resultaat\ntt-blog"
  },
  {
    "objectID": "posts/2023-03-18-quarto-blog/schrijf_publiseer_blog.html#tot-slot",
    "href": "posts/2023-03-18-quarto-blog/schrijf_publiseer_blog.html#tot-slot",
    "title": "Maken en publiceren van een blog",
    "section": "Tot slot",
    "text": "Tot slot\nHet maken van een blog lijkt heel erg op het maken van een boek of een website (zie ook mijn blog van februari over het maken en verspreiden van een boek). Als je het een kunt, wordt het andere product makkelijker om te maken. Werk vanuit een project met verschillende mappen; zorg ervoor dat je altijd met GitHub werkt en doe alles stap voor stap. Begin met een klein project en werk het daarna uit. Om te publiceren heb ik tot nu toe met Netlify. Het moet ook met GitHub Pageskunnen. Quarto zelf heeft ook een eigen systeem om te publiceren (Quarto Pub) en dan is er nog Posit Connect en zijn er nog allerlei andere diensten waarmee je jouw producten onder de aandacht kunt brengen. Later zal ik nog eens een blog schrijven over deze mogelijkheden."
  },
  {
    "objectID": "posts/2023-02-20-quarto-book/schrijf_publiseer_blog.html",
    "href": "posts/2023-02-20-quarto-book/schrijf_publiseer_blog.html",
    "title": "Maken en publiceren van een boek",
    "section": "",
    "text": "Met Quarto ben je in staat om inhoud en code naar verschillende wetenschappelijke producten om te zetten. Dat kunnen artikelen, rapporten, blogs, websites of b.v. dashboards zijn. Met Quarto kun je ook boeken maken in verschillende vormen (html, pdf, word of epub). In dit blog laat ik jullie zien hoe je een html-boek kunt maken. Om meer over Quarto te leren verwijs ik je naar de Quarto-website."
  },
  {
    "objectID": "posts/2023-02-20-quarto-book/schrijf_publiseer_blog.html#tt_book",
    "href": "posts/2023-02-20-quarto-book/schrijf_publiseer_blog.html#tt_book",
    "title": "Maken en publiceren van een boek",
    "section": "TT_book",
    "text": "TT_book\nDit is overigens het boek dat ik met Quarto heb gemaakt TT-book, het eindresultaat van deze tutorial. Al het materiaal (teksten, plaatjes, ondersteunend materiaal) vind je op Github."
  },
  {
    "objectID": "posts/2023-02-20-quarto-book/schrijf_publiseer_blog.html#achtergrond-van-het-boek",
    "href": "posts/2023-02-20-quarto-book/schrijf_publiseer_blog.html#achtergrond-van-het-boek",
    "title": "Maken en publiceren van een boek",
    "section": "Achtergrond van het boek",
    "text": "Achtergrond van het boek\nDe NSC-R werkgroep organiseert regelmatig bijeenkomsten om medewerkers en betrokkenen te informeren over ontwikkelingen op het terrein van moderne data-analyse met inzet van het programma R. Het werk van deze werkgroep vind je op NSC-R Workshops.\nOnderdeel van deze workshops zijn NSC-R Tidy Tuesday-bijeenkomsten. Iemand leidt mensen door een bepaald analysescript en laat zo zien wat en hoe je de analyse met R kunt uitvoeren. In de periode januari 2022-januari 2023 zijn verschillende van deze bijeenkomsten georganiseerd. In enkele stappen heb ik deze workshopteksten naar een boek omgezet en dit boek heb ik als product via internet gepubliceerd. Een aantal stappen zijn essentieel:\n\nAllereerst heb ik de verschillende scripts genomen en in Quarto omgezet (Tekstschrijven met Quarto).\n\nVervolgens heb ik het boekformat van Quarto binnengehaald en dat als uitgangspunt voor dit boek genomen. Vervolgens heb ik van de workshopteksten hoofdstukken gemaakt, ik heb er een inleiding en samenvatting aan toegevoegd en ik heb de referenties eraan toegevoegd (Creëren van een boek).\nBelangrijk is dat je het project op GitHub zet (Werken vanaf GitHub).\nAls je het project als repository of GitHub heb staan, kun je het product vervolgens via Netlify publiceren (Publiceren).\nDaarna heb ik de opmaak nog wat aangepast (Opmaak).\n\nHieronder ga ik wat uitgebreider op de vijf stappen in."
  },
  {
    "objectID": "posts/2023-02-20-quarto-book/schrijf_publiseer_blog.html#tekstschrijven-met-quarto",
    "href": "posts/2023-02-20-quarto-book/schrijf_publiseer_blog.html#tekstschrijven-met-quarto",
    "title": "Maken en publiceren van een boek",
    "section": "1. Tekstschrijven met Quarto",
    "text": "1. Tekstschrijven met Quarto\nQuarto is de nieuwe versie van R Markdown, het populaire pakket van R om wetenschappelijk te kunnen schrijven. R Markdown is er nu tien/twaalf jaar en blijft bestaan (ook ik zal het blijven gebruiken). Echter, Quarto heeft enkele kenmerken dat deze innovatie interessant maakt. Een van de voordelen is b.v. dat je er niet alleen in RStudio mee kunt werken maar ook met Python of VS Code. In deze tutorial wordt gebruik gemaakt van RStudio. In de nieuwe versies van RStudio is Quarto al opgenomen (dus dat hoef je niet apart te installeren). Als je met R Markdown werkt moet je steeds een ander pakket binnenhalen (voor het maken van een blog weer een ander pakket dan voor een boek). Bij Quarto hoef je dat niet steeds te doen. Nog een ander voordeel, ook het werken met Quarto zelf heeft enkele voordelen (bv dat je snel kunt wisselen tussen resultaat (Visual) en code (Soure) bv).\n\nQuarto-documenten sla je op als .qmd-documenten.Verder werkt het veelal het zelfde als .rmd-documenten. Ook dit systeem werkt met Markdown tekst. De code chunks schrijf je wel net wat anders.Je begint {r} en de opdracht zet je eronder met #| zols hieronder.\n#| warning: false\n#| echo: true\n#| label: fig-agecat\n#| fig-cap: \"Age categories and their numbers\"\n#| cap-location: margin\nUitgebreide informatie vind je over Schrijven en coderen op de website van Quarto. Je kunt ook de teksten van het boek nemen en zien hoe ik dit heb gedaan."
  },
  {
    "objectID": "posts/2023-02-20-quarto-book/schrijf_publiseer_blog.html#creëren-van-een-boek",
    "href": "posts/2023-02-20-quarto-book/schrijf_publiseer_blog.html#creëren-van-een-boek",
    "title": "Maken en publiceren van een boek",
    "section": "2. Creëren van een boek",
    "text": "2. Creëren van een boek\nNet als andere wetenschappelijke producten (zoals artikelen en rapporten) kun je ook boek in verschillende formaten creëren: html, pdf, word of epub. In deze tutorial heb ik een boek in html-formaat gemaakt.Crëer eerst binnen RStudio een nieuw project en kies voor New Directory.\n\n\n\nNieuw project\n\n\nVervolgens kies je voor Quarto Book.\n En dit boek geef je dan een naam, in ons geval TT_book\n\n\n\nTT_book\n\n\nAls je dit project opent, zie je dat er verschillende bestanden zijn binnengehaald.\n Als je dit project vervolgens opent en het document index.qmdrendert, zal het bestand -book worden toegevoegd.\n\n\n\n_book\n\n\nAls je dan _book/index.html opent, zie je het resultaat.\nDit project heb ik als uitgangspunt genomen en vervolgens heb ik de workshopsbestanden, de inleiding, de samenvatting en de referenties hieraan toegevoegd. Dat moet je steeds stap voor stap doen. Maar hier zie je hoe de projectmap er uiteindelijk uitziet. Belangrijk is wel dat je het yaml-document goed aanpast en dat je daarin omschrijft wat er aan hoofdstukken in het boek moet worden opgenomen.\nZo ziet dan het hele boek eruit, met een intro, tien hoofdstukken en een samenvatting. Over andere zaken zal ik straks nog wat bij de opmaak opmerken.\n Belangrijk is wel dat je het yaml-document aanpast aan de opzet die jij wilt maken.Vergelijk zelf het yaml-document dat je krijgt als je Quarto book opent en de yaml voor dit TT_book. Alle hoofdstukken moeten in het yaml-document zijn omschreven. Hier zie je het yaml document. Bij de opmaak zeg ik hier nog wat over. Hier zie een deel van yaml-opzet. Let voor op de hoofdstukken. Bij de opmaak kom ik nog op dit document terug.\n\n\n\nyaml-deel"
  },
  {
    "objectID": "posts/2023-02-20-quarto-book/schrijf_publiseer_blog.html#vanuit-github-werken",
    "href": "posts/2023-02-20-quarto-book/schrijf_publiseer_blog.html#vanuit-github-werken",
    "title": "Maken en publiceren van een boek",
    "section": "3. Vanuit GitHub werken",
    "text": "3. Vanuit GitHub werken\nOok voor deze klus is het belangrijk dat je vanaf GitHub werkt. Dat is het versie controle programma waarover ik al eens vaker heb geschreven.Zie bijvoorbeeld hier. Het is belangrijk om stap voor stap te kunnen werken, om alles goed bij te houden (versie-controle), eventueel met anderen in het project te kunnen samenwerken, maar ook om het eindproduct te kunnen communiceren.\nHet is natuurlijk goed als je vanaf het begin het project in GitHub hebt opgeslagen. In dat geval begin je met het openen van een nieuwe repository om GitHub, koppel je het met jouw lokale projectmap en voeg je er nieuwe bestanden aan toe. Je zorgt steeds dat de nieuwste versie op GitHub staat. Je moet dan wel een account daar hebben.\nHet kan ook zijn dat je het project al op jouw lokale computer hebt staan en het vervolgens op GitHub moet plaatsen. In dat geval kun je ook het pakket usethis gebruiken. [Usethis](https://usethis.r-lib.org/. Dat pakket moet je dan wel hebben geinstalleerd en geöpend in de projectmap van het boek.\nStart met het commando\nusethis::use_git()\nHij vraagt dan of je commit your changes toestaat en herstart dan RStudio en accepteer dit.\nGeef dan het volgende commando om een Github repository te creëren om alles op GitHub op te slaan.\nusethis::use_github()\nNogmaals, alles moet op GitHub staan voordat je het boek kunt publiceren."
  },
  {
    "objectID": "posts/2023-02-20-quarto-book/schrijf_publiseer_blog.html#publiceren",
    "href": "posts/2023-02-20-quarto-book/schrijf_publiseer_blog.html#publiceren",
    "title": "Maken en publiceren van een boek",
    "section": "4. Publiceren",
    "text": "4. Publiceren\nVoor het publiceren van het boek heb ik het programma Netlifygebruikt. Ook hier moet je een account hebben (wel gratis net als GitHub). Vervolgens moet je ervoor zorgen dat ingelogd bent.\nZeg dan dat je een nieuwe site wilt openen (je moet het boek in dit geval als nieuwe site zien).\n.\nBij het inloggen kun je het beste jouw GitHub account gebruiken. Verbind het met jouw GitHub account\n\n\n\nVia GitHub\n\n\nKies uit jouw repositories in dit geval de TT-repository (de projectmap waarin alles zit)\n\n\n\nTT-bookrepository\n\n\nKies jouw settings uit. In dit geval is -book de basisrepository en is _book/ de directory. Klik op deploy (publiceren).\n\n\n\nkies setting\n\n\nVervolgens kun je ook de naam veranderen (van de onbegrijpelijke Netlifynaam naar jouw herkenbare TT_book naam).\n\n\n\nNaam veranderen\n\n\nDe naam is dus veranderd in TT_book en uiteindelijk is dit het adres geworden dat je van Netlify krijgt: https://ttbook.netlify.app en dit is het resultaan\nTT_book"
  },
  {
    "objectID": "posts/2023-02-20-quarto-book/schrijf_publiseer_blog.html#opmaak",
    "href": "posts/2023-02-20-quarto-book/schrijf_publiseer_blog.html#opmaak",
    "title": "Maken en publiceren van een boek",
    "section": "5. Opmaak",
    "text": "5. Opmaak\n\nReferenties\nHeel kort iets over referenties. Zelf werk ik met het Zotero om referenties in op te slaan. Als je dat gedaan hebt, moet je dit bestand vervolgens als bib-bestand opslaan. Als je in de map van TT_book kijkt, vind je het referencestt.bib bestand. Daarin zitten alle referenties van de workshops. In de tekst zelf zet je dan bv @bernasco_nsc-r_2022 tussen haakjes ([]) als je wilt verwijzen naar een bijeenkomst die Bernasco in 2022 heeft gegeven. In de _yml schrijf je dan dat de bibliografie te vinden is in het bestand referencestt.bib.\nReferenties kunnen op verschillende manieren beschreven worden. Op internet kun je verschillende stijlen vinden citation styles. Hier is gekozen voor de apa stijl (apa.csl). Dit kun je als document binnenhalen en daar kun je in jouw yml-bestand naar verwijzen (csl:apa.csl).\n\n\n\nReferenties\n\n\n\n\nStijl\nDe stijl van het product kun je op verschillende manieren defiëren. In Quarto zelf zitten er verschillende thema’s waar je voor kunt kiezen. In mijn geval heb ik voor cosmo gekozen. Dat vermeld je dan in het yml-bestand. Je kunt in datzelfde bestand ook lettergroottes, -types, achtergrondkleuren of andere stijlaspecten definiëren. Wat je ook kunt doen is een apart css-bestand toevoegen waarin je de stijl van, in dit geval boek, definieert. Ook over dit onderwerp zou veel meer te vertellen zijn, maar dat laat ik voor nu even zitten."
  },
  {
    "objectID": "posts/2023-02-20-quarto-book/schrijf_publiseer_blog.html#tot-slot",
    "href": "posts/2023-02-20-quarto-book/schrijf_publiseer_blog.html#tot-slot",
    "title": "Maken en publiceren van een boek",
    "section": "Tot slot",
    "text": "Tot slot\nHet maken van een boek lijkt heel erg op het maken van een blog of een website. Als je het een kunt, wordt het andere product makkelijker om te maken. Werk vanuit een project met verschillende mappen; zorg ervoor dat je altijd met GitHub werkt en doe alles stap voor stap. Begin met een klein project en werk het daarna uit. Om te publiceren heb ik tot nu toe met Netlify en GitHub Pages gewerkt. Quarto heeft ook een eigen systeem om te publiceren (Quarto Pub) en dan is er nog Posit Connect en zijn er nog allerlei andere diensten waarmee je jouw producten onder de aandacht kunt brengen. Later zal ik nog eens een blog schrijven over deze mogelijkheden."
  },
  {
    "objectID": "posts/2023-03-18-quarto-blog/schrijf_publiseer_blog.html#presentatie-over-quarto-blog",
    "href": "posts/2023-03-18-quarto-blog/schrijf_publiseer_blog.html#presentatie-over-quarto-blog",
    "title": "Maken en publiceren van een blog",
    "section": "Presentatie over Quarto Blog",
    "text": "Presentatie over Quarto Blog\nOver deze blog maken met Quarto heb ik een aparte presentatie gemaakt. Het resultaat vind je hier. Op GitHub heb ik deze presentatie geplaatst, hier"
  },
  {
    "objectID": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#regression-and-other-stories",
    "href": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#regression-and-other-stories",
    "title": "Regressie en nog zo iets",
    "section": "",
    "text": "Vijftien jaar geleden schreven Gelman en Hill Data analysis using regression and multilevel/hierarchical models, een klassieker over moderne data-analyse. Ze gebruikte R en WinBugs voor lineaire en logistische, hierarchische regressieanalyse en causale inferentie. Ze lieten zien hoe je dat op de frequentistische en Bayesiaanse manier kunt doen. Het boek werd voor mij een naslagwerk dat ik steeds maar weer uit de kast trok. Vorig jaar dacht ik, laat ik eens zien of Gelman al weer iets nieuws heeft geschreven en toen zag ik dat Regression and other stories hiernet uit was is. Dat heeft Andrew Gelman weer met Jennifer Hill geschreven maar nu ook met de Fin Aki Vehtari. Ik was er nog niet aan toe gekomen om het te lezen. Dat heb ik deze maand gedaan. Ook dit boek zal ik vaker uit de kast trekken. Dit boek gaat over allerlei aspecten van regressie. Het is een theoretisch én praktisch boek. Je leert, wat ze noemen, voorspellende modellen beter begrijpen, toepassen in verschillende praktische problemen en je leert het simuleren. Je leert het opbouwen vanaf de basis en daarna kun je het in verschillende situaties toepassen. Het wil kritisch zijn, zonder nihilistisch te worden en vooral laten zien dat je van statistische analyse kunt leren. Ook dit boek staat op twee benen: frequentisch en Bayesiaans en laat zien hoe informatie wordt gebruikt in het schattingsproces, de assumpties die eraan ten grondslag liggen en hoe schattingen en voorspellingen kunnen worden geïnterpreteerd in beide raamwerken. Beide kunnen worden gebruikt, maar het is ook duidelijk dat de voorkeur bij Bayesiaanse benadering ligt. Dan kun je ook andere informatie gebruiken om te schatten of te voorspellen. En omdat je simuleert (het model duizenden keren draait) kunt je met de Bayesiaanse techniek meer zeggen over onzekerheid. Dat maakt deze techniek zeer geschikt voor regressieanalyses zoals in dit boek gepresenteerd. Wat ik zelf van dit boek heb geleerd zijn de mogelijkheden om op basis van gegevens te voorspellen. Vooral hoofdstuk 9 (Voorspellen en Bayesiaanse inferentie) vond ik interessant. Maar het boek zit vol informatie en kennis en laat zich amper samenvatten. Het lijkt erop dat het een eerste deel is en ik verwacht dat er later nog een tweede deel komt dat de nadruk legt op multilevel analyse. We zullen zien Bij het boek zit ook nog een website met data en scripts om zelf uit te proberen, prachtig onderwijsmateriaal opgesteld door Aki Vehtari hier."
  },
  {
    "objectID": "posts/2023-02-20-quarto-book/schrijf_publiseer_boek.html#quarto",
    "href": "posts/2023-02-20-quarto-book/schrijf_publiseer_boek.html#quarto",
    "title": "Maken en publiceren van een boek",
    "section": "",
    "text": "Met Quarto ben je in staat om inhoud en code naar verschillende wetenschappelijke producten om te zetten. Dat kunnen artikelen, rapporten, blogs, websites of b.v. dashboards zijn. Met Quarto kun je ook boeken maken in verschillende vormen (html, pdf, word of epub). In dit blog laat ik jullie zien hoe je een html-boek kunt maken. Om meer over Quarto te leren verwijs ik je naar de Quarto-website."
  },
  {
    "objectID": "posts/2022-05-31-van-distill-naar-quarto/van-distill-naar-quarto.html#footnotes",
    "href": "posts/2022-05-31-van-distill-naar-quarto/van-distill-naar-quarto.html#footnotes",
    "title": "Van distill naar quarto?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nJa↩︎\nNee, maar ze kunnen recursief zijn[^3]↩︎\nVoor het geval je geïnteresseerd bent: de “Welkom op mijn blog” post in de start blog vermeldt de datum als date: \"20/04/2022\", die wordt verwerkt als een letterlijke string wanneer de post wordt gebouwd (d.w.z., de postdatum wordt weergegeven als “20/04/2022”), logisch genoeg. Echter, wanneer je de hele site bouwt, wordt deze weergegeven als “4 mei 2023”.↩︎\nWat ik eerlijk gezegd niet doe, maar ik ben ook dom en probeer dingen toch↩︎"
  },
  {
    "objectID": "posts/2021-04-22-tidymodels-opnieuw/tidymodels-opnieuw.html#wat-is-tidymodels",
    "href": "posts/2021-04-22-tidymodels-opnieuw/tidymodels-opnieuw.html#wat-is-tidymodels",
    "title": "Tidymodels opnieuw",
    "section": "",
    "text": "Net als tidyverse, dat uit verschillende pakketten bestaat zoals ggplot2 en dplyr, zitten er ook in tidymodels enkele kernpakketten, zoals\n\nrsample: voor het uit elkaar halen van een datasample (b.v. train/test of cross-validatie);\nrecipes: voor pre-procesfuncties;\nparsnip: voor het specificeren van het model;\nyardstick: voor het evalueren van van het model;\ntune: voor het afstemmen van parameters;\nworkflow: om alles samen te brengen.\n\nNet zoals je de hele suite aan pakketten van tidyverse kunt binnenhalen door library(tidyverse) in te tikken. tidymodels bestaat dus uit verschillende pakketten en soms zal ik hieronder individuele pakketten noemen."
  },
  {
    "objectID": "posts/2021-04-22-tidymodels-opnieuw/tidymodels-opnieuw.html#eerst-maar-eens-de-boel-klaarzetten",
    "href": "posts/2021-04-22-tidymodels-opnieuw/tidymodels-opnieuw.html#eerst-maar-eens-de-boel-klaarzetten",
    "title": "Tidymodels opnieuw",
    "section": "",
    "text": "Als je deze pakketten nog niet hebt geïnstalleerd, moet je dat wel eerst doen (slechts één keer) door install.packages(\"tidymodels\") te gebruiken. Vervolgens laad je bepaalde bibliotheken: tidymodels en tidyverse.\n\n# laad de relevante tidymodels pakketten\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.1.3\n\n\n-- Attaching packages -------------------------------------- tidymodels 0.2.0 --\n\n\nv broom        0.8.0     v recipes      0.2.0\nv dials        1.0.0     v rsample      0.1.1\nv dplyr        1.0.9     v tibble       3.1.7\nv ggplot2      3.3.6     v tidyr        1.2.0\nv infer        1.0.2     v tune         0.2.0\nv modeldata    0.1.1     v workflows    0.2.6\nv parsnip      1.0.0     v workflowsets 0.2.1\nv purrr        0.3.4     v yardstick    1.0.0\n\n\nWarning: package 'broom' was built under R version 4.1.3\n\n\nWarning: package 'dials' was built under R version 4.1.3\n\n\nWarning: package 'scales' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'infer' was built under R version 4.1.3\n\n\nWarning: package 'parsnip' was built under R version 4.1.3\n\n\nWarning: package 'recipes' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'tune' was built under R version 4.1.3\n\n\nWarning: package 'workflows' was built under R version 4.1.3\n\n\nWarning: package 'workflowsets' was built under R version 4.1.3\n\n\nWarning: package 'yardstick' was built under R version 4.1.3\n\n\n-- Conflicts ----------------------------------------- tidymodels_conflicts() --\nx purrr::discard() masks scales::discard()\nx dplyr::filter()  masks stats::filter()\nx dplyr::lag()     masks stats::lag()\nx recipes::step()  masks stats::step()\n* Use suppressPackageStartupMessages() to eliminate package startup messages\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv readr   2.1.2     v forcats 0.5.1\nv stringr 1.4.1     \n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'stringr' was built under R version 4.1.3\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx readr::col_factor() masks scales::col_factor()\nx purrr::discard()    masks scales::discard()\nx dplyr::filter()     masks stats::filter()\nx stringr::fixed()    masks recipes::fixed()\nx dplyr::lag()        masks stats::lag()\nx readr::spec()       masks yardstick::spec()\n\n\n\n# laad de Pima Indians dataset van de mlbench dataset\nlibrary(mlbench)\ndata(PimaIndiansDiabetes)\n#Wat zit erin\nglimpse(PimaIndiansDiabetes)\n\nRows: 768\nColumns: 9\n$ pregnant &lt;dbl&gt; 6, 1, 8, 1, 0, 5, 3, 10, 2, 8, 4, 10, 10, 1, 5, 7, 0, 7, 1, 1~\n$ glucose  &lt;dbl&gt; 148, 85, 183, 89, 137, 116, 78, 115, 197, 125, 110, 168, 139,~\n$ pressure &lt;dbl&gt; 72, 66, 64, 66, 40, 74, 50, 0, 70, 96, 92, 74, 80, 60, 72, 0,~\n$ triceps  &lt;dbl&gt; 35, 29, 0, 23, 35, 0, 32, 0, 45, 0, 0, 0, 0, 23, 19, 0, 47, 0~\n$ insulin  &lt;dbl&gt; 0, 0, 0, 94, 168, 0, 88, 0, 543, 0, 0, 0, 0, 846, 175, 0, 230~\n$ mass     &lt;dbl&gt; 33.6, 26.6, 23.3, 28.1, 43.1, 25.6, 31.0, 35.3, 30.5, 0.0, 37~\n$ pedigree &lt;dbl&gt; 0.627, 0.351, 0.672, 0.167, 2.288, 0.201, 0.248, 0.134, 0.158~\n$ age      &lt;dbl&gt; 50, 31, 32, 21, 33, 30, 26, 29, 53, 54, 30, 34, 57, 59, 51, 3~\n$ diabetes &lt;fct&gt; pos, neg, pos, neg, pos, neg, pos, neg, pos, pos, neg, pos, n~\n\n\nWe zullen gebruik maken van de Pima Indian Women’s diabetes-dataset dat informatie bevat over de diabetes status van 768 Pima Indian vrouwen(diabetes). In de dataset zitten daarnaast enkele predictoren zoals het aantal zwangerschappen (pregnant), concentratie glucose (glucose), diastolische bloeddruk (pressure), triceps huidplooidikte (triceps), 2 uur serum insuline (insuline), BMI (mass), diabetes stamboom functie (pedigree) en hun leeftijd (age). Voor het geval je het je afvraagt, de Pima Indianen zijn een groep indianen die leven in een gebied dat bestaat uit wat nu centraal en zuidelijk Arizona is. De korte naam “Pima” zou afkomstig zijn van een zinsnede die “ik weet het niet” betekent, die ze herhaaldelijk gebruikten in hun eerste ontmoetingen met Spaanse kolonisten. Wikipedia bedankt!\n\n# Geef de dataset een kortere naam omdat we wat lui zijn\ndiabetes_orig &lt;- PimaIndiansDiabetes\n\n\ndiabetes_orig\n\n    pregnant glucose pressure triceps insulin mass pedigree age diabetes\n1          6     148       72      35       0 33.6    0.627  50      pos\n2          1      85       66      29       0 26.6    0.351  31      neg\n3          8     183       64       0       0 23.3    0.672  32      pos\n4          1      89       66      23      94 28.1    0.167  21      neg\n5          0     137       40      35     168 43.1    2.288  33      pos\n6          5     116       74       0       0 25.6    0.201  30      neg\n7          3      78       50      32      88 31.0    0.248  26      pos\n8         10     115        0       0       0 35.3    0.134  29      neg\n9          2     197       70      45     543 30.5    0.158  53      pos\n10         8     125       96       0       0  0.0    0.232  54      pos\n11         4     110       92       0       0 37.6    0.191  30      neg\n12        10     168       74       0       0 38.0    0.537  34      pos\n13        10     139       80       0       0 27.1    1.441  57      neg\n14         1     189       60      23     846 30.1    0.398  59      pos\n15         5     166       72      19     175 25.8    0.587  51      pos\n16         7     100        0       0       0 30.0    0.484  32      pos\n17         0     118       84      47     230 45.8    0.551  31      pos\n18         7     107       74       0       0 29.6    0.254  31      pos\n19         1     103       30      38      83 43.3    0.183  33      neg\n20         1     115       70      30      96 34.6    0.529  32      pos\n21         3     126       88      41     235 39.3    0.704  27      neg\n22         8      99       84       0       0 35.4    0.388  50      neg\n23         7     196       90       0       0 39.8    0.451  41      pos\n24         9     119       80      35       0 29.0    0.263  29      pos\n25        11     143       94      33     146 36.6    0.254  51      pos\n26        10     125       70      26     115 31.1    0.205  41      pos\n27         7     147       76       0       0 39.4    0.257  43      pos\n28         1      97       66      15     140 23.2    0.487  22      neg\n29        13     145       82      19     110 22.2    0.245  57      neg\n30         5     117       92       0       0 34.1    0.337  38      neg\n31         5     109       75      26       0 36.0    0.546  60      neg\n32         3     158       76      36     245 31.6    0.851  28      pos\n33         3      88       58      11      54 24.8    0.267  22      neg\n34         6      92       92       0       0 19.9    0.188  28      neg\n35        10     122       78      31       0 27.6    0.512  45      neg\n36         4     103       60      33     192 24.0    0.966  33      neg\n37        11     138       76       0       0 33.2    0.420  35      neg\n38         9     102       76      37       0 32.9    0.665  46      pos\n39         2      90       68      42       0 38.2    0.503  27      pos\n40         4     111       72      47     207 37.1    1.390  56      pos\n41         3     180       64      25      70 34.0    0.271  26      neg\n42         7     133       84       0       0 40.2    0.696  37      neg\n43         7     106       92      18       0 22.7    0.235  48      neg\n44         9     171      110      24     240 45.4    0.721  54      pos\n45         7     159       64       0       0 27.4    0.294  40      neg\n46         0     180       66      39       0 42.0    1.893  25      pos\n47         1     146       56       0       0 29.7    0.564  29      neg\n48         2      71       70      27       0 28.0    0.586  22      neg\n49         7     103       66      32       0 39.1    0.344  31      pos\n50         7     105        0       0       0  0.0    0.305  24      neg\n51         1     103       80      11      82 19.4    0.491  22      neg\n52         1     101       50      15      36 24.2    0.526  26      neg\n53         5      88       66      21      23 24.4    0.342  30      neg\n54         8     176       90      34     300 33.7    0.467  58      pos\n55         7     150       66      42     342 34.7    0.718  42      neg\n56         1      73       50      10       0 23.0    0.248  21      neg\n57         7     187       68      39     304 37.7    0.254  41      pos\n58         0     100       88      60     110 46.8    0.962  31      neg\n59         0     146       82       0       0 40.5    1.781  44      neg\n60         0     105       64      41     142 41.5    0.173  22      neg\n61         2      84        0       0       0  0.0    0.304  21      neg\n62         8     133       72       0       0 32.9    0.270  39      pos\n63         5      44       62       0       0 25.0    0.587  36      neg\n64         2     141       58      34     128 25.4    0.699  24      neg\n65         7     114       66       0       0 32.8    0.258  42      pos\n66         5      99       74      27       0 29.0    0.203  32      neg\n67         0     109       88      30       0 32.5    0.855  38      pos\n68         2     109       92       0       0 42.7    0.845  54      neg\n69         1      95       66      13      38 19.6    0.334  25      neg\n70         4     146       85      27     100 28.9    0.189  27      neg\n71         2     100       66      20      90 32.9    0.867  28      pos\n72         5     139       64      35     140 28.6    0.411  26      neg\n73        13     126       90       0       0 43.4    0.583  42      pos\n74         4     129       86      20     270 35.1    0.231  23      neg\n75         1      79       75      30       0 32.0    0.396  22      neg\n76         1       0       48      20       0 24.7    0.140  22      neg\n77         7      62       78       0       0 32.6    0.391  41      neg\n78         5      95       72      33       0 37.7    0.370  27      neg\n79         0     131        0       0       0 43.2    0.270  26      pos\n80         2     112       66      22       0 25.0    0.307  24      neg\n81         3     113       44      13       0 22.4    0.140  22      neg\n82         2      74        0       0       0  0.0    0.102  22      neg\n83         7      83       78      26      71 29.3    0.767  36      neg\n84         0     101       65      28       0 24.6    0.237  22      neg\n85         5     137      108       0       0 48.8    0.227  37      pos\n86         2     110       74      29     125 32.4    0.698  27      neg\n87        13     106       72      54       0 36.6    0.178  45      neg\n88         2     100       68      25      71 38.5    0.324  26      neg\n89        15     136       70      32     110 37.1    0.153  43      pos\n90         1     107       68      19       0 26.5    0.165  24      neg\n91         1      80       55       0       0 19.1    0.258  21      neg\n92         4     123       80      15     176 32.0    0.443  34      neg\n93         7      81       78      40      48 46.7    0.261  42      neg\n94         4     134       72       0       0 23.8    0.277  60      pos\n95         2     142       82      18      64 24.7    0.761  21      neg\n96         6     144       72      27     228 33.9    0.255  40      neg\n97         2      92       62      28       0 31.6    0.130  24      neg\n98         1      71       48      18      76 20.4    0.323  22      neg\n99         6      93       50      30      64 28.7    0.356  23      neg\n100        1     122       90      51     220 49.7    0.325  31      pos\n101        1     163       72       0       0 39.0    1.222  33      pos\n102        1     151       60       0       0 26.1    0.179  22      neg\n103        0     125       96       0       0 22.5    0.262  21      neg\n104        1      81       72      18      40 26.6    0.283  24      neg\n105        2      85       65       0       0 39.6    0.930  27      neg\n106        1     126       56      29     152 28.7    0.801  21      neg\n107        1      96      122       0       0 22.4    0.207  27      neg\n108        4     144       58      28     140 29.5    0.287  37      neg\n109        3      83       58      31      18 34.3    0.336  25      neg\n110        0      95       85      25      36 37.4    0.247  24      pos\n111        3     171       72      33     135 33.3    0.199  24      pos\n112        8     155       62      26     495 34.0    0.543  46      pos\n113        1      89       76      34      37 31.2    0.192  23      neg\n114        4      76       62       0       0 34.0    0.391  25      neg\n115        7     160       54      32     175 30.5    0.588  39      pos\n116        4     146       92       0       0 31.2    0.539  61      pos\n117        5     124       74       0       0 34.0    0.220  38      pos\n118        5      78       48       0       0 33.7    0.654  25      neg\n119        4      97       60      23       0 28.2    0.443  22      neg\n120        4      99       76      15      51 23.2    0.223  21      neg\n121        0     162       76      56     100 53.2    0.759  25      pos\n122        6     111       64      39       0 34.2    0.260  24      neg\n123        2     107       74      30     100 33.6    0.404  23      neg\n124        5     132       80       0       0 26.8    0.186  69      neg\n125        0     113       76       0       0 33.3    0.278  23      pos\n126        1      88       30      42      99 55.0    0.496  26      pos\n127        3     120       70      30     135 42.9    0.452  30      neg\n128        1     118       58      36      94 33.3    0.261  23      neg\n129        1     117       88      24     145 34.5    0.403  40      pos\n130        0     105       84       0       0 27.9    0.741  62      pos\n131        4     173       70      14     168 29.7    0.361  33      pos\n132        9     122       56       0       0 33.3    1.114  33      pos\n133        3     170       64      37     225 34.5    0.356  30      pos\n134        8      84       74      31       0 38.3    0.457  39      neg\n135        2      96       68      13      49 21.1    0.647  26      neg\n136        2     125       60      20     140 33.8    0.088  31      neg\n137        0     100       70      26      50 30.8    0.597  21      neg\n138        0      93       60      25      92 28.7    0.532  22      neg\n139        0     129       80       0       0 31.2    0.703  29      neg\n140        5     105       72      29     325 36.9    0.159  28      neg\n141        3     128       78       0       0 21.1    0.268  55      neg\n142        5     106       82      30       0 39.5    0.286  38      neg\n143        2     108       52      26      63 32.5    0.318  22      neg\n144       10     108       66       0       0 32.4    0.272  42      pos\n145        4     154       62      31     284 32.8    0.237  23      neg\n146        0     102       75      23       0  0.0    0.572  21      neg\n147        9      57       80      37       0 32.8    0.096  41      neg\n148        2     106       64      35     119 30.5    1.400  34      neg\n149        5     147       78       0       0 33.7    0.218  65      neg\n150        2      90       70      17       0 27.3    0.085  22      neg\n151        1     136       74      50     204 37.4    0.399  24      neg\n152        4     114       65       0       0 21.9    0.432  37      neg\n153        9     156       86      28     155 34.3    1.189  42      pos\n154        1     153       82      42     485 40.6    0.687  23      neg\n155        8     188       78       0       0 47.9    0.137  43      pos\n156        7     152       88      44       0 50.0    0.337  36      pos\n157        2      99       52      15      94 24.6    0.637  21      neg\n158        1     109       56      21     135 25.2    0.833  23      neg\n159        2      88       74      19      53 29.0    0.229  22      neg\n160       17     163       72      41     114 40.9    0.817  47      pos\n161        4     151       90      38       0 29.7    0.294  36      neg\n162        7     102       74      40     105 37.2    0.204  45      neg\n163        0     114       80      34     285 44.2    0.167  27      neg\n164        2     100       64      23       0 29.7    0.368  21      neg\n165        0     131       88       0       0 31.6    0.743  32      pos\n166        6     104       74      18     156 29.9    0.722  41      pos\n167        3     148       66      25       0 32.5    0.256  22      neg\n168        4     120       68       0       0 29.6    0.709  34      neg\n169        4     110       66       0       0 31.9    0.471  29      neg\n170        3     111       90      12      78 28.4    0.495  29      neg\n171        6     102       82       0       0 30.8    0.180  36      pos\n172        6     134       70      23     130 35.4    0.542  29      pos\n173        2      87        0      23       0 28.9    0.773  25      neg\n174        1      79       60      42      48 43.5    0.678  23      neg\n175        2      75       64      24      55 29.7    0.370  33      neg\n176        8     179       72      42     130 32.7    0.719  36      pos\n177        6      85       78       0       0 31.2    0.382  42      neg\n178        0     129      110      46     130 67.1    0.319  26      pos\n179        5     143       78       0       0 45.0    0.190  47      neg\n180        5     130       82       0       0 39.1    0.956  37      pos\n181        6      87       80       0       0 23.2    0.084  32      neg\n182        0     119       64      18      92 34.9    0.725  23      neg\n183        1       0       74      20      23 27.7    0.299  21      neg\n184        5      73       60       0       0 26.8    0.268  27      neg\n185        4     141       74       0       0 27.6    0.244  40      neg\n186        7     194       68      28       0 35.9    0.745  41      pos\n187        8     181       68      36     495 30.1    0.615  60      pos\n188        1     128       98      41      58 32.0    1.321  33      pos\n189        8     109       76      39     114 27.9    0.640  31      pos\n190        5     139       80      35     160 31.6    0.361  25      pos\n191        3     111       62       0       0 22.6    0.142  21      neg\n192        9     123       70      44      94 33.1    0.374  40      neg\n193        7     159       66       0       0 30.4    0.383  36      pos\n194       11     135        0       0       0 52.3    0.578  40      pos\n195        8      85       55      20       0 24.4    0.136  42      neg\n196        5     158       84      41     210 39.4    0.395  29      pos\n197        1     105       58       0       0 24.3    0.187  21      neg\n198        3     107       62      13      48 22.9    0.678  23      pos\n199        4     109       64      44      99 34.8    0.905  26      pos\n200        4     148       60      27     318 30.9    0.150  29      pos\n201        0     113       80      16       0 31.0    0.874  21      neg\n202        1     138       82       0       0 40.1    0.236  28      neg\n203        0     108       68      20       0 27.3    0.787  32      neg\n204        2      99       70      16      44 20.4    0.235  27      neg\n205        6     103       72      32     190 37.7    0.324  55      neg\n206        5     111       72      28       0 23.9    0.407  27      neg\n207        8     196       76      29     280 37.5    0.605  57      pos\n208        5     162      104       0       0 37.7    0.151  52      pos\n209        1      96       64      27      87 33.2    0.289  21      neg\n210        7     184       84      33       0 35.5    0.355  41      pos\n211        2      81       60      22       0 27.7    0.290  25      neg\n212        0     147       85      54       0 42.8    0.375  24      neg\n213        7     179       95      31       0 34.2    0.164  60      neg\n214        0     140       65      26     130 42.6    0.431  24      pos\n215        9     112       82      32     175 34.2    0.260  36      pos\n216       12     151       70      40     271 41.8    0.742  38      pos\n217        5     109       62      41     129 35.8    0.514  25      pos\n218        6     125       68      30     120 30.0    0.464  32      neg\n219        5      85       74      22       0 29.0    1.224  32      pos\n220        5     112       66       0       0 37.8    0.261  41      pos\n221        0     177       60      29     478 34.6    1.072  21      pos\n222        2     158       90       0       0 31.6    0.805  66      pos\n223        7     119        0       0       0 25.2    0.209  37      neg\n224        7     142       60      33     190 28.8    0.687  61      neg\n225        1     100       66      15      56 23.6    0.666  26      neg\n226        1      87       78      27      32 34.6    0.101  22      neg\n227        0     101       76       0       0 35.7    0.198  26      neg\n228        3     162       52      38       0 37.2    0.652  24      pos\n229        4     197       70      39     744 36.7    2.329  31      neg\n230        0     117       80      31      53 45.2    0.089  24      neg\n231        4     142       86       0       0 44.0    0.645  22      pos\n232        6     134       80      37     370 46.2    0.238  46      pos\n233        1      79       80      25      37 25.4    0.583  22      neg\n234        4     122       68       0       0 35.0    0.394  29      neg\n235        3      74       68      28      45 29.7    0.293  23      neg\n236        4     171       72       0       0 43.6    0.479  26      pos\n237        7     181       84      21     192 35.9    0.586  51      pos\n238        0     179       90      27       0 44.1    0.686  23      pos\n239        9     164       84      21       0 30.8    0.831  32      pos\n240        0     104       76       0       0 18.4    0.582  27      neg\n241        1      91       64      24       0 29.2    0.192  21      neg\n242        4      91       70      32      88 33.1    0.446  22      neg\n243        3     139       54       0       0 25.6    0.402  22      pos\n244        6     119       50      22     176 27.1    1.318  33      pos\n245        2     146       76      35     194 38.2    0.329  29      neg\n246        9     184       85      15       0 30.0    1.213  49      pos\n247       10     122       68       0       0 31.2    0.258  41      neg\n248        0     165       90      33     680 52.3    0.427  23      neg\n249        9     124       70      33     402 35.4    0.282  34      neg\n250        1     111       86      19       0 30.1    0.143  23      neg\n251        9     106       52       0       0 31.2    0.380  42      neg\n252        2     129       84       0       0 28.0    0.284  27      neg\n253        2      90       80      14      55 24.4    0.249  24      neg\n254        0      86       68      32       0 35.8    0.238  25      neg\n255       12      92       62       7     258 27.6    0.926  44      pos\n256        1     113       64      35       0 33.6    0.543  21      pos\n257        3     111       56      39       0 30.1    0.557  30      neg\n258        2     114       68      22       0 28.7    0.092  25      neg\n259        1     193       50      16     375 25.9    0.655  24      neg\n260       11     155       76      28     150 33.3    1.353  51      pos\n261        3     191       68      15     130 30.9    0.299  34      neg\n262        3     141        0       0       0 30.0    0.761  27      pos\n263        4      95       70      32       0 32.1    0.612  24      neg\n264        3     142       80      15       0 32.4    0.200  63      neg\n265        4     123       62       0       0 32.0    0.226  35      pos\n266        5      96       74      18      67 33.6    0.997  43      neg\n267        0     138        0       0       0 36.3    0.933  25      pos\n268        2     128       64      42       0 40.0    1.101  24      neg\n269        0     102       52       0       0 25.1    0.078  21      neg\n270        2     146        0       0       0 27.5    0.240  28      pos\n271       10     101       86      37       0 45.6    1.136  38      pos\n272        2     108       62      32      56 25.2    0.128  21      neg\n273        3     122       78       0       0 23.0    0.254  40      neg\n274        1      71       78      50      45 33.2    0.422  21      neg\n275       13     106       70       0       0 34.2    0.251  52      neg\n276        2     100       70      52      57 40.5    0.677  25      neg\n277        7     106       60      24       0 26.5    0.296  29      pos\n278        0     104       64      23     116 27.8    0.454  23      neg\n279        5     114       74       0       0 24.9    0.744  57      neg\n280        2     108       62      10     278 25.3    0.881  22      neg\n281        0     146       70       0       0 37.9    0.334  28      pos\n282       10     129       76      28     122 35.9    0.280  39      neg\n283        7     133       88      15     155 32.4    0.262  37      neg\n284        7     161       86       0       0 30.4    0.165  47      pos\n285        2     108       80       0       0 27.0    0.259  52      pos\n286        7     136       74      26     135 26.0    0.647  51      neg\n287        5     155       84      44     545 38.7    0.619  34      neg\n288        1     119       86      39     220 45.6    0.808  29      pos\n289        4      96       56      17      49 20.8    0.340  26      neg\n290        5     108       72      43      75 36.1    0.263  33      neg\n291        0      78       88      29      40 36.9    0.434  21      neg\n292        0     107       62      30      74 36.6    0.757  25      pos\n293        2     128       78      37     182 43.3    1.224  31      pos\n294        1     128       48      45     194 40.5    0.613  24      pos\n295        0     161       50       0       0 21.9    0.254  65      neg\n296        6     151       62      31     120 35.5    0.692  28      neg\n297        2     146       70      38     360 28.0    0.337  29      pos\n298        0     126       84      29     215 30.7    0.520  24      neg\n299       14     100       78      25     184 36.6    0.412  46      pos\n300        8     112       72       0       0 23.6    0.840  58      neg\n301        0     167        0       0       0 32.3    0.839  30      pos\n302        2     144       58      33     135 31.6    0.422  25      pos\n303        5      77       82      41      42 35.8    0.156  35      neg\n304        5     115       98       0       0 52.9    0.209  28      pos\n305        3     150       76       0       0 21.0    0.207  37      neg\n306        2     120       76      37     105 39.7    0.215  29      neg\n307       10     161       68      23     132 25.5    0.326  47      pos\n308        0     137       68      14     148 24.8    0.143  21      neg\n309        0     128       68      19     180 30.5    1.391  25      pos\n310        2     124       68      28     205 32.9    0.875  30      pos\n311        6      80       66      30       0 26.2    0.313  41      neg\n312        0     106       70      37     148 39.4    0.605  22      neg\n313        2     155       74      17      96 26.6    0.433  27      pos\n314        3     113       50      10      85 29.5    0.626  25      neg\n315        7     109       80      31       0 35.9    1.127  43      pos\n316        2     112       68      22      94 34.1    0.315  26      neg\n317        3      99       80      11      64 19.3    0.284  30      neg\n318        3     182       74       0       0 30.5    0.345  29      pos\n319        3     115       66      39     140 38.1    0.150  28      neg\n320        6     194       78       0       0 23.5    0.129  59      pos\n321        4     129       60      12     231 27.5    0.527  31      neg\n322        3     112       74      30       0 31.6    0.197  25      pos\n323        0     124       70      20       0 27.4    0.254  36      pos\n324       13     152       90      33      29 26.8    0.731  43      pos\n325        2     112       75      32       0 35.7    0.148  21      neg\n326        1     157       72      21     168 25.6    0.123  24      neg\n327        1     122       64      32     156 35.1    0.692  30      pos\n328       10     179       70       0       0 35.1    0.200  37      neg\n329        2     102       86      36     120 45.5    0.127  23      pos\n330        6     105       70      32      68 30.8    0.122  37      neg\n331        8     118       72      19       0 23.1    1.476  46      neg\n332        2      87       58      16      52 32.7    0.166  25      neg\n333        1     180        0       0       0 43.3    0.282  41      pos\n334       12     106       80       0       0 23.6    0.137  44      neg\n335        1      95       60      18      58 23.9    0.260  22      neg\n336        0     165       76      43     255 47.9    0.259  26      neg\n337        0     117        0       0       0 33.8    0.932  44      neg\n338        5     115       76       0       0 31.2    0.343  44      pos\n339        9     152       78      34     171 34.2    0.893  33      pos\n340        7     178       84       0       0 39.9    0.331  41      pos\n341        1     130       70      13     105 25.9    0.472  22      neg\n342        1      95       74      21      73 25.9    0.673  36      neg\n343        1       0       68      35       0 32.0    0.389  22      neg\n344        5     122       86       0       0 34.7    0.290  33      neg\n345        8      95       72       0       0 36.8    0.485  57      neg\n346        8     126       88      36     108 38.5    0.349  49      neg\n347        1     139       46      19      83 28.7    0.654  22      neg\n348        3     116        0       0       0 23.5    0.187  23      neg\n349        3      99       62      19      74 21.8    0.279  26      neg\n350        5       0       80      32       0 41.0    0.346  37      pos\n351        4      92       80       0       0 42.2    0.237  29      neg\n352        4     137       84       0       0 31.2    0.252  30      neg\n353        3      61       82      28       0 34.4    0.243  46      neg\n354        1      90       62      12      43 27.2    0.580  24      neg\n355        3      90       78       0       0 42.7    0.559  21      neg\n356        9     165       88       0       0 30.4    0.302  49      pos\n357        1     125       50      40     167 33.3    0.962  28      pos\n358       13     129        0      30       0 39.9    0.569  44      pos\n359       12      88       74      40      54 35.3    0.378  48      neg\n360        1     196       76      36     249 36.5    0.875  29      pos\n361        5     189       64      33     325 31.2    0.583  29      pos\n362        5     158       70       0       0 29.8    0.207  63      neg\n363        5     103      108      37       0 39.2    0.305  65      neg\n364        4     146       78       0       0 38.5    0.520  67      pos\n365        4     147       74      25     293 34.9    0.385  30      neg\n366        5      99       54      28      83 34.0    0.499  30      neg\n367        6     124       72       0       0 27.6    0.368  29      pos\n368        0     101       64      17       0 21.0    0.252  21      neg\n369        3      81       86      16      66 27.5    0.306  22      neg\n370        1     133      102      28     140 32.8    0.234  45      pos\n371        3     173       82      48     465 38.4    2.137  25      pos\n372        0     118       64      23      89  0.0    1.731  21      neg\n373        0      84       64      22      66 35.8    0.545  21      neg\n374        2     105       58      40      94 34.9    0.225  25      neg\n375        2     122       52      43     158 36.2    0.816  28      neg\n376       12     140       82      43     325 39.2    0.528  58      pos\n377        0      98       82      15      84 25.2    0.299  22      neg\n378        1      87       60      37      75 37.2    0.509  22      neg\n379        4     156       75       0       0 48.3    0.238  32      pos\n380        0      93      100      39      72 43.4    1.021  35      neg\n381        1     107       72      30      82 30.8    0.821  24      neg\n382        0     105       68      22       0 20.0    0.236  22      neg\n383        1     109       60       8     182 25.4    0.947  21      neg\n384        1      90       62      18      59 25.1    1.268  25      neg\n385        1     125       70      24     110 24.3    0.221  25      neg\n386        1     119       54      13      50 22.3    0.205  24      neg\n387        5     116       74      29       0 32.3    0.660  35      pos\n388        8     105      100      36       0 43.3    0.239  45      pos\n389        5     144       82      26     285 32.0    0.452  58      pos\n390        3     100       68      23      81 31.6    0.949  28      neg\n391        1     100       66      29     196 32.0    0.444  42      neg\n392        5     166       76       0       0 45.7    0.340  27      pos\n393        1     131       64      14     415 23.7    0.389  21      neg\n394        4     116       72      12      87 22.1    0.463  37      neg\n395        4     158       78       0       0 32.9    0.803  31      pos\n396        2     127       58      24     275 27.7    1.600  25      neg\n397        3      96       56      34     115 24.7    0.944  39      neg\n398        0     131       66      40       0 34.3    0.196  22      pos\n399        3      82       70       0       0 21.1    0.389  25      neg\n400        3     193       70      31       0 34.9    0.241  25      pos\n401        4      95       64       0       0 32.0    0.161  31      pos\n402        6     137       61       0       0 24.2    0.151  55      neg\n403        5     136       84      41      88 35.0    0.286  35      pos\n404        9      72       78      25       0 31.6    0.280  38      neg\n405        5     168       64       0       0 32.9    0.135  41      pos\n406        2     123       48      32     165 42.1    0.520  26      neg\n407        4     115       72       0       0 28.9    0.376  46      pos\n408        0     101       62       0       0 21.9    0.336  25      neg\n409        8     197       74       0       0 25.9    1.191  39      pos\n410        1     172       68      49     579 42.4    0.702  28      pos\n411        6     102       90      39       0 35.7    0.674  28      neg\n412        1     112       72      30     176 34.4    0.528  25      neg\n413        1     143       84      23     310 42.4    1.076  22      neg\n414        1     143       74      22      61 26.2    0.256  21      neg\n415        0     138       60      35     167 34.6    0.534  21      pos\n416        3     173       84      33     474 35.7    0.258  22      pos\n417        1      97       68      21       0 27.2    1.095  22      neg\n418        4     144       82      32       0 38.5    0.554  37      pos\n419        1      83       68       0       0 18.2    0.624  27      neg\n420        3     129       64      29     115 26.4    0.219  28      pos\n421        1     119       88      41     170 45.3    0.507  26      neg\n422        2      94       68      18      76 26.0    0.561  21      neg\n423        0     102       64      46      78 40.6    0.496  21      neg\n424        2     115       64      22       0 30.8    0.421  21      neg\n425        8     151       78      32     210 42.9    0.516  36      pos\n426        4     184       78      39     277 37.0    0.264  31      pos\n427        0      94        0       0       0  0.0    0.256  25      neg\n428        1     181       64      30     180 34.1    0.328  38      pos\n429        0     135       94      46     145 40.6    0.284  26      neg\n430        1      95       82      25     180 35.0    0.233  43      pos\n431        2      99        0       0       0 22.2    0.108  23      neg\n432        3      89       74      16      85 30.4    0.551  38      neg\n433        1      80       74      11      60 30.0    0.527  22      neg\n434        2     139       75       0       0 25.6    0.167  29      neg\n435        1      90       68       8       0 24.5    1.138  36      neg\n436        0     141        0       0       0 42.4    0.205  29      pos\n437       12     140       85      33       0 37.4    0.244  41      neg\n438        5     147       75       0       0 29.9    0.434  28      neg\n439        1      97       70      15       0 18.2    0.147  21      neg\n440        6     107       88       0       0 36.8    0.727  31      neg\n441        0     189      104      25       0 34.3    0.435  41      pos\n442        2      83       66      23      50 32.2    0.497  22      neg\n443        4     117       64      27     120 33.2    0.230  24      neg\n444        8     108       70       0       0 30.5    0.955  33      pos\n445        4     117       62      12       0 29.7    0.380  30      pos\n446        0     180       78      63      14 59.4    2.420  25      pos\n447        1     100       72      12      70 25.3    0.658  28      neg\n448        0      95       80      45      92 36.5    0.330  26      neg\n449        0     104       64      37      64 33.6    0.510  22      pos\n450        0     120       74      18      63 30.5    0.285  26      neg\n451        1      82       64      13      95 21.2    0.415  23      neg\n452        2     134       70       0       0 28.9    0.542  23      pos\n453        0      91       68      32     210 39.9    0.381  25      neg\n454        2     119        0       0       0 19.6    0.832  72      neg\n455        2     100       54      28     105 37.8    0.498  24      neg\n456       14     175       62      30       0 33.6    0.212  38      pos\n457        1     135       54       0       0 26.7    0.687  62      neg\n458        5      86       68      28      71 30.2    0.364  24      neg\n459       10     148       84      48     237 37.6    1.001  51      pos\n460        9     134       74      33      60 25.9    0.460  81      neg\n461        9     120       72      22      56 20.8    0.733  48      neg\n462        1      71       62       0       0 21.8    0.416  26      neg\n463        8      74       70      40      49 35.3    0.705  39      neg\n464        5      88       78      30       0 27.6    0.258  37      neg\n465       10     115       98       0       0 24.0    1.022  34      neg\n466        0     124       56      13     105 21.8    0.452  21      neg\n467        0      74       52      10      36 27.8    0.269  22      neg\n468        0      97       64      36     100 36.8    0.600  25      neg\n469        8     120        0       0       0 30.0    0.183  38      pos\n470        6     154       78      41     140 46.1    0.571  27      neg\n471        1     144       82      40       0 41.3    0.607  28      neg\n472        0     137       70      38       0 33.2    0.170  22      neg\n473        0     119       66      27       0 38.8    0.259  22      neg\n474        7     136       90       0       0 29.9    0.210  50      neg\n475        4     114       64       0       0 28.9    0.126  24      neg\n476        0     137       84      27       0 27.3    0.231  59      neg\n477        2     105       80      45     191 33.7    0.711  29      pos\n478        7     114       76      17     110 23.8    0.466  31      neg\n479        8     126       74      38      75 25.9    0.162  39      neg\n480        4     132       86      31       0 28.0    0.419  63      neg\n481        3     158       70      30     328 35.5    0.344  35      pos\n482        0     123       88      37       0 35.2    0.197  29      neg\n483        4      85       58      22      49 27.8    0.306  28      neg\n484        0      84       82      31     125 38.2    0.233  23      neg\n485        0     145        0       0       0 44.2    0.630  31      pos\n486        0     135       68      42     250 42.3    0.365  24      pos\n487        1     139       62      41     480 40.7    0.536  21      neg\n488        0     173       78      32     265 46.5    1.159  58      neg\n489        4      99       72      17       0 25.6    0.294  28      neg\n490        8     194       80       0       0 26.1    0.551  67      neg\n491        2      83       65      28      66 36.8    0.629  24      neg\n492        2      89       90      30       0 33.5    0.292  42      neg\n493        4      99       68      38       0 32.8    0.145  33      neg\n494        4     125       70      18     122 28.9    1.144  45      pos\n495        3      80        0       0       0  0.0    0.174  22      neg\n496        6     166       74       0       0 26.6    0.304  66      neg\n497        5     110       68       0       0 26.0    0.292  30      neg\n498        2      81       72      15      76 30.1    0.547  25      neg\n499        7     195       70      33     145 25.1    0.163  55      pos\n500        6     154       74      32     193 29.3    0.839  39      neg\n501        2     117       90      19      71 25.2    0.313  21      neg\n502        3      84       72      32       0 37.2    0.267  28      neg\n503        6       0       68      41       0 39.0    0.727  41      pos\n504        7      94       64      25      79 33.3    0.738  41      neg\n505        3      96       78      39       0 37.3    0.238  40      neg\n506       10      75       82       0       0 33.3    0.263  38      neg\n507        0     180       90      26      90 36.5    0.314  35      pos\n508        1     130       60      23     170 28.6    0.692  21      neg\n509        2      84       50      23      76 30.4    0.968  21      neg\n510        8     120       78       0       0 25.0    0.409  64      neg\n511       12      84       72      31       0 29.7    0.297  46      pos\n512        0     139       62      17     210 22.1    0.207  21      neg\n513        9      91       68       0       0 24.2    0.200  58      neg\n514        2      91       62       0       0 27.3    0.525  22      neg\n515        3      99       54      19      86 25.6    0.154  24      neg\n516        3     163       70      18     105 31.6    0.268  28      pos\n517        9     145       88      34     165 30.3    0.771  53      pos\n518        7     125       86       0       0 37.6    0.304  51      neg\n519       13      76       60       0       0 32.8    0.180  41      neg\n520        6     129       90       7     326 19.6    0.582  60      neg\n521        2      68       70      32      66 25.0    0.187  25      neg\n522        3     124       80      33     130 33.2    0.305  26      neg\n523        6     114        0       0       0  0.0    0.189  26      neg\n524        9     130       70       0       0 34.2    0.652  45      pos\n525        3     125       58       0       0 31.6    0.151  24      neg\n526        3      87       60      18       0 21.8    0.444  21      neg\n527        1      97       64      19      82 18.2    0.299  21      neg\n528        3     116       74      15     105 26.3    0.107  24      neg\n529        0     117       66      31     188 30.8    0.493  22      neg\n530        0     111       65       0       0 24.6    0.660  31      neg\n531        2     122       60      18     106 29.8    0.717  22      neg\n532        0     107       76       0       0 45.3    0.686  24      neg\n533        1      86       66      52      65 41.3    0.917  29      neg\n534        6      91        0       0       0 29.8    0.501  31      neg\n535        1      77       56      30      56 33.3    1.251  24      neg\n536        4     132        0       0       0 32.9    0.302  23      pos\n537        0     105       90       0       0 29.6    0.197  46      neg\n538        0      57       60       0       0 21.7    0.735  67      neg\n539        0     127       80      37     210 36.3    0.804  23      neg\n540        3     129       92      49     155 36.4    0.968  32      pos\n541        8     100       74      40     215 39.4    0.661  43      pos\n542        3     128       72      25     190 32.4    0.549  27      pos\n543       10      90       85      32       0 34.9    0.825  56      pos\n544        4      84       90      23      56 39.5    0.159  25      neg\n545        1      88       78      29      76 32.0    0.365  29      neg\n546        8     186       90      35     225 34.5    0.423  37      pos\n547        5     187       76      27     207 43.6    1.034  53      pos\n548        4     131       68      21     166 33.1    0.160  28      neg\n549        1     164       82      43      67 32.8    0.341  50      neg\n550        4     189      110      31       0 28.5    0.680  37      neg\n551        1     116       70      28       0 27.4    0.204  21      neg\n552        3      84       68      30     106 31.9    0.591  25      neg\n553        6     114       88       0       0 27.8    0.247  66      neg\n554        1      88       62      24      44 29.9    0.422  23      neg\n555        1      84       64      23     115 36.9    0.471  28      neg\n556        7     124       70      33     215 25.5    0.161  37      neg\n557        1      97       70      40       0 38.1    0.218  30      neg\n558        8     110       76       0       0 27.8    0.237  58      neg\n559       11     103       68      40       0 46.2    0.126  42      neg\n560       11      85       74       0       0 30.1    0.300  35      neg\n561        6     125       76       0       0 33.8    0.121  54      pos\n562        0     198       66      32     274 41.3    0.502  28      pos\n563        1      87       68      34      77 37.6    0.401  24      neg\n564        6      99       60      19      54 26.9    0.497  32      neg\n565        0      91       80       0       0 32.4    0.601  27      neg\n566        2      95       54      14      88 26.1    0.748  22      neg\n567        1      99       72      30      18 38.6    0.412  21      neg\n568        6      92       62      32     126 32.0    0.085  46      neg\n569        4     154       72      29     126 31.3    0.338  37      neg\n570        0     121       66      30     165 34.3    0.203  33      pos\n571        3      78       70       0       0 32.5    0.270  39      neg\n572        2     130       96       0       0 22.6    0.268  21      neg\n573        3     111       58      31      44 29.5    0.430  22      neg\n574        2      98       60      17     120 34.7    0.198  22      neg\n575        1     143       86      30     330 30.1    0.892  23      neg\n576        1     119       44      47      63 35.5    0.280  25      neg\n577        6     108       44      20     130 24.0    0.813  35      neg\n578        2     118       80       0       0 42.9    0.693  21      pos\n579       10     133       68       0       0 27.0    0.245  36      neg\n580        2     197       70      99       0 34.7    0.575  62      pos\n581        0     151       90      46       0 42.1    0.371  21      pos\n582        6     109       60      27       0 25.0    0.206  27      neg\n583       12     121       78      17       0 26.5    0.259  62      neg\n584        8     100       76       0       0 38.7    0.190  42      neg\n585        8     124       76      24     600 28.7    0.687  52      pos\n586        1      93       56      11       0 22.5    0.417  22      neg\n587        8     143       66       0       0 34.9    0.129  41      pos\n588        6     103       66       0       0 24.3    0.249  29      neg\n589        3     176       86      27     156 33.3    1.154  52      pos\n590        0      73        0       0       0 21.1    0.342  25      neg\n591       11     111       84      40       0 46.8    0.925  45      pos\n592        2     112       78      50     140 39.4    0.175  24      neg\n593        3     132       80       0       0 34.4    0.402  44      pos\n594        2      82       52      22     115 28.5    1.699  25      neg\n595        6     123       72      45     230 33.6    0.733  34      neg\n596        0     188       82      14     185 32.0    0.682  22      pos\n597        0      67       76       0       0 45.3    0.194  46      neg\n598        1      89       24      19      25 27.8    0.559  21      neg\n599        1     173       74       0       0 36.8    0.088  38      pos\n600        1     109       38      18     120 23.1    0.407  26      neg\n601        1     108       88      19       0 27.1    0.400  24      neg\n602        6      96        0       0       0 23.7    0.190  28      neg\n603        1     124       74      36       0 27.8    0.100  30      neg\n604        7     150       78      29     126 35.2    0.692  54      pos\n605        4     183        0       0       0 28.4    0.212  36      pos\n606        1     124       60      32       0 35.8    0.514  21      neg\n607        1     181       78      42     293 40.0    1.258  22      pos\n608        1      92       62      25      41 19.5    0.482  25      neg\n609        0     152       82      39     272 41.5    0.270  27      neg\n610        1     111       62      13     182 24.0    0.138  23      neg\n611        3     106       54      21     158 30.9    0.292  24      neg\n612        3     174       58      22     194 32.9    0.593  36      pos\n613        7     168       88      42     321 38.2    0.787  40      pos\n614        6     105       80      28       0 32.5    0.878  26      neg\n615       11     138       74      26     144 36.1    0.557  50      pos\n616        3     106       72       0       0 25.8    0.207  27      neg\n617        6     117       96       0       0 28.7    0.157  30      neg\n618        2      68       62      13      15 20.1    0.257  23      neg\n619        9     112       82      24       0 28.2    1.282  50      pos\n620        0     119        0       0       0 32.4    0.141  24      pos\n621        2     112       86      42     160 38.4    0.246  28      neg\n622        2      92       76      20       0 24.2    1.698  28      neg\n623        6     183       94       0       0 40.8    1.461  45      neg\n624        0      94       70      27     115 43.5    0.347  21      neg\n625        2     108       64       0       0 30.8    0.158  21      neg\n626        4      90       88      47      54 37.7    0.362  29      neg\n627        0     125       68       0       0 24.7    0.206  21      neg\n628        0     132       78       0       0 32.4    0.393  21      neg\n629        5     128       80       0       0 34.6    0.144  45      neg\n630        4      94       65      22       0 24.7    0.148  21      neg\n631        7     114       64       0       0 27.4    0.732  34      pos\n632        0     102       78      40      90 34.5    0.238  24      neg\n633        2     111       60       0       0 26.2    0.343  23      neg\n634        1     128       82      17     183 27.5    0.115  22      neg\n635       10      92       62       0       0 25.9    0.167  31      neg\n636       13     104       72       0       0 31.2    0.465  38      pos\n637        5     104       74       0       0 28.8    0.153  48      neg\n638        2      94       76      18      66 31.6    0.649  23      neg\n639        7      97       76      32      91 40.9    0.871  32      pos\n640        1     100       74      12      46 19.5    0.149  28      neg\n641        0     102       86      17     105 29.3    0.695  27      neg\n642        4     128       70       0       0 34.3    0.303  24      neg\n643        6     147       80       0       0 29.5    0.178  50      pos\n644        4      90        0       0       0 28.0    0.610  31      neg\n645        3     103       72      30     152 27.6    0.730  27      neg\n646        2     157       74      35     440 39.4    0.134  30      neg\n647        1     167       74      17     144 23.4    0.447  33      pos\n648        0     179       50      36     159 37.8    0.455  22      pos\n649       11     136       84      35     130 28.3    0.260  42      pos\n650        0     107       60      25       0 26.4    0.133  23      neg\n651        1      91       54      25     100 25.2    0.234  23      neg\n652        1     117       60      23     106 33.8    0.466  27      neg\n653        5     123       74      40      77 34.1    0.269  28      neg\n654        2     120       54       0       0 26.8    0.455  27      neg\n655        1     106       70      28     135 34.2    0.142  22      neg\n656        2     155       52      27     540 38.7    0.240  25      pos\n657        2     101       58      35      90 21.8    0.155  22      neg\n658        1     120       80      48     200 38.9    1.162  41      neg\n659       11     127      106       0       0 39.0    0.190  51      neg\n660        3      80       82      31      70 34.2    1.292  27      pos\n661       10     162       84       0       0 27.7    0.182  54      neg\n662        1     199       76      43       0 42.9    1.394  22      pos\n663        8     167      106      46     231 37.6    0.165  43      pos\n664        9     145       80      46     130 37.9    0.637  40      pos\n665        6     115       60      39       0 33.7    0.245  40      pos\n666        1     112       80      45     132 34.8    0.217  24      neg\n667        4     145       82      18       0 32.5    0.235  70      pos\n668       10     111       70      27       0 27.5    0.141  40      pos\n669        6      98       58      33     190 34.0    0.430  43      neg\n670        9     154       78      30     100 30.9    0.164  45      neg\n671        6     165       68      26     168 33.6    0.631  49      neg\n672        1      99       58      10       0 25.4    0.551  21      neg\n673       10      68      106      23      49 35.5    0.285  47      neg\n674        3     123      100      35     240 57.3    0.880  22      neg\n675        8      91       82       0       0 35.6    0.587  68      neg\n676        6     195       70       0       0 30.9    0.328  31      pos\n677        9     156       86       0       0 24.8    0.230  53      pos\n678        0      93       60       0       0 35.3    0.263  25      neg\n679        3     121       52       0       0 36.0    0.127  25      pos\n680        2     101       58      17     265 24.2    0.614  23      neg\n681        2      56       56      28      45 24.2    0.332  22      neg\n682        0     162       76      36       0 49.6    0.364  26      pos\n683        0      95       64      39     105 44.6    0.366  22      neg\n684        4     125       80       0       0 32.3    0.536  27      pos\n685        5     136       82       0       0  0.0    0.640  69      neg\n686        2     129       74      26     205 33.2    0.591  25      neg\n687        3     130       64       0       0 23.1    0.314  22      neg\n688        1     107       50      19       0 28.3    0.181  29      neg\n689        1     140       74      26     180 24.1    0.828  23      neg\n690        1     144       82      46     180 46.1    0.335  46      pos\n691        8     107       80       0       0 24.6    0.856  34      neg\n692       13     158      114       0       0 42.3    0.257  44      pos\n693        2     121       70      32      95 39.1    0.886  23      neg\n694        7     129       68      49     125 38.5    0.439  43      pos\n695        2      90       60       0       0 23.5    0.191  25      neg\n696        7     142       90      24     480 30.4    0.128  43      pos\n697        3     169       74      19     125 29.9    0.268  31      pos\n698        0      99        0       0       0 25.0    0.253  22      neg\n699        4     127       88      11     155 34.5    0.598  28      neg\n700        4     118       70       0       0 44.5    0.904  26      neg\n701        2     122       76      27     200 35.9    0.483  26      neg\n702        6     125       78      31       0 27.6    0.565  49      pos\n703        1     168       88      29       0 35.0    0.905  52      pos\n704        2     129        0       0       0 38.5    0.304  41      neg\n705        4     110       76      20     100 28.4    0.118  27      neg\n706        6      80       80      36       0 39.8    0.177  28      neg\n707       10     115        0       0       0  0.0    0.261  30      pos\n708        2     127       46      21     335 34.4    0.176  22      neg\n709        9     164       78       0       0 32.8    0.148  45      pos\n710        2      93       64      32     160 38.0    0.674  23      pos\n711        3     158       64      13     387 31.2    0.295  24      neg\n712        5     126       78      27      22 29.6    0.439  40      neg\n713       10     129       62      36       0 41.2    0.441  38      pos\n714        0     134       58      20     291 26.4    0.352  21      neg\n715        3     102       74       0       0 29.5    0.121  32      neg\n716        7     187       50      33     392 33.9    0.826  34      pos\n717        3     173       78      39     185 33.8    0.970  31      pos\n718       10      94       72      18       0 23.1    0.595  56      neg\n719        1     108       60      46     178 35.5    0.415  24      neg\n720        5      97       76      27       0 35.6    0.378  52      pos\n721        4      83       86      19       0 29.3    0.317  34      neg\n722        1     114       66      36     200 38.1    0.289  21      neg\n723        1     149       68      29     127 29.3    0.349  42      pos\n724        5     117       86      30     105 39.1    0.251  42      neg\n725        1     111       94       0       0 32.8    0.265  45      neg\n726        4     112       78      40       0 39.4    0.236  38      neg\n727        1     116       78      29     180 36.1    0.496  25      neg\n728        0     141       84      26       0 32.4    0.433  22      neg\n729        2     175       88       0       0 22.9    0.326  22      neg\n730        2      92       52       0       0 30.1    0.141  22      neg\n731        3     130       78      23      79 28.4    0.323  34      pos\n732        8     120       86       0       0 28.4    0.259  22      pos\n733        2     174       88      37     120 44.5    0.646  24      pos\n734        2     106       56      27     165 29.0    0.426  22      neg\n735        2     105       75       0       0 23.3    0.560  53      neg\n736        4      95       60      32       0 35.4    0.284  28      neg\n737        0     126       86      27     120 27.4    0.515  21      neg\n738        8      65       72      23       0 32.0    0.600  42      neg\n739        2      99       60      17     160 36.6    0.453  21      neg\n740        1     102       74       0       0 39.5    0.293  42      pos\n741       11     120       80      37     150 42.3    0.785  48      pos\n742        3     102       44      20      94 30.8    0.400  26      neg\n743        1     109       58      18     116 28.5    0.219  22      neg\n744        9     140       94       0       0 32.7    0.734  45      pos\n745       13     153       88      37     140 40.6    1.174  39      neg\n746       12     100       84      33     105 30.0    0.488  46      neg\n747        1     147       94      41       0 49.3    0.358  27      pos\n748        1      81       74      41      57 46.3    1.096  32      neg\n749        3     187       70      22     200 36.4    0.408  36      pos\n750        6     162       62       0       0 24.3    0.178  50      pos\n751        4     136       70       0       0 31.2    1.182  22      pos\n752        1     121       78      39      74 39.0    0.261  28      neg\n753        3     108       62      24       0 26.0    0.223  25      neg\n754        0     181       88      44     510 43.3    0.222  26      pos\n755        8     154       78      32       0 32.4    0.443  45      pos\n756        1     128       88      39     110 36.5    1.057  37      pos\n757        7     137       90      41       0 32.0    0.391  39      neg\n758        0     123       72       0       0 36.3    0.258  52      pos\n759        1     106       76       0       0 37.5    0.197  26      neg\n760        6     190       92       0       0 35.5    0.278  66      pos\n761        2      88       58      26      16 28.4    0.766  22      neg\n762        9     170       74      31       0 44.0    0.403  43      pos\n763        9      89       62       0       0 22.5    0.142  33      neg\n764       10     101       76      48     180 32.9    0.171  63      neg\n765        2     122       70      27       0 36.8    0.340  27      neg\n766        5     121       72      23     112 26.2    0.245  30      neg\n767        1     126       60       0       0 30.1    0.349  47      pos\n768        1      93       70      31       0 30.4    0.315  23      neg\n\n\nEen snelle verkenning van de dataset toont aan dat er meer nullen in de gegevens zitten dan verwacht (vooral omdat een BMI of tricep huiddikte van 0 onmogelijk is), wat betekent dat ontbrekende waarden als nullen worden geregistreerd. Zie bijvoorbeeld het histogram van de tricep huidplooidikte, waar de nullen voor dikte opvallen.\n\nggplot(diabetes_orig) +\n  geom_histogram(aes(x = triceps))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nDit fenomeen is ook te zien in de glucose-, druk-, insuline- en massavariabelen. We zetten eerst de 0-scores in alle variabelen (behalve “zwanger”) over naar NA (missende waarde). Daarvoor gebruiken we de mutate_at()functie (die binnenkort wordt vervangen door mutate() met across()) om aan te geven op welke variabelen we onze muterende functie willen toepassen. We gebruiken de if_else()functie om aan te geven waar we de waarde mee moeten vervangen als de voorwaarde waar of onwaar is.\n\ndiabetes_clean &lt;- diabetes_orig %&gt;%\n  mutate_at(vars(triceps, glucose, pressure, insulin, mass), \n            function(.var) { \n              if_else(condition = (.var == 0), # als waar (bv als het 0 is)\n                      true = as.numeric(NA),  # zet er de waarde NA voor in de plaats\n                      false = .var # anders laat het zoals het is\n                      )\n            })\n\nOnze gegevens zijn klaar. Laten we beginnen met het maken van een aantal tidymodels!"
  },
  {
    "objectID": "posts/2021-04-22-tidymodels-opnieuw/tidymodels-opnieuw.html#haal-traintest-sets-uit-elkaar",
    "href": "posts/2021-04-22-tidymodels-opnieuw/tidymodels-opnieuw.html#haal-traintest-sets-uit-elkaar",
    "title": "Tidymodels opnieuw",
    "section": "",
    "text": "Laten we onze data verdelen in trainings- en testdata. De trainingsdata worden gebruikt om ons model te vinden en de parameters in te stellen (tune). De testdata gebruiken we alleen om de werking van het finale model vast te stellen. Dat splitten kunnen we doen door de inital_split() functie (van het rsample pakket). Dat creëert een speciaal “split” object.\n\nset.seed(234589)\n# deel de data op in trainng (75%) en testing (25%)\ndiabetes_split &lt;- initial_split(diabetes_clean, \n                                prop = 3/4)\ndiabetes_split\n\n&lt;Analysis/Assess/Total&gt;\n&lt;576/192/768&gt;\n\n\ndiabetes_split, ons gesplitste object, vertelt ons hoeveel waarnemingen we hebben in de trainingsset, de testset en de gehele dataset: &lt;train/test/totaal&gt; (576/192/768).\nDe trainings- en testsets kunnen uit het “split”-object worden gehaald met behulp van de training() en testing() functies. Hoewel we deze objecten niet echt zullen gebruiken in de pipeline (daarvoor zullen we het diabetes_split-object zelf gebruiken).\n\n# haal training en testing sets uit elkaar\ndiabetes_train &lt;- training(diabetes_split)\ndiabetes_test &lt;- testing(diabetes_split)\n\nOp een gegeven moment zullen we de parameters hiervan wat willen tuenen (afstemmen). Dat doen we met cross-validatie. Zo ontstaat er met vfold_cv() een cross-validatie versie van de trainingsset waar we zo op terugkomen.\n\n# creeer CV object van training data\ndiabetes_cv &lt;- vfold_cv(diabetes_train)"
  },
  {
    "objectID": "posts/2021-04-22-tidymodels-opnieuw/tidymodels-opnieuw.html#defineeer-een-recipe",
    "href": "posts/2021-04-22-tidymodels-opnieuw/tidymodels-opnieuw.html#defineeer-een-recipe",
    "title": "Tidymodels opnieuw",
    "section": "",
    "text": "Met het pakket recipes kun je de variabelen een rol geven, als uitkomst of voorspellende variabele (gebruik een “formule”) b.v.. Maar met recipe kun je ook andere voorbereidingsstappen zetten die je nodig acht (zoals standaardiseren, imputeren, PCA, etc). Een recipe voer je uit in delen (gelaagd op elkaar door pipes %&gt;% te gebruiken):\n\nSpecificeer de formule (recipe()): specificeer eerst wat is de uitkomstvariabele en wat zijn de predictoren;\nSpecificeer pre-processing steps (step_zzz()): defineer voorbereidingsstappen, zoals imputatie, creëren van dummy variabelen, schalen en wat al niet meer\n\nZo kunnen we bijvoorbeeld de volgende recipe maken.\n\n# defineer de `recipe`\ndiabetes_recipe &lt;- \n  # dat bestaat uit de volgende formule (uitkomst ~ predictoren)\n  recipe(diabetes ~ pregnant + glucose + pressure + triceps + \n           insulin + mass + pedigree + age, \n         data = diabetes_clean) %&gt;%\n  # en voeren we enkele voorbereidingsstappen uit (normaliseren en imputeren)\n  step_normalize(all_numeric()) %&gt;%\n  step_knnimpute(all_predictors())\n\nWarning: `step_knnimpute()` was deprecated in recipes 0.1.16.\nPlease use `step_impute_knn()` instead.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\n\nAls je ooit eerder formules hebt gezien (bijvoorbeeld met behulp van de lm() functie in R), dan weet je misschien dat we onze formule veel efficiënter hadden kunnen schrijven met behulp van een shortcut, waarbij de . alle variabelen in de gegevens vertegenwoordigt: outcome ~ .\nDe volledige lijst van beschikbare voorbewerkingsstappen is hier te vinden. In de bovenstaande chunck hebben we de functies all_numeric() en all_predictors() gebruikt als argumenten van voorbereiding. Deze worden “rolselecties” genoemd en geven aan dat we de stap willen toepassen op “alle numerieke” variabelen of “alle predictoren”. De lijst van alle potentiële rolselectoren kan worden gevonden door ?selectis in je console te typen.\nMerk op dat we het originele diabetes_clean data-object hebben gebruikt (we stellen recipe(..., data = diabetes_clean)), in plaats van het diabetes_train-object of het diabetes_split-object. Het blijkt dat we deze allemaal hadden kunnen gebruiken. Alle recipes die op dit punt uit het dataobject worden gehaald zijn de namen en rollen van de uitkomst en de voorspellende variabelen. We zullen deze recipe later toepassen op specifieke datasets. Dit betekent dat voor grote datasets een kleinere dataset gebruikt wordt om tijd en geheugen te besparen.\nInderdaad, als we een samenvatting van het diabetes_recipe object printen, dan laat het ons gewoon zien hoeveel voorspellingsvariabelen we hebben gespecificeerd en welke stappen we hebben gespecificeerd (maar het implementeert ze eigenlijk nog niet!).\n\ndiabetes_recipe\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          8\n\nOperations:\n\nCentering and scaling for all_numeric()\nK-nearest neighbor imputation for all_predictors()\n\n\nAls je de voorbewerkte dataset zelf wilt extraheren, kunt je eerst prep() het recept voor een specifieke dataset en juice() het voorbewerkte recept om de voorbewerkte gegevens te extraheren. Het blijkt dat het extraheren van de voorbewerkte data eigenlijk niet nodig is voor de pipeline, omdat dit onder de motorkap gebeurt als het model geschikt is. Soms is het toch nuttig.\n\ndiabetes_train_preprocessed &lt;- diabetes_recipe %&gt;%\n  # apply the recipe to the training data\n  prep(diabetes_train) %&gt;%\n  # extract the pre-processed training dataset\n  juice()\ndiabetes_train_preprocessed\n\n# A tibble: 576 x 9\n   pregnant glucose pressure triceps insulin     mass pedigree     age diabetes\n      &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;   \n 1   1.23   -0.390    0.262   0.892   -0.317 -0.656      0.502 -0.201  pos     \n 2   0.0447  1.09    -0.0616 -0.0348  -0.219 -0.168     -0.400  0.301  neg     \n 3   1.82    1.91    -0.224   0.595    0.146  0.379     -0.813  0.301  neg     \n 4  -1.14   -0.0948  -0.710  -0.591   -0.522 -0.311      3.76  -1.04   neg     \n 5  -0.548   0.233    0.424   0.706    0.239  1.56       2.25  -0.201  pos     \n 6   0.637  -0.0620  -1.84   -0.683    0.190 -0.771      2.53  -0.0335 pos     \n 7  -0.844  -1.64    -2.01   -1.05    -0.629 -1.73      -0.445 -0.953  neg     \n 8  -0.548  -0.423   -1.68   -0.313   -0.735  0.00505   -0.460 -0.953  neg     \n 9  -1.14    0.463   -0.386   1.17     0.796  1.41      -0.320 -0.786  pos     \n10   1.53    1.41     0.424   0.150    0.877  0.0482    -0.968  0.969  pos     \n# ... with 566 more rows"
  },
  {
    "objectID": "posts/2021-04-22-tidymodels-opnieuw/tidymodels-opnieuw.html#specificeer-het-model",
    "href": "posts/2021-04-22-tidymodels-opnieuw/tidymodels-opnieuw.html#specificeer-het-model",
    "title": "Tidymodels opnieuw",
    "section": "",
    "text": "Tot nu toe hebben we onze data verdeeld in training en test-sets en onze pre-proces stappen gespecificeerd door een recipe te gebruiken. Nu willen we ons model definiëren en daarvoor gebruiken we het parsnip pakket dat in tidymodels zit.\nParsnip biedt een uniforme interface voor de enorme verscheidenheid aan modellen die er in R bestaan. Dit betekent dat je slechts één manier hoeft te leren om een model te specificeren en dan kun je dit gebruiken voor allerlei verschillende modellen, vaak met enkele coderegel.\nEr zijn een paar primaire componenten in de modelspecificatie opgeslagen:\n\nHet model type: wat voor soort model wil je gebruiken, zoals rand_forest() voor het random forest-model, logistic_reg() voor het logistisch regressie-model, svm_poly() voor een polynomiaal SVM-model, enz. De volledige lijst van modellen die beschikbaar zijn via parsnip kan [hier] (link naar website) vinden.\nDe arguments: de model parameter waarden (de benaming is consistent over verschillende modellen), door het gebruik van set_args().\nDe engine: het onderliggende pakket waar het model van wegkomt (bv. “ranger” voor implementatie van Random Forest), door het gebuik van set_engine().\nDe mode: het type voorspelling - omdat verschillende pakketten zowel classificatie (binaire/categoriale voorspelling) en regressie (continue voorspelling) kunnen uitvoeren, door het gebruik van set_mode().\n\nAls we bijvoorbeeld een random forest model willen gebruiken, zoals dat in het ranger pakket zit, met als doel classificatie en we willen de try parameter tunen (het afstemmen van het aantal willekeurig gekozen variabelen dat bij elke splitsing in aanmerking moet worden genomen), dan moeten we de volgende modelspecificatie definiëren:\n\nrf_model &lt;- \n  # specificeren dat het model random forest is\n  rand_forest() %&gt;%\n  # specificeren dat we de `mtry` parameter moeten afstemmen\n  set_args(mtry = tune()) %&gt;%\n  # selecteren van de motor van het pakket dat onder het model zit\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  # kiezen dat je voor continue analyse (regressie) of categoriale analyse (classificatie) gaat\n  set_mode(\"classification\") \n\nAls je later het variabele belang van jouw uiteindelijke model wilt kunnen onderzoeken, moet je het engine argument opnieuw instellen. De volgende code specificeert bijvoorbeeld een logistisch regressiemodel uit het glm pakket.\n\nlr_model &lt;- \n  # specificeer een logistisch regressiemodel\n  logistic_reg() %&gt;%\n  # selecteer het pakket dat bij dit model hoort\n  set_engine(\"glm\") %&gt;%\n  # kies voor een continue regressie of binaire classificatie wijze\n  set_mode(\"classification\") \n\nDeze code draait niet het model. Net als de recipe, is het veel meer een beschrijving van het model. Echter, wanneer je een parameter op tune() zet wordt het later gestemd in de stemfase van de pipeline (bv. om de waarde vast te stellen van de parameter die de beste performance geeft). Je kunt ook zelf een bepaalde waarde aan de parameter geven wanneer je het niet wilt afstemmen, bv door set_args(mtry = 4) te gebruiken. Een ander ding om op te merken is dat niets wat deze modelspecificatie betreft specifiek is voor de diabetes-dataset."
  },
  {
    "objectID": "posts/2021-04-22-tidymodels-opnieuw/tidymodels-opnieuw.html#alles-in-een-workflow-samenbrengen",
    "href": "posts/2021-04-22-tidymodels-opnieuw/tidymodels-opnieuw.html#alles-in-een-workflow-samenbrengen",
    "title": "Tidymodels opnieuw",
    "section": "",
    "text": "We zijn klaar om het model en de recipes in een workflow te plaatsen. Een workflow zet je op door het gebruik van workflow() (van het workflows pakket) en dan kun je een recipe en een model toevoegen.\n\n# zet de workflow op\nrf_workflow &lt;- workflow() %&gt;%\n  # voeg de `recipe` toe\n  add_recipe(diabetes_recipe) %&gt;%\n  # voeg het `model` toe\n  add_model(rf_model)\n\nMerk op dat we de voorbewerkingsstappen nog niet in de recipe hebben geïmplementeerd noch dat we het model hebben gepast. We hebben alleen maar het raamwerk geschreven. Pas als we de parameters hebben afgestemd of in het model hebben gepast, worden het recept en het model daadwerkelijk geïmplementeerd."
  },
  {
    "objectID": "posts/2021-04-22-tidymodels-opnieuw/tidymodels-opnieuw.html#afstemmen-van-de-parameters",
    "href": "posts/2021-04-22-tidymodels-opnieuw/tidymodels-opnieuw.html#afstemmen-van-de-parameters",
    "title": "Tidymodels opnieuw",
    "section": "",
    "text": "Omdat er een parameter is ontwikkeld om af te stemmen (mtry), moeten we dat daar voor gebruiken (bv. de waarde kiezen die de beste performance laat zien) voordat we het model passen. Als je geen parameters hebt om af te stemmen, kun je dit deel overslaan.\nDat afstemmen doen we door een cross-validation object (diabetes_cv) te kiezen. Om dat te doen specificeren we de range van mtry waarden die we willen gebruiken en dan voegen we een stemmingslaag toe aan onze workflow door tune_grid() te gebruiken (van het tune pakket). We richten ons op twee maten: accuracy en roc_auc (van het yardstick pakket). Die vertellen ons welke maten we het beste kunnen gebruiken.\n\n# specificeer de waarden die je wilt gebruiken\nrf_grid &lt;- expand.grid(mtry = c(3, 4, 5))\n# extraheer resultaten\nrf_tune_results &lt;- rf_workflow %&gt;%\n  tune_grid(resamples = diabetes_cv, #CV object\n            grid = rf_grid, # grid van waarden om te proberen\n            metrics = metric_set(accuracy, roc_auc) # maten waar we naar moeten kijken\n            )\n\nWarning: package 'ranger' was built under R version 4.1.3\n\n\nJe kunt verschillende parameters afstemmen door verschillende parameters aan de expand.grid() functie toe te voegen, bv. expand.grid(mtry = c(3, 4, 5), trees = c(100, 500)).\nHet is altijd goed om de resultaten van de cross-validatie goed te onderzoeken. collect_metrics() is echt een handige functie die in verschillende omstandigheden kan worden gebruikt om te vergelijken die zijn berekend in het object dat is gebruikt. In dit geval komen de maten van de cross-validatie performance over de verschillende waarden van de performance.\n\n# print results\nrf_tune_results %&gt;%\n  collect_metrics()\n\n# A tibble: 6 x 7\n   mtry .metric  .estimator  mean     n std_err .config             \n  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1     3 accuracy binary     0.766    10 0.00832 Preprocessor1_Model1\n2     3 roc_auc  binary     0.842    10 0.0116  Preprocessor1_Model1\n3     4 accuracy binary     0.774    10 0.0127  Preprocessor1_Model2\n4     4 roc_auc  binary     0.842    10 0.0123  Preprocessor1_Model2\n5     5 accuracy binary     0.771    10 0.0121  Preprocessor1_Model3\n6     5 roc_auc  binary     0.841    10 0.0134  Preprocessor1_Model3\n\n\nTen opzichte van accuracy en AUC laat mtry = 4 de beste performance zien (hoogste gemiddelde waarden)."
  },
  {
    "objectID": "posts/2021-04-22-tidymodels-opnieuw/tidymodels-opnieuw.html#afronden-van-de-workflow",
    "href": "posts/2021-04-22-tidymodels-opnieuw/tidymodels-opnieuw.html#afronden-van-de-workflow",
    "title": "Tidymodels opnieuw",
    "section": "",
    "text": "We willen een laag aan onze workflow toevoegen die overeenkomt met de afgestemde parameter, d.w.z. dat we mtry instellen als de waarde die de beste resultaten opleverde. Als je geen parameters hebt afgestemd, kun je deze stap overslaan.\nWe kunnen de beste waarde voor de nauwkeurigheidsmetriek extraheren door de select_best()functie toe te passen op het afstemmingsobject.\n\nparam_final &lt;- rf_tune_results %&gt;%\n  select_best(metric = \"accuracy\")\nparam_final\n\n# A tibble: 1 x 2\n   mtry .config             \n  &lt;dbl&gt; &lt;chr&gt;               \n1     4 Preprocessor1_Model2\n\n\nDan kunnen we deze parameter aan de workflow toevoegen door de finalize_workflow() functie te gebruiken.\n\nrf_workflow &lt;- rf_workflow %&gt;%\n  finalize_workflow(param_final)"
  },
  {
    "objectID": "posts/2021-04-22-tidymodels-opnieuw/tidymodels-opnieuw.html#evalueren-van-het-model-op-de-test-set",
    "href": "posts/2021-04-22-tidymodels-opnieuw/tidymodels-opnieuw.html#evalueren-van-het-model-op-de-test-set",
    "title": "Tidymodels opnieuw",
    "section": "",
    "text": "Nu we ons recipe en ons model hebben gedefinieerd en de parameters van het model hebben getuned, zijn we klaar om daadwerkelijk het uiteindelijke model te draaien. Aangezien al deze informatie in het workflow-object zit, zullen we de last_fit() functie toepassen op onze workflow en ons train/test-splitsingsobject. Dit zal automatisch het door de workflow gespecificeerde model trainen met behulp van de trainingsgegevens en evaluaties produceren op basis van de testset.\n\nrf_fit &lt;- rf_workflow %&gt;%\n  # draaien op de trainingsset en evalueren op de test set\n  last_fit(diabetes_split)\n\nMerk op dat het object dat wordt gecreëerd een data-frame-achtig object is; het is een tibble met listkolommen.\n\nrf_fit\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 x 6\n  splits            id               .metrics .notes   .predictions .workflow \n  &lt;list&gt;            &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [576/192]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\n\nDit is echt een aardige eigenschap van tidymodels (en ook waarom je zo goed kunt werken met tidyverse) omdat je al je nette handelingen op het modelobject kunt uitvoeren.\nAangezien we het trainings/testobject al hebben geleverd op het moment dat we in de workflow werken, worden de maten geëvalueerd op de testset. Wanneer we nu de collect_metrics() functie gebruiken (herinner ons dat we deze hebben gebruikt bij het afstemmen van onze parameters), haalt deze de prestaties van het uiteindelijke model (aangezien rf_fit nu bestaat uit een enkel definitief model) toegepast op de test set.\n\ntest_performance &lt;- rf_fit %&gt;% collect_metrics()\ntest_performance\n\n# A tibble: 2 x 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.781 Preprocessor1_Model1\n2 roc_auc  binary         0.847 Preprocessor1_Model1\n\n\nOverall is de performance heel goed, met een accuracy van 0.74 en een AUC van 0.82. Maar deze waarden zijn vaak lager dan in de trainingsset.\nJe kunt de test set voorspellingen zelf gebruiken met de collect_predictions() functie. Let op dat er 192 rijen in het voorspellingsobject zitten dat overeenkomt met de test set observaties (juist om jou te laten zien dat deze gebaseerd zijn op de testset meer dan op de trainingsset).\n\n# genereer voorspellingen vanuit de test set\ntest_predictions &lt;- rf_fit %&gt;% collect_predictions()\ntest_predictions\n\n# A tibble: 192 x 7\n   id               .pred_neg .pred_pos  .row .pred_class diabetes .config      \n   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;    &lt;chr&gt;        \n 1 train/test split     0.347    0.653      3 pos         pos      Preprocessor~\n 2 train/test split     0.210    0.790      9 pos         pos      Preprocessor~\n 3 train/test split     0.783    0.217     11 neg         neg      Preprocessor~\n 4 train/test split     0.508    0.492     13 neg         neg      Preprocessor~\n 5 train/test split     0.624    0.376     18 neg         pos      Preprocessor~\n 6 train/test split     0.364    0.636     26 pos         pos      Preprocessor~\n 7 train/test split     0.218    0.782     32 pos         pos      Preprocessor~\n 8 train/test split     0.673    0.327     50 neg         neg      Preprocessor~\n 9 train/test split     0.976    0.0240    53 neg         neg      Preprocessor~\n10 train/test split     0.166    0.834     55 pos         neg      Preprocessor~\n# ... with 182 more rows\n\n\nOmndat dit een normaal data frame/tibble object is, kunnen we de samenvattingen genereren en een confusie matrix plotten.\n\n# genereer een confusie matrix\ntest_predictions %&gt;% \n  conf_mat(truth = diabetes, estimate = .pred_class)\n\n          Truth\nPrediction neg pos\n       neg 107  19\n       pos  23  43\n\n\nWe kunnen ook de voorspelde kansverdelingen voor elke klasse in kaart brengen.\n\ntest_predictions %&gt;%\n  ggplot() +\n  geom_density(aes(x = .pred_pos, fill = diabetes), \n               alpha = 0.5)\n\n\n\n\nDe voorspellingen kun je ook als volgt laten zien:\n\ntest_predictions &lt;- rf_fit %&gt;% pull(.predictions)\ntest_predictions\n\n[[1]]\n# A tibble: 192 x 6\n   .pred_neg .pred_pos  .row .pred_class diabetes .config             \n       &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;    &lt;chr&gt;               \n 1     0.347    0.653      3 pos         pos      Preprocessor1_Model1\n 2     0.210    0.790      9 pos         pos      Preprocessor1_Model1\n 3     0.783    0.217     11 neg         neg      Preprocessor1_Model1\n 4     0.508    0.492     13 neg         neg      Preprocessor1_Model1\n 5     0.624    0.376     18 neg         pos      Preprocessor1_Model1\n 6     0.364    0.636     26 pos         pos      Preprocessor1_Model1\n 7     0.218    0.782     32 pos         pos      Preprocessor1_Model1\n 8     0.673    0.327     50 neg         neg      Preprocessor1_Model1\n 9     0.976    0.0240    53 neg         neg      Preprocessor1_Model1\n10     0.166    0.834     55 pos         neg      Preprocessor1_Model1\n# ... with 182 more rows"
  },
  {
    "objectID": "posts/2021-04-22-tidymodels-opnieuw/tidymodels-opnieuw.html#het-laatste-model",
    "href": "posts/2021-04-22-tidymodels-opnieuw/tidymodels-opnieuw.html#het-laatste-model",
    "title": "Tidymodels opnieuw",
    "section": "",
    "text": "In de vorige paragraaf is het model dat is getraind op de trainingsgegevens geëvalueerd aan de hand van de testgegevens. Maar als je eenmaal jouw definitieve model hebt bepaald, wil je het vaak trainen op je volledige dataset en het dan gebruiken om de respons voor nieuwe gegevens te voorspellen.\nAls je jouw model wilt gebruiken om de respons voor nieuwe waarnemingen te voorspellen, moet je de fit()functie op jouw workflow gebruiken en de dataset waarop je het uiteindelijke model wilt laten passen (bijvoorbeeld de volledige training + testdataset).\n\nfinal_model &lt;- fit(rf_workflow, diabetes_clean)\n\nHet final_model object bevat een aantal zaken, waaronder het ranger-object dat getraind is met de parameters die via de workflow in rf_workflow zijn vastgelegd op basis van de gegevens in diabetes_clean (de gecombineerde trainings- en testgegevens).\n\nfinal_model\n\n== Workflow [trained] ==========================================================\nPreprocessor: Recipe\nModel: rand_forest()\n\n-- Preprocessor ----------------------------------------------------------------\n2 Recipe Steps\n\n* step_normalize()\n* step_impute_knn()\n\n-- Model -----------------------------------------------------------------------\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~4,      x), importance = ~\"impurity\", num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  500 \nSample size:                      768 \nNumber of independent variables:  8 \nMtry:                             4 \nTarget node size:                 10 \nVariable importance mode:         impurity \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.1595594 \n\n\nAls we de diabetes status van een nieuwe vrouw willen voorspellen, kunnen we de predict() functie gebruiken.\nBijvoorbeeld, definieren we de data voor een nieuwe vrouw.\n\nnew_woman &lt;- tribble(~pregnant, ~glucose, ~pressure, ~triceps, ~insulin, ~mass, ~pedigree, ~age,\n                     2, 95, 70, 31, 102, 28.2, 0.67, 47)\nnew_woman\n\n# A tibble: 1 x 8\n  pregnant glucose pressure triceps insulin  mass pedigree   age\n     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1        2      95       70      31     102  28.2     0.67    47\n\n\nDe voorspelde diabetes status van deze nieuwe vrouw is “negatief”.\n\npredict(final_model, new_data = new_woman)\n\n# A tibble: 1 x 1\n  .pred_class\n  &lt;fct&gt;      \n1 neg"
  },
  {
    "objectID": "posts/2021-04-22-tidymodels-opnieuw/tidymodels-opnieuw.html#variabele-belang",
    "href": "posts/2021-04-22-tidymodels-opnieuw/tidymodels-opnieuw.html#variabele-belang",
    "title": "Tidymodels opnieuw",
    "section": "",
    "text": "Als je de belangrijkheid van een variabele uit je model wilt vaststellen, voor zover je dat kan zien, moet je het modelobject uit het fit() object halen (dat voor ons final_model heet). De functie die het model extraheert is pull_workflow_fit() en dan moet je het fit-object pakken dat de output bevat.\n\nranger_obj &lt;- pull_workflow_fit(final_model)$fit\n\nWarning: `pull_workflow_fit()` was deprecated in workflows 0.2.3.\nPlease use `extract_fit_parsnip()` instead.\n\nranger_obj\n\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~4,      x), importance = ~\"impurity\", num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  500 \nSample size:                      768 \nNumber of independent variables:  8 \nMtry:                             4 \nTarget node size:                 10 \nVariable importance mode:         impurity \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.1595594 \n\n\nVervolgens kun je het belang van de variabele uit het ranger-object zelf halen (variable.importance is een specifiek object in de ranger-output - dit zal moeten worden aangepast voor het specifieke objecttype van andere modellen).\n\nranger_obj$variable.importance\n\npregnant  glucose pressure  triceps  insulin     mass pedigree      age \n15.64236 79.84557 17.51228 21.64400 53.15198 43.77916 29.70545 32.32099"
  },
  {
    "objectID": "posts/2022-02-11-multilevelrstan/multilevelrstan.html",
    "href": "posts/2022-02-11-multilevelrstan/multilevelrstan.html",
    "title": "Multilevel modeling met STAN",
    "section": "",
    "text": "Een paar jaar geleden schreven Joonho Lee en collega’s van de Berkley Universiteit onder leiding van Sophia Rabe-Hesketh (van wie ik al eerder veel geleerd heb over multilevel analyse) een mooie tutorial over het gebruik van rstanarm in multilevelanalyse. Omdat deze tutorial in vele opzichten leerzaam is, heb ik deze vertaald. Zo maakte ik mij het werken met rstanarm eigen maar ook hoe je zo’n tutorial in rmarkdown zet. Alle credits gaan natuurlijk naar Joonhoo Lee e.a.. De oorspronkelijke tutorial van 24 april 2018 vind je hier. Later vond ik ook op github de syntax en kon ik diverse eigen fouten weer bijstellen hier."
  },
  {
    "objectID": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#footnotes",
    "href": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#footnotes",
    "title": "Multilevel modeling met STAN",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe werken meer over prioriteitsverdelingen uit in hoofdstuk 3. 2↩︎\nGebruikers die niet vertrouwd zijn met de syntaxis van ggplot2 verwijzen we naar hier↩︎\nVoor meer informatie over reguliere expressies, zie hier↩︎\nDe Dirichlet-verdeling is een multivariate veralgemening van de bètaverdeling met één concentratieparameter, die kan worden geïnterpreteerd als voorafgaande tellingen van een multinomiale willekeurige variabele (de simplexvector in onze context), zie voor details hier.↩︎"
  },
  {
    "objectID": "posts/2021-07-21-kaart-van-zwitserland/kaart-van-zwitserland.html#kaarten-maken",
    "href": "posts/2021-07-21-kaart-van-zwitserland/kaart-van-zwitserland.html#kaarten-maken",
    "title": "Kaart van Zwitserland",
    "section": "",
    "text": "Op Medium verscheen eind 2020 dit duidelijke blog van Giulia Ruggeri Hier de link. Ik wilde weer eens met ggplot2 en sf werken en Giulia’s blog vond ik interessant en heb ik vervolgens bewerkt.\nIn de afgelopen jaren is het maken van mooie kaarten in R vrij eenvoudig geworden, dankzij het {sf} pakket. In dit artikel gaan we de ruimtelijke verspreiding van de COVID-19 incidentie van de laatste 14 dagen in Zwitserland visualiseren door een thematische kaart te maken, een choropleth kaart zoals dat heet. We maken daarbij gebruik van {sf} en {ggplot2} als onze belangrijkste hulpmiddelen.\n{sf}, wat staat voor simple feature (eenvoudige eigenschap), is de ‘go-to’ bibliotheek om om te gaan met ruimtelijke vectoriële gegevens, dat zijn gegevens die geografische geometrieën beschrijven als een reeks van punten, die worden beschreven door hun lengteg- en breedtegraad coördinaten. Hiermee kunnen geografische vormen worden geïmporteerd, gemanipuleerd en geplot en kunnen we gegevens verwerken in een tabel-achtig formaat, net als een data.frame. Wat een opluchting!\nIn deze kleine oefening gebruiken we {readxl} om het Excel bestand te importeren en binnen te halen van de website van het Zwitsers Federaal Bureau van Publieke Gezondheid.\n{rcartocolor} is de R bibliotheek die mooi uitziende kleurschalen bevat, die zijn ontwikkeld voor cartografie. Zoals David Letterman zou zeggen, {tidyverse}‘needs no introduction’. Dit zijn de programma’s die we hier binnenhalen.\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(rcartocolor)\nlibrary(readxl)\n\nLaten we beginnen met het laden van de gegevens, met read_excel(), waarin we de exacte naam van het blad dat we willen laden kunnen opgeven, hoeveel regels we mogen overslaan en hoeveel regels we in totaal willen behouden.\nWe hebben een rij per kanton en een rij voor de titel, wat betekent dat we slechts 27 rijen hoeven te behouden (er zijn 26 Zwitserse kantons).\nWe schonen ook de kolomnamen een beetje op met clean_names uit het {janitor} pakket en gebruiken transmute om de gewenste kolommen te hernoemen en de andere te laten vallen.\n\nTot nu toe, eenvoudige dataimport en manipulatie.\n\n\ncovid_incidence &lt;- read_excel(\"resources/200325_dati di base_grafica_COVID-19-rapporto.xlsx\", \n    sheet = \"COVID19 casi per cantone\", skip = 6, \n    n_max = 27) %&gt;% \n  janitor::clean_names() %&gt;% \n  transmute(canton = cantone, incidence = incidenza_100_000_6)\n\nNew names:\n* `Casi confermati` -&gt; `Casi confermati...2`\n* `Incidenza/100 000` -&gt; `Incidenza/100 000...3`\n* `` -&gt; `...4`\n* `Casi confermati` -&gt; `Casi confermati...5`\n* `Incidenza/100 000` -&gt; `Incidenza/100 000...6`\n\nhead(covid_incidence)\n\n# A tibble: 6 x 2\n  canton incidence\n  &lt;chr&gt;      &lt;dbl&gt;\n1 AG          45.1\n2 AI          99.1\n3 AR          92.3\n4 BE          73  \n5 BL          33  \n6 BS          47.7\n\n\nWij hebben nu een variabele die de kantoncodes bevat en een variabele die de incidentie per 100.000, per kanton, van COVID-19 in de laatste 14 dagen bevat.\nWe zijn nu klaar om de shapefiles te laden.\n\nWacht even, wat zijn de shapefiles?\n\nShapefiles, zijn de bestanden die de geografische vormen bevatten die we willen plotten. We willen de data van de kantons plotten, dus hebben we de Zwitserse kantons nodig, die kunnen worden gedownload van [hier] (https://www.bfs.admin.ch/bfs/en/home/services/geostat/swiss-federal-statistics-geodata/administrative-boundaries/generalized-boundaries-local-regional-authorities.html).\nShapefiles zijn eigenlijk een set van bestanden, die verschillende geografische informatie bevatten (b.v. info over de projecties). Een van deze bestanden heeft de extensie .shp en dit is het bestand dat we gaan laden.\n\nLet op dat je alle andere bestanden in dezelfde map hebt staan.\n\nNu kunnen we dus 2 shapefiles laden, één die de vormen van de kantongrenzen bevat en één die de vorm van de grote meren van Zwitserland bevat.\nLaten we ze eens laden en kijken hoe ze eruit zien.\n\nswiss_lakes &lt;- st_read(\"resources/g2s15.shp\")\n\nReading layer `g2s15' from data source \n  `C:\\FilesHarrie\\HHQuarto\\posts\\2021-07-21-kaart-van-zwitserland\\resources\\g2s15.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 22 features and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 500253.8 ymin: 63872.4 xmax: 774495.3 ymax: 297632.2\nProjected CRS: CH1903 / LV03\n\nswiss_cantons &lt;- st_read(\"resources/G1K09.shp\")\n\nReading layer `G1K09' from data source \n  `C:\\FilesHarrie\\HHQuarto\\posts\\2021-07-21-kaart-van-zwitserland\\resources\\G1K09.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 26 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 485414 ymin: 75286 xmax: 833837 ymax: 295935\nProjected CRS: CH1903 / LV03\n\n\n\nclass(swiss_cantons)\n\n[1] \"sf\"         \"data.frame\"\n\n\nswiss_cantons en swiss_lakes, worden opgeslagen als sf data.frames (als dataframe van sf dus), zodat we ze kunnen manipuleren, net zoals we tibbles (of data.frames) kunnen manipuleren. Dit is mogelijk omdat geometrieën worden opgeslagen op een zeer nette manier: als een geneste variabele meestal genaamd geometry. Dit zal je enige speciale variabele zijn, de andere (die attributen worden genoemd) zullen gewoon normale variabelen zijn. Bijvoorbeeld, elk kanton heeft zijn naam en code gekoppeld aan de geometrie die het beschrijft.\n\nhead(swiss_cantons)\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 546871 ymin: 130593 xmax: 768722 ymax: 295935\nProjected CRS: CH1903 / LV03\n  KT         NAME KURZ                       geometry\n1 17   St. Gallen   SG MULTIPOLYGON (((738559 1968...\n2 12  Basel-Stadt   BS MULTIPOLYGON (((608728 2681...\n3  7    Nidwalden   NW MULTIPOLYGON (((671030 1822...\n4  2         Bern   BE MULTIPOLYGON (((572954 1936...\n5 14 Schaffhausen   SH MULTIPOLYGON (((684561 2726...\n6 10     Fribourg   FR MULTIPOLYGON (((584435 1976...\n\n\n\nHet voordeel van {sf} te gebruiken als ons hoofdgereedschap om met deze datatypes om te gaan? We kunnen {ggplot2} gebruiken om ze te plotten!\n\n\nggplot()+\n  geom_sf(data = swiss_cantons)\n\n\n\n\nEn, net als met elke {ggplot2} grafiek, kunnen we het bouwen van de kaart laag voor laag opbouwen. Laten we nu de Zwitserse meren toevoegen bovenop de kantonvormen en theme_void() gebruiken om de achtergrond en de as te verwijderen. In deze stap kunnen we ook de kantons transparant maken door het fill argument op NA te zetten en een lichte groenblauwe kleur toe te voegen om de meren te vullen. geom_sf() werkt inderdaad net als elke andere geom_ functie, geen alarmen en geen verrassingen hier.\n\nggplot()+\n  geom_sf(data = swiss_cantons, fill = NA) +\n  geom_sf(data = swiss_lakes,  fill = \"#d1eeea\", color = \"#d1eeea\") +\n  theme_void()\n\n\n\n\nHoe kunnen we nu elk kanton kleuren naar de grootte van de COVID-19 incidentie per 100’000 mensen?\nWe hoeven alleen maar de covid_incidence tabel en de swiss_cantons tabel samen te voegen, met de kantoncode als verbindingsvariabele. Hiermee kunnen we de variabele incidentie in kaart brengen naar de fill esthetiek en ereen choropleth kaart van maken, d.w.z. een thematische kaart.\n\nswiss_cantons &lt;- swiss_cantons %&gt;% \n  left_join(covid_incidence, c(\"KURZ\" = \"canton\"))\n\nOm onze kaart er mooi te laten uitzien, verdelen wij, in plaats van een numerieke variabele te gebruiken, de incidentie in categorieën. Zo is het voor de gebruiker gemakkelijker te zien in welke categorie elk kanton valt.\nDit is een typische praktijk voor choropleth kaarten en het kan op verschillende manieren worden gedaan. In dit geval kiezen we voor een brute kracht aanpak, we doen het handmatig.\n\nswiss_cantons &lt;- swiss_cantons %&gt;% \n  mutate(incidence_cat = case_when(\n    incidence &lt;= 50 ~ \"0-50\",\n    incidence &lt;= 100 ~ \"51-100\",\n    incidence &lt;= 150 ~ \"101-150\",\n    incidence &lt;= 300 ~ \"251-300\"\n  )) %&gt;% \n  mutate(incidence_cat = factor(incidence_cat, levels = c(\"0-50\", \"51-100\",\"101-150\",\"151-200\",\"251-300\")))\n\nNu kunnen we de kleur toewijzen aan de incidence_cat variabele en de eerste choropleth kaart maken.\n\nggplot(swiss_cantons) +\n  geom_sf(aes(fill = incidence_cat), size = 0.3) +\n  scale_fill_carto_d(palette = \"BrwnYl\") +\n  geom_sf(data = swiss_lakes, fill = \"#d1eeea\", color = \"#d1eeea\")+\n  theme_void() +\n  theme(legend.title = element_blank(),\n        legend.position = \"bottom\") \n\n\n\n\nEn we hebben onze eerste choropleth kaart, gebouwd met alleen {sf} en {ggplot2}. Laten we nog wat puntjes op de i zetten: we zijn niet tevreden met hoe de legende eruit ziet en we kunnen die veranderen met guide_legend().\n\nggplot(swiss_cantons) +\n  geom_sf(aes(fill = incidence_cat), size = 0.3) +\n  scale_fill_carto_d(palette = \"BrwnYl\",\n                     guide = guide_legend(direction = \"horizontal\",\n            keyheight = unit(2, units = \"mm\"),\n            keywidth = unit(70 / 5, units = \"mm\"),\n            title.position = 'top',\n            title.hjust = 0.5,\n            label.hjust = 0.5,\n            nrow = 1,\n            byrow = T,\n            label.position = \"bottom\")) +\n  geom_sf(data = swiss_lakes, fill = \"#d1eeea\", color = \"#d1eeea\")+\n  theme_void() +\n  theme(legend.title = element_blank(),\n        legend.position = \"bottom\") \n\n\n\n\nNu kunnen we een titel, een ondertitel en labels toevoegen aan de bovenkant van elke kanton. We zullen {ggrepel} gebruiken om ervoor te zorgen dat de labels elkaar niet overlappen, we zullen ook {ggtext} gebruiken zodat we markdown syntax kunnen gebruiken voor onze titel en ondertitel.\n\nggplot(swiss_cantons) +\n  geom_sf(aes(fill = incidence_cat), size = 0.3) +\n  scale_fill_carto_d(palette = \"BrwnYl\",\n                     guide = guide_legend(direction = \"horizontal\",\n            keyheight = unit(2, units = \"mm\"),\n            keywidth = unit(70 / 5, units = \"mm\"),\n            title.position = 'top',\n            title.hjust = 0.5,\n            label.hjust = 0.5,\n            nrow = 1,\n            byrow = T,\n            label.position = \"bottom\")) +\n  geom_sf(data = swiss_lakes, fill = \"#d1eeea\", color = \"#d1eeea\")+\n  ggrepel::geom_label_repel(\n    data = swiss_cantons,\n    aes(label = paste0(KURZ,\":\",round(incidence, digits = 0)), \n        geometry = geometry),\n    stat = \"sf_coordinates\",\n    min.segment.length = 0.2,\n    colour = \"#541f3f\",\n    size = 3,\n    segment.alpha = 0.5\n  ) +\n  labs(title = \"&lt;b style='color:#541f3f'&gt; COVID-19 gevallen per kanton, laatste 14 dagen &lt;/b&gt;\",\n       subtitle = \"&lt;span style='font-size:10pt'&gt;Incidentie per 100'000 inwoners,per canton &lt;/span&gt;\",\n       caption = \"Bron: OFSP | updated 12.10.2020\") +\n  theme_void() +\n  theme(legend.title = element_blank(),\n        legend.position = \"bottom\",\n        plot.title = ggtext::element_markdown(),\n        plot.subtitle = ggtext::element_markdown()) \n\n\n\n\nWe hebben nu de code om een choropleth kaart te maken en we hebben gezien hoe we die stap voor stap kunnen bouwen met {ggplot2}. Met een beetje maatwerk hebben we nu een statische kaart die we in een ander formaat opslaan en delen.\nAls je geïnteresseerd bent in het omgaan met geografische gegevens, is een van de beste vrij beschikbare bronnen het Geocomputation with R-boek. De auteurs van het boek maken veel gebruik van verschillende pakketten voor het plotten van thematische kaarten, met name {tmap}, dat ook de moeite waard is om te onderzoeken. Als u pakketten zoals {ggtext} wilt gebruiken om uw plots aan te passen, is {ggplot2} de bibliotheek waarop u wilt vertrouwen, vooral als u al gewend bent om ermee te werken.\nIk hoop dat je dit artikel met plezier las en blijf op de hoogte van meer voorbeelden over hoe kaarten in R zijn te maken."
  },
  {
    "objectID": "posts/2022-01-03-multilevel-modeling/multilevel-modeling.html#het-schatten-van-multilevel-modellen-voor-verandering-in-r",
    "href": "posts/2022-01-03-multilevel-modeling/multilevel-modeling.html#het-schatten-van-multilevel-modellen-voor-verandering-in-r",
    "title": "Multilevel modeling",
    "section": "",
    "text": "Longitudinale gegevens zijn heel boeiend omdat je ermee kunt kijken naar verandering in de tijd, een beter begrip krijgt van causale verbanden en gebeurtenissen en hun timing ermee kunt verklaren. Om dit te kunnen doen, moeten we verder gaan dan de klassieke statistische methoden, zoals OLS regressie en ANOVA, en modellen gebruiken die beter kunnen omgaan met complexiteit van de gegevens. Alexander Cernat schreef er een blog over hier die ik hier in het Nederlands overzet en waarbij ik alcoholdata van Willet en Singer gebruik/\nEen populair model voor de analyse van longitudinale gegevens is het Multilevel Model voor Verandering; MultiLevel Model for Change (MLMC). Dit model maakt de schatting van verandering in de tijd mogelijk, terwijl rekening wordt gehouden met de hiërarchische aard van de gegevens (meerdere punten in de tijd genest binnen individuen). Het is vergelijkbaar met het Latente Groei Model; Latent Growth Model zie deze post, maar hier wordt geschat met behulp van het multilevel model raamwerk (ook bekend als hiërarchische modellering of random effecten). Deze techniek maakt gebruik van het lange dataformaat (elke rij is is een rij gegevens op een specifiek tijdstip voor een individu).\nMeer in het bijzonder kan het MLMC helpen:\n- te begrijpen hoe de individuele en geaggregeerde verandering in de tijd verlopen;\n- verandering te verklaren met behulp van tijdsvariërende (bv. tijd) en tijdsconstante (b.v. geslacht) voorspellers;\n- variantie te ontleden in tussen- en binnen- variatie;\n- gemakkelijk om te gaan met continue tijd, onevenwichtige gegevens (niet alle individuen zijn op alle tijdstippen aanwezig) en verschillende timings (niet iedereen geeft gegevens op precies hetzelfde moment).\nHier volgt een korte inleiding op MLMC, hoe hiermee te werken in R en hoe veranderingen zijn te visualiseren.\nLaten we eerst de R-pakketten laden (deze moeten dus wel geïnstalleerd zijn). We zullen tidyverse gebruiken voor het opschonen en visualiseren van de data, lme4 voor het uitvoeren van de MLMC in R en sjstats voor het schatten van intra class correlation (icc)\nJe kunt pakketten installeren met het install.packages() commando.\nLaten we vervolgens de pakketten binnenhalen die wij bij deze analyse zullen gebruiken.\n\n# pakket voor dataopschonen en visualiseren\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv ggplot2 3.3.6     v purrr   0.3.4\nv tibble  3.1.7     v dplyr   1.0.9\nv tidyr   1.2.0     v stringr 1.4.1\nv readr   2.1.2     v forcats 0.5.1\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'stringr' was built under R version 4.1.3\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\n# pakket voor multilevel-modeleren\nlibrary(lme4)\n\nWarning: package 'lme4' was built under R version 4.1.3\n\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n# pakket om intra class correlation makkelijk vast te stellen\nlibrary(sjstats)\n\nLaten we, voordat we aan de MLMC beginnen, eerst kijken naar de data die we willen analyseren. Hier gebruik ik alcuse (alcoholgebruik) met drie metingen. Dit is een longitudinale dataset van 82 jongeren.\nWe willen weten hoe alcoholgebruik in de tijd verandert en we willen die verandering begrijpen. Hierbij maken we een onderscheid tussen tussenvariatie (hoe de jongeren ten opzichte van elkaar veranderen) en binnenvariatie (hoe jongeren veranderen ten opzichte van hun eigen gemiddelde/trend).\nLaten we deze gegevens eens onderzoeken. We kijken hiervoor naar de gegevens in lang formaat, die we zullen gebruiken voor de modellering en de grafieken. Hieronder zie je de eerste tien gegevens:\n\nalcohol1 &lt;- read.table(\"https://stats.idre.ucla.edu/stat/r/examples/alda/data/alcohol1_pp.txt\", header=T, sep=\",\")\nattach(alcohol1)\nhead(alcohol1, n=10)\n\n   id age coa male age_14   alcuse      peer      cpeer  ccoa\n1   1  14   1    0      0 1.732051 1.2649111  0.2469111 0.549\n2   1  15   1    0      1 2.000000 1.2649111  0.2469111 0.549\n3   1  16   1    0      2 2.000000 1.2649111  0.2469111 0.549\n4   2  14   1    1      0 0.000000 0.8944272 -0.1235728 0.549\n5   2  15   1    1      1 0.000000 0.8944272 -0.1235728 0.549\n6   2  16   1    1      2 1.000000 0.8944272 -0.1235728 0.549\n7   3  14   1    1      0 1.000000 0.8944272 -0.1235728 0.549\n8   3  15   1    1      1 2.000000 0.8944272 -0.1235728 0.549\n9   3  16   1    1      2 3.316625 0.8944272 -0.1235728 0.549\n10  4  14   1    1      0 0.000000 1.7888544  0.7708544 0.549\n\n\nWe zien dat elke rij een combinatie is van een jongere (variabele id (van 1 tot en met 4, die we in de analyse gebruiken)) en tijd (variabele age_14, alcoholgebruik op 14-jarige leeftijd). Dit is ook het formaat dat we nodig hebben voor een visualisatie met ggplot2.\nOm te zien wat we gaan modelleren, kunnen we een eenvoudige grafiek maken met een gemiddelde veranderingslijn in tijd voor de hele dataset en een wirwar van lijnen voor de verandering van elk individu:\n\nggplot(alcohol1, aes(age, alcuse, group = id)) +\n  geom_line(alpha = 0.1) + # voeg individuele lijn met transparantie toe\n  stat_summary( # voeg gemiddelde lijn toe\n    aes(group = 1),\n    fun = mean,\n    geom = \"line\",\n    size = 1.0,\n    color = \"red\"\n  ) +\n  theme_bw() + # goed theme voor visualisatie\n  labs(x = \"Leeftijd\", y = \"Alcoholgebrui\") # de labels\n\n\n\n\nWe zien dus een hele lichte positieve en constante verandering in de tijd, maar vooral ook heel wat variatie in de manier waarop jongeren alcohol gebruiken (grijze lijnen). MLMC is in staat om beide dingen (het structurele en individuele) tegelijk te schatten!"
  },
  {
    "objectID": "posts/2021-02-19-website-met-distill/website-met-distill.html#distill",
    "href": "posts/2021-02-19-website-met-distill/website-met-distill.html#distill",
    "title": "Website met distill",
    "section": "",
    "text": "Mijn eigen Harrie’s Hoekje blog, over een aantal ontwikkelingen in de dataanalyse, maak ik ook met het gebruik van het pakket Distill. Met Distill kun je wetenschappelijke websites maken, een blog en artikelen schrijven. Over hoe je dat doet schreef Lisa Lendway een kort en krachtige blog. Dat kan ik niet beter. Dank je, Lisa, hiervoor. Haar blog staat hier"
  },
  {
    "objectID": "posts/2023-03-18-quarto-blog/schrijf_publiseer_blog.html#quarto",
    "href": "posts/2023-03-18-quarto-blog/schrijf_publiseer_blog.html#quarto",
    "title": "Maken en publiceren van een blog",
    "section": "",
    "text": "Met Quarto ben je in staat om inhoud en code naar verschillende wetenschappelijke producten om te zetten. Dat kunnen artikelen, rapporten, boeken, websites of b.v. dashboards zijn. Met Quarto kun je ook een blog maken. Hier laat ik jullie zien hoe je zo’n blog kunt maken. Om meer over Quarto te leren verwijs ik je naar de uitgebreide Quarto-website. Ondertussen heb ik verschillende blogs/websites en boeken met Quarto gemaakt, waaronder: - Mijn eigen website\n\nHarrie’s Hoekje\nNSCR-workshops\nDemocratie en Onderwijs\nQuarto Boek]"
  },
  {
    "objectID": "posts/2023-02-13-quarto-presentatie-toelichting/Quatropresentatie.html#quatro",
    "href": "posts/2023-02-13-quarto-presentatie-toelichting/Quatropresentatie.html#quatro",
    "title": "Begin van mijn Quartoreis, een opzet voor een Quarto presentatie",
    "section": "",
    "text": "Afgelopen weken heb ik wat met Quarto geëxperimenteerd. Zo heb ik met Quarto een boek gemaakt en daarover zal ik nog een andere blog schrijven. Ook heb ik met Quarto wat presentaties gemaakt. Meghan Hall heeft mij hier veel geleerd. Zij heeft een presentatie in Quarto gezet hier en de repository daarvan vind je hier. Hoe ze deze aantrekkelijke presentatie heeft opgemaakt, beschrijft ze in haar blog.\nHIER VIND JE DIE PRESENTATIE\nDe presentatie heb ik vertaald en in andere kleuren gezet hier. De blogopmaak heb ik zelf wat aangepast. De aangepaste presentatie en blog vind je op mijn github-site hier. Hartelijke dank Meghan Hall voor jouw duidelijke uitleg, veel van geleerd."
  },
  {
    "objectID": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#eigen-inleiding",
    "href": "posts/2022-06-27-ggplot-stap-voor-stap/ggplot-stap-voor-stap.html#eigen-inleiding",
    "title": "ggplot stap voor stap",
    "section": "",
    "text": "De afgelopen maand toch nog weer eens naar datavisualisatie gekeken en hoe je dat met R het beste kunt doen. Twee boeken nogeens gelezen hierover. Allereerst het boek van Rob Kabacoff Data Visualization with R. Het is een handige introducte op ggploten vooral een handige tutorial voor het visualiseren van data met R. Daarnaast het boek van Clause Wilke Fundamentals of Data Visualization dat meer een algemene reflectie is op data visualisatie. Het ga je op verschillende manieren van data naar visualisatie, wat zijn de onderliggende principes en waaraan moeten we bij datavisualisatie vooral denken. Toch heb ik de afgelopen maand vooral veel geleerd van Cédric Scherer. Ik zag dat hij op de conferentie van R binnenkort een inleiding geeft op het onderwerp datavisualisatie en zo zag ik via zijn Github site verschillend materiaal over datavisualisatie. Mooie en duidelijke inleidingen en twee ervan heb ik in het Nederlands overgezet. Hier vind je in ieder geval een algemene inleiding op ggplot. zie"
  },
  {
    "objectID": "posts/2022-11-07-statistisch-denken/Stats21.html#inleiding",
    "href": "posts/2022-11-07-statistisch-denken/Stats21.html#inleiding",
    "title": "Statistisch denken in de 21ste eeuw",
    "section": "",
    "text": "Een paar jaar geleden schreef Russell A. Poldrack Statistical Thinking for the 21st Century. Dat vind ik een inspirerend boek. Poldrack wil ons de basisideeën van statistisch denken laten begrijpen. Zijn boek presenteert een systematische manier van denken over het beschrijven van de wereld met behulp van data, hoe we deze kunnen gebruiken bij het nemen van beslissingen en het doen van voorspellingen en dit alles in de context van onzekerheid die er in de wereld bestaat. Met de traditionele statistiekboeken kon hij niet uit de voeten en daarom besloot hij zijn eigen boek te maken. Hij gebruikt aansprekende datasets in zijn boek en moderne manieren om hiermee om te gaan.\nOm je deze nieuwe manier van statistisch denken eigen te maken, moet je ermee werken, je moet het doen."
  },
  {
    "objectID": "posts/2022-11-07-statistisch-denken/Stats21.html#footnotes",
    "href": "posts/2022-11-07-statistisch-denken/Stats21.html#footnotes",
    "title": "Statistisch denken in de 21ste eeuw",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHet werkt ook met Python en Quarto maar dat kon ik niet in deze blog laten zien.↩︎"
  },
  {
    "objectID": "posts/2021-07-18-classificeren-van-palmer-penguins/classificeren-van-palmer-penguins.html#palmer-penguins-classificatie",
    "href": "posts/2021-07-18-classificeren-van-palmer-penguins/classificeren-van-palmer-penguins.html#palmer-penguins-classificatie",
    "title": "Classificeren van Palmer penguins",
    "section": "",
    "text": "Hier kun je overigen haar opnmame vinden. Julia Silge on youtube\nDe laatste tijd heeft Julia Silge een aantal videoopnamen gemaakt die laten zien hoe het tidymodels raamwerk is te gebruiken.Het zijn opnamen over de eerste stappen in het modelleren tot hoe complexe modellen zijn te evalueren. Deze videoopname is goed voor mensen die net beginnen met tidymodels. Ze maakt daarbij gebruik van een #TidyTuesday dataset over pinguïns. Hier gaat het om classificeren.\nHier kun je haar opnmame vinden. Julia Silge on youtube\nEerst maar eens enkele pakketten laden en het databestand openen.\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv ggplot2 3.3.6     v purrr   0.3.4\nv tibble  3.1.7     v dplyr   1.0.9\nv tidyr   1.2.0     v stringr 1.4.1\nv readr   2.1.2     v forcats 0.5.1\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'stringr' was built under R version 4.1.3\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(palmerpenguins)\n\nWarning: package 'palmerpenguins' was built under R version 4.1.3\n\npenguins\n\n# A tibble: 344 x 8\n   species island    bill_length_mm bill_depth_mm flipper_~1 body_~2 sex    year\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;      &lt;int&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7        181    3750 male   2007\n 2 Adelie  Torgersen           39.5          17.4        186    3800 fema~  2007\n 3 Adelie  Torgersen           40.3          18          195    3250 fema~  2007\n 4 Adelie  Torgersen           NA            NA           NA      NA &lt;NA&gt;   2007\n 5 Adelie  Torgersen           36.7          19.3        193    3450 fema~  2007\n 6 Adelie  Torgersen           39.3          20.6        190    3650 male   2007\n 7 Adelie  Torgersen           38.9          17.8        181    3625 fema~  2007\n 8 Adelie  Torgersen           39.2          19.6        195    4675 male   2007\n 9 Adelie  Torgersen           34.1          18.1        193    3475 &lt;NA&gt;   2007\n10 Adelie  Torgersen           42            20.2        190    4250 &lt;NA&gt;   2007\n# ... with 334 more rows, and abbreviated variable names 1: flipper_length_mm,\n#   2: body_mass_g\n\n\nAls je een classificatiemodel voor soorten pinquins probeert op te stellen, zul je waarschijnlijk een bijna perfecte pasvorm vinden, omdat dit soort waarnemingen in feite de verschillende soorten onderscheiden. sex (geslacht) daarentegen geeft een wat rommeliger beeld, vandaar dat hier deze uitkomstvariabelen op basis van predictoren wordt voorspeld.\n\npenguins %&gt;%\n  filter(!is.na(sex)) %&gt;%\n  ggplot(aes(flipper_length_mm, bill_length_mm, color = sex, size = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~species)\n\n\n\n\nHet ziet er naar uit dat de vrouwelijke pinguïnflippers kleiner zijn met kleinere snavels, maar laten we ons klaarmaken voor het modelleren om meer te weten te komen! De informatie over het eiland of het jaar zullen we niet gebruiken in ons model. Die halen we eruit.\n\npenguins_df &lt;- penguins %&gt;%\n  filter(!is.na(sex)) %&gt;%\n  select(-year, -island)\n\n\n\nWe zullen ook het tidymodels metapakket laden en vervolgens onze gegevens splitsen in een trainings- en testingssets.\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.1.3\n\n\n-- Attaching packages -------------------------------------- tidymodels 0.2.0 --\n\n\nv broom        0.8.0     v rsample      0.1.1\nv dials        1.0.0     v tune         0.2.0\nv infer        1.0.2     v workflows    0.2.6\nv modeldata    0.1.1     v workflowsets 0.2.1\nv parsnip      1.0.0     v yardstick    1.0.0\nv recipes      0.2.0     \n\n\nWarning: package 'broom' was built under R version 4.1.3\n\n\nWarning: package 'dials' was built under R version 4.1.3\n\n\nWarning: package 'scales' was built under R version 4.1.3\n\n\nWarning: package 'infer' was built under R version 4.1.3\n\n\nWarning: package 'parsnip' was built under R version 4.1.3\n\n\nWarning: package 'recipes' was built under R version 4.1.3\n\n\nWarning: package 'tune' was built under R version 4.1.3\n\n\nWarning: package 'workflows' was built under R version 4.1.3\n\n\nWarning: package 'workflowsets' was built under R version 4.1.3\n\n\nWarning: package 'yardstick' was built under R version 4.1.3\n\n\n-- Conflicts ----------------------------------------- tidymodels_conflicts() --\nx scales::discard() masks purrr::discard()\nx dplyr::filter()   masks stats::filter()\nx recipes::fixed()  masks stringr::fixed()\nx dplyr::lag()      masks stats::lag()\nx yardstick::spec() masks readr::spec()\nx recipes::step()   masks stats::step()\n* Learn how to get started at https://www.tidymodels.org/start/\n\nset.seed(123)\npenguin_split &lt;- initial_split(penguins_df, strata = sex)\npenguin_train &lt;- training(penguin_split)\npenguin_test &lt;- testing(penguin_split)\n\nOmdat het een relatieve kleine dataset betreft (zeker de testset), maken we vervolgens hier gebruik van bootstrap-resamples van de trainingsgegevens, om onze modellen te evalueren.\n\nset.seed(123)\npenguin_boot &lt;- bootstraps(penguin_train)\npenguin_boot\n\n# Bootstrap sampling \n# A tibble: 25 x 2\n   splits           id         \n   &lt;list&gt;           &lt;chr&gt;      \n 1 &lt;split [249/93]&gt; Bootstrap01\n 2 &lt;split [249/91]&gt; Bootstrap02\n 3 &lt;split [249/90]&gt; Bootstrap03\n 4 &lt;split [249/91]&gt; Bootstrap04\n 5 &lt;split [249/85]&gt; Bootstrap05\n 6 &lt;split [249/87]&gt; Bootstrap06\n 7 &lt;split [249/94]&gt; Bootstrap07\n 8 &lt;split [249/88]&gt; Bootstrap08\n 9 &lt;split [249/95]&gt; Bootstrap09\n10 &lt;split [249/89]&gt; Bootstrap10\n# ... with 15 more rows\n\n\nLaten we eens twee verschillende modellen vergelijken, een logistisch regressiemodel en een random forest model. We beginnen met het maken van de modelspecificaties voor beide modellen.\n\nglm_spec &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\")\n\nglm_spec\n\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\n\nrf_spec &lt;- rand_forest() %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"ranger\")\n\nrf_spec\n\nRandom Forest Model Specification (classification)\n\nComputational engine: ranger \n\n\nLaten we nu beginnen met het samenstellen van een tidymodels workflow(), een object dat helpt om modelleer-pijplijnen te beheren met stukjes die in elkaar passen als Lego-blokjes. Merk op dat er nog geen model is:\n\npenguin_wf &lt;- workflow() %&gt;%\n  add_formula(sex ~ .)\n\npenguin_wf\n\n== Workflow ====================================================================\nPreprocessor: Formula\nModel: None\n\n-- Preprocessor ----------------------------------------------------------------\nsex ~ .\n\n\nNu kunnen we een model toevoegen, en de fit voor elk van de resamples. Eerst kunnen we het logistische regressiemodel passen.\n\nglm_rs &lt;- penguin_wf %&gt;%\n  add_model(glm_spec) %&gt;%\n  fit_resamples(\n    resamples = penguin_boot,\n    control = control_resamples(save_pred = TRUE)\n  )\n\n! Bootstrap05: preprocessor 1/1, model 1/1: glm.fit: fitted probabilities numerically 0...\n\n\n! Bootstrap08: preprocessor 1/1, model 1/1: glm.fit: fitted probabilities numerically 0...\n\n\n! Bootstrap23: preprocessor 1/1, model 1/1: glm.fit: fitted probabilities numerically 0...\n\nglm_rs\n\n# Resampling results\n# Bootstrap sampling \n# A tibble: 25 x 5\n   splits           id          .metrics         .notes           .predictions\n   &lt;list&gt;           &lt;chr&gt;       &lt;list&gt;           &lt;list&gt;           &lt;list&gt;      \n 1 &lt;split [249/93]&gt; Bootstrap01 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 3]&gt; &lt;tibble&gt;    \n 2 &lt;split [249/91]&gt; Bootstrap02 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 3]&gt; &lt;tibble&gt;    \n 3 &lt;split [249/90]&gt; Bootstrap03 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 3]&gt; &lt;tibble&gt;    \n 4 &lt;split [249/91]&gt; Bootstrap04 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 3]&gt; &lt;tibble&gt;    \n 5 &lt;split [249/85]&gt; Bootstrap05 &lt;tibble [2 x 4]&gt; &lt;tibble [1 x 3]&gt; &lt;tibble&gt;    \n 6 &lt;split [249/87]&gt; Bootstrap06 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 3]&gt; &lt;tibble&gt;    \n 7 &lt;split [249/94]&gt; Bootstrap07 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 3]&gt; &lt;tibble&gt;    \n 8 &lt;split [249/88]&gt; Bootstrap08 &lt;tibble [2 x 4]&gt; &lt;tibble [1 x 3]&gt; &lt;tibble&gt;    \n 9 &lt;split [249/95]&gt; Bootstrap09 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 3]&gt; &lt;tibble&gt;    \n10 &lt;split [249/89]&gt; Bootstrap10 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 3]&gt; &lt;tibble&gt;    \n# ... with 15 more rows\n\nThere were issues with some computations:\n\n  - Warning(s) x3: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nUse `collect_notes(object)` for more information.\n\n\nTen tweede kunnen we het random forest model toepassen.\n\nrf_rs &lt;- penguin_wf %&gt;%\n  add_model(rf_spec) %&gt;%\n  fit_resamples(\n    resamples = penguin_boot,\n    control = control_resamples(save_pred = TRUE)\n  )\n\nWarning: package 'ranger' was built under R version 4.1.3\n\nrf_rs\n\n# Resampling results\n# Bootstrap sampling \n# A tibble: 25 x 5\n   splits           id          .metrics         .notes           .predictions\n   &lt;list&gt;           &lt;chr&gt;       &lt;list&gt;           &lt;list&gt;           &lt;list&gt;      \n 1 &lt;split [249/93]&gt; Bootstrap01 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 3]&gt; &lt;tibble&gt;    \n 2 &lt;split [249/91]&gt; Bootstrap02 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 3]&gt; &lt;tibble&gt;    \n 3 &lt;split [249/90]&gt; Bootstrap03 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 3]&gt; &lt;tibble&gt;    \n 4 &lt;split [249/91]&gt; Bootstrap04 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 3]&gt; &lt;tibble&gt;    \n 5 &lt;split [249/85]&gt; Bootstrap05 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 3]&gt; &lt;tibble&gt;    \n 6 &lt;split [249/87]&gt; Bootstrap06 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 3]&gt; &lt;tibble&gt;    \n 7 &lt;split [249/94]&gt; Bootstrap07 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 3]&gt; &lt;tibble&gt;    \n 8 &lt;split [249/88]&gt; Bootstrap08 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 3]&gt; &lt;tibble&gt;    \n 9 &lt;split [249/95]&gt; Bootstrap09 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 3]&gt; &lt;tibble&gt;    \n10 &lt;split [249/89]&gt; Bootstrap10 &lt;tibble [2 x 4]&gt; &lt;tibble [0 x 3]&gt; &lt;tibble&gt;    \n# ... with 15 more rows\n\n\nWij hebben elk van onze kandidaat-modellen aangepast aan onze opnieuw bemonsterde trainingsreeks!\n\n\n\nLaten we nu eens kijken hoe we het gedaan hebben. Eerst het logistisch regressiemodel.\n\ncollect_metrics(glm_rs)\n\n# A tibble: 2 x 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.918    25 0.00639 Preprocessor1_Model1\n2 roc_auc  binary     0.979    25 0.00254 Preprocessor1_Model1\n\n\nGoed zo! De functie collect_metrics() extraheert en formatteert de .metrics kolom van resampling resultaten zoals hierboven voor het glm-model. Nu het random-forest model.\n\ncollect_metrics(rf_rs)\n\n# A tibble: 2 x 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.912    25 0.00547 Preprocessor1_Model1\n2 roc_auc  binary     0.977    25 0.00202 Preprocessor1_Model1\n\n\nDus… ook geweldig! Als ik in een situatie zit waarin een complexer model (zoals een random forest) hetzelfde presteert als een eenvoudiger model (zoals logistische regressie), dan kies ik het eenvoudiger model. Laten we eens dieper ingaan op hoe het het doet. Bijvoorbeeld, hoe voorspelt het glm-model de twee klassen?\n\nglm_rs %&gt;%\n  conf_mat_resampled()\n\n# A tibble: 4 x 3\n  Prediction Truth   Freq\n  &lt;fct&gt;      &lt;fct&gt;  &lt;dbl&gt;\n1 female     female  41.1\n2 female     male     3  \n3 male       female   4.4\n4 male       male    42.3\n\n\nOngeveer hetzelfde, wat goed is. We kunnen ook een ROC curve maken.\n\nglm_rs %&gt;%\n  collect_predictions() %&gt;%\n  group_by(id) %&gt;%\n  roc_curve(sex, .pred_female) %&gt;%\n  ggplot(aes(1 - specificity, sensitivity, color = id)) +\n  geom_abline(lty = 2, color = \"gray80\", size = 1.5) +\n  geom_path(show.legend = FALSE, alpha = 0.6, size = 1.2) +\n  coord_equal()\n\n\n\n\nDeze ROC-curve is grilliger dan andere die u wellicht hebt gezien omdat de dataset klein is.\nHet is eindelijk tijd om terug te keren naar de testset. Merk op dat we de testset tijdens deze hele analyse nog niet hebben gebruikt; de testset is kostbaar en kan alleen worden gebruikt om de prestaties op nieuwe gegevens in te schatten. Laten we nog een keer passen op de trainingsgegevens en evalueren op de testgegevens met behulp van de functie last_fit().\n\npenguin_final &lt;- penguin_wf %&gt;%\n  add_model(glm_spec) %&gt;%\n  last_fit(penguin_split)\n\npenguin_final\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 x 6\n  splits           id               .metrics .notes   .predictions .workflow \n  &lt;list&gt;           &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [249/84]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\n\nDe metriek en voorspellingen hier zijn op de testgegevens.\n\ncollect_metrics(penguin_final)\n\n# A tibble: 2 x 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.857 Preprocessor1_Model1\n2 roc_auc  binary         0.938 Preprocessor1_Model1\n\n\n\ncollect_predictions(penguin_final) %&gt;%\n  conf_mat(sex, .pred_class)\n\n          Truth\nPrediction female male\n    female     37    7\n    male        5   35\n\n\nDe coëfficiënten (die we eruit kunnen halen met tidy()) zijn geschat met behulp van de trainingsdata. Als we exponentiate = TRUE gebruiken, hebben we odds ratio’s.\n\npenguin_final$.workflow[[1]] %&gt;%\n  tidy(exponentiate = TRUE)\n\n# A tibble: 7 x 5\n  term              estimate std.error statistic     p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 (Intercept)       5.75e-46  19.6        -5.31  0.000000110\n2 speciesChinstrap  1.37e- 4   2.34       -3.79  0.000148   \n3 speciesGentoo     1.14e- 5   3.75       -3.03  0.00243    \n4 bill_length_mm    1.91e+ 0   0.180       3.60  0.000321   \n5 bill_depth_mm     8.36e+ 0   0.478       4.45  0.00000868 \n6 flipper_length_mm 1.06e+ 0   0.0611      0.926 0.355      \n7 body_mass_g       1.01e+ 0   0.00176     4.59  0.00000442 \n\n\n\nDe grootste kansverhouding geldt voor de snaveldiepte, en de op één na grootste voor de snavellengte. Een toename van 1 mm snaveldiepte komt overeen met bijna 4x meer kans om een mannetje te zijn. De kenmerken van de bek van een pinguïn moeten geassocieerd zijn met het geslacht.\n\nWe hebben geen sterke aanwijzingen dat de lengte van de vleugels verschillend is tussen mannelijke en vrouwelijke pinguïns, als we de andere maten controleren; misschien moeten we dat onderzoeken door de eerste grafiek te veranderen!\n\n\npenguins %&gt;%\n  filter(!is.na(sex)) %&gt;%\n  ggplot(aes(bill_depth_mm, bill_length_mm, color = sex, size = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~species)\n\n\n\n\nJa, de mannetjes- en vrouwtjespinguïns zijn nu veel meer gescheiden."
  },
  {
    "objectID": "posts/2022-01-03-latente-groei-modeling/latente-groei-modeling.html#schatten-en-visualiseren-van-verandering-in-de-tijd-met-behulp-van-latente-groeimodellen-met-r",
    "href": "posts/2022-01-03-latente-groei-modeling/latente-groei-modeling.html#schatten-en-visualiseren-van-verandering-in-de-tijd-met-behulp-van-latente-groeimodellen-met-r",
    "title": "Latente Groei Modeling",
    "section": "",
    "text": "Longitudinale gegevens zijn zo interessant omdat ze ons in staat stellen te kijken naar verandering in de tijd, je krijgt er een beter inzicht mee in causale verbanden en je kunt gebeurtenissen en hun timing ermee verklaren. Om gebruik te maken van dit soort gegevens, moeten we verder gaan dan de klassieke statistische methoden, zoals OLS regressie en ANOVA. Dan moeten we gebruik maken van modellen die de extra complexiteit van de gegevens ook echt aan kunnen. Alexandru Cernat schreef ook hier een duidelijke blog over dat ik heb bewerkt en waarbij ik ook de alcoholdata van Singer en Willet heb gebruik.\nEen populair model voor de analyse van longitudinale gegevens is het Latente Groei Model (Latent Growth Model, LGM). Hiermee kan de verandering in de tijd worden geschat, terwijl rekening wordt gehouden met de hiërarchische aard van de gegevens (meerdere punten in de tijd die genest zijn binnen individuen). Het is vergelijkbaar met het multilevel model van verandering, maar hier wordt de schatting gedaan met behulp van het Structural Equation Modeling (SEM)-raamwerk. Dit raamwerk maakt gebruik van gegevens in het brede formaat (elke rij is een individu en de diverse metingen in de tijd verschijnen als verschillende kolommen).\nMeer in het bijzonder kan het LGM helpen\n- te begrijpen hoe verandering in de tijd verloopt;\n- verandering verklaren met behulp van tijdvariërende en tijdconstante voorspellers;\n- variantie ontleden in tussen- en binnenvariatie;\n- en het model kan makkelijk worden uitgebreid naar andere analysemodellen.\nHieronder volgt een korte inleiding op LGM, hoe de uitkomsten zijn te schatten en hoe de schattingen van verandering zijn te visualiseren.\nLaten we eerst de benodigde pakketten eens laden. We zullen tidyverse gebruiken voor het opschonen en visualiseren van de gegevens en lavaan voor het uitvoeren van de LGM in R.\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv ggplot2 3.3.6     v purrr   0.3.4\nv tibble  3.1.7     v dplyr   1.0.9\nv tidyr   1.2.0     v stringr 1.4.1\nv readr   2.1.2     v forcats 0.5.1\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'stringr' was built under R version 4.1.3\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(lavaan)\n\nWarning: package 'lavaan' was built under R version 4.1.3\n\n\nThis is lavaan 0.6-11\nlavaan is FREE software! Please report any bugs.\n\n\nLaten we, voordat we aan de LGM beginnen, eens kijken naar het soort gegevens dat we zouden willen analyseren. Hier gebruik ik alcoholdata van jongeren met de drie metingen van Singer en Willet voor die vrij toegankelijk zijn op internet.\nStel dat we geïnteresseerd zijn in hoe alcoholscore in de tijd verandert. Om het preciezer te formuleren willen laten zien hoe alcoholgebruik onder jongeren gemiddeld verandert, en tegelijk willen we een onderscheid maken tussen variatie, hoe jongeren veranderen ten opzichte van anderen. Maar tegelijk willen we ook iets zeggen over binnenvariatie en hoe jongeren veranderen ten opzichte van hun eigen gemiddelde/trend.\nLaten we eerst eens kijken hoe de gegevens eruit zien. Laten we eens kijken naar de brede gegevens, dit zijn de gegevens die gebruikt worden om LGM uit te voeren en laten we ook maar meteen het lange bestand bekijken:\n\nalcohol1 &lt;- read.table(\"https://stats.idre.ucla.edu/stat/r/examples/alda/data/alcohol1_pp.txt\", header=T, sep=\",\")\nattach(alcohol1)\n\nWe beginnen met het lange formaat, waar elke rij een combinatie is van individu en tijd. Dit is het formaat dat we nodig hebben voor visualisatie met ggplot2, en voor andere modellen (zoals het multilevel model voor verandering).\n\nhead(alcohol1)\n\n  id age coa male age_14   alcuse      peer      cpeer  ccoa\n1  1  14   1    0      0 1.732051 1.2649111  0.2469111 0.549\n2  1  15   1    0      1 2.000000 1.2649111  0.2469111 0.549\n3  1  16   1    0      2 2.000000 1.2649111  0.2469111 0.549\n4  2  14   1    1      0 0.000000 0.8944272 -0.1235728 0.549\n5  2  15   1    1      1 0.000000 0.8944272 -0.1235728 0.549\n6  2  16   1    1      2 1.000000 0.8944272 -0.1235728 0.549\n\n\nOm een idee te krijgen van wat we gaan modelleren, maken we een eenvoudige grafiek met de gemiddelde verandering in de tijd en de trend voor elk individu.\n\nggplot(alcohol1, aes(age_14, alcuse, group = id)) + \n  geom_line(alpha = 0.1) + # add individual line with transparency\n  stat_summary( # add average line\n    aes(group = 1),\n    fun = mean,\n    geom = \"line\",\n    size = 1.5,\n    color = \"red\"\n  ) +\n  theme_bw() + # nice theme\n  labs(x = \"Leeftijd\", y = \"Alcoholgebruik\") # nice labels\n\n\n\n\nWe zien hier een gemiddelde verandering in de tijd. Tegelijk willen we ook zichtbaar maken wat variatie is in de manier waarop mensen veranderen. LGM is in staat beide tegelijk te schatten!"
  },
  {
    "objectID": "posts/2023-06-27-multilevel/Multilevel.html",
    "href": "posts/2023-06-27-multilevel/Multilevel.html",
    "title": "Introductie op multilevel analyse in R met lme4 en tidyverse",
    "section": "",
    "text": "Ik had al een tijdje geen multilevel-analyses meer gedaan en wilde dat weer eens doen met aangepaste technieken. Ben zelf ‘opgeleid’ met het prachtige werk van de multilevelgroep uit Bristol (Goldstein, Rabash, Brown e.a.) en ook dat van Joop Hox, die ik op dit gebied van dichtbij heb meegemaakt. Toen ik wat rondkeek, kwam ik de blog van Rafaella Vacca tegen, die ik inzichtelijk en vernieuwend vond. Hij werkt met het pakket lme4 en tidyverse. Hieronder vind je mijn bewerkte versie. Dank je wel Rafaella.\n\n\nDit is een introductie op multilevel analyse met R voor de seminars die Raffaele Vacca eerder gaf op de UniMi NASP graduate school en Behave Lab.\nHier staat het oorspronkelijke materiaal dat je hier (GitHub) kunt downloaden.\n\nR pakketten en literatuur:\n\nDeze tutorial richt zich op (1) het lme4 pakket voor (Restricted) Maximum Likelihood Estimation van lineaire multilevel modellen [@bates_fitting_2015; @bates_lme4:_2012] en (2) integreren van lme4 met tidyverse, een verzameling van R pakketten voor data wetenschap (waaronder dplyr, ggplot2, en purrr) met een gezamenlijke taal en een set van principles [@wickham_r_2017].\nHet is gebaseerd op de discussie ‘(linear) multilevel modeling’ van @fox_fitting_2018 en @fox_linear_2016. Het voorbeeld dat hier gebruikt wordt komt oorspronkelijk van @raudenbush_hierarchical_2002. Een deel van de code is ook geïnsprieerd door Wickham and Grolemund’s [-@wickham_r_2017] behandeling van statististisch modeleren met R (vooral Hfd. 20).\nDe data komen van MathAchieve en MathAchSchool data-frames in het nlme pakket. Daar komen ze weer via het “High School and Beyond” onderzoek met 7185 studenten in 160 V.S. middelbare scholen, inclusief 70 Katholieke en 90 Openbare scholen [@fox_linear_2016; @raudenbush_hierarchical_2002]. Kijk naar de links en referenties hierboven als je meer documentatie zoekt voor deze data.\n\nNog wat litertuur en bronnen:\n\nVoor statistische theorie, details over schattingmethodes en meer gedetailleerd behandeling van multilevel modelellen die in deze introductie worden behandeld (in chronologische volgorde): @raudenbush_hierarchical_2002; @gelman_data_2006; @rasbash_lemma:_2008; @goldstein_multilevel_2010; @snijders_multilevel_2012; @simonoff_sage_2013; @fox_applied_2016 (Ch. 23-24).\nVoor meer informatie over de R-implementatie van multilevel modellen, inclusief verschillende pakketten en schattingsmethodes: @finch_multilevel_2014; @fox_fitting_2018; Ben Bolker‘s FAQ page over ’Generalized Linear Mixed Models’.\n\n\n\nVoor deze workshop is het nodig dat je:\n\nDat je de laatste versie binnenhaalt van R hier (selecteer een locatie bij je in de buurt).\n\nVolg de instructies om R op jouw computer te installeren.\n\nDownload RStudio (vrije versie) hier.\n\nVolg de instructies om RStudio op jouw computer te installeren.\n\nInstalleer de R pakkettem genoemd onder.\n\nOpen RStudio en ga naarTop menu &gt; Tools &gt; Install packages....\nInstalleer elk pakket van de lijst.\n\nBreng de laptop mee naar de workshop.\nDownload de workshop project folder hier\n\nKlik op de link &gt; Klik op de groene Clone knop &gt; Download ZIP &gt; Dan ‘unzip’ de folder op jouw computer.\nIk zou dat de in de klas doen aan het begin van de workshop zodat je de laatste versie van folder download.\n\n\nEen keer in de klas, ga naar de workshop project folder en dubbelklik op de workshop R project file (Multilevel_with_R.Rproj). Dit zal RStudio openen.\n\n\n\n\nAlgemeen:\n\nbroom om de model resultaten als tidy tibbles te zien.\n\nmagrittr voor ‘pipe’ en gerelateerde handelingen.\n\ntidyverse.\n\n\nOm multilevel modellen te draaien en de resultaten te zien:\n\nbroom.mixed.\n\ncar voor testen van significantie.\n\nggeffects om de geschatte waarden (‘predicted values’) te berekenen en te visualiseren.\n\nlme4 voor specificeren en schatten van multilevel modellen.\n\nlmerTest voor testen van significatie van multilevel modellen."
  },
  {
    "objectID": "posts/2023-06-27-multilevel/Multilevel.html#set-up-instructies",
    "href": "posts/2023-06-27-multilevel/Multilevel.html#set-up-instructies",
    "title": "Introductie op multilevel analyse in R met lme4 en tidyverse",
    "section": "",
    "text": "Voor deze workshop is het nodig dat je:\n\nDat je de laatste versie binnenhaalt van R hier (selecteer een locatie bij je in de buurt).\n\nVolg de instructies om R op jouw computer te installeren.\n\nDownload RStudio (vrije versie) hier.\n\nVolg de instructies om RStudio op jouw computer te installeren.\n\nInstalleer de R pakkettem genoemd onder.\n\nOpen RStudio en ga naarTop menu &gt; Tools &gt; Install packages....\nInstalleer elk pakket van de lijst.\n\nBreng de laptop mee naar de workshop.\nDownload de workshop project folder hier\n\nKlik op de link &gt; Klik op de groene Clone knop &gt; Download ZIP &gt; Dan ‘unzip’ de folder op jouw computer.\nIk zou dat de in de klas doen aan het begin van de workshop zodat je de laatste versie van folder download.\n\n\nEen keer in de klas, ga naar de workshop project folder en dubbelklik op de workshop R project file (Multilevel_with_R.Rproj). Dit zal RStudio openen."
  },
  {
    "objectID": "posts/2023-06-27-multilevel/Multilevel.html#packages",
    "href": "posts/2023-06-27-multilevel/Multilevel.html#packages",
    "title": "Introductie op multilevel analyse in R met lme4 en tidyverse",
    "section": "",
    "text": "Algemeen:\n\nbroom om de model resultaten als tidy tibbles te zien.\n\nmagrittr voor ‘pipe’ en gerelateerde handelingen.\n\ntidyverse.\n\n\nOm multilevel modellen te draaien en de resultaten te zien:\n\nbroom.mixed.\n\ncar voor testen van significantie.\n\nggeffects om de geschatte waarden (‘predicted values’) te berekenen en te visualiseren.\n\nlme4 voor specificeren en schatten van multilevel modellen.\n\nlmerTest voor testen van significatie van multilevel modellen."
  },
  {
    "objectID": "posts/2023-07-03-regressie_analyses/Moderatie.html",
    "href": "posts/2023-07-03-regressie_analyses/Moderatie.html",
    "title": "Enkelvoudige, meervoudige en moderatie regressieanalyse",
    "section": "",
    "text": "Onlangs heb ik een groep studenten van Forensische Orthopedagogiek van de Universeriteit van Amsterdam begeleid bij het schrijven van hun masterscriptie. Ze hadden met elkaar een sample verzameld. Ze moesten op zoek naar de relatie tussen cannabisgebruik en online-blootstelling aan cannabis. Daarboven op moesten ze op zoek naar of bepaalde variabelen deze invloed beïnvloeden. Daarvoor moesten ze een moderatieanalyse uitvoeren. In deze korte blog laat ik zien hoe ik deze techniek zou uitvoeren. Dat heb ik de studenten ook laten zien en hier deel ik het met anderen. Eerst introduceer ik heel kort moderatie analyse als onderdeel van regressieanalyse en wat de achterliggende ideeën ervan. Dan ligt ik kort de dataset uit die gebruikt is. Dan laat ik zien welke pakketten van R ik hierbij heb gebruikt en hoe deze pakketten ons werk hierbij kunnen ondersteunen. Dan laat ik twee soorten modernatieanalyses zien. Één analyse waarbij de moderator een categoriale variabele en een andere analyse waarbij de moderator een continue variabele is. Dan laat ik kort de tekst zien hoe de tekst er dan uitziet hier."
  },
  {
    "objectID": "posts/2023-07-03-regressie_analyses/Moderatie.html#enkelvoudige-regressie",
    "href": "posts/2023-07-03-regressie_analyses/Moderatie.html#enkelvoudige-regressie",
    "title": "Enkelvoudige, meervoudige en moderatie regressieanalyse",
    "section": "Enkelvoudige regressie",
    "text": "Enkelvoudige regressie\n\nmodel4 &lt;- lm(can ~ online_can_c, data=df) \n\n\nsumm(model4)\n\n\n\n\n\nObservations\n136 (17 missing obs. deleted)\n\n\nDependent variable\ncan\n\n\nType\nOLS linear regression\n\n\n\n\n\n\n\n\nF(1,134)\n14.56\n\n\nR²\n0.10\n\n\nAdj. R²\n0.09\n\n\n\n\n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n0.97\n0.15\n6.55\n0.00\n\n\nonline_can_c\n0.33\n0.09\n3.82\n0.00\n\n\n\n Standard errors: OLS"
  },
  {
    "objectID": "posts/2023-07-03-regressie_analyses/Moderatie.html#meervoudige-regressie",
    "href": "posts/2023-07-03-regressie_analyses/Moderatie.html#meervoudige-regressie",
    "title": "Enkelvoudige, meervoudige en moderatie regressieanalyse",
    "section": "Meervoudige regressie",
    "text": "Meervoudige regressie\n\nmodel5 &lt;- lm(can ~ online_can_c + sm_freq_c, data=df) \n\n\nsumm(model5)\n\n\n\n\n\nObservations\n136 (17 missing obs. deleted)\n\n\nDependent variable\ncan\n\n\nType\nOLS linear regression\n\n\n\n\n\n\n\n\nF(2,133)\n7.27\n\n\nR²\n0.10\n\n\nAdj. R²\n0.09\n\n\n\n\n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n0.97\n0.15\n6.52\n0.00\n\n\nonline_can_c\n0.32\n0.09\n3.56\n0.00\n\n\nsm_freq_c\n0.07\n0.24\n0.30\n0.76\n\n\n\n Standard errors: OLS"
  },
  {
    "objectID": "posts/2023-07-03-regressie_analyses/Moderatie.html#gemodereerde-multiple-regressie",
    "href": "posts/2023-07-03-regressie_analyses/Moderatie.html#gemodereerde-multiple-regressie",
    "title": "Enkelvoudige, meervoudige en moderatie regressieanalyse",
    "section": "Gemodereerde multiple regressie",
    "text": "Gemodereerde multiple regressie\n\nmodel6 &lt;-lm(can ~ online_can_c + sm_freq_c + online_can_c*sm_freq_c, data=df)\nsumm(model6)\n\n\n\n\n\nObservations\n136 (17 missing obs. deleted)\n\n\nDependent variable\ncan\n\n\nType\nOLS linear regression\n\n\n\n\n\n\n\n\nF(3,132)\n4.94\n\n\nR²\n0.10\n\n\nAdj. R²\n0.08\n\n\n\n\n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n0.94\n0.16\n6.08\n0.00\n\n\nonline_can_c\n0.31\n0.09\n3.27\n0.00\n\n\nsm_freq_c\n0.06\n0.24\n0.23\n0.82\n\n\nonline_can_c:sm_freq_c\n0.08\n0.14\n0.59\n0.56\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\nAls we deze modellen naast elkaar zetten, zien we dat de meervoudige en de moderatieanalyse geen significantie opleveren voor de variabelen die zijn toegevoegd.\n\nexport_summs(model4, model5, model6)\n\n\n\n\n\nModel 1\nModel 2\nModel 3\n\n\n(Intercept)\n0.97 ***\n0.97 ***\n0.94 ***\n\n\n\n(0.15)   \n(0.15)   \n(0.16)   \n\n\nonline_can_c\n0.33 ***\n0.32 ***\n0.31 ** \n\n\n\n(0.09)   \n(0.09)   \n(0.09)   \n\n\nsm_freq_c\n       \n0.07    \n0.06    \n\n\n\n       \n(0.24)   \n(0.24)   \n\n\nonline_can_c:sm_freq_c\n       \n       \n0.08    \n\n\n\n       \n       \n(0.14)   \n\n\nN\n136       \n136       \n136       \n\n\nR2\n0.10    \n0.10    \n0.10    \n\n\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05.\n\n\n\n\n\n\n\nOok de grafiek laat zien dat de regressielijnen voor de verschillende groepen vrijwel gelijk lopen (geen interactieeffect).\n\nlibrary(interactions)\n\ninteract_plot(model6, \"online_can_c\", \"sm_freq_c\", plot.points = TRUE)"
  },
  {
    "objectID": "posts/2023-07-03-regressie_analyses/Moderatie.html#enkelvoudige-regressie-1",
    "href": "posts/2023-07-03-regressie_analyses/Moderatie.html#enkelvoudige-regressie-1",
    "title": "Enkelvoudige, meervoudige en moderatie regressieanalyse",
    "section": "Enkelvoudige regressie",
    "text": "Enkelvoudige regressie\nDe eerste regressie ziet er zo uit:\n\nmodel1 &lt;- lm(can ~ online_can, data=df) \n\nDit zijn de resultaten. jstools geeft deze resultaten helder weer.\n\nsumm(model1)\n\n\n\n\n\nObservations\n136 (17 missing obs. deleted)\n\n\nDependent variable\ncan\n\n\nType\nOLS linear regression\n\n\n\n\n\n\n\n\nF(1,134)\n14.56\n\n\nR²\n0.10\n\n\nAdj. R²\n0.09\n\n\n\n\n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n0.49\n0.19\n2.53\n0.01\n\n\nonline_can\n0.33\n0.09\n3.82\n0.00\n\n\n\n Standard errors: OLS"
  },
  {
    "objectID": "posts/2023-07-03-regressie_analyses/Moderatie.html#meervoudige-regressie-1",
    "href": "posts/2023-07-03-regressie_analyses/Moderatie.html#meervoudige-regressie-1",
    "title": "Enkelvoudige, meervoudige en moderatie regressieanalyse",
    "section": "Meervoudige regressie",
    "text": "Meervoudige regressie\nVervolgens voeg ik er een onafhankelijke variabele aan toe:\n\nmodel2 &lt;- lm(can ~ online_can + ouders_can, data=df) \n\nEn dat ziet er zo uit:\n\nsumm(model2)\n\n\n\n\n\nObservations\n136 (17 missing obs. deleted)\n\n\nDependent variable\ncan\n\n\nType\nOLS linear regression\n\n\n\n\n\n\n\n\nF(2,133)\n10.91\n\n\nR²\n0.14\n\n\nAdj. R²\n0.13\n\n\n\n\n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n0.29\n0.21\n1.39\n0.17\n\n\nonline_can\n0.29\n0.09\n3.41\n0.00\n\n\nouders_canwel\n0.82\n0.32\n2.58\n0.01\n\n\n\n Standard errors: OLS"
  },
  {
    "objectID": "posts/2023-07-03-regressie_analyses/Moderatie.html#gemodereerde-multiple-regressie-1",
    "href": "posts/2023-07-03-regressie_analyses/Moderatie.html#gemodereerde-multiple-regressie-1",
    "title": "Enkelvoudige, meervoudige en moderatie regressieanalyse",
    "section": "Gemodereerde multiple regressie",
    "text": "Gemodereerde multiple regressie\nTot slot voer ik de moderatieanalyse uit, zoals hier:\n\nmodel3 &lt;-lm(can ~ online_can + ouders_can + online_can*ouders_can, data=df)\n\nMet dit als resultaat:\n\nsumm(model3)\n\n\n\n\n\nObservations\n136 (17 missing obs. deleted)\n\n\nDependent variable\ncan\n\n\nType\nOLS linear regression\n\n\n\n\n\n\n\n\nF(3,132)\n9.63\n\n\nR²\n0.18\n\n\nAdj. R²\n0.16\n\n\n\n\n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n0.45\n0.21\n2.11\n0.04\n\n\nonline_can\n0.16\n0.10\n1.66\n0.10\n\n\nouders_canwel\n0.02\n0.45\n0.04\n0.97\n\n\nonline_can:ouders_canwel\n0.47\n0.19\n2.49\n0.01\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\nWe kunnen de resultaten in een duidelijke tabel terug zien. De modellen netjes naast elkaar, met sterkte van de coëfficienten, met p-waardes en met verklaarde variantie. We zien dat model 3 duidelijk het sterkste is.\n\nexport_summs(model1, model2, model3)\n\n\n\n\n\nModel 1\nModel 2\nModel 3\n\n\n(Intercept)\n0.49 *  \n0.29    \n0.45 *\n\n\n\n(0.19)   \n(0.21)   \n(0.21) \n\n\nonline_can\n0.33 ***\n0.29 ***\n0.16  \n\n\n\n(0.09)   \n(0.09)   \n(0.10) \n\n\nouders_canwel\n       \n0.82 *  \n0.02  \n\n\n\n       \n(0.32)   \n(0.45) \n\n\nonline_can:ouders_canwel\n       \n       \n0.47 *\n\n\n\n       \n       \n(0.19) \n\n\nN\n136       \n136       \n136     \n\n\nR2\n0.10    \n0.14    \n0.18  \n\n\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05.\n\n\n\n\n\n\n\nHet interactieffect is in deze grafiek goed te zien:\n\nlibrary(interactions)\n\ninteract_plot(model3, \"online_can\", \"ouders_can\", plot.points = TRUE)"
  },
  {
    "objectID": "posts/2023-08-02-multilevel/multilevel.html",
    "href": "posts/2023-08-02-multilevel/multilevel.html",
    "title": "Introductie op multilevel analyse in R met lme4 en tidyverse",
    "section": "",
    "text": "Ik had al een tijdje geen multilevel-analyses meer gedaan en wilde dat weer eens doen met aangepaste technieken. Ben zelf ‘opgeleid’ met het prachtige werk van de multilevelgroep uit Bristol (Goldstein, Rabash, Brown e.a.) en ook dat van Joop Hox, die ik op dit gebied van dichtbij heb meegemaakt. Toen ik wat rondkeek, kwam ik de blog van Rafaella Vacca (Universiteit van Milaan) tegen, die ik inzichtelijk en vernieuwend vond. Hij werkt met het pakket lme4 en tidyverse. Hieronder vind je mijn bewerkte versie. Dank je wel Rafaella.\nHier kun je de blog vinden en hier de codes."
  }
]