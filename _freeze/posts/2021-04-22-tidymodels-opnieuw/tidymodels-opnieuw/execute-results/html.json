{
  "hash": "4e00dccb906bde42ae7b878f391cf03b",
  "result": {
    "markdown": "---\ntitle: \"Tidymodels opnieuw\"\ndescription: |\n    Blog van Rebecca Barter onder de titel 'Tidymodels: tidy machine learning in R'\nauthor: \"Rebecca Barter, bewerking Harrie Jonkman\"\ndate: \"04-22-2021\"\ncategories: [analyse]\nimage: \"Screenshot.png\"\n---\n\n\n\n# Tidymodels: een nette en consistente manier om met machine learning in R te werken\n\n`Tidyverse` is misschien wel een van de grootste successen van R de laatste jaren. Het is een basispakket (een suite van pakketten) waarmee je heel veel statistiscche bewerkingen goed en betrekkelijk eenvoudig kunt uitvoeren. De laatste jaren is `tidymodels` ontwikkeld dat voor het modelleren van data het basispakket moet worden en het ontwikkelt zich vergelijkbaar de gereedschapskist van `tidyverse` maar dan op het gebied van machine learning.\n\nWaarom `tidymodels`? Nou, het blijkt dat R een consistentieprobleem heeft. Omdat alles rondom machine learning door verschillende mensen is gemaakt, allemaal met verschillende principes, heeft alles een net iets andere interface gekregenen. Om de boel in lijn te houden is onderhand een frustrerende bezigheid. Enkele jaren geleden ontwikkelde Max Kuhn (nu bij RStudio in dienst) het `caret` R-pakket, dat is zo'n uniforme interface voor een groot aantal machine learning-modellen die er in R zijn. Het programma `caret` bestaat nog steeds, was in veel opzichten geweldig en is nog steeds goed te gebruiken. Maar in andere opzichten is het beperkt. Zo kan het vrij traag zijn, zelfs bij gebuik van data in bescheiden omvang.\n\n`caret` was een geweldig uitgangspunt, dus RStudio heeft Max Kuhn ingehuurd om te werken aan een `tidy` versie van `caret`. Hij en veel anderen ontwikkelden de afgelopen jaren `tidymodels`.`tidymodels` is al een paar jaar in ontwikkeling en delen ervan waren al eerder uitgebracht. Die volledige versie is in het voorjaar van 2020 gepresenteerd en Barter schreef vlak daarvoor deze tutoriol. Ondertussen is het voldoende ontwikkeld als je het wil leren! Terwijl `caret` niet verder ontwikkeld wordt (je kunt `caret` blijven gebruiken en je bestaande `caret`-code werkt nog steeds, het pakket wordt alleen niet onderhouden), zal `tidymodels` het uiteindelijk overbodig maken.\n\nDeze tutorial van Barter is gebaseerd op Alison Hill's dia's van `Introduction to Machine Learning with the Tidyverse`, die alle dia's bevat voor de cursus die ze met Garrett Grolemund voor RStudio heeft voorbereid::conf(2020), en Edgar Ruiz's `Gentle introduction to tidymodels` op de website van RStudio. In deze tutorial gaat zij ervan uit dat de gebruiker bepaalde basiskennis heeft, voornamelijk omgaan met `dplyr` (b.v. piping `%>%` en een functie zoals `mutate()`). \n\n## Wat is `tidymodels`?\nNet als `tidyverse`, dat uit verschillende pakketten bestaat zoals `ggplot2` en `dplyr`, zitten er ook in `tidymodels` enkele kernpakketten, zoals\n\n- `rsample`: voor het uit elkaar halen van een datasample (b.v. train/test of cross-validatie);\n\n- `recipes`: voor pre-procesfuncties;\n\n- `parsnip`: voor het specificeren van het model;\n\n- `yardstick`: voor het evalueren van van het model;\n\n- `tune`: voor het afstemmen van parameters;\n\n- `workflow`: om alles samen te brengen.\n\nNet zoals je de hele suite aan pakketten van `tidyverse` kunt binnenhalen door `library(tidyverse)` in te tikken. `tidymodels` bestaat dus uit verschillende pakketten en soms zal ik hieronder individuele pakketten noemen.\n\n## Eerst maar eens de boel klaarzetten\n\nAls je deze pakketten nog niet hebt geïnstalleerd, moet je dat wel eerst doen (slechts één keer) door `install.packages(\"tidymodels\")` te gebruiken. Vervolgens laad je bepaalde bibliotheken: `tidymodels` en `tidyverse`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# laad de relevante tidymodels pakketten\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'tidymodels' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n-- Attaching packages -------------------------------------- tidymodels 0.2.0 --\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nv broom        0.8.0     v recipes      0.2.0\nv dials        1.0.0     v rsample      0.1.1\nv dplyr        1.0.9     v tibble       3.1.7\nv ggplot2      3.3.6     v tidyr        1.2.0\nv infer        1.0.2     v tune         0.2.0\nv modeldata    0.1.1     v workflows    0.2.6\nv parsnip      1.0.0     v workflowsets 0.2.1\nv purrr        0.3.4     v yardstick    1.0.0\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'broom' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'dials' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'scales' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'dplyr' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'ggplot2' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'infer' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'parsnip' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'recipes' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'tibble' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'tidyr' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'tune' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'workflows' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'workflowsets' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'yardstick' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n-- Conflicts ----------------------------------------- tidymodels_conflicts() --\nx purrr::discard() masks scales::discard()\nx dplyr::filter()  masks stats::filter()\nx dplyr::lag()     masks stats::lag()\nx recipes::step()  masks stats::step()\n* Use suppressPackageStartupMessages() to eliminate package startup messages\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'tidyverse' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nv readr   2.1.2     v forcats 0.5.1\nv stringr 1.4.1     \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'readr' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'stringr' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx readr::col_factor() masks scales::col_factor()\nx purrr::discard()    masks scales::discard()\nx dplyr::filter()     masks stats::filter()\nx stringr::fixed()    masks recipes::fixed()\nx dplyr::lag()        masks stats::lag()\nx readr::spec()       masks yardstick::spec()\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# laad de Pima Indians dataset van de mlbench dataset\nlibrary(mlbench)\ndata(PimaIndiansDiabetes)\n#Wat zit erin\nglimpse(PimaIndiansDiabetes)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 768\nColumns: 9\n$ pregnant <dbl> 6, 1, 8, 1, 0, 5, 3, 10, 2, 8, 4, 10, 10, 1, 5, 7, 0, 7, 1, 1~\n$ glucose  <dbl> 148, 85, 183, 89, 137, 116, 78, 115, 197, 125, 110, 168, 139,~\n$ pressure <dbl> 72, 66, 64, 66, 40, 74, 50, 0, 70, 96, 92, 74, 80, 60, 72, 0,~\n$ triceps  <dbl> 35, 29, 0, 23, 35, 0, 32, 0, 45, 0, 0, 0, 0, 23, 19, 0, 47, 0~\n$ insulin  <dbl> 0, 0, 0, 94, 168, 0, 88, 0, 543, 0, 0, 0, 0, 846, 175, 0, 230~\n$ mass     <dbl> 33.6, 26.6, 23.3, 28.1, 43.1, 25.6, 31.0, 35.3, 30.5, 0.0, 37~\n$ pedigree <dbl> 0.627, 0.351, 0.672, 0.167, 2.288, 0.201, 0.248, 0.134, 0.158~\n$ age      <dbl> 50, 31, 32, 21, 33, 30, 26, 29, 53, 54, 30, 34, 57, 59, 51, 3~\n$ diabetes <fct> pos, neg, pos, neg, pos, neg, pos, neg, pos, pos, neg, pos, n~\n```\n:::\n:::\n\n\n\nWe zullen gebruik maken van de Pima Indian Women's diabetes-dataset dat informatie bevat over de diabetes status van 768 Pima Indian vrouwen(`diabetes`). In de dataset zitten daarnaast enkele predictoren zoals het aantal zwangerschappen (`pregnant`), concentratie glucose (`glucose`), diastolische bloeddruk (`pressure`), triceps huidplooidikte (`triceps`), 2 uur serum insuline (`insuline`), BMI (`mass`), diabetes stamboom functie (`pedigree`) en hun leeftijd (`age`). Voor het geval je het je afvraagt, de Pima Indianen zijn een groep indianen die leven in een gebied dat bestaat uit wat nu centraal en zuidelijk Arizona is. De korte naam \"Pima\" zou afkomstig zijn van een zinsnede die \"ik weet het niet\" betekent, die ze herhaaldelijk gebruikten in hun eerste ontmoetingen met Spaanse kolonisten. Wikipedia bedankt!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Geef de dataset een kortere naam omdat we wat lui zijn\ndiabetes_orig <- PimaIndiansDiabetes\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndiabetes_orig\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    pregnant glucose pressure triceps insulin mass pedigree age diabetes\n1          6     148       72      35       0 33.6    0.627  50      pos\n2          1      85       66      29       0 26.6    0.351  31      neg\n3          8     183       64       0       0 23.3    0.672  32      pos\n4          1      89       66      23      94 28.1    0.167  21      neg\n5          0     137       40      35     168 43.1    2.288  33      pos\n6          5     116       74       0       0 25.6    0.201  30      neg\n7          3      78       50      32      88 31.0    0.248  26      pos\n8         10     115        0       0       0 35.3    0.134  29      neg\n9          2     197       70      45     543 30.5    0.158  53      pos\n10         8     125       96       0       0  0.0    0.232  54      pos\n11         4     110       92       0       0 37.6    0.191  30      neg\n12        10     168       74       0       0 38.0    0.537  34      pos\n13        10     139       80       0       0 27.1    1.441  57      neg\n14         1     189       60      23     846 30.1    0.398  59      pos\n15         5     166       72      19     175 25.8    0.587  51      pos\n16         7     100        0       0       0 30.0    0.484  32      pos\n17         0     118       84      47     230 45.8    0.551  31      pos\n18         7     107       74       0       0 29.6    0.254  31      pos\n19         1     103       30      38      83 43.3    0.183  33      neg\n20         1     115       70      30      96 34.6    0.529  32      pos\n21         3     126       88      41     235 39.3    0.704  27      neg\n22         8      99       84       0       0 35.4    0.388  50      neg\n23         7     196       90       0       0 39.8    0.451  41      pos\n24         9     119       80      35       0 29.0    0.263  29      pos\n25        11     143       94      33     146 36.6    0.254  51      pos\n26        10     125       70      26     115 31.1    0.205  41      pos\n27         7     147       76       0       0 39.4    0.257  43      pos\n28         1      97       66      15     140 23.2    0.487  22      neg\n29        13     145       82      19     110 22.2    0.245  57      neg\n30         5     117       92       0       0 34.1    0.337  38      neg\n31         5     109       75      26       0 36.0    0.546  60      neg\n32         3     158       76      36     245 31.6    0.851  28      pos\n33         3      88       58      11      54 24.8    0.267  22      neg\n34         6      92       92       0       0 19.9    0.188  28      neg\n35        10     122       78      31       0 27.6    0.512  45      neg\n36         4     103       60      33     192 24.0    0.966  33      neg\n37        11     138       76       0       0 33.2    0.420  35      neg\n38         9     102       76      37       0 32.9    0.665  46      pos\n39         2      90       68      42       0 38.2    0.503  27      pos\n40         4     111       72      47     207 37.1    1.390  56      pos\n41         3     180       64      25      70 34.0    0.271  26      neg\n42         7     133       84       0       0 40.2    0.696  37      neg\n43         7     106       92      18       0 22.7    0.235  48      neg\n44         9     171      110      24     240 45.4    0.721  54      pos\n45         7     159       64       0       0 27.4    0.294  40      neg\n46         0     180       66      39       0 42.0    1.893  25      pos\n47         1     146       56       0       0 29.7    0.564  29      neg\n48         2      71       70      27       0 28.0    0.586  22      neg\n49         7     103       66      32       0 39.1    0.344  31      pos\n50         7     105        0       0       0  0.0    0.305  24      neg\n51         1     103       80      11      82 19.4    0.491  22      neg\n52         1     101       50      15      36 24.2    0.526  26      neg\n53         5      88       66      21      23 24.4    0.342  30      neg\n54         8     176       90      34     300 33.7    0.467  58      pos\n55         7     150       66      42     342 34.7    0.718  42      neg\n56         1      73       50      10       0 23.0    0.248  21      neg\n57         7     187       68      39     304 37.7    0.254  41      pos\n58         0     100       88      60     110 46.8    0.962  31      neg\n59         0     146       82       0       0 40.5    1.781  44      neg\n60         0     105       64      41     142 41.5    0.173  22      neg\n61         2      84        0       0       0  0.0    0.304  21      neg\n62         8     133       72       0       0 32.9    0.270  39      pos\n63         5      44       62       0       0 25.0    0.587  36      neg\n64         2     141       58      34     128 25.4    0.699  24      neg\n65         7     114       66       0       0 32.8    0.258  42      pos\n66         5      99       74      27       0 29.0    0.203  32      neg\n67         0     109       88      30       0 32.5    0.855  38      pos\n68         2     109       92       0       0 42.7    0.845  54      neg\n69         1      95       66      13      38 19.6    0.334  25      neg\n70         4     146       85      27     100 28.9    0.189  27      neg\n71         2     100       66      20      90 32.9    0.867  28      pos\n72         5     139       64      35     140 28.6    0.411  26      neg\n73        13     126       90       0       0 43.4    0.583  42      pos\n74         4     129       86      20     270 35.1    0.231  23      neg\n75         1      79       75      30       0 32.0    0.396  22      neg\n76         1       0       48      20       0 24.7    0.140  22      neg\n77         7      62       78       0       0 32.6    0.391  41      neg\n78         5      95       72      33       0 37.7    0.370  27      neg\n79         0     131        0       0       0 43.2    0.270  26      pos\n80         2     112       66      22       0 25.0    0.307  24      neg\n81         3     113       44      13       0 22.4    0.140  22      neg\n82         2      74        0       0       0  0.0    0.102  22      neg\n83         7      83       78      26      71 29.3    0.767  36      neg\n84         0     101       65      28       0 24.6    0.237  22      neg\n85         5     137      108       0       0 48.8    0.227  37      pos\n86         2     110       74      29     125 32.4    0.698  27      neg\n87        13     106       72      54       0 36.6    0.178  45      neg\n88         2     100       68      25      71 38.5    0.324  26      neg\n89        15     136       70      32     110 37.1    0.153  43      pos\n90         1     107       68      19       0 26.5    0.165  24      neg\n91         1      80       55       0       0 19.1    0.258  21      neg\n92         4     123       80      15     176 32.0    0.443  34      neg\n93         7      81       78      40      48 46.7    0.261  42      neg\n94         4     134       72       0       0 23.8    0.277  60      pos\n95         2     142       82      18      64 24.7    0.761  21      neg\n96         6     144       72      27     228 33.9    0.255  40      neg\n97         2      92       62      28       0 31.6    0.130  24      neg\n98         1      71       48      18      76 20.4    0.323  22      neg\n99         6      93       50      30      64 28.7    0.356  23      neg\n100        1     122       90      51     220 49.7    0.325  31      pos\n101        1     163       72       0       0 39.0    1.222  33      pos\n102        1     151       60       0       0 26.1    0.179  22      neg\n103        0     125       96       0       0 22.5    0.262  21      neg\n104        1      81       72      18      40 26.6    0.283  24      neg\n105        2      85       65       0       0 39.6    0.930  27      neg\n106        1     126       56      29     152 28.7    0.801  21      neg\n107        1      96      122       0       0 22.4    0.207  27      neg\n108        4     144       58      28     140 29.5    0.287  37      neg\n109        3      83       58      31      18 34.3    0.336  25      neg\n110        0      95       85      25      36 37.4    0.247  24      pos\n111        3     171       72      33     135 33.3    0.199  24      pos\n112        8     155       62      26     495 34.0    0.543  46      pos\n113        1      89       76      34      37 31.2    0.192  23      neg\n114        4      76       62       0       0 34.0    0.391  25      neg\n115        7     160       54      32     175 30.5    0.588  39      pos\n116        4     146       92       0       0 31.2    0.539  61      pos\n117        5     124       74       0       0 34.0    0.220  38      pos\n118        5      78       48       0       0 33.7    0.654  25      neg\n119        4      97       60      23       0 28.2    0.443  22      neg\n120        4      99       76      15      51 23.2    0.223  21      neg\n121        0     162       76      56     100 53.2    0.759  25      pos\n122        6     111       64      39       0 34.2    0.260  24      neg\n123        2     107       74      30     100 33.6    0.404  23      neg\n124        5     132       80       0       0 26.8    0.186  69      neg\n125        0     113       76       0       0 33.3    0.278  23      pos\n126        1      88       30      42      99 55.0    0.496  26      pos\n127        3     120       70      30     135 42.9    0.452  30      neg\n128        1     118       58      36      94 33.3    0.261  23      neg\n129        1     117       88      24     145 34.5    0.403  40      pos\n130        0     105       84       0       0 27.9    0.741  62      pos\n131        4     173       70      14     168 29.7    0.361  33      pos\n132        9     122       56       0       0 33.3    1.114  33      pos\n133        3     170       64      37     225 34.5    0.356  30      pos\n134        8      84       74      31       0 38.3    0.457  39      neg\n135        2      96       68      13      49 21.1    0.647  26      neg\n136        2     125       60      20     140 33.8    0.088  31      neg\n137        0     100       70      26      50 30.8    0.597  21      neg\n138        0      93       60      25      92 28.7    0.532  22      neg\n139        0     129       80       0       0 31.2    0.703  29      neg\n140        5     105       72      29     325 36.9    0.159  28      neg\n141        3     128       78       0       0 21.1    0.268  55      neg\n142        5     106       82      30       0 39.5    0.286  38      neg\n143        2     108       52      26      63 32.5    0.318  22      neg\n144       10     108       66       0       0 32.4    0.272  42      pos\n145        4     154       62      31     284 32.8    0.237  23      neg\n146        0     102       75      23       0  0.0    0.572  21      neg\n147        9      57       80      37       0 32.8    0.096  41      neg\n148        2     106       64      35     119 30.5    1.400  34      neg\n149        5     147       78       0       0 33.7    0.218  65      neg\n150        2      90       70      17       0 27.3    0.085  22      neg\n151        1     136       74      50     204 37.4    0.399  24      neg\n152        4     114       65       0       0 21.9    0.432  37      neg\n153        9     156       86      28     155 34.3    1.189  42      pos\n154        1     153       82      42     485 40.6    0.687  23      neg\n155        8     188       78       0       0 47.9    0.137  43      pos\n156        7     152       88      44       0 50.0    0.337  36      pos\n157        2      99       52      15      94 24.6    0.637  21      neg\n158        1     109       56      21     135 25.2    0.833  23      neg\n159        2      88       74      19      53 29.0    0.229  22      neg\n160       17     163       72      41     114 40.9    0.817  47      pos\n161        4     151       90      38       0 29.7    0.294  36      neg\n162        7     102       74      40     105 37.2    0.204  45      neg\n163        0     114       80      34     285 44.2    0.167  27      neg\n164        2     100       64      23       0 29.7    0.368  21      neg\n165        0     131       88       0       0 31.6    0.743  32      pos\n166        6     104       74      18     156 29.9    0.722  41      pos\n167        3     148       66      25       0 32.5    0.256  22      neg\n168        4     120       68       0       0 29.6    0.709  34      neg\n169        4     110       66       0       0 31.9    0.471  29      neg\n170        3     111       90      12      78 28.4    0.495  29      neg\n171        6     102       82       0       0 30.8    0.180  36      pos\n172        6     134       70      23     130 35.4    0.542  29      pos\n173        2      87        0      23       0 28.9    0.773  25      neg\n174        1      79       60      42      48 43.5    0.678  23      neg\n175        2      75       64      24      55 29.7    0.370  33      neg\n176        8     179       72      42     130 32.7    0.719  36      pos\n177        6      85       78       0       0 31.2    0.382  42      neg\n178        0     129      110      46     130 67.1    0.319  26      pos\n179        5     143       78       0       0 45.0    0.190  47      neg\n180        5     130       82       0       0 39.1    0.956  37      pos\n181        6      87       80       0       0 23.2    0.084  32      neg\n182        0     119       64      18      92 34.9    0.725  23      neg\n183        1       0       74      20      23 27.7    0.299  21      neg\n184        5      73       60       0       0 26.8    0.268  27      neg\n185        4     141       74       0       0 27.6    0.244  40      neg\n186        7     194       68      28       0 35.9    0.745  41      pos\n187        8     181       68      36     495 30.1    0.615  60      pos\n188        1     128       98      41      58 32.0    1.321  33      pos\n189        8     109       76      39     114 27.9    0.640  31      pos\n190        5     139       80      35     160 31.6    0.361  25      pos\n191        3     111       62       0       0 22.6    0.142  21      neg\n192        9     123       70      44      94 33.1    0.374  40      neg\n193        7     159       66       0       0 30.4    0.383  36      pos\n194       11     135        0       0       0 52.3    0.578  40      pos\n195        8      85       55      20       0 24.4    0.136  42      neg\n196        5     158       84      41     210 39.4    0.395  29      pos\n197        1     105       58       0       0 24.3    0.187  21      neg\n198        3     107       62      13      48 22.9    0.678  23      pos\n199        4     109       64      44      99 34.8    0.905  26      pos\n200        4     148       60      27     318 30.9    0.150  29      pos\n201        0     113       80      16       0 31.0    0.874  21      neg\n202        1     138       82       0       0 40.1    0.236  28      neg\n203        0     108       68      20       0 27.3    0.787  32      neg\n204        2      99       70      16      44 20.4    0.235  27      neg\n205        6     103       72      32     190 37.7    0.324  55      neg\n206        5     111       72      28       0 23.9    0.407  27      neg\n207        8     196       76      29     280 37.5    0.605  57      pos\n208        5     162      104       0       0 37.7    0.151  52      pos\n209        1      96       64      27      87 33.2    0.289  21      neg\n210        7     184       84      33       0 35.5    0.355  41      pos\n211        2      81       60      22       0 27.7    0.290  25      neg\n212        0     147       85      54       0 42.8    0.375  24      neg\n213        7     179       95      31       0 34.2    0.164  60      neg\n214        0     140       65      26     130 42.6    0.431  24      pos\n215        9     112       82      32     175 34.2    0.260  36      pos\n216       12     151       70      40     271 41.8    0.742  38      pos\n217        5     109       62      41     129 35.8    0.514  25      pos\n218        6     125       68      30     120 30.0    0.464  32      neg\n219        5      85       74      22       0 29.0    1.224  32      pos\n220        5     112       66       0       0 37.8    0.261  41      pos\n221        0     177       60      29     478 34.6    1.072  21      pos\n222        2     158       90       0       0 31.6    0.805  66      pos\n223        7     119        0       0       0 25.2    0.209  37      neg\n224        7     142       60      33     190 28.8    0.687  61      neg\n225        1     100       66      15      56 23.6    0.666  26      neg\n226        1      87       78      27      32 34.6    0.101  22      neg\n227        0     101       76       0       0 35.7    0.198  26      neg\n228        3     162       52      38       0 37.2    0.652  24      pos\n229        4     197       70      39     744 36.7    2.329  31      neg\n230        0     117       80      31      53 45.2    0.089  24      neg\n231        4     142       86       0       0 44.0    0.645  22      pos\n232        6     134       80      37     370 46.2    0.238  46      pos\n233        1      79       80      25      37 25.4    0.583  22      neg\n234        4     122       68       0       0 35.0    0.394  29      neg\n235        3      74       68      28      45 29.7    0.293  23      neg\n236        4     171       72       0       0 43.6    0.479  26      pos\n237        7     181       84      21     192 35.9    0.586  51      pos\n238        0     179       90      27       0 44.1    0.686  23      pos\n239        9     164       84      21       0 30.8    0.831  32      pos\n240        0     104       76       0       0 18.4    0.582  27      neg\n241        1      91       64      24       0 29.2    0.192  21      neg\n242        4      91       70      32      88 33.1    0.446  22      neg\n243        3     139       54       0       0 25.6    0.402  22      pos\n244        6     119       50      22     176 27.1    1.318  33      pos\n245        2     146       76      35     194 38.2    0.329  29      neg\n246        9     184       85      15       0 30.0    1.213  49      pos\n247       10     122       68       0       0 31.2    0.258  41      neg\n248        0     165       90      33     680 52.3    0.427  23      neg\n249        9     124       70      33     402 35.4    0.282  34      neg\n250        1     111       86      19       0 30.1    0.143  23      neg\n251        9     106       52       0       0 31.2    0.380  42      neg\n252        2     129       84       0       0 28.0    0.284  27      neg\n253        2      90       80      14      55 24.4    0.249  24      neg\n254        0      86       68      32       0 35.8    0.238  25      neg\n255       12      92       62       7     258 27.6    0.926  44      pos\n256        1     113       64      35       0 33.6    0.543  21      pos\n257        3     111       56      39       0 30.1    0.557  30      neg\n258        2     114       68      22       0 28.7    0.092  25      neg\n259        1     193       50      16     375 25.9    0.655  24      neg\n260       11     155       76      28     150 33.3    1.353  51      pos\n261        3     191       68      15     130 30.9    0.299  34      neg\n262        3     141        0       0       0 30.0    0.761  27      pos\n263        4      95       70      32       0 32.1    0.612  24      neg\n264        3     142       80      15       0 32.4    0.200  63      neg\n265        4     123       62       0       0 32.0    0.226  35      pos\n266        5      96       74      18      67 33.6    0.997  43      neg\n267        0     138        0       0       0 36.3    0.933  25      pos\n268        2     128       64      42       0 40.0    1.101  24      neg\n269        0     102       52       0       0 25.1    0.078  21      neg\n270        2     146        0       0       0 27.5    0.240  28      pos\n271       10     101       86      37       0 45.6    1.136  38      pos\n272        2     108       62      32      56 25.2    0.128  21      neg\n273        3     122       78       0       0 23.0    0.254  40      neg\n274        1      71       78      50      45 33.2    0.422  21      neg\n275       13     106       70       0       0 34.2    0.251  52      neg\n276        2     100       70      52      57 40.5    0.677  25      neg\n277        7     106       60      24       0 26.5    0.296  29      pos\n278        0     104       64      23     116 27.8    0.454  23      neg\n279        5     114       74       0       0 24.9    0.744  57      neg\n280        2     108       62      10     278 25.3    0.881  22      neg\n281        0     146       70       0       0 37.9    0.334  28      pos\n282       10     129       76      28     122 35.9    0.280  39      neg\n283        7     133       88      15     155 32.4    0.262  37      neg\n284        7     161       86       0       0 30.4    0.165  47      pos\n285        2     108       80       0       0 27.0    0.259  52      pos\n286        7     136       74      26     135 26.0    0.647  51      neg\n287        5     155       84      44     545 38.7    0.619  34      neg\n288        1     119       86      39     220 45.6    0.808  29      pos\n289        4      96       56      17      49 20.8    0.340  26      neg\n290        5     108       72      43      75 36.1    0.263  33      neg\n291        0      78       88      29      40 36.9    0.434  21      neg\n292        0     107       62      30      74 36.6    0.757  25      pos\n293        2     128       78      37     182 43.3    1.224  31      pos\n294        1     128       48      45     194 40.5    0.613  24      pos\n295        0     161       50       0       0 21.9    0.254  65      neg\n296        6     151       62      31     120 35.5    0.692  28      neg\n297        2     146       70      38     360 28.0    0.337  29      pos\n298        0     126       84      29     215 30.7    0.520  24      neg\n299       14     100       78      25     184 36.6    0.412  46      pos\n300        8     112       72       0       0 23.6    0.840  58      neg\n301        0     167        0       0       0 32.3    0.839  30      pos\n302        2     144       58      33     135 31.6    0.422  25      pos\n303        5      77       82      41      42 35.8    0.156  35      neg\n304        5     115       98       0       0 52.9    0.209  28      pos\n305        3     150       76       0       0 21.0    0.207  37      neg\n306        2     120       76      37     105 39.7    0.215  29      neg\n307       10     161       68      23     132 25.5    0.326  47      pos\n308        0     137       68      14     148 24.8    0.143  21      neg\n309        0     128       68      19     180 30.5    1.391  25      pos\n310        2     124       68      28     205 32.9    0.875  30      pos\n311        6      80       66      30       0 26.2    0.313  41      neg\n312        0     106       70      37     148 39.4    0.605  22      neg\n313        2     155       74      17      96 26.6    0.433  27      pos\n314        3     113       50      10      85 29.5    0.626  25      neg\n315        7     109       80      31       0 35.9    1.127  43      pos\n316        2     112       68      22      94 34.1    0.315  26      neg\n317        3      99       80      11      64 19.3    0.284  30      neg\n318        3     182       74       0       0 30.5    0.345  29      pos\n319        3     115       66      39     140 38.1    0.150  28      neg\n320        6     194       78       0       0 23.5    0.129  59      pos\n321        4     129       60      12     231 27.5    0.527  31      neg\n322        3     112       74      30       0 31.6    0.197  25      pos\n323        0     124       70      20       0 27.4    0.254  36      pos\n324       13     152       90      33      29 26.8    0.731  43      pos\n325        2     112       75      32       0 35.7    0.148  21      neg\n326        1     157       72      21     168 25.6    0.123  24      neg\n327        1     122       64      32     156 35.1    0.692  30      pos\n328       10     179       70       0       0 35.1    0.200  37      neg\n329        2     102       86      36     120 45.5    0.127  23      pos\n330        6     105       70      32      68 30.8    0.122  37      neg\n331        8     118       72      19       0 23.1    1.476  46      neg\n332        2      87       58      16      52 32.7    0.166  25      neg\n333        1     180        0       0       0 43.3    0.282  41      pos\n334       12     106       80       0       0 23.6    0.137  44      neg\n335        1      95       60      18      58 23.9    0.260  22      neg\n336        0     165       76      43     255 47.9    0.259  26      neg\n337        0     117        0       0       0 33.8    0.932  44      neg\n338        5     115       76       0       0 31.2    0.343  44      pos\n339        9     152       78      34     171 34.2    0.893  33      pos\n340        7     178       84       0       0 39.9    0.331  41      pos\n341        1     130       70      13     105 25.9    0.472  22      neg\n342        1      95       74      21      73 25.9    0.673  36      neg\n343        1       0       68      35       0 32.0    0.389  22      neg\n344        5     122       86       0       0 34.7    0.290  33      neg\n345        8      95       72       0       0 36.8    0.485  57      neg\n346        8     126       88      36     108 38.5    0.349  49      neg\n347        1     139       46      19      83 28.7    0.654  22      neg\n348        3     116        0       0       0 23.5    0.187  23      neg\n349        3      99       62      19      74 21.8    0.279  26      neg\n350        5       0       80      32       0 41.0    0.346  37      pos\n351        4      92       80       0       0 42.2    0.237  29      neg\n352        4     137       84       0       0 31.2    0.252  30      neg\n353        3      61       82      28       0 34.4    0.243  46      neg\n354        1      90       62      12      43 27.2    0.580  24      neg\n355        3      90       78       0       0 42.7    0.559  21      neg\n356        9     165       88       0       0 30.4    0.302  49      pos\n357        1     125       50      40     167 33.3    0.962  28      pos\n358       13     129        0      30       0 39.9    0.569  44      pos\n359       12      88       74      40      54 35.3    0.378  48      neg\n360        1     196       76      36     249 36.5    0.875  29      pos\n361        5     189       64      33     325 31.2    0.583  29      pos\n362        5     158       70       0       0 29.8    0.207  63      neg\n363        5     103      108      37       0 39.2    0.305  65      neg\n364        4     146       78       0       0 38.5    0.520  67      pos\n365        4     147       74      25     293 34.9    0.385  30      neg\n366        5      99       54      28      83 34.0    0.499  30      neg\n367        6     124       72       0       0 27.6    0.368  29      pos\n368        0     101       64      17       0 21.0    0.252  21      neg\n369        3      81       86      16      66 27.5    0.306  22      neg\n370        1     133      102      28     140 32.8    0.234  45      pos\n371        3     173       82      48     465 38.4    2.137  25      pos\n372        0     118       64      23      89  0.0    1.731  21      neg\n373        0      84       64      22      66 35.8    0.545  21      neg\n374        2     105       58      40      94 34.9    0.225  25      neg\n375        2     122       52      43     158 36.2    0.816  28      neg\n376       12     140       82      43     325 39.2    0.528  58      pos\n377        0      98       82      15      84 25.2    0.299  22      neg\n378        1      87       60      37      75 37.2    0.509  22      neg\n379        4     156       75       0       0 48.3    0.238  32      pos\n380        0      93      100      39      72 43.4    1.021  35      neg\n381        1     107       72      30      82 30.8    0.821  24      neg\n382        0     105       68      22       0 20.0    0.236  22      neg\n383        1     109       60       8     182 25.4    0.947  21      neg\n384        1      90       62      18      59 25.1    1.268  25      neg\n385        1     125       70      24     110 24.3    0.221  25      neg\n386        1     119       54      13      50 22.3    0.205  24      neg\n387        5     116       74      29       0 32.3    0.660  35      pos\n388        8     105      100      36       0 43.3    0.239  45      pos\n389        5     144       82      26     285 32.0    0.452  58      pos\n390        3     100       68      23      81 31.6    0.949  28      neg\n391        1     100       66      29     196 32.0    0.444  42      neg\n392        5     166       76       0       0 45.7    0.340  27      pos\n393        1     131       64      14     415 23.7    0.389  21      neg\n394        4     116       72      12      87 22.1    0.463  37      neg\n395        4     158       78       0       0 32.9    0.803  31      pos\n396        2     127       58      24     275 27.7    1.600  25      neg\n397        3      96       56      34     115 24.7    0.944  39      neg\n398        0     131       66      40       0 34.3    0.196  22      pos\n399        3      82       70       0       0 21.1    0.389  25      neg\n400        3     193       70      31       0 34.9    0.241  25      pos\n401        4      95       64       0       0 32.0    0.161  31      pos\n402        6     137       61       0       0 24.2    0.151  55      neg\n403        5     136       84      41      88 35.0    0.286  35      pos\n404        9      72       78      25       0 31.6    0.280  38      neg\n405        5     168       64       0       0 32.9    0.135  41      pos\n406        2     123       48      32     165 42.1    0.520  26      neg\n407        4     115       72       0       0 28.9    0.376  46      pos\n408        0     101       62       0       0 21.9    0.336  25      neg\n409        8     197       74       0       0 25.9    1.191  39      pos\n410        1     172       68      49     579 42.4    0.702  28      pos\n411        6     102       90      39       0 35.7    0.674  28      neg\n412        1     112       72      30     176 34.4    0.528  25      neg\n413        1     143       84      23     310 42.4    1.076  22      neg\n414        1     143       74      22      61 26.2    0.256  21      neg\n415        0     138       60      35     167 34.6    0.534  21      pos\n416        3     173       84      33     474 35.7    0.258  22      pos\n417        1      97       68      21       0 27.2    1.095  22      neg\n418        4     144       82      32       0 38.5    0.554  37      pos\n419        1      83       68       0       0 18.2    0.624  27      neg\n420        3     129       64      29     115 26.4    0.219  28      pos\n421        1     119       88      41     170 45.3    0.507  26      neg\n422        2      94       68      18      76 26.0    0.561  21      neg\n423        0     102       64      46      78 40.6    0.496  21      neg\n424        2     115       64      22       0 30.8    0.421  21      neg\n425        8     151       78      32     210 42.9    0.516  36      pos\n426        4     184       78      39     277 37.0    0.264  31      pos\n427        0      94        0       0       0  0.0    0.256  25      neg\n428        1     181       64      30     180 34.1    0.328  38      pos\n429        0     135       94      46     145 40.6    0.284  26      neg\n430        1      95       82      25     180 35.0    0.233  43      pos\n431        2      99        0       0       0 22.2    0.108  23      neg\n432        3      89       74      16      85 30.4    0.551  38      neg\n433        1      80       74      11      60 30.0    0.527  22      neg\n434        2     139       75       0       0 25.6    0.167  29      neg\n435        1      90       68       8       0 24.5    1.138  36      neg\n436        0     141        0       0       0 42.4    0.205  29      pos\n437       12     140       85      33       0 37.4    0.244  41      neg\n438        5     147       75       0       0 29.9    0.434  28      neg\n439        1      97       70      15       0 18.2    0.147  21      neg\n440        6     107       88       0       0 36.8    0.727  31      neg\n441        0     189      104      25       0 34.3    0.435  41      pos\n442        2      83       66      23      50 32.2    0.497  22      neg\n443        4     117       64      27     120 33.2    0.230  24      neg\n444        8     108       70       0       0 30.5    0.955  33      pos\n445        4     117       62      12       0 29.7    0.380  30      pos\n446        0     180       78      63      14 59.4    2.420  25      pos\n447        1     100       72      12      70 25.3    0.658  28      neg\n448        0      95       80      45      92 36.5    0.330  26      neg\n449        0     104       64      37      64 33.6    0.510  22      pos\n450        0     120       74      18      63 30.5    0.285  26      neg\n451        1      82       64      13      95 21.2    0.415  23      neg\n452        2     134       70       0       0 28.9    0.542  23      pos\n453        0      91       68      32     210 39.9    0.381  25      neg\n454        2     119        0       0       0 19.6    0.832  72      neg\n455        2     100       54      28     105 37.8    0.498  24      neg\n456       14     175       62      30       0 33.6    0.212  38      pos\n457        1     135       54       0       0 26.7    0.687  62      neg\n458        5      86       68      28      71 30.2    0.364  24      neg\n459       10     148       84      48     237 37.6    1.001  51      pos\n460        9     134       74      33      60 25.9    0.460  81      neg\n461        9     120       72      22      56 20.8    0.733  48      neg\n462        1      71       62       0       0 21.8    0.416  26      neg\n463        8      74       70      40      49 35.3    0.705  39      neg\n464        5      88       78      30       0 27.6    0.258  37      neg\n465       10     115       98       0       0 24.0    1.022  34      neg\n466        0     124       56      13     105 21.8    0.452  21      neg\n467        0      74       52      10      36 27.8    0.269  22      neg\n468        0      97       64      36     100 36.8    0.600  25      neg\n469        8     120        0       0       0 30.0    0.183  38      pos\n470        6     154       78      41     140 46.1    0.571  27      neg\n471        1     144       82      40       0 41.3    0.607  28      neg\n472        0     137       70      38       0 33.2    0.170  22      neg\n473        0     119       66      27       0 38.8    0.259  22      neg\n474        7     136       90       0       0 29.9    0.210  50      neg\n475        4     114       64       0       0 28.9    0.126  24      neg\n476        0     137       84      27       0 27.3    0.231  59      neg\n477        2     105       80      45     191 33.7    0.711  29      pos\n478        7     114       76      17     110 23.8    0.466  31      neg\n479        8     126       74      38      75 25.9    0.162  39      neg\n480        4     132       86      31       0 28.0    0.419  63      neg\n481        3     158       70      30     328 35.5    0.344  35      pos\n482        0     123       88      37       0 35.2    0.197  29      neg\n483        4      85       58      22      49 27.8    0.306  28      neg\n484        0      84       82      31     125 38.2    0.233  23      neg\n485        0     145        0       0       0 44.2    0.630  31      pos\n486        0     135       68      42     250 42.3    0.365  24      pos\n487        1     139       62      41     480 40.7    0.536  21      neg\n488        0     173       78      32     265 46.5    1.159  58      neg\n489        4      99       72      17       0 25.6    0.294  28      neg\n490        8     194       80       0       0 26.1    0.551  67      neg\n491        2      83       65      28      66 36.8    0.629  24      neg\n492        2      89       90      30       0 33.5    0.292  42      neg\n493        4      99       68      38       0 32.8    0.145  33      neg\n494        4     125       70      18     122 28.9    1.144  45      pos\n495        3      80        0       0       0  0.0    0.174  22      neg\n496        6     166       74       0       0 26.6    0.304  66      neg\n497        5     110       68       0       0 26.0    0.292  30      neg\n498        2      81       72      15      76 30.1    0.547  25      neg\n499        7     195       70      33     145 25.1    0.163  55      pos\n500        6     154       74      32     193 29.3    0.839  39      neg\n501        2     117       90      19      71 25.2    0.313  21      neg\n502        3      84       72      32       0 37.2    0.267  28      neg\n503        6       0       68      41       0 39.0    0.727  41      pos\n504        7      94       64      25      79 33.3    0.738  41      neg\n505        3      96       78      39       0 37.3    0.238  40      neg\n506       10      75       82       0       0 33.3    0.263  38      neg\n507        0     180       90      26      90 36.5    0.314  35      pos\n508        1     130       60      23     170 28.6    0.692  21      neg\n509        2      84       50      23      76 30.4    0.968  21      neg\n510        8     120       78       0       0 25.0    0.409  64      neg\n511       12      84       72      31       0 29.7    0.297  46      pos\n512        0     139       62      17     210 22.1    0.207  21      neg\n513        9      91       68       0       0 24.2    0.200  58      neg\n514        2      91       62       0       0 27.3    0.525  22      neg\n515        3      99       54      19      86 25.6    0.154  24      neg\n516        3     163       70      18     105 31.6    0.268  28      pos\n517        9     145       88      34     165 30.3    0.771  53      pos\n518        7     125       86       0       0 37.6    0.304  51      neg\n519       13      76       60       0       0 32.8    0.180  41      neg\n520        6     129       90       7     326 19.6    0.582  60      neg\n521        2      68       70      32      66 25.0    0.187  25      neg\n522        3     124       80      33     130 33.2    0.305  26      neg\n523        6     114        0       0       0  0.0    0.189  26      neg\n524        9     130       70       0       0 34.2    0.652  45      pos\n525        3     125       58       0       0 31.6    0.151  24      neg\n526        3      87       60      18       0 21.8    0.444  21      neg\n527        1      97       64      19      82 18.2    0.299  21      neg\n528        3     116       74      15     105 26.3    0.107  24      neg\n529        0     117       66      31     188 30.8    0.493  22      neg\n530        0     111       65       0       0 24.6    0.660  31      neg\n531        2     122       60      18     106 29.8    0.717  22      neg\n532        0     107       76       0       0 45.3    0.686  24      neg\n533        1      86       66      52      65 41.3    0.917  29      neg\n534        6      91        0       0       0 29.8    0.501  31      neg\n535        1      77       56      30      56 33.3    1.251  24      neg\n536        4     132        0       0       0 32.9    0.302  23      pos\n537        0     105       90       0       0 29.6    0.197  46      neg\n538        0      57       60       0       0 21.7    0.735  67      neg\n539        0     127       80      37     210 36.3    0.804  23      neg\n540        3     129       92      49     155 36.4    0.968  32      pos\n541        8     100       74      40     215 39.4    0.661  43      pos\n542        3     128       72      25     190 32.4    0.549  27      pos\n543       10      90       85      32       0 34.9    0.825  56      pos\n544        4      84       90      23      56 39.5    0.159  25      neg\n545        1      88       78      29      76 32.0    0.365  29      neg\n546        8     186       90      35     225 34.5    0.423  37      pos\n547        5     187       76      27     207 43.6    1.034  53      pos\n548        4     131       68      21     166 33.1    0.160  28      neg\n549        1     164       82      43      67 32.8    0.341  50      neg\n550        4     189      110      31       0 28.5    0.680  37      neg\n551        1     116       70      28       0 27.4    0.204  21      neg\n552        3      84       68      30     106 31.9    0.591  25      neg\n553        6     114       88       0       0 27.8    0.247  66      neg\n554        1      88       62      24      44 29.9    0.422  23      neg\n555        1      84       64      23     115 36.9    0.471  28      neg\n556        7     124       70      33     215 25.5    0.161  37      neg\n557        1      97       70      40       0 38.1    0.218  30      neg\n558        8     110       76       0       0 27.8    0.237  58      neg\n559       11     103       68      40       0 46.2    0.126  42      neg\n560       11      85       74       0       0 30.1    0.300  35      neg\n561        6     125       76       0       0 33.8    0.121  54      pos\n562        0     198       66      32     274 41.3    0.502  28      pos\n563        1      87       68      34      77 37.6    0.401  24      neg\n564        6      99       60      19      54 26.9    0.497  32      neg\n565        0      91       80       0       0 32.4    0.601  27      neg\n566        2      95       54      14      88 26.1    0.748  22      neg\n567        1      99       72      30      18 38.6    0.412  21      neg\n568        6      92       62      32     126 32.0    0.085  46      neg\n569        4     154       72      29     126 31.3    0.338  37      neg\n570        0     121       66      30     165 34.3    0.203  33      pos\n571        3      78       70       0       0 32.5    0.270  39      neg\n572        2     130       96       0       0 22.6    0.268  21      neg\n573        3     111       58      31      44 29.5    0.430  22      neg\n574        2      98       60      17     120 34.7    0.198  22      neg\n575        1     143       86      30     330 30.1    0.892  23      neg\n576        1     119       44      47      63 35.5    0.280  25      neg\n577        6     108       44      20     130 24.0    0.813  35      neg\n578        2     118       80       0       0 42.9    0.693  21      pos\n579       10     133       68       0       0 27.0    0.245  36      neg\n580        2     197       70      99       0 34.7    0.575  62      pos\n581        0     151       90      46       0 42.1    0.371  21      pos\n582        6     109       60      27       0 25.0    0.206  27      neg\n583       12     121       78      17       0 26.5    0.259  62      neg\n584        8     100       76       0       0 38.7    0.190  42      neg\n585        8     124       76      24     600 28.7    0.687  52      pos\n586        1      93       56      11       0 22.5    0.417  22      neg\n587        8     143       66       0       0 34.9    0.129  41      pos\n588        6     103       66       0       0 24.3    0.249  29      neg\n589        3     176       86      27     156 33.3    1.154  52      pos\n590        0      73        0       0       0 21.1    0.342  25      neg\n591       11     111       84      40       0 46.8    0.925  45      pos\n592        2     112       78      50     140 39.4    0.175  24      neg\n593        3     132       80       0       0 34.4    0.402  44      pos\n594        2      82       52      22     115 28.5    1.699  25      neg\n595        6     123       72      45     230 33.6    0.733  34      neg\n596        0     188       82      14     185 32.0    0.682  22      pos\n597        0      67       76       0       0 45.3    0.194  46      neg\n598        1      89       24      19      25 27.8    0.559  21      neg\n599        1     173       74       0       0 36.8    0.088  38      pos\n600        1     109       38      18     120 23.1    0.407  26      neg\n601        1     108       88      19       0 27.1    0.400  24      neg\n602        6      96        0       0       0 23.7    0.190  28      neg\n603        1     124       74      36       0 27.8    0.100  30      neg\n604        7     150       78      29     126 35.2    0.692  54      pos\n605        4     183        0       0       0 28.4    0.212  36      pos\n606        1     124       60      32       0 35.8    0.514  21      neg\n607        1     181       78      42     293 40.0    1.258  22      pos\n608        1      92       62      25      41 19.5    0.482  25      neg\n609        0     152       82      39     272 41.5    0.270  27      neg\n610        1     111       62      13     182 24.0    0.138  23      neg\n611        3     106       54      21     158 30.9    0.292  24      neg\n612        3     174       58      22     194 32.9    0.593  36      pos\n613        7     168       88      42     321 38.2    0.787  40      pos\n614        6     105       80      28       0 32.5    0.878  26      neg\n615       11     138       74      26     144 36.1    0.557  50      pos\n616        3     106       72       0       0 25.8    0.207  27      neg\n617        6     117       96       0       0 28.7    0.157  30      neg\n618        2      68       62      13      15 20.1    0.257  23      neg\n619        9     112       82      24       0 28.2    1.282  50      pos\n620        0     119        0       0       0 32.4    0.141  24      pos\n621        2     112       86      42     160 38.4    0.246  28      neg\n622        2      92       76      20       0 24.2    1.698  28      neg\n623        6     183       94       0       0 40.8    1.461  45      neg\n624        0      94       70      27     115 43.5    0.347  21      neg\n625        2     108       64       0       0 30.8    0.158  21      neg\n626        4      90       88      47      54 37.7    0.362  29      neg\n627        0     125       68       0       0 24.7    0.206  21      neg\n628        0     132       78       0       0 32.4    0.393  21      neg\n629        5     128       80       0       0 34.6    0.144  45      neg\n630        4      94       65      22       0 24.7    0.148  21      neg\n631        7     114       64       0       0 27.4    0.732  34      pos\n632        0     102       78      40      90 34.5    0.238  24      neg\n633        2     111       60       0       0 26.2    0.343  23      neg\n634        1     128       82      17     183 27.5    0.115  22      neg\n635       10      92       62       0       0 25.9    0.167  31      neg\n636       13     104       72       0       0 31.2    0.465  38      pos\n637        5     104       74       0       0 28.8    0.153  48      neg\n638        2      94       76      18      66 31.6    0.649  23      neg\n639        7      97       76      32      91 40.9    0.871  32      pos\n640        1     100       74      12      46 19.5    0.149  28      neg\n641        0     102       86      17     105 29.3    0.695  27      neg\n642        4     128       70       0       0 34.3    0.303  24      neg\n643        6     147       80       0       0 29.5    0.178  50      pos\n644        4      90        0       0       0 28.0    0.610  31      neg\n645        3     103       72      30     152 27.6    0.730  27      neg\n646        2     157       74      35     440 39.4    0.134  30      neg\n647        1     167       74      17     144 23.4    0.447  33      pos\n648        0     179       50      36     159 37.8    0.455  22      pos\n649       11     136       84      35     130 28.3    0.260  42      pos\n650        0     107       60      25       0 26.4    0.133  23      neg\n651        1      91       54      25     100 25.2    0.234  23      neg\n652        1     117       60      23     106 33.8    0.466  27      neg\n653        5     123       74      40      77 34.1    0.269  28      neg\n654        2     120       54       0       0 26.8    0.455  27      neg\n655        1     106       70      28     135 34.2    0.142  22      neg\n656        2     155       52      27     540 38.7    0.240  25      pos\n657        2     101       58      35      90 21.8    0.155  22      neg\n658        1     120       80      48     200 38.9    1.162  41      neg\n659       11     127      106       0       0 39.0    0.190  51      neg\n660        3      80       82      31      70 34.2    1.292  27      pos\n661       10     162       84       0       0 27.7    0.182  54      neg\n662        1     199       76      43       0 42.9    1.394  22      pos\n663        8     167      106      46     231 37.6    0.165  43      pos\n664        9     145       80      46     130 37.9    0.637  40      pos\n665        6     115       60      39       0 33.7    0.245  40      pos\n666        1     112       80      45     132 34.8    0.217  24      neg\n667        4     145       82      18       0 32.5    0.235  70      pos\n668       10     111       70      27       0 27.5    0.141  40      pos\n669        6      98       58      33     190 34.0    0.430  43      neg\n670        9     154       78      30     100 30.9    0.164  45      neg\n671        6     165       68      26     168 33.6    0.631  49      neg\n672        1      99       58      10       0 25.4    0.551  21      neg\n673       10      68      106      23      49 35.5    0.285  47      neg\n674        3     123      100      35     240 57.3    0.880  22      neg\n675        8      91       82       0       0 35.6    0.587  68      neg\n676        6     195       70       0       0 30.9    0.328  31      pos\n677        9     156       86       0       0 24.8    0.230  53      pos\n678        0      93       60       0       0 35.3    0.263  25      neg\n679        3     121       52       0       0 36.0    0.127  25      pos\n680        2     101       58      17     265 24.2    0.614  23      neg\n681        2      56       56      28      45 24.2    0.332  22      neg\n682        0     162       76      36       0 49.6    0.364  26      pos\n683        0      95       64      39     105 44.6    0.366  22      neg\n684        4     125       80       0       0 32.3    0.536  27      pos\n685        5     136       82       0       0  0.0    0.640  69      neg\n686        2     129       74      26     205 33.2    0.591  25      neg\n687        3     130       64       0       0 23.1    0.314  22      neg\n688        1     107       50      19       0 28.3    0.181  29      neg\n689        1     140       74      26     180 24.1    0.828  23      neg\n690        1     144       82      46     180 46.1    0.335  46      pos\n691        8     107       80       0       0 24.6    0.856  34      neg\n692       13     158      114       0       0 42.3    0.257  44      pos\n693        2     121       70      32      95 39.1    0.886  23      neg\n694        7     129       68      49     125 38.5    0.439  43      pos\n695        2      90       60       0       0 23.5    0.191  25      neg\n696        7     142       90      24     480 30.4    0.128  43      pos\n697        3     169       74      19     125 29.9    0.268  31      pos\n698        0      99        0       0       0 25.0    0.253  22      neg\n699        4     127       88      11     155 34.5    0.598  28      neg\n700        4     118       70       0       0 44.5    0.904  26      neg\n701        2     122       76      27     200 35.9    0.483  26      neg\n702        6     125       78      31       0 27.6    0.565  49      pos\n703        1     168       88      29       0 35.0    0.905  52      pos\n704        2     129        0       0       0 38.5    0.304  41      neg\n705        4     110       76      20     100 28.4    0.118  27      neg\n706        6      80       80      36       0 39.8    0.177  28      neg\n707       10     115        0       0       0  0.0    0.261  30      pos\n708        2     127       46      21     335 34.4    0.176  22      neg\n709        9     164       78       0       0 32.8    0.148  45      pos\n710        2      93       64      32     160 38.0    0.674  23      pos\n711        3     158       64      13     387 31.2    0.295  24      neg\n712        5     126       78      27      22 29.6    0.439  40      neg\n713       10     129       62      36       0 41.2    0.441  38      pos\n714        0     134       58      20     291 26.4    0.352  21      neg\n715        3     102       74       0       0 29.5    0.121  32      neg\n716        7     187       50      33     392 33.9    0.826  34      pos\n717        3     173       78      39     185 33.8    0.970  31      pos\n718       10      94       72      18       0 23.1    0.595  56      neg\n719        1     108       60      46     178 35.5    0.415  24      neg\n720        5      97       76      27       0 35.6    0.378  52      pos\n721        4      83       86      19       0 29.3    0.317  34      neg\n722        1     114       66      36     200 38.1    0.289  21      neg\n723        1     149       68      29     127 29.3    0.349  42      pos\n724        5     117       86      30     105 39.1    0.251  42      neg\n725        1     111       94       0       0 32.8    0.265  45      neg\n726        4     112       78      40       0 39.4    0.236  38      neg\n727        1     116       78      29     180 36.1    0.496  25      neg\n728        0     141       84      26       0 32.4    0.433  22      neg\n729        2     175       88       0       0 22.9    0.326  22      neg\n730        2      92       52       0       0 30.1    0.141  22      neg\n731        3     130       78      23      79 28.4    0.323  34      pos\n732        8     120       86       0       0 28.4    0.259  22      pos\n733        2     174       88      37     120 44.5    0.646  24      pos\n734        2     106       56      27     165 29.0    0.426  22      neg\n735        2     105       75       0       0 23.3    0.560  53      neg\n736        4      95       60      32       0 35.4    0.284  28      neg\n737        0     126       86      27     120 27.4    0.515  21      neg\n738        8      65       72      23       0 32.0    0.600  42      neg\n739        2      99       60      17     160 36.6    0.453  21      neg\n740        1     102       74       0       0 39.5    0.293  42      pos\n741       11     120       80      37     150 42.3    0.785  48      pos\n742        3     102       44      20      94 30.8    0.400  26      neg\n743        1     109       58      18     116 28.5    0.219  22      neg\n744        9     140       94       0       0 32.7    0.734  45      pos\n745       13     153       88      37     140 40.6    1.174  39      neg\n746       12     100       84      33     105 30.0    0.488  46      neg\n747        1     147       94      41       0 49.3    0.358  27      pos\n748        1      81       74      41      57 46.3    1.096  32      neg\n749        3     187       70      22     200 36.4    0.408  36      pos\n750        6     162       62       0       0 24.3    0.178  50      pos\n751        4     136       70       0       0 31.2    1.182  22      pos\n752        1     121       78      39      74 39.0    0.261  28      neg\n753        3     108       62      24       0 26.0    0.223  25      neg\n754        0     181       88      44     510 43.3    0.222  26      pos\n755        8     154       78      32       0 32.4    0.443  45      pos\n756        1     128       88      39     110 36.5    1.057  37      pos\n757        7     137       90      41       0 32.0    0.391  39      neg\n758        0     123       72       0       0 36.3    0.258  52      pos\n759        1     106       76       0       0 37.5    0.197  26      neg\n760        6     190       92       0       0 35.5    0.278  66      pos\n761        2      88       58      26      16 28.4    0.766  22      neg\n762        9     170       74      31       0 44.0    0.403  43      pos\n763        9      89       62       0       0 22.5    0.142  33      neg\n764       10     101       76      48     180 32.9    0.171  63      neg\n765        2     122       70      27       0 36.8    0.340  27      neg\n766        5     121       72      23     112 26.2    0.245  30      neg\n767        1     126       60       0       0 30.1    0.349  47      pos\n768        1      93       70      31       0 30.4    0.315  23      neg\n```\n:::\n:::\n\n\n\nEen snelle verkenning van de dataset toont aan dat er meer nullen in de gegevens zitten dan verwacht (vooral omdat een BMI of tricep huiddikte van 0 onmogelijk is), wat betekent dat ontbrekende waarden als nullen worden geregistreerd. Zie bijvoorbeeld het histogram van de tricep huidplooidikte, waar de nullen voor dikte opvallen.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(diabetes_orig) +\n  geom_histogram(aes(x = triceps))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](tidymodels-opnieuw_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nDit fenomeen is ook te zien in de glucose-, druk-, insuline- en massavariabelen. We zetten eerst de 0-scores in alle variabelen (behalve \"zwanger\") over naar `NA` (missende waarde). Daarvoor gebruiken we de `mutate_at()`functie (die binnenkort wordt vervangen door `mutate()` met `across())` om aan te geven op welke variabelen we onze muterende functie willen toepassen. We gebruiken de `if_else()`functie om aan te geven waar we de waarde mee moeten vervangen als de voorwaarde waar of onwaar is.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiabetes_clean <- diabetes_orig %>%\n  mutate_at(vars(triceps, glucose, pressure, insulin, mass), \n            function(.var) { \n              if_else(condition = (.var == 0), # als waar (bv als het 0 is)\n                      true = as.numeric(NA),  # zet er de waarde NA voor in de plaats\n                      false = .var # anders laat het zoals het is\n                      )\n            })\n```\n:::\n\n\nOnze gegevens zijn klaar. Laten we beginnen met het maken van een aantal `tidymodels`!\n\n\n## Haal train/test sets uit elkaar\n\nLaten we onze data verdelen in trainings- en testdata. De trainingsdata worden gebruikt om ons model te vinden en de parameters in te stellen (`tune`). De testdata gebruiken we alleen om de werking van het finale model vast te stellen. Dat splitten kunnen we doen door de `inital_split()` functie (van het `rsample` pakket). Dat creëert een speciaal “split” object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(234589)\n# deel de data op in trainng (75%) en testing (25%)\ndiabetes_split <- initial_split(diabetes_clean, \n                                prop = 3/4)\ndiabetes_split\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Analysis/Assess/Total>\n<576/192/768>\n```\n:::\n:::\n\n\n\n`diabetes_split`, ons gesplitste object, vertelt ons hoeveel waarnemingen we hebben in de trainingsset, de testset en de gehele dataset: `<train/test/totaal>` (576/192/768).\n\nDe trainings- en testsets kunnen uit het \"split\"-object worden gehaald met behulp van de `training()` en `testing()` functies. Hoewel we deze objecten niet echt zullen gebruiken in de `pipeline` (daarvoor zullen we het `diabetes_split`-object zelf gebruiken).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# haal training en testing sets uit elkaar\ndiabetes_train <- training(diabetes_split)\ndiabetes_test <- testing(diabetes_split)\n```\n:::\n\n\nOp een gegeven moment zullen we de parameters hiervan wat willen `tuenen` (afstemmen). Dat doen we met cross-validatie. Zo ontstaat er met `vfold_cv()` een cross-validatie versie van de trainingsset waar we zo op terugkomen.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# creeer CV object van training data\ndiabetes_cv <- vfold_cv(diabetes_train)\n```\n:::\n\n\n\n## Defineeer een `recipe`\n\nMet het pakket `recipes` kun je de variabelen een rol geven, als uitkomst of voorspellende variabele (gebruik een “formule”) b.v.. Maar met `recipe` kun je ook andere voorbereidingsstappen zetten die je nodig acht (zoals standaardiseren, imputeren, PCA, etc). Een `recipe` voer je uit in delen (gelaagd op elkaar door pipes `%>%` te gebruiken):\n\n1. **Specificeer de formule** (`recipe()`): specificeer eerst wat is de uitkomstvariabele en wat zijn de predictoren;   \n\n2. **Specificeer pre-processing** `steps (step_zzz()`): defineer voorbereidingsstappen, zoals imputatie, creëren van dummy variabelen, schalen en wat al niet meer   \n\nZo kunnen we bijvoorbeeld de volgende `recipe` maken.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# defineer de `recipe`\ndiabetes_recipe <- \n  # dat bestaat uit de volgende formule (uitkomst ~ predictoren)\n  recipe(diabetes ~ pregnant + glucose + pressure + triceps + \n           insulin + mass + pedigree + age, \n         data = diabetes_clean) %>%\n  # en voeren we enkele voorbereidingsstappen uit (normaliseren en imputeren)\n  step_normalize(all_numeric()) %>%\n  step_knnimpute(all_predictors())\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: `step_knnimpute()` was deprecated in recipes 0.1.16.\nPlease use `step_impute_knn()` instead.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n```\n:::\n:::\n\n\nAls je ooit eerder formules hebt gezien (bijvoorbeeld met behulp van de `lm()` functie in R), dan weet je misschien dat we onze formule veel efficiënter hadden kunnen schrijven met behulp van een shortcut, waarbij de `.` alle variabelen in de gegevens vertegenwoordigt: `outcome ~` .\n\nDe volledige lijst van beschikbare voorbewerkingsstappen is hier te vinden. In de bovenstaande chunck hebben we de functies `all_numeric()` en `all_predictors()` gebruikt als argumenten van voorbereiding. Deze worden \"rolselecties\" genoemd en geven aan dat we de stap willen toepassen op \"alle numerieke\" variabelen of \"alle predictoren\". De lijst van alle potentiële rolselectoren kan worden gevonden door `?selectis` in je console te typen.\n\nMerk op dat we het originele `diabetes_clean` data-object hebben gebruikt (we stellen `recipe(..., data = diabetes_clean)`), in plaats van het `diabetes_train`-object of het `diabetes_split`-object. Het blijkt dat we deze allemaal hadden kunnen gebruiken. Alle `recipes` die op dit punt uit het dataobject worden gehaald zijn de namen en rollen van de uitkomst en de voorspellende variabelen. We zullen deze `recipe` later toepassen op specifieke datasets. Dit betekent dat voor grote datasets een kleinere dataset gebruikt wordt om tijd en geheugen te besparen.\n\nInderdaad, als we een samenvatting van `het diabetes_recipe` object printen, dan laat het ons gewoon zien hoeveel voorspellingsvariabelen we hebben gespecificeerd en welke stappen we hebben gespecificeerd (maar het implementeert ze eigenlijk nog niet!).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiabetes_recipe\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          8\n\nOperations:\n\nCentering and scaling for all_numeric()\nK-nearest neighbor imputation for all_predictors()\n```\n:::\n:::\n\n\nAls je de voorbewerkte dataset zelf wilt extraheren, kunt je eerst `prep()` het recept voor een specifieke dataset en `juice()` het voorbewerkte recept om de voorbewerkte gegevens te extraheren. Het blijkt dat het extraheren van de voorbewerkte data eigenlijk niet nodig is voor de pipeline, omdat dit onder de motorkap gebeurt als het model geschikt is. Soms is het toch nuttig.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiabetes_train_preprocessed <- diabetes_recipe %>%\n  # apply the recipe to the training data\n  prep(diabetes_train) %>%\n  # extract the pre-processed training dataset\n  juice()\ndiabetes_train_preprocessed\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 576 x 9\n   pregnant glucose pressure triceps insulin     mass pedigree     age diabetes\n      <dbl>   <dbl>    <dbl>   <dbl>   <dbl>    <dbl>    <dbl>   <dbl> <fct>   \n 1   1.23   -0.390    0.262   0.892   -0.317 -0.656      0.502 -0.201  pos     \n 2   0.0447  1.09    -0.0616 -0.0348  -0.219 -0.168     -0.400  0.301  neg     \n 3   1.82    1.91    -0.224   0.595    0.146  0.379     -0.813  0.301  neg     \n 4  -1.14   -0.0948  -0.710  -0.591   -0.522 -0.311      3.76  -1.04   neg     \n 5  -0.548   0.233    0.424   0.706    0.239  1.56       2.25  -0.201  pos     \n 6   0.637  -0.0620  -1.84   -0.683    0.190 -0.771      2.53  -0.0335 pos     \n 7  -0.844  -1.64    -2.01   -1.05    -0.629 -1.73      -0.445 -0.953  neg     \n 8  -0.548  -0.423   -1.68   -0.313   -0.735  0.00505   -0.460 -0.953  neg     \n 9  -1.14    0.463   -0.386   1.17     0.796  1.41      -0.320 -0.786  pos     \n10   1.53    1.41     0.424   0.150    0.877  0.0482    -0.968  0.969  pos     \n# ... with 566 more rows\n```\n:::\n:::\n\n\n\n\n## Specificeer het model\n\nTot nu toe hebben we onze data verdeeld in training en test-sets en onze pre-proces stappen gespecificeerd door een `recipe` te gebruiken. Nu willen we ons model definiëren en daarvoor gebruiken we het `parsnip` pakket dat in `tidymodels` zit.\n\n`Parsnip` biedt een uniforme interface voor de enorme verscheidenheid aan modellen die er in R bestaan. Dit betekent dat je slechts één manier hoeft te leren om een model te specificeren en dan kun je dit gebruiken voor allerlei verschillende modellen, vaak met enkele coderegel.\n\nEr zijn een paar primaire componenten in de modelspecificatie opgeslagen:\n\n1. Het **model type**: wat voor soort model wil je gebruiken, zoals `rand_forest()` voor het `random forest`-model, `logistic_reg()` voor het logistisch regressie-model, `svm_poly()` voor een polynomiaal SVM-model, enz. De volledige lijst van modellen die beschikbaar zijn via `parsnip` kan [hier] (link naar website) vinden.\n\n2. De **arguments**: de model parameter waarden (de benaming is consistent over verschillende modellen), door het gebruik van `set_args()`.\n\n3. De **engine**: het onderliggende pakket waar het model van wegkomt (bv. “ranger” voor implementatie van Random Forest), door het gebuik van `set_engine()`.\n\n4. De **mode**: het type voorspelling - omdat verschillende pakketten zowel classificatie (binaire/categoriale voorspelling) en regressie (continue voorspelling) kunnen uitvoeren, door het gebruik van `set_mode()`.\n\nAls we bijvoorbeeld een `random forest` model willen gebruiken, zoals dat in het `ranger` pakket zit, met als doel classificatie en we willen de `try` parameter `tunen` (het afstemmen van het aantal willekeurig gekozen variabelen dat bij elke splitsing in aanmerking moet worden genomen), dan moeten we de volgende modelspecificatie definiëren:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_model <- \n  # specificeren dat het model random forest is\n  rand_forest() %>%\n  # specificeren dat we de `mtry` parameter moeten afstemmen\n  set_args(mtry = tune()) %>%\n  # selecteren van de motor van het pakket dat onder het model zit\n  set_engine(\"ranger\", importance = \"impurity\") %>%\n  # kiezen dat je voor continue analyse (regressie) of categoriale analyse (classificatie) gaat\n  set_mode(\"classification\") \n```\n:::\n\n\nAls je later het variabele belang van jouw uiteindelijke model wilt kunnen onderzoeken, moet je het `engine` argument opnieuw instellen. De volgende code specificeert bijvoorbeeld een logistisch regressiemodel uit het `glm` pakket.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlr_model <- \n  # specificeer een logistisch regressiemodel\n  logistic_reg() %>%\n  # selecteer het pakket dat bij dit model hoort\n  set_engine(\"glm\") %>%\n  # kies voor een continue regressie of binaire classificatie wijze\n  set_mode(\"classification\") \n```\n:::\n\n\nDeze code draait niet het model. Net als de `recipe`, is het veel meer een beschrijving van het model. Echter, wanneer je een parameter op `tune()` zet wordt het later gestemd in de stemfase van de pipeline (bv. om de waarde vast te stellen van de parameter die de beste performance geeft). Je kunt ook zelf een bepaalde waarde aan de parameter geven wanneer je het niet wilt afstemmen, bv door `set_args(mtry = 4)` te gebruiken. Een ander ding om op te merken is dat niets wat deze modelspecificatie betreft specifiek is voor de diabetes-dataset.\n\n\n## Alles in een workflow samenbrengen\n\nWe zijn klaar om het model en de `recipes` in een workflow te plaatsen. Een workflow zet je op door het gebruik van `workflow()` (van het `workflows` pakket) en dan kun je een `recipe` en een `model` toevoegen. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# zet de workflow op\nrf_workflow <- workflow() %>%\n  # voeg de `recipe` toe\n  add_recipe(diabetes_recipe) %>%\n  # voeg het `model` toe\n  add_model(rf_model)\n```\n:::\n\n\nMerk op dat we de voorbewerkingsstappen nog niet in de `recipe` hebben geïmplementeerd noch dat we het `model` hebben gepast. We hebben alleen maar het raamwerk geschreven. Pas als we de parameters hebben afgestemd of in het model hebben gepast, worden het recept en het model daadwerkelijk geïmplementeerd.\n\n## Afstemmen van de parameters\n\nOmdat er een parameter is ontwikkeld om af te stemmen `(mtry)`, moeten we dat daar voor gebruiken (bv. de waarde kiezen die de beste performance laat zien) voordat we het model passen. Als je geen parameters hebt om af te stemmen, kun je dit deel overslaan.\n\nDat afstemmen doen we door een cross-validation object (`diabetes_cv`) te kiezen. Om dat te doen specificeren we de range van `mtry` waarden die we willen gebruiken en dan voegen we een stemmingslaag toe aan onze workflow door `tune_grid()` te gebruiken (van het `tune` pakket). We richten ons op twee maten: `accuracy` en `roc_auc` (van het `yardstick` pakket). Die vertellen ons welke maten we het beste kunnen gebruiken.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# specificeer de waarden die je wilt gebruiken\nrf_grid <- expand.grid(mtry = c(3, 4, 5))\n# extraheer resultaten\nrf_tune_results <- rf_workflow %>%\n  tune_grid(resamples = diabetes_cv, #CV object\n            grid = rf_grid, # grid van waarden om te proberen\n            metrics = metric_set(accuracy, roc_auc) # maten waar we naar moeten kijken\n            )\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'ranger' was built under R version 4.1.3\n```\n:::\n:::\n\n\nJe kunt verschillende parameters afstemmen door verschillende parameters aan de `expand.grid()` functie toe te voegen, bv. `expand.grid(mtry = c(3, 4, 5), trees = c(100, 500))`.\n\nHet is altijd goed om de resultaten van de cross-validatie goed te onderzoeken. `collect_metrics()` is echt een handige functie die in verschillende omstandigheden kan worden gebruikt om te vergelijken die zijn berekend in het object dat is gebruikt. In dit geval komen de maten van de cross-validatie performance over de verschillende waarden van de performance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# print results\nrf_tune_results %>%\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 7\n   mtry .metric  .estimator  mean     n std_err .config             \n  <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1     3 accuracy binary     0.766    10 0.00832 Preprocessor1_Model1\n2     3 roc_auc  binary     0.842    10 0.0116  Preprocessor1_Model1\n3     4 accuracy binary     0.774    10 0.0127  Preprocessor1_Model2\n4     4 roc_auc  binary     0.842    10 0.0123  Preprocessor1_Model2\n5     5 accuracy binary     0.771    10 0.0121  Preprocessor1_Model3\n6     5 roc_auc  binary     0.841    10 0.0134  Preprocessor1_Model3\n```\n:::\n:::\n\n\nTen opzichte van `accuracy` en `AUC` laat `mtry = 4` de beste performance zien (hoogste gemiddelde waarden).\n\n## Afronden van de workflow\n\nWe willen een laag aan onze workflow toevoegen die overeenkomt met de afgestemde parameter, d.w.z. dat we `mtry` instellen als de waarde die de beste resultaten opleverde. Als je geen parameters hebt afgestemd, kun je deze stap overslaan.\n\nWe kunnen de beste waarde voor de nauwkeurigheidsmetriek extraheren door de `select_best()`functie toe te passen op het afstemmingsobject.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparam_final <- rf_tune_results %>%\n  select_best(metric = \"accuracy\")\nparam_final\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 2\n   mtry .config             \n  <dbl> <chr>               \n1     4 Preprocessor1_Model2\n```\n:::\n:::\n\n\nDan kunnen we deze parameter aan de workflow toevoegen door de `finalize_workflow()` functie te gebruiken.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_workflow <- rf_workflow %>%\n  finalize_workflow(param_final)\n```\n:::\n\n\n\n## Evalueren van het model op de test set\n\nNu we ons `recipe` en ons `model` hebben gedefinieerd en de parameters van het model hebben ge`tune`d, zijn we klaar om daadwerkelijk het uiteindelijke model te draaien. Aangezien al deze informatie in het workflow-object zit, zullen we de `last_fit()` functie toepassen op onze workflow en ons train/test-splitsingsobject. Dit zal automatisch het door de workflow gespecificeerde model trainen met behulp van de trainingsgegevens en evaluaties produceren op basis van de testset.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_fit <- rf_workflow %>%\n  # draaien op de trainingsset en evalueren op de test set\n  last_fit(diabetes_split)\n```\n:::\n\n\nMerk op dat het object dat wordt gecreëerd een data-frame-achtig object is; het is een `tibble` met listkolommen.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Resampling results\n# Manual resampling \n# A tibble: 1 x 6\n  splits            id               .metrics .notes   .predictions .workflow \n  <list>            <chr>            <list>   <list>   <list>       <list>    \n1 <split [576/192]> train/test split <tibble> <tibble> <tibble>     <workflow>\n```\n:::\n:::\n\n\nDit is echt een aardige eigenschap van `tidymodels` (en ook waarom je zo goed kunt werken met `tidyverse`) omdat je al je nette handelingen op het modelobject kunt uitvoeren. \n\nAangezien we het trainings/testobject al hebben geleverd op het moment dat we in de workflow werken, worden de maten geëvalueerd op de testset. Wanneer we nu de `collect_metrics()` functie gebruiken (herinner ons dat we deze hebben gebruikt bij het afstemmen van onze parameters), haalt deze de prestaties van het uiteindelijke model (aangezien `rf_fit` nu bestaat uit een enkel definitief model) toegepast op de *test* set.\n \n\n::: {.cell}\n\n```{.r .cell-code}\ntest_performance <- rf_fit %>% collect_metrics()\ntest_performance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.781 Preprocessor1_Model1\n2 roc_auc  binary         0.847 Preprocessor1_Model1\n```\n:::\n:::\n\n\nOverall is de performance heel goed, met een `accuracy` van 0.74 en een AUC van 0.82. Maar deze waarden zijn vaak lager dan in de trainingsset.\n\nJe kunt de test set voorspellingen zelf gebruiken met de `collect_predictions()` functie. Let op dat er 192 rijen in het voorspellingsobject zitten dat overeenkomt met de *test set* observaties (juist om jou te laten zien dat deze gebaseerd zijn op de testset meer dan op de trainingsset).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# genereer voorspellingen vanuit de test set\ntest_predictions <- rf_fit %>% collect_predictions()\ntest_predictions\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 192 x 7\n   id               .pred_neg .pred_pos  .row .pred_class diabetes .config      \n   <chr>                <dbl>     <dbl> <int> <fct>       <fct>    <chr>        \n 1 train/test split     0.347    0.653      3 pos         pos      Preprocessor~\n 2 train/test split     0.210    0.790      9 pos         pos      Preprocessor~\n 3 train/test split     0.783    0.217     11 neg         neg      Preprocessor~\n 4 train/test split     0.508    0.492     13 neg         neg      Preprocessor~\n 5 train/test split     0.624    0.376     18 neg         pos      Preprocessor~\n 6 train/test split     0.364    0.636     26 pos         pos      Preprocessor~\n 7 train/test split     0.218    0.782     32 pos         pos      Preprocessor~\n 8 train/test split     0.673    0.327     50 neg         neg      Preprocessor~\n 9 train/test split     0.976    0.0240    53 neg         neg      Preprocessor~\n10 train/test split     0.166    0.834     55 pos         neg      Preprocessor~\n# ... with 182 more rows\n```\n:::\n:::\n\n\nOmndat dit een normaal data frame/tibble object is, kunnen we de samenvattingen genereren en een confusie matrix plotten.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# genereer een confusie matrix\ntest_predictions %>% \n  conf_mat(truth = diabetes, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Truth\nPrediction neg pos\n       neg 107  19\n       pos  23  43\n```\n:::\n:::\n\n\nWe kunnen ook de voorspelde kansverdelingen voor elke klasse in kaart brengen.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_predictions %>%\n  ggplot() +\n  geom_density(aes(x = .pred_pos, fill = diabetes), \n               alpha = 0.5)\n```\n\n::: {.cell-output-display}\n![](tidymodels-opnieuw_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n De voorspellingen kun je ook als volgt laten zien:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_predictions <- rf_fit %>% pull(.predictions)\ntest_predictions\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n# A tibble: 192 x 6\n   .pred_neg .pred_pos  .row .pred_class diabetes .config             \n       <dbl>     <dbl> <int> <fct>       <fct>    <chr>               \n 1     0.347    0.653      3 pos         pos      Preprocessor1_Model1\n 2     0.210    0.790      9 pos         pos      Preprocessor1_Model1\n 3     0.783    0.217     11 neg         neg      Preprocessor1_Model1\n 4     0.508    0.492     13 neg         neg      Preprocessor1_Model1\n 5     0.624    0.376     18 neg         pos      Preprocessor1_Model1\n 6     0.364    0.636     26 pos         pos      Preprocessor1_Model1\n 7     0.218    0.782     32 pos         pos      Preprocessor1_Model1\n 8     0.673    0.327     50 neg         neg      Preprocessor1_Model1\n 9     0.976    0.0240    53 neg         neg      Preprocessor1_Model1\n10     0.166    0.834     55 pos         neg      Preprocessor1_Model1\n# ... with 182 more rows\n```\n:::\n:::\n\n\n## Het laatste model\n\nIn de vorige paragraaf is het model dat is getraind op de trainingsgegevens geëvalueerd aan de hand van de testgegevens. Maar als je eenmaal jouw definitieve model hebt bepaald, wil je het vaak trainen op je volledige dataset en het dan gebruiken om de respons voor *nieuwe* gegevens te voorspellen.\n\nAls je jouw model wilt gebruiken om de respons voor nieuwe waarnemingen te voorspellen, moet je de `fit()`functie op jouw workflow gebruiken en de dataset waarop je het uiteindelijke model wilt laten passen (bijvoorbeeld de volledige training + testdataset).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_model <- fit(rf_workflow, diabetes_clean)\n```\n:::\n\n\nHet `final_model` object bevat een aantal zaken, waaronder het ranger-object dat getraind is met de parameters die via de workflow in `rf_workflow` zijn vastgelegd op basis van de gegevens in `diabetes_clean` (de gecombineerde trainings- en testgegevens).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n== Workflow [trained] ==========================================================\nPreprocessor: Recipe\nModel: rand_forest()\n\n-- Preprocessor ----------------------------------------------------------------\n2 Recipe Steps\n\n* step_normalize()\n* step_impute_knn()\n\n-- Model -----------------------------------------------------------------------\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~4,      x), importance = ~\"impurity\", num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  500 \nSample size:                      768 \nNumber of independent variables:  8 \nMtry:                             4 \nTarget node size:                 10 \nVariable importance mode:         impurity \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.1595594 \n```\n:::\n:::\n\n\nAls we de diabetes status van een nieuwe vrouw willen voorspellen, kunnen we de `predict()` functie gebruiken.\n\nBijvoorbeeld, definieren we de data voor een nieuwe vrouw.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_woman <- tribble(~pregnant, ~glucose, ~pressure, ~triceps, ~insulin, ~mass, ~pedigree, ~age,\n                     2, 95, 70, 31, 102, 28.2, 0.67, 47)\nnew_woman\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 8\n  pregnant glucose pressure triceps insulin  mass pedigree   age\n     <dbl>   <dbl>    <dbl>   <dbl>   <dbl> <dbl>    <dbl> <dbl>\n1        2      95       70      31     102  28.2     0.67    47\n```\n:::\n:::\n\n\nDe voorspelde diabetes status van deze nieuwe vrouw is “negatief”.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(final_model, new_data = new_woman)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 1\n  .pred_class\n  <fct>      \n1 neg        \n```\n:::\n:::\n\n\n## Variabele belang\nAls je de belangrijkheid van een variabele uit je model wilt vaststellen, voor zover je dat kan zien, moet je het modelobject uit het `fit()` object halen (dat voor ons `final_model` heet). De functie die het model extraheert is `pull_workflow_fit()` en dan moet je het `fit`-object pakken dat de output bevat.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nranger_obj <- pull_workflow_fit(final_model)$fit\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: `pull_workflow_fit()` was deprecated in workflows 0.2.3.\nPlease use `extract_fit_parsnip()` instead.\n```\n:::\n\n```{.r .cell-code}\nranger_obj\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~4,      x), importance = ~\"impurity\", num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  500 \nSample size:                      768 \nNumber of independent variables:  8 \nMtry:                             4 \nTarget node size:                 10 \nVariable importance mode:         impurity \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.1595594 \n```\n:::\n:::\n\n\nVervolgens kun je het belang van de variabele uit het ranger-object zelf halen (`variable.importance` is een specifiek object in de ranger-output - dit zal moeten worden aangepast voor het specifieke objecttype van andere modellen).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nranger_obj$variable.importance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\npregnant  glucose pressure  triceps  insulin     mass pedigree      age \n15.64236 79.84557 17.51228 21.64400 53.15198 43.77916 29.70545 32.32099 \n```\n:::\n:::\n",
    "supporting": [
      "tidymodels-opnieuw_files\\figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}