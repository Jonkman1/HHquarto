{
  "hash": "4ee1d022cdbf7ef338f562373644e8b5",
  "result": {
    "markdown": "---\ntitle: \"Hiërarchische logistische regressie met Bayesiaanse technieken\"\ndescription: |\n    Dit is een blog over hoe hiërarchische logistische regressie werkt met gebruik van Bayesiaanse technieken\nauthor: \"Johnson e.a. en Harrie Jonkmann\"\ndate: \"2022-03-21\"\ncategories: [bayes]\nimage: \"Screenshot.PNG\"\n---\n\n\n## Introductie\nOnlangs verscheen een prachtig boek van Alicia A. Johnson, Miles Q. Ot en Mine Dogucu onder de titel *Bayes Rules! An Introduction to Applied Bayesian Modeling* en het verscheen bij CRC Press (2022). Eerdere versies stonden kon je al via bookdown bekijken (https://www.bayesrulesbook.com/) en vanaf de eerste keer dat ik het zag, was ik hier heel enthousiast over. Het boek heb ik direct besteld en vorige week kon ik het ophalen. \n\nHet boek bestaat uit vier duidelijke delen. Het eerste deel gaat in op de fundamenten van het Bayesiaanse perspectief. Het leert je denken als een Bayesiaan en het gaat in op die belangrijke Bayesiaanse regel $posterior=\\frac{prior.likelihood}{normaliserende constante}$. Aan de hand van enkele voorbeelden gaan Johnson e.a. in op hoe het in de praktijk werkt. Daarna gaat het in hoe kennis en data op elkaar inwerken en het laat enkele basisanalyses zien en hoe dat in deze vorm van statistiek werkt (normaal, binair en poisson). \nHet tweede deel is een meer technisch hoofdstuk en laat je ook onder de moterkap van deze techniek kijken. Het gaat ook op de wetenschappelijke principes van de benadering, waar je hier op moet letten, hoe je hiermee hypothesen kunt testen (niet alleen tov van een nulhypothese, maar hoeveel beter de ene hypothese is ten opzichte van de andere hyposthese) en hoe je hiermee ook kunt voorspellen. De twee volgende delen (Deel drie en vier) zijn praktische delen. Deel drie gaat in op regressieanalyses voor continue variabelen en classificatieanalyses voor binaire variabelen. Het vierde deel ten slotte gaat in op geclusterde datasets en hoe je hierarchische Bayesiaanse regressie en classificatieanalyses uitvoert.\n\nNatuurlijk, er zijn onderhand al verschillende interessante boeken te krijgen die je laten zien hoe Bayesiaanse denken in de praktijk kan werken. De boeken van Gelman, McElreath, Spiegelhalter en Kruschke verschenen de afgelopen tien/vijftine jaar en leren je dit. Maar *Bayes Rules!* vind ik op dit moment als introductieboek mogelijk wel het beste. \n\nNu het boek bij mij op het bureau ligt, kan ik er binnenkort een keer een korte recensie over schrijven. Voor nu heb ik uit elk deel een hoofdstuk genomen en het vertaald en bewerkt. Hieronder zie je een bewerking van een deel van het achttiende hoofdstuk van het vierde deel (*Non-Normal Hierarchical Regression & Classification*).Hoofdstukken zo overzetten is voor mij niet alleen een goede manier om het mij eigen te maken, maar ook een manier om het boek anderen aan te raden. Dus lezen en gebruiken deze *Bayes Rules! An Introduction to Applied Bayesian Modeling* \n\n## Hierarchical logistic regression\n\nEerst maar een enkele pakketten laden:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Laden van pakketten\nlibrary(bayesrules)\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'tidyverse' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nv ggplot2 3.3.6     v purrr   0.3.4\nv tibble  3.1.7     v dplyr   1.0.9\nv tidyr   1.2.0     v stringr 1.4.1\nv readr   2.1.2     v forcats 0.5.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'ggplot2' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'tibble' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'tidyr' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'readr' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'dplyr' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'stringr' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nlibrary(bayesplot)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'bayesplot' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThis is bayesplot version 1.9.0\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n- Online documentation and vignettes at mc-stan.org/bayesplot\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n- bayesplot theme set to bayesplot::theme_default()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n   * Does _not_ affect other ggplot2 plots\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n   * See ?bayesplot_theme_set for details on theme setting\n```\n:::\n\n```{.r .cell-code}\nlibrary(rstanarm)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'rstanarm' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: Rcpp\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'Rcpp' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThis is rstanarm version 2.21.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n- See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n- Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n- For execution on a local, multicore CPU with excess RAM we recommend calling\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n  options(mc.cores = parallel::detectCores())\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidybayes)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'tidybayes' was built under R version 4.1.3\n```\n:::\n\n```{.r .cell-code}\nlibrary(broom.mixed)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'broom.mixed' was built under R version 4.1.3\n```\n:::\n\n```{.r .cell-code}\nlibrary(janitor)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'janitor'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n```\n:::\n:::\n\n\nBergbeklimmers proberen grote hoogten te beklimmen in de majestueuze Nepalese Himalaya. Dit doen ze vanwege de sensatie van ijle lucht, de uitdaging of het buitenleven. Succes is niet gegarandeerd; slecht weer, defecte uitrusting, verwondingen of gewoon pech zorgen ervoor dat niet alle klimmers hun bestemming bereiken. Dit roept enkele vragen op. Hoe groot is de kans dat een bergbeklimmer de top haalt? Welke factoren kunnen bijdragen aan een hoger succespercentage? Naast het vage gevoel dat een gemiddelde klimmer 50% kans op succes heeft, wegen we dit zwak informatief inzicht af tegen data van klimmers die in het **bayesrules** pakket zitten. Dit deel van de data is beschikbaar gesteld door 'The Himalayan Database' (2020) en verspreid via het **#tidytuesday** project (R for Data Science 2020b):\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Binnenhalen, herbenoemen & opschonen van data\ndata(climbers_sub)\nclimbers <- climbers_sub %>% \n  select(expedition_id, member_id, success, year, season,\n         age, expedition_role, oxygen_used)\n```\n:::\n\n\nDeze dataset bevat de resultaten van 2076 klimmers vanaf 1978. Slechts 38,87% van hen slaagde erin de top te bereiken:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnrow(climbers)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2076\n```\n:::\n\n```{.r .cell-code}\nclimbers %>% \n  tabyl(success)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n success    n   percent\n   FALSE 1269 0.6112717\n    TRUE  807 0.3887283\n```\n:::\n:::\n\n\nOmdat `member_id` in essentie een rij van klimmersid is en we maar één observatie per klimmer hebben, is dit geen groepsvariabele. Verder, hoewel het seizon (`seison`), rol bij de expeditie (`expedition_role`) en het gebruik van zuurstof (`oxygen_used)` categorische variabelen zijn meerdere malen geobserveerd, zijn dit potentiële *voorspellers* van van succes (`succes`), ook geen groepsvariabele. Dan blijft expeditie_id (`expedition_id`) over - dit *is* wel een groepsvariabele. De dataset beslaat 200 verschillende expedities:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Omvang per expeditie\nclimbers_per_expedition <- climbers %>% \n  group_by(expedition_id) %>% \n  summarize(count = n())\n\n# Aantal expedities\nnrow(climbers_per_expedition)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 200\n```\n:::\n:::\n\n\nElke expeditie bestaat uit meerdere klimmers. Zo vertrokken onze eerste drie expedities met respectievelijk 5, 6 en 12 klimmers:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclimbers_per_expedition %>% \n  head(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 x 2\n  expedition_id count\n  <chr>         <int>\n1 AMAD03107         5\n2 AMAD03327         6\n3 AMAD05338        12\n```\n:::\n:::\n\n\nHet zou *fout* zijn om deze groepsstructuur te negeren en er anders van uit te gaan dat de individuele klimmers onafhankelijke resultaten boeken. Aangezien elke expeditie als een *team* werkt, hangt het succes of falen van de ene klimmer in díe expeditie gedeeltelijk af van het succes of falen van anderen in de groep. Bovendien vertrekken alle leden van een expeditie met dezelfde bestemming, met dezelfde leiders en onder dezelfde weersomstandigheden, en zijn dus onderhevig aan dezelfde externe succesfactoren. Het is dus niet alleen juist om rekening te houden met de groepering van de gegevens, maar het kan ook duidelijk maken in welke mate deze factoren variabiliteit veroorzaken in de succespercentages *tussen* expedities. Meer dan 75 van onze 200 expedities hadden een 0% succesratio - m.a.w. geen enkele klimmer in deze expedities slaagde erin de top te bereiken. Daarentegen hadden bijna 20 expedities een 100% succespercentage. Tussen deze extremen in, is er heel wat variatie in het succespercentage van de expedities.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bereken de slagingskans voor elke expeditie\nexpedition_success <- climbers %>% \n  group_by(expedition_id) %>% \n  summarize(success_rate = mean(success))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot de slagingskansen over de expedities\nggplot(expedition_success, aes(x = success_rate)) + \n  geom_histogram(color = \"white\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](hirarchische-logistische-regressie-met-bayesiaanse-technieken_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n### Model bouw en simulatie\n\nOm de 'gegroepeerde' aard van onze gegevens te weerspiegelen, laat $Y_ij$ aangeven of klimmer $i$ in expeditie  \n$j$ succesvol de top van hun piek bereikt:\n\n\n\n$$\\\nY_ij = \\begin{cases}\n1 Ja \\\\\n0 Nee\\\\\n\\end{cases}\n\\]$$\nEr zijn verschillende potentiële voorspellers voor het succes van klimmers in onze dataset. We kijken hier naar slechts twee voorspellers: de leeftijd van de klimmer en of hij extra zuurstof heeft gekregen om gemakkelijker te kunnen ademen op grote hoogte. Als zodanig, definiëren we: \n$$ X_ij1=leeftijd van klimmer *i* in expeditie *j*$$\n\n$$X_ij2=of de klimmer in *i* in expeditie *j* zuurstof (oxygen) heeft gekregen$$\nDoor het aandeel van succes te berekenen bij elke combinatie van leeftijd en zuurstofgebruik, krijgen we een idee van hoe deze factoren gerelateerd zijn aan het klimmerssucces (zij het een wankel idee gezien de kleine steekproefgroottes van sommige combinaties). Kort samengevat lijkt het erop dat het succes van klimmers afneemt met de leeftijd en sterk toeneemt met het gebruik van zuurstof:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bereken het slagingspercentage per leeftijd en zuurstofgebruik\ndata_by_age_oxygen <- climbers %>% \n  group_by(age, oxygen_used) %>% \n  summarize(success_rate = mean(success))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`summarise()` has grouped output by 'age'. You can override using the `.groups`\nargument.\n```\n:::\n\n```{.r .cell-code}\n# Plot deze relatie\nggplot(data_by_age_oxygen, aes(x = age, y = success_rate, \n                               color = oxygen_used)) + \n  geom_point()\n```\n\n::: {.cell-output-display}\n![](hirarchische-logistische-regressie-met-bayesiaanse-technieken_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\nOm een Bayesiaans model van deze relatie op te stellen, erkennen we eerst dat het Bernoulli model redelijk is voor onze binaire responsvariabele $Y_ij$. Stel $\\pi_ij$ de *waarschijnlijkheid* is dat klimmer$i$ in expeditie$j$ zijn piek succesvol beklimt, d.w.z. dat $Y_ij=1$,\n\n$$Y_ij|\\pi_ij \\sim \\Bern{\\pi_ij}$$\n\nDit is een **complete pooling** benadering waarbij een simpel model wordt omgezet in een **logistisch regressie model** van $Y$ met enkele voorspellers $X$\n\n\n$$Y_ij|\\beta_0,\\beta_1,beta_2 \\sim^{ind} \\Bernoulli(\\pi_ij) with log(\\frac{\\pi_ij}{1-\\pi_ij})=\\beta_0+\\beta_1X_ij1+\\beta_2X_ij2) \\\\\n \\beta_0c \\sim N(m_0,s_0^2) \\\\\n \\beta_1 \\sim N(m_1, s_1^2) \\\\\n \\beta_2 \\sim N(m_2, s_1^2)$$\n \nDit is een goed begin, MAAR het houdt geen rekening met de groepsstructuur van onze data. Overweeg in plaats daarvan het volgende hiërarchische alternatief met onafhankelijke, zwak informatieve priors hieronder afgestemd via `stan_glmer()` en met een prior model voor $beta_0$ uitgedrukt via het gecentreerde intercept $beta_0c$. Het is immers zinvoller om na te denken over de baseline succesratio bij de *typische/gemiddelde* klimmer, $\\beta_0c$, dan bij 0-jarige klimmers die geen zuurstof gebruiken, $\\beta_0$$. Daarom begonnen we onze analyse met de zwakke veronderstelling dat de typische klimmer een kans op succes heeft van 0,5, of met log(kans op succes)=0.\n \n\n xxx\n \n*Net zo goed* kunnen we dit logistische regressiemodel met **willekeurige intercepts** omvormen door de expeditiespecifieke intercepties uit te drukken als *aanpassingen* op het algemene intercept,\n\n$$log(\\frac{\\pi_ij}{1-\\pi_ij})=(\\beta_0+b_0j) +\\beta_1X_ij1 + \\beta_2X_ij2$$\n\nmet $\\beta_0j|\\sigma_0 \\sim^{ind} N(0,\\sigma_0^2)$ Laten we eens naar de betekenis van en de veronderstellingen achter de modelparameters kijken:\n\n- De **expeditie-specifieke** intercepten $\\beta_0j$ beschrijven de onderliggende succespercentages, zoals gemeten door de log(kans op succes), voor elke expeditie$j$. Hiermee wordt erkend dat sommige expedities inherent succesvoller zijn dan andere.    \n\n- De expeditiespecifieke intervallen $\\beta_0j$ worden verondersteld normaal verdeeld te zijn rond een gemiddeld intercept $\\beta_0$ met standaardafwijking $\\sigma_0$. Daarmee beschrijft $\\beta_0$ het *typische* basissucces over alle expedities, en $\\sigma_0$ de **tussen-groep variabiliteit** in succespercentages van expeditie tot expeditie.      \n\n- Beta_1$ beschrijft het **gemiddelde** verband tussen succes en leeftijd wanneer gecontroleerd wordt voor zuurstofgebruik. Op dezelfde manier beschrijft $beta_2$ de gemiddelde relatie tussen succes en zuurstofverbruik wanneer gecontroleerd wordt voor leeftijd.      \n\nSamengevat maakt ons logistisch regressiemodel met willekeurige intercepten de vereenvoudigende (maar volgens ons redelijke) veronderstelling dat expedities *unieke intercepten* $\\beta_0j$ kunnen hebben, maar delen de *gemeenschappelijke* regressieparameters $\\beta_1$ en $\\beta_2$. Anders gezegd, hoewel de onderliggende succespercentages kunnen verschillen van expeditie tot expeditie, zijn jonger zijn of zuurstof gebruiken niet voordeliger in de ene expeditie dan in de andere.\n\nOm de posterior van het model te simuleren, combineert de `stan_glmer()` code hieronder het beste van twee werelden: `family = binomial` geeft aan dat het om een *logistisch* regressiemodel gaat (à la Hoofdstuk 13) en de `(1 | expeditie_id)` term in de modelformule incorporeert onze hiërarchische groeperingsstructuur (à la Hoofdstuk 17):\nConsider the meaning of, and assumptions behind, the model parameters:\n\n- De **expeditie-specifieke** intercepts $\\beta_0j$ beschrijven de onderliggende succespercentages, zoals gemeten door de log(kans op succes), voor elke expeditie$j$. Hiermee wordt erkend dat sommige expedities inherent succesvoller zijn dan andere.   \n\n- De expeditiespecifieke intervallen $\\beta_0j$ worden verondersteld normaal verdeeld te zijn rond een globaal intercept $\\beta_0$ met standaardafwijking $\\sigma_0$. Daarmee beschrijft $\\beta_0$ het *typische* basissucces over alle expedities, en $\\sigma_0$ de **tussen-groep variabiliteit** in succespercentages van expeditie tot expeditie.    \n\n- Beta_1$ beschrijft het **globale** verband tussen succes en leeftijd wanneer gecontroleerd wordt voor zuurstofgebruik. Op dezelfde manier beschrijft $beta_2$ de globale relatie tussen succes en zuurstofverbruik wanneer gecontroleerd wordt voor leeftijd.   \n\nSamengevat maakt ons logistisch regressiemodel met willekeurige intercepten de vereenvoudigende (maar volgens ons redelijke) veronderstelling dat expedities *unieke intercepten* $\\beta_0j$ kunnen hebben, maar *gemeenschappelijke* regressieparameters $\\beta_1$ en $\\beta_2$ delen. In gewone taal, hoewel de onderliggende succespercentages kunnen verschillen van expeditie tot expeditie, zijn jonger zijn of zuurstof gebruiken niet voordeliger in de ene expeditie dan in de andere.\n\nOm de posterior van het model te simuleren, combineert de `stan_glmer()` code, zie hieronder, het beste van twee werelden: `family = binomial` geeft aan dat het om een *logistisch* regressiemodel gaat en de `(1 | expeditie_id)` term in de modelformule incorporeert onze hiërarchische groepstructuur:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclimb_model <- stan_glmer(\n  success ~ age + oxygen_used + (1 | expedition_id), \n  data = climbers, family = binomial,\n  prior_intercept = normal(0, 2.5, autoscale = TRUE),\n  prior = normal(0, 2.5, autoscale = TRUE), \n  prior_covariance = decov(reg = 1, conc = 1, shape = 1, scale = 1),\n  chains = 4, iter = 5000*2, seed = 84735\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'bernoulli' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.001 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 10 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 79.289 seconds (Warm-up)\nChain 1:                72.661 seconds (Sampling)\nChain 1:                151.95 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'bernoulli' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 78.588 seconds (Warm-up)\nChain 2:                72.631 seconds (Sampling)\nChain 2:                151.219 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'bernoulli' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0.001 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 10 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 80.316 seconds (Warm-up)\nChain 3:                74.563 seconds (Sampling)\nChain 3:                154.879 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'bernoulli' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 81.826 seconds (Warm-up)\nChain 4:                75.631 seconds (Sampling)\nChain 4:                157.457 seconds (Total)\nChain 4: \n```\n:::\n:::\n\n\nJe wordt aangemoedigd deze simulatie te volgen met de uitvoering van de code hierboven en te kijken naar enkele MCMC-diagnoses die hieronder staan. De r:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bevestig prior specificaties\nprior_summary(climb_model)\n\n# MCMC diagnostiek\nmcmc_trace(climb_model, size = 0.1)\n```\n\n::: {.cell-output-display}\n![](hirarchische-logistische-regressie-met-bayesiaanse-technieken_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmcmc_dens_overlay(climb_model)\n```\n\n::: {.cell-output-display}\n![](hirarchische-logistische-regressie-met-bayesiaanse-technieken_files/figure-html/unnamed-chunk-10-2.png){width=672}\n:::\n\n```{.r .cell-code}\nmcmc_acf(climb_model)\n```\n\n::: {.cell-output-display}\n![](hirarchische-logistische-regressie-met-bayesiaanse-technieken_files/figure-html/unnamed-chunk-10-3.png){width=672}\n:::\n\n```{.r .cell-code}\nneff_ratio(climb_model)\nrhat(climb_model)\n```\n:::\n\n\nTerwijl deze diagnostiek bevestigt dat onze MCMC simulatie op het juiste spoor zit, geeft een **posterior predictive check** hieronder aan dat ons model op het juiste spoor zit. Van elk van de 100 posterior gesimuleerde datasets, stellen we de proportie klimmers vast die succesvol waren met de `success_rate()` functie. Deze succespercentages variëren van ruwweg 37% tot 41%, in een klein venster rond het werkelijk waargenomen succespercentage van 38.9% in de `klimmers` data. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Defineer slagingspercentage functie\nsuccess_rate <- function(x){mean(x == 1)}\n\n# Posterior predictive check\npp_check(climb_model, nreps = 100,\n         plotfun = \"stat\", stat = \"success_rate\") + \n  xlab(\"succes score\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: 'nreps' is ignored for this PPC\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](hirarchische-logistische-regressie-met-bayesiaanse-technieken_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n### Posterior analyse\nIn onze posterior analyse van het succes van bergbeklimmers, concentreren we ons op het geheel. Behalve dat we gerustgesteld zijn door het feit dat we correct rekening houden met de groepsstructuur van onze gegevens, zijn we niet geïnteresseerd in een specifieke expeditie. Hieronder volgen enkele posterior samenvattingen voor onze regressieparameters $\\beta_0$, $\\beta_1$ en $\\beta_2$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(climb_model, effects = \"fixed\", conf.int = TRUE, conf.level = 0.80)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 x 5\n  term            estimate std.error conf.low conf.high\n  <chr>              <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)      -1.41     0.476    -2.04     -0.802 \n2 age              -0.0475   0.00940  -0.0596   -0.0358\n3 oxygen_usedTRUE   5.80     0.485     5.21      6.46  \n```\n:::\n:::\n\n\nOm te beginnen zien we dat het 80% posterior 'çredible' (geloofwaardigheids) interval (CI) voor de `age` coëfficiënt $\\beta_1$ ruim onder 0 ligt. We hebben dus *significant* posterior bewijs dat, wanneer we controleren of een klimmer al dan niet zuurstof gebruikt, de kans op succes afneemt met de leeftijd. Meer specifiek, als we de informatie in $\\beta_1$ vertalen van de *log*(kansen) naar de *kans* schaal, is er 80% kans dat de kans op een succesvolle beklimming daalt tussen 3,5% en 5,8% voor elk jaar extra leeftijd: $e^{-0,0594}, e^{-0,0358}=(0,942, 0,965)$.\n\nOp dezelfde manier levert het 80% posterior geloofwaardig interval voor de oxygen_usedTRUE coëfficiënt $beta_2$ *significant* posterior bewijs dat, wanneer gecontroleerd wordt voor leeftijd, het gebruik van zuurstof de kans op het beklimmen van de top drastisch verhoogt. Er is een kans van 80% dat het gebruik van zuurstof kan overeenkomen met een 182- tot 617-voudige toename van de kans op succes: $e^{5.2}}, e^{6.43}=(182,617)$, Zuurstof alstublieft!\n\nDoor onze waarnemingen voor $\\beta_1$ en $\\beta_2$ te combineren, wordt het posterior mediaan model voor de relatie tussen de log(kans op succes) van de klimmers en hun leeftijd ($X_1$)en zuurstofgebruik ($X_2$)\n\n$$log(\\frac{\\pi}{1-\\pi})=-1.42-0.0474X_1+5.79X_2$$\n\nOf, op de schaal van waarschijnlijkheid:\n$$\\pi=\\frac{e^{-1.42-0.0474X_1+5.79X_2}}{1+e^{-1.42-0.0474X_1+5.79X_2}}$$\n\nDit posterior mediaan model vertegenwoordigt slechts het *midden* van een *bereik* van posterior plausibele relaties tussen succes, leeftijd en zuurstofgebruik. Om een idee te krijgen van dit bereik, toont figuur hieronder 100 posterior plausibele alternatieve modellen. Zowel met als zonder zuurstof neemt de kans op succes af met de leeftijd. Bovendien, op elke leeftijd, is de kans op succes *dramatisch* hoger wanneer klimmers zuurstof gebruiken. Echter, onze zekerheid over deze trends varieert nogal per leeftijd. We hebben *veel* minder zekerheid over de slaagkans voor oudere klimmers met zuurstof dan voor jongere klimmers met zuurstof, voor wie de slaagkans over het geheel hoog is. Op dezelfde manier, maar minder drastisch, hebben we minder zekerheid over de slaagkans voor jongere klimmers die geen zuurstof gebruiken dan voor oudere klimmers die geen zuurstof gebruiken, voor wie de slaagkans uniform laag is.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclimbers %>%\n  add_fitted_draws(climb_model, n = 100, re_formula = NA) %>%\n  ggplot(aes(x = age, y = success, color = oxygen_used)) +\n    geom_line(aes(y = .value, group = paste(oxygen_used, .draw)), \n              alpha = 0.1) + \n    labs(y = \"waarschijnlijkheid van succes\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: `fitted_draws` and `add_fitted_draws` are deprecated as their names were confusing.\nUse [add_]epred_draws() to get the expectation of the posterior predictive.\nUse [add_]linpred_draws() to get the distribution of the linear predictor.\nFor example, you used [add_]fitted_draws(..., scale = \"response\"), which\nmeans you most likely want [add_]epred_draws(...).\n```\n:::\n\n::: {.cell-output-display}\n![](hirarchische-logistische-regressie-met-bayesiaanse-technieken_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n### Posterior classificatie\n\nStel dat vier klimmers op een nieuwe expeditie gaan. Twee van hen zijn 20 jaar oud en twee zijn 60 jaar. Van beide leeftijdsgroepen is één klimmer van plan zuurstof te gebruiken en de andere niet:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Nieuwe expeditie\nnew_expedition <- data.frame(\n  age = c(20, 20, 60, 60), oxygen_used = c(FALSE, TRUE, FALSE, TRUE), \n  expedition_id = rep(\"new\", 4))\nnew_expedition\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  age oxygen_used expedition_id\n1  20       FALSE           new\n2  20        TRUE           new\n3  60       FALSE           new\n4  60        TRUE           new\n```\n:::\n:::\n\n\nNatuurlijk willen ze allemaal weten hoe groot de kans is dat ze de top zullen bereiken. Om dit vast te stellen werken we hier met de `posterior_predict()` snelkoppelingsfunctie om 20.000 posterior voorspellingen (0 of 1) te simuleren voor elk van onze 4 nieuwe klimmers:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Posterior voorspellingen van binaire uitkomst\nset.seed(84735)\nbinary_prediction <- posterior_predict(climb_model, newdata = new_expedition)\n\n# Eerste drie voorspellingen\nhead(binary_prediction, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     1 2 3 4\n[1,] 0 1 0 0\n[2,] 1 1 0 1\n[3,] 1 1 0 1\n```\n:::\n:::\n\n\n\nVoor elke klimmer wordt de kans op succes benaderd door het geobserveerde aandeel van succes onder hun 20.000 posterieure voorspellingen. Aangezien deze kansen de onzekerheid in het basissuccespercentage van de nieuwe expeditie omvatten, zijn ze gematigder dan de algemene trends die we eerder zichtbaar maakten.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Vat de posterior voorspellingen van Y samen:\ncolMeans(binary_prediction)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      1       2       3       4 \n0.27815 0.80110 0.14630 0.64710 \n```\n:::\n:::\n\n\nDeze voorspellingen geven meer inzicht in de verbanden tussen leeftijd, zuurstof, en succes. Bijvoorbeeld, onze posterior voorspelling is dat klimmer 1, die 20 jaar oud is en *niet* van plan is om zuurstof te gebruiken, 27.88% kans heeft om de top te halen. Deze kans is natuurlijk lager dan voor klimmer 2, die ook 20 is maar *wel* van plan is om zuurstof te gebruiken. Het is hoger dan de posterior voorspelling van succes voor klimmer 3, die ook niet van plan is zuurstof te gebruiken maar wel 60 jaar oud is. Over het algemeen is de voorspelling van succes *het hoogst* voor klimmer 2, die jonger is en van plan is zuurstof te gebruiken, en *het laagst* voor klimmer 3, die ouder is en niet van plan is zuurstof te gebruiken.\n\nPosterior *kans* voorspellingen kunnen omgezet worden in **posterior classificaties** van binaire uitkomsten: ja of nee, verwachtingen of de klimmer zal slagen of niet? Als we een eenvoudige cut-off van 0,5 zouden gebruiken om dit te bepalen, dan zouden we klimmers 1 en 3 aanraden *niet* aan de expeditie deel te nemen (tenminste, niet zonder zuurstof) en klimmers 2 en 4 het groene licht geven. Maar in deze specifieke context moeten we het waarschijnlijk aan de individuele klimmers overlaten om hun eigen resultaten te interpreteren en hun eigen ja-of-nee beslissingen te nemen over het al dan niet voortzetten van hun expeditie. Zo kan een kans op succes van 65,16% voor sommigen de moeite en het risico waard zijn, maar voor anderen niet.\n\n### Model evaluatie\n\nOm onze klimanalyse af te ronden, vragen we ons af: Is ons hiërarchisch-logistisch model een goed model? Lang verhaal kort, het antwoord is ja. \n- Ten eerste, ons model is **eerlijk**. De gegevens die we hebben gebruikt zijn openbaar en we verwachten niet dat onze analyse een negatief effect zal hebben op individuen of de samenleving. (Nogmaals, saaie antwoorden op de vraag naar eerlijkheid zijn de beste soort.)    \n- Ten tweede Posterior Predictive Checque controle toonde aan dat ons model **niet al te verkeerd** lijkt - onze posterior gesimuleerde succespercentages schommelen rond de waargenomen succespercentages in onze gegevens.     \n- Tenslotte, voor de vraag naar **posterior classificatie nauwkeurigheid**, kunnen we onze posterior classificaties van succes vergelijken met de werkelijke uitkomsten voor de 2076 klimmers in onze dataset. Standaard beginnen we met een **kans cut-off** van 0.5 - als de kans op succes van een klimmer groter is dan 0.5, voorspellen we dat hij zal slagen. We implementeren en evalueren deze classificatieregel met `classification_summary()` hieronder.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(84735)\nclassification_summary(data = climbers, model = climb_model, cutoff = 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$confusion_matrix\n     y    0   1\n FALSE 1172  97\n  TRUE   77 730\n\n$accuracy_rates\n                          \nsensitivity      0.9045849\nspecificity      0.9235619\noverall_accuracy 0.9161850\n```\n:::\n:::\n\n\nIn het algemeen voorspelt ons model met deze classificatieregel de resultaten goed voor 91,61% van onze klimmers. Dit ziet er behoorlijk fantastisch uit gezien het feit dat we enkel informatie gebruiken over de leeftijd en het zuurstofverbruik van de klimmers (terwijl er nog andere voorspellers te bedenken zijn (bv. bestemming, seizoen, enz.). Maar gezien de gevolgen van een foute classificatie in deze specifieke context (bv. risico op verwondingen), moeten we voorrang geven aan **specificiteit**, ons vermogen om te anticiperen wanneer een klimmer *niet* zou slagen. Om dit te bereiken voorspelde ons model slechts 92.51% van de mislukte beklimmingen correct. Om dit percentage te verhogen, kunnen we de waarschijnlijkheidsgrens in onze classificatieregel aanpassen.\n\nIn het algemeen kunnen we, om de specificiteit te verhogen, de waarschijnlijkheidsdrempel *verhogen*, waardoor het *moeilijker* wordt om \"succes\" te voorspellen. Na wat trial and error lijkt het erop dat cut-offs van ruwweg 0.65 of hoger een gewenst specificiteitsniveau van 95% zullen bereiken. Deze overschakeling naar 0.65 verlaagt natuurlijk de gevoeligheid van onze posterior classificaties, van 90.46% naar 81.54%, en dus ons vermogen om te detecteren wanneer een klimmer *succesvol* zal zijn. Wij denken dat de extra voorzichtigheid hier van belang is.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(84735)\nclassification_summary(data = climbers, model = climb_model, cutoff = 0.65)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$confusion_matrix\n     y    0   1\n FALSE 1213  56\n  TRUE  149 658\n\n$accuracy_rates\n                          \nsensitivity      0.8153656\nspecificity      0.9558708\noverall_accuracy 0.9012524\n```\n:::\n:::\n\n\n## Literatuur\n\nFast, Shannon, and Thomas Hegland. 2011. “Book Challenges: A Statistical Examination.” *Project for Statistics 316-Advanced Statistical Modeling, St. Olaf College*.\n\nLegler, Julie, and Paul Roback. 2021. *Beyond Multiple Linear Regression: Applied Generalized Linear Models and Multilevel Models in R.* Chapman; Hall/CRC. https://bookdown.org/roback/bookdown-BeyondMLR/.\n———. \n2020b. “Himalayan Climbing Expeditions.” *TidyTuesday Github Repostitory*. https://github.com/rfordatascience/tidytuesday/tree/master/data/2020/2020-09-22.\n\nThe Himalayan Database. 2020. https://www.himalayandatabase.com/.\nTrinh, Ly, and Pony Ameri. 2016. “AirBnB Price Determinants: A Multilevel Modeling Approach.” *Project for Statistics 316-Advanced Statistical Modeling, St. Olaf College*.\n\n\n",
    "supporting": [
      "hirarchische-logistische-regressie-met-bayesiaanse-technieken_files\\figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}