[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Harrie’s Hoekje",
    "section": "",
    "text": "analyse\n\n\n\n\nEen blog over de principes van de Bayesiaanse theorie\n\n\n\n\n\n\nMar 7, 2022\n\n\nJohnson e.a. en Harrie Jonkman\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nanalyse\n\n\n\n\nDit is een blog naar aanleiding van Gelman/Hill/Vehtari nieuwe boek Regresion and other stories\n\n\n\n\n\n\nFeb 10, 2022\n\n\nHarrie Jonkman\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nanalyse\n\n\n\n\nIntroductie op multilevel modelleren met gebruik van rstanarm: Een tutorial voor onderwijsonderzoekers\n\n\n\n\n\n\nFeb 6, 2022\n\n\nJoonHo Lee, Nicholas Sim, Feng Ji, and Sophia Rabe-Hesketh, vertaling HarrieJonkman\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nanalyse\n\n\n\n\nDit is een post over multilevel analyse van longitudinale data met betrekking tot alcoholgebruik van jongeren.\n\n\n\n\n\n\nJan 4, 2022\n\n\nAlexander Cernat, bewerking Harrie Jonkman\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nanalyse\n\n\n\n\nDit is een blog over Latente Groei Modeling van longitudinale data van alcoholgebruik van jongeren\n\n\n\n\n\n\nJan 3, 2022\n\n\nAlexander Cernat, bewerking Harrie Jonkman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Harrie’s Hoekje",
    "section": "",
    "text": "Dr. Harrie Jonkman studeerde sociologie en onderwijskunde en werkte aan de Rijksuniversiteit Groningen, in het onderwijs, het Nationaal Comité 4 en 5 mei en het Nederlands Jeugdinstituut (NJi). Sinds 2008 is hij verbonden aan het Verwey-Jonker Instituut. Zijn werk richt zich op de sociale en cognitieve ontwikkeling van kinderen en jongeren, sociale determinanten en preventie van ontwikkelings- en gedragsproblemen. In 2012 ontving hij van het National Institute on Drug Abuse (VS) een beurs als ‘Distinguished Researcher’. Hij was landelijk projectleider van de Community that Care strategie in Nederland, ondersteunde deze ook in andere landen en schreef zijn promotieonderzoek naar de effecten van de preventiestrategie Communities that Care. Bij het VerweyJonker Instituut is hij betrokken bij experimenten en evaluaties van sociale programma’s, longitudinale studies, en internationaal vergelijkende studies. Hij ondersteunt preventiewerk in verschillende landen, was adviseur van de International Task Force on Prevention van de Society of Prevention Research en werkte tientallen jaren als supervisor in stuurgroepen van onderwijsinstellingen. De laatste jaren specialiseerde hij zich in moderne data-analyse. Zijn interesse gaat daarbij uit naar multilevel-analyse, effectonderzoek en, sinds kort, machine-learning, zowel frequentisch als Bayesiaans. Maar ook vormgeving en visualisatie heeft zijn interesse. Hij ziet zichzelf meer als liefhebber dan als expert.\nDit zijn blogs die hij graag volgt:\nSimply Statistics\nRbloggers\nR4stats\nDataCamp\nJASP\nMLwiN\nSTATA\nIndividuen die hij graag volgt zijn onder andere:\nKieran Healey\nRens van de Schoot\nDavid Spiegelhalter\nAlison Hill\nHier boeken die op dit moment voor hem belangrijk zijn:\nJohnson, A.A., Ott, M., Dogucu, M. (2021). Bayes Rules! Introduction to Bayesian Modeling with R. https://www.bayesrulesbook.com/\nBatra, N. (ed, 2021). The Epidemiologist R Handbook. https://epirhandbook.com/index.html\nBaumer, B.S., Kaplan, D.T. & Horton, N.J. (2018). Modern Data Science with R. Boca Raton: CRC Press.\nFreeman, M. & Ross, J. (2019). Programming Skills for Data Science. Start writing code to wrangle, analyze, and visualize data with R. Boston: Addison Wesley.\nGillespie, C. & Lovelace, R. (2017). Efficient R Programming. (https://csgillespie.github.io/efficientR/)\nGrolemund, G. & Wickham, H. (2019). R for Data Science. (https://r4ds.had.co.nz/)\nHealey, K. (2019). Data Visualization: A Practical Introduction. Princeton: Princeton University Press.\nIrizarry, R.A. (2019). Introduction to Data Science. Data Analysis and Prediction Algorithms with R. (https://rafalab.github.io/dsbook/)\nLovelace, R., Nowosad, J. & Muenchow, J. (2019). Geocomputation with R. (https://geocompr.robinlovelace.net/)\nMcElreath, R. (2019). Statistical Rethinking. A Bayesian Course with Examples in R and Stan. (Second edition). Boca Raton: Chapman and Hall/CRC Vooral met brms, ggplot2 and the tidyverse https://bookdown.org/connect/#/apps/1850/access\nPoldrack, R.A. (2021). Statistical Thinking for the 21st Century.https://statsthinking21.github.io/statsthinking21-core-site/\nSpiegelhalter, D. (2019). The Art of Statistics. Learning from Data. https://github.com/dspiegel29/ArtofStatistics\nXie, Y., Allaire, J.J. & Grolemund, G.(2019). R Markdown: The Definitive Guide. https://bookdown.org/yihui/rmarkdown/\nXie, Y., Dervieux, C. & Riederer, E. (2021). RMarkdown Cookbook. https://bookdown.org/yihui/rmarkdown-cookbook/"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Exploreren",
    "section": "",
    "text": "De tekst vind je hier: http://www.harriejonkman.nl/wp-content/uploads/2018/01/MDtotaal.pdf"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Thomas Bayes was een Engels wiskundige en presbyteriaans predikant die zichzelf in de tweede helft van de 18e eeuw enkele belangrijke vragen stelde, zoals hoe moeten we onze overtuigingen aanpassen als we nieuwe informatie krijgen en hoe houden we vast aan oude veronderstellingen, ook als ze onhoudbaar zijn geworden. Of laten we die overtuigingen heel makkelijk los, ook als we een klein beetje gaan twijfelen?\nBayes maakt ons ervan bewust dat we onze opvattingen steeds geleidelijk aan passen aan de werkelijkheid. Zijn stelling, of theorema, is een belangrijk instrument geworden voor allerlei wetenschappers en de Bayesiaanse regel staat niet zelden heel stoer op een t-shirt afgebeeld:\n\\[P(A|B) = \\frac {P(A) \\times P(B|A)}{P(B)}\\]\nBayes draaide dingen om en wilde iets zeggen over een hypothese gegeven het bewijsmateriaal dat we hebben. Hoe moet je een veronderstelling begrijpen in het licht van empirisch materiaal? Bayes’ theorie kent vervolgens een lange, ingewikkelde en verrassende geschiedenis die tot op de dag van vandaag doorgaat. Die geschiedenis heeft Sharon Bertsch McGrayne, een Amerikaanse wetenschapsjournalist, heel mooi in beeld gebracht. Maar het was niet Bayes zelf, maar zijn vriend Richard Price, een amateur wiskundige, die deze ideeën openbaar maakte na Bayes’ dood. Zonder Price had die theorie helemaal niet bestaan. Price stuurde Bayes wetenschappelijke werk in en zorgde er zo voor dat er een publicatie van kwam. Naar die stelling werd vervolgens helemaal niet meer omgekeken tot de beroemde Franse wiskundige Pierre Simon Laplace in het begin van de 19e eeuw deze ideeën uitbreidde en op slimme manieren toepaste. Daarna was het maatschappelijk aan en uit met die theorie, werd het gebruikt en raakte het ook weer uit de mode. De ideeën werden op het ene na het andere gebied toegepast, om vervolgens weer te worden veroordeeld omdat het zou werken met vage, subjectieve of onwetenschappelijke uitgangspunten. Rivaliserende kampen (Frequentisten en Baysianen) voerden hier lange tijd grote strijd over.\nDe theorie zelf kan eenvoudig worden uitgelegd. Je hebt een bepaalde hypothese, bijvoorbeeld over een munt. Je denkt dat de kans op kop of munt hetzelde is. Daar ga je van te voren van uit. Als je vijf keer gooit en je gooit steeds kop dan denk je nog dat het toeval is en je past jouw veronderstelling nog niet aan. Anders wordt het als dit 100 keer gebeurt, dan ga je toch echt twijfelen over de munt en ga je denken dat het misschien wel alleen maar kop kent. Bij waarschijnlijkheid gaat het om uitsluitende mogelijkheden die je toekent. Je hebt van tevoren een idee over iets (prior), vervolgens heb je de data (likelihood) en dat zorgt vervolgens voor jouw geupdated kennis (posterior). Bayes’ theorie is een manier om de waarschijnlijkheid steeds op een consistent en logische manier te herberekenen. Dat herberekenen van de hypothese (of kennis die we hebben) vindt dus steeds plaats in het licht van nieuwe bewijzen. Deze geupdated of aangepaste waarschijnlijkheid wordt de posterior probability of gewoon de posterior genoemd.\nOm het iets ingewikkelder te zeggen, de stelling van Bayes’s houdt nu in dat de posterior waarschijnlijkheid van een hypothese gelijk is aan het product van de voorafgaande waarschijnlijkheid van de hypothese (dus wat je weet al, de prior) en de waarschijnlijkheid van het bewijs gegeven de hypothese (de data, de likelihood). Dit deel je vervolgens door de waarschijnlijkheid van alle bewijzen. Dat laatste gebeurt omdat je zo steeds een waarde tussen 0 en 1 te krijgen, en hoe dichter bij 1 hoe groter de kans. Dat is precies wat op dat stoere t-shirt staat.\nHeel veel kennis is er opgebouwd met munten, kaarten en dergelijke. Moeilijker wordt het wanneer je de stelling van Bayes toepast op het echte leven. Maar ook hier gebeurt het op een zelfde manier. Stel dat je naar buitenloopt en ziet dat jouw tuinpad nat is. Dan denk je misschien dat het geregend heeft. Als je verder loopt en ziet dat de straten droog zijn, ga je toch denken dat jouw vrouw misschien de tuin heeft gesproeid. Wanneer je het weerbericht hebt gehoord (waarin regen wordt voorspeld) voordat je naar buitenloopt, denk je nog eerder dat het geregend heeft als je over het natte tuinpad loopt. Met dat weersbericht en dat natte tuinpad word je minder snel van jouw regengedachte afgehaald als je de droge straat ziet. In het dagelijks leven worden gedachten en veronderstellingen voortdurend geüpdated. Kennis wordt gebruikt en nieuw bewijsmateriaal wordt voortdurend daarmee gefilterd en verwerkt in jouw Bayesiaanse hoofd. Steeds meer doen we kennis op over hoe kansen zijn toe te wijzen en bewijs kan worden geëvalueerd in situaties die veel ingewikkelder zijn dan het gooien van munten of het inschatten van regen.\nMcGrayne besteedt veel aandacht aan allerlei bijdragen van individuele wetenschappers aan die boeiende geschiedenis van de Bayesiaanse theorie. Zo bespreekt ze uitgebreid hoe het in oorlogsvoering is gebruikt, in het proces van kolonel Dreyfus, hoe Alan Turing met deze theorie Duitse codes kraakte en er met dat kleine groepje slimme mensen voor zorgde dat de Tweede Wereldoorlog minder lang duurde. Maar veel meer voorbeelden komen aan de orde.\nBayesiaanse theorie wordt tegenwoordig op allerlei gebieden van wetenschap toegepast. Ook in onze dagelijks leven hebben we ermee te maken via vertaalmachines, spamfilters en stuurloze auto’s bijvoorbeeld. We zijn er onszelf nauwelijks van bewust. McGrayne heeft niet alleen oog voor statistici die zich succesvol wijdden aan Bayesiaanse statistiek. Ze laat ook zien hoe deze door andere statici (zoals Fischer, als vertegenwoordiger van de zogenaamde Frequentisten) heftig worden tegengewerkt. De kern van de verschillen tussen deze twee groepen is dat volgens Bayesianen de prior een subjectieve uitdrukking kan zijn van de mate van geloof in een hypothese. Je moet de kennis gebruiken die je al hebt, het is vreemd om steeds maar opnieuw en van voren af aan te beginnen. Frequentisten erkennen dit subjectieve element niet. Voor hen moet wetenschap een objectieve basis hebben, liefste in relatieve frequentie van gebeurtenissen in herhaalbare, welomschreven experimenten.Dat subjectieve element staat daar ver van.\nMcGrayne’s boek is een prachtig wetenschapshistorisch boek Zie ook haar presentatie hier. Ze laat zien dat de Bayesiaanse theorie weer helemaal terug is en dat komt natuurlijk vooral ook door de ontwikkeling van de computer en moderne algoritmen waarmee de theorie niet enkel theorie blijft maar ook praktisch kan worden toegepast. David Spiegelhalter en zijn biostatistische onderzoeksgroep hebben daar op een ongekende manier aan bijgedragen. De Bayesiaanse methode wordt in allerlei onderzoek gebruikt en in verschillende omstandigheden heeft het grote voordelen zien ten opzichte van de frequentistische statistiek. Er wordt vandaag de dag veel pragmatischer mee hiermee omgegaan. De tegenwoordige wetenschapper loopt steeds vaker met twee gereedschapskisten rond en op basis van het probleem besluit hij of zij tot een onderzoekswijze."
  },
  {
    "objectID": "posts/Bayes/index.html",
    "href": "posts/Bayes/index.html",
    "title": "Bayes",
    "section": "",
    "text": "Thomas Bayes was een Engels wiskundige en presbyteriaans predikant die zichzelf in de tweede helft van de 18e eeuw enkele belangrijke vragen stelde, zoals hoe moeten we onze overtuigingen aanpassen als we nieuwe informatie krijgen en hoe houden we vast aan oude veronderstellingen, ook als ze onhoudbaar zijn geworden. Of laten we die overtuigingen heel makkelijk los, ook als we een klein beetje gaan twijfelen?\nBayes maakt ons ervan bewust dat we onze opvattingen steeds geleidelijk aan passen aan de werkelijkheid. Zijn stelling, of theorema, is een belangrijk instrument geworden voor allerlei wetenschappers en de Bayesiaanse regel staat niet zelden heel stoer op een t-shirt afgebeeld:\n\\[P(A|B) = \\frac {P(A) \\times P(B|A)}{P(B)}\\]\nBayes draaide dingen om en wilde iets zeggen over een hypothese gegeven het bewijsmateriaal dat we hebben. Hoe moet je een veronderstelling begrijpen in het licht van empirisch materiaal? Bayes’ theorie kent vervolgens een lange, ingewikkelde en verrassende geschiedenis die tot op de dag van vandaag doorgaat. Die geschiedenis heeft Sharon Bertsch McGrayne, een Amerikaanse wetenschapsjournalist, heel mooi in beeld gebracht. Maar het was niet Bayes zelf, maar zijn vriend Richard Price, een amateur wiskundige, die deze ideeën openbaar maakte na Bayes’ dood. Zonder Price had die theorie helemaal niet bestaan. Price stuurde Bayes wetenschappelijke werk in en zorgde er zo voor dat er een publicatie van kwam. Naar die stelling werd vervolgens helemaal niet meer omgekeken tot de beroemde Franse wiskundige Pierre Simon Laplace in het begin van de 19e eeuw deze ideeën uitbreidde en op slimme manieren toepaste. Daarna was het maatschappelijk aan en uit met die theorie, werd het gebruikt en raakte het ook weer uit de mode. De ideeën werden op het ene na het andere gebied toegepast, om vervolgens weer te worden veroordeeld omdat het zou werken met vage, subjectieve of onwetenschappelijke uitgangspunten. Rivaliserende kampen (Frequentisten en Baysianen) voerden hier lange tijd grote strijd over.\nDe theorie zelf kan eenvoudig worden uitgelegd. Je hebt een bepaalde hypothese, bijvoorbeeld over een munt. Je denkt dat de kans op kop of munt hetzelde is. Daar ga je van te voren van uit. Als je vijf keer gooit en je gooit steeds kop dan denk je nog dat het toeval is en je past jouw veronderstelling nog niet aan. Anders wordt het als dit 100 keer gebeurt, dan ga je toch echt twijfelen over de munt en ga je denken dat het misschien wel alleen maar kop kent. Bij waarschijnlijkheid gaat het om uitsluitende mogelijkheden die je toekent. Je hebt van tevoren een idee over iets (prior), vervolgens heb je de data (likelihood) en dat zorgt vervolgens voor jouw geupdated kennis (posterior). Bayes’ theorie is een manier om de waarschijnlijkheid steeds op een consistent en logische manier te herberekenen. Dat herberekenen van de hypothese (of kennis die we hebben) vindt dus steeds plaats in het licht van nieuwe bewijzen. Deze geupdated of aangepaste waarschijnlijkheid wordt de posterior probability of gewoon de posterior genoemd.\nOm het iets ingewikkelder te zeggen, de stelling van Bayes’s houdt nu in dat de posterior waarschijnlijkheid van een hypothese gelijk is aan het product van de voorafgaande waarschijnlijkheid van de hypothese (dus wat je weet al, de prior) en de waarschijnlijkheid van het bewijs gegeven de hypothese (de data, de likelihood). Dit deel je vervolgens door de waarschijnlijkheid van alle bewijzen. Dat laatste gebeurt omdat je zo steeds een waarde tussen 0 en 1 te krijgen, en hoe dichter bij 1 hoe groter de kans. Dat is precies wat op dat stoere t-shirt staat.\nHeel veel kennis is er opgebouwd met munten, kaarten en dergelijke. Moeilijker wordt het wanneer je de stelling van Bayes toepast op het echte leven. Maar ook hier gebeurt het op een zelfde manier. Stel dat je naar buitenloopt en ziet dat jouw tuinpad nat is. Dan denk je misschien dat het geregend heeft. Als je verder loopt en ziet dat de straten droog zijn, ga je toch denken dat jouw vrouw misschien de tuin heeft gesproeid. Wanneer je het weerbericht hebt gehoord (waarin regen wordt voorspeld) voordat je naar buitenloopt, denk je nog eerder dat het geregend heeft als je over het natte tuinpad loopt. Met dat weersbericht en dat natte tuinpad word je minder snel van jouw regengedachte afgehaald als je de droge straat ziet. In het dagelijks leven worden gedachten en veronderstellingen voortdurend geüpdated. Kennis wordt gebruikt en nieuw bewijsmateriaal wordt voortdurend daarmee gefilterd en verwerkt in jouw Bayesiaanse hoofd. Steeds meer doen we kennis op over hoe kansen zijn toe te wijzen en bewijs kan worden geëvalueerd in situaties die veel ingewikkelder zijn dan het gooien van munten of het inschatten van regen.\nMcGrayne besteedt veel aandacht aan allerlei bijdragen van individuele wetenschappers aan die boeiende geschiedenis van de Bayesiaanse theorie. Zo bespreekt ze uitgebreid hoe het in oorlogsvoering is gebruikt, in het proces van kolonel Dreyfus, hoe Alan Turing met deze theorie Duitse codes kraakte en er met dat kleine groepje slimme mensen voor zorgde dat de Tweede Wereldoorlog minder lang duurde. Maar veel meer voorbeelden komen aan de orde.\nBayesiaanse theorie wordt tegenwoordig op allerlei gebieden van wetenschap toegepast. Ook in onze dagelijks leven hebben we ermee te maken via vertaalmachines, spamfilters en stuurloze auto’s bijvoorbeeld. We zijn er onszelf nauwelijks van bewust. McGrayne heeft niet alleen oog voor statistici die zich succesvol wijdden aan Bayesiaanse statistiek. Ze laat ook zien hoe deze door andere statici (zoals Fischer, als vertegenwoordiger van de zogenaamde Frequentisten) heftig worden tegengewerkt. De kern van de verschillen tussen deze twee groepen is dat volgens Bayesianen de prior een subjectieve uitdrukking kan zijn van de mate van geloof in een hypothese. Je moet de kennis gebruiken die je al hebt, het is vreemd om steeds maar opnieuw en van voren af aan te beginnen. Frequentisten erkennen dit subjectieve element niet. Voor hen moet wetenschap een objectieve basis hebben, liefste in relatieve frequentie van gebeurtenissen in herhaalbare, welomschreven experimenten.Dat subjectieve element staat daar ver van.\nMcGrayne’s boek is een prachtig wetenschapshistorisch boek Zie ook haar presentatie hier. Ze laat zien dat de Bayesiaanse theorie weer helemaal terug is en dat komt natuurlijk vooral ook door de ontwikkeling van de computer en moderne algoritmen waarmee de theorie niet enkel theorie blijft maar ook praktisch kan worden toegepast. David Spiegelhalter en zijn biostatistische onderzoeksgroep hebben daar op een ongekende manier aan bijgedragen. De Bayesiaanse methode wordt in allerlei onderzoek gebruikt en in verschillende omstandigheden heeft het grote voordelen zien ten opzichte van de frequentistische statistiek. Er wordt vandaag de dag veel pragmatischer mee hiermee omgegaan. De tegenwoordige wetenschapper loopt steeds vaker met twee gereedschapskisten rond en op basis van het probleem besluit hij of zij tot een onderzoekswijze."
  },
  {
    "objectID": "posts/Latex/latex.html",
    "href": "posts/Latex/latex.html",
    "title": "Latex",
    "section": "",
    "text": "Hier vind je een snelle introductie op de werking van Latex"
  },
  {
    "objectID": "posts/Exploreren/index.html",
    "href": "posts/Exploreren/index.html",
    "title": "Exploreren",
    "section": "",
    "text": "De tekst vind je hier: http://www.harriejonkman.nl/wp-content/uploads/2018/01/MDtotaal.pdf"
  },
  {
    "objectID": "posts/Latex/index.html",
    "href": "posts/Latex/index.html",
    "title": "Latex",
    "section": "",
    "text": "Hier vind je een snelle introductie op de werking van Latex"
  },
  {
    "objectID": "posts/Op-weg-naar-infografieken/index.html",
    "href": "posts/Op-weg-naar-infografieken/index.html",
    "title": "Op weg naar infografieken",
    "section": "",
    "text": "Hier vind je een goed document"
  },
  {
    "objectID": "posts/Psm/index.html",
    "href": "posts/Psm/index.html",
    "title": "Propensity Score Matching",
    "section": "",
    "text": "## Impactevaluatie Om precies het effect van een aanpak of politieke keuze vast te stellen is een ingewikkelde kwestie. Toch is er dat soort onderzoek nodig om de keuze voor programma’s te legitimeren. Tegenwoordig is er een heel spectrum van technieken om de impact van programma’s vast te stellen. Dit zijn technieken die kunnen worden gebruikt binnen hele verschillende soorten impactstudies. Het is goed daar kennis van te nemen, zeker nu steeds meer mogelijk is omdat er meer data beschikbaar zijn waarop deze evaluaties gebaseerd kunnen worden. Impactstudies worden uitgevoerd om vast te stellen of programma’s de effecten opleveren die ze nastreven, om te begrijpen of en waarom deze programma’s werken, om vast te stellen in hoeverre veranderingen zijn toe te schrijven aan de inzet van het programma en ook om vast te stellen of de gelden op een goede manier worden besteed. Op dit terrein is er natuurlijk een enorme hoeveelheid literatuur en enkele uitgaven geven ons hiervan een goed en up-to-date overzicht1. Experimentele studies kunnen natuurlijk goede impactstudies zijn, met sterke punten en beperkingen. Maar er zijn ook aanvullende methodes die in quasi-experimentele of observationele studies kunnen worden toegepast. Zo zijn er panel datamethodes die gebruikt kunnen worden, regressie discontinu?teit methodes en instrumentele variabelen methodes. Daarnaast zijn er verschillende matchingsmethodes die in impactstudies worden gebruikt. Hier stellen we zo’n matchingsmethode voor die goed gebruikt kan worden in verschillende soorten impactstudies en laten we zien hoe deze uitgevoerd kan worden."
  },
  {
    "objectID": "posts/Psm/index.html#propensity-score-matching-als-optie",
    "href": "posts/Psm/index.html#propensity-score-matching-als-optie",
    "title": "Propensity Score Matching",
    "section": "Propensity Score Matching als optie",
    "text": "Propensity Score Matching als optie\nPropensity Score Matching is een statische techniek waarin een individu (of andere eenheid) die behandeld wordt of die ergens aan meedoet, wordt gematcht met een of meer respondenten uit de controlegroep op basis van de propensity score. Deze matchingstechniek versterkt causale argumenten in quasi-experimentele en observationele studies omdat ze selectie-bias reduceert. In dit artikel concentreren we ons op hoe we propensity score matching uitvoeren via een voorbeeld uit het onderwijsveld. Het doel van dit artikel is om informatie te geven zodat het gebruikt kan worden door onderzoekers en mensen die evaluatiestudies uitvoeren2 3."
  },
  {
    "objectID": "posts/Psm/index.html#stap-voor-stap",
    "href": "posts/Psm/index.html#stap-voor-stap",
    "title": "Propensity Score Matching",
    "section": "Stap voor stap",
    "text": "Stap voor stap\nPropensity score matching is een statistische techniek waarin een geval uit de behandelgroep wordt gematcht met een of meer gevallen uit de controlegroep gebaseerd op ieders propensity score. Elders zijn er veel uitleg en onderbouwingen te vinden voor propensity score (Adelson, 20134; Holland, 1986 5; Rubin, 2005 6; Rudner & Peyton, 20067; Shadish, Cook, & Campbell, 20028; Stone & Tang, 20139 ). In dit artikel concentreren we ons op de vraag hoe we propensity score matching moeten uitvoeren door een voorbeeld uit het onderwijsveld te geven. In dit document willen we vooral een stap-voor-stap voorbeeld geven van de uitvoering van propensity score matching in R. We gebruiken hierbij het MatchIt-pakket met ‘nearest-neighbor 1-to-1 matching’. Terwijl er andere software is dan R voor het uitvoeren van propensity score matching, hebben we voor R gekozen omdat het open-source software is en omdat het breed gebruikt wordt door data-wetenschappers binnen verschillende disciplines. Het doel van dit artikel is om informatie te verschaffen zodat propensity score matching binnen het bereik komt van onderzoek en evaluatie."
  },
  {
    "objectID": "posts/Psm/index.html#informatie-over-de-gebruikte-dataset",
    "href": "posts/Psm/index.html#informatie-over-de-gebruikte-dataset",
    "title": "Propensity Score Matching",
    "section": "Informatie over de gebruikte dataset",
    "text": "Informatie over de gebruikte dataset\nData van een observationele studie door Falbe (2014) worden hier gebruikt om te illustreren hoe propensity score matching werkt. In die studie gebruikt Falbe algemeen toegankelijke schooldata van verschillende staten om te onderzoeken of een bepaalde interventie een voorspeller was van succes in de resultaten op lezen of rekenen/wiskunde wanneer er voor de schoolgrootte worden gecontrolleerd (tot), percentage studenten van minderheden (min) en het percentage studenten dat een vrije of in prijs gereduceerde lunch ontvangt (dis)10. Voor het voorbeeld hier gebruiken we alleen Falbe’s schooldata van de staat New York. In de New York data set, zijn er 25 stw scholen en 560 niet-stw scholen (stw refereert hier naar een bepaalde interventie). Als matchingsvariabelen koos Falbe schoolgrootte, percentage minderheids studenten en het percentage studenten dat een vrije of een in prijs gereduceerde lunch ontvangt. De reden dat ze voor deze matchings variabelen koos, was dat vorig onderzoek had aangetoond dat deze variabelen samenhingen met academische resultaten. Door op deze variabelen te matchen, kon ze, dat was haar doel, selectie-bias reduceren tussen experimentele (“treated”, bijvoorbeeld stw) en controle groep (“control”, bijvoorbeeld niet-stw-scholen). Let op dat ondanks Falbe in haar studie naar correlaties zocht en niet-experimenteel van opzet was, we hier de termen behandeling (treated) en controle (control) gebruiken omdat deze termen worden gebruikt in het programma en de output van het MatchIt pakket."
  },
  {
    "objectID": "posts/Psm/index.html#de-stappen-in-het-uitvoeren-van-propensity-score-matching-in-r",
    "href": "posts/Psm/index.html#de-stappen-in-het-uitvoeren-van-propensity-score-matching-in-r",
    "title": "Propensity Score Matching",
    "section": "De stappen in het uitvoeren van Propensity Score Matching in R",
    "text": "De stappen in het uitvoeren van Propensity Score Matching in R"
  },
  {
    "objectID": "posts/Reproducable-research/index.html#instructies-bij-het-installeren",
    "href": "posts/Reproducable-research/index.html#instructies-bij-het-installeren",
    "title": "Dynamische documenten maken met RMarkdown en knitr",
    "section": "Instructies bij het installeren",
    "text": "Instructies bij het installeren\nVoordat je gaat werken met de workshopmaterialen, zorg ervoor dat je het volgende hebt gedaan:\n\nOpen RStudio.\n\nInstalleer en download het devtools R pakket door het volgende commando te runnen.\n\n\ninstall.packages(\"devtools\") # Nodig voor deze sessie\nlibrary(\"devtools\")          # Nodig voor deze sessie   \n\n\nCheck of je de goede versie hebt van R en RStudio door devtools::session_info() in de R console te draaien.\nHier geeft devtools:: aan om de session_info() functie in R te gebruiken ipv het devtools pakket en de sessionInfo() functie binnen het utils pakket. Het runnen van devtools::session_info() stelt ons in staat de versie van R en RStudio vast te stellen.\n\nHeb je de volgende versie van R en RStudio?\n\nR: Versie 3.3.0 (2016-05-03)\n\nRStudio: 0.99.1172\n\nZo ja dan kun je van start gaan!\n\nZo nee dan heb je nieuwe versies van R en RStudio nodig, volg dan Setup in dit document.\n\n\n\nInstalleer andere R pakketten die nodig zijn voor deze workshop.\n\n\n## Installeer de goede pakketten\ninstall.packages(\"rmarkdown\")  # Dit zorgt voor koele dynamische documenten\ninstall.packages(\"knitr\")      # Hier kun je R code Chunks mee runnen\ninstall.packages(\"ggplot2\")    # Voor het plotten van mooie figuren\ninstall.packages(\"DT\")         # Om interactieve HTML tabellen te maken\n\n\n## Deze pakketten ook laden om er zeker van te zijn dat je de goede pakketten hebt\nlibrary(\"rmarkdown\")           # Dit zorgt voor koele dynamische documenten\nlibrary(\"knitr\")               # Hier kun je R code Chunks mee runnen\nlibrary(\"ggplot2\")             # Voor het plotten van mooie figuren\nlibrary(\"DT\")                  # Om interactieve HTML tabellen te maken\n\n\nAls je de pakketten zonder fouten hebt geladen, ben je klaar voor deze workshop!\n\nZijn er nog problemen, meld het ajb!"
  },
  {
    "objectID": "posts/Reproducable-research/index.html#reproduceerbaar-onderzoek",
    "href": "posts/Reproducable-research/index.html#reproduceerbaar-onderzoek",
    "title": "Dynamische documenten maken met RMarkdown en knitr",
    "section": "Reproduceerbaar onderzoek",
    "text": "Reproduceerbaar onderzoek\nReproduceerbaar onderzoek is een mogelijk product van dynamische documenten, echter, goed resultaat is niet gegarandeerd!\nGoede uitvoering van reproduceerbaar onderzoek houdt in ieder geval in:\n\nHet hele project in een directory plaatsen die wordt ondersteund door de ‘version control’.\n\nCode en data vrijlaten.\n\nAlles documenteren en de code als documentatie gebruiken!\n\nFiguren, tabellen en de statistiek zijn het resultaat van scripts en codes die in de tekst staan.\n\nSchrijf in de codes de paden die worden gebruikt.\n\nStel ‘seed’ in zodat een volgende persoon dezelfde resultaten krijgt.\n\nLaat ook informatie zien waarmee de code file wordt uitgevoerd. Je kunt bijvoorbeeld de devtools::session_info() gebruiken.\n\nOm meer over reproduceerbaarheid en datamanagement te lezen, kun je Vince Buffalo’s Boek erop naslaan[@Buffalo2015]."
  },
  {
    "objectID": "posts/Reproducable-research/index.html#waarom-r-markdown",
    "href": "posts/Reproducable-research/index.html#waarom-r-markdown",
    "title": "Dynamische documenten maken met RMarkdown en knitr",
    "section": "Waarom R Markdown?",
    "text": "Waarom R Markdown?\nEen aantrekkelijk gereedschap voor reproduceerbare en dynamische rapporten!\n\nTerwijl het was gemaakt voor R, accepteert het veel programmeertalen. Om het eenvoudig te houden, werken we vandaag alleen met R.\n\nEen code kan op een aantal manieren worden uitgevoerd:\n\nInline Code: Een korte code die in de geschreven tekst van het document wordt uitgevoerd.\nCode Chunks: Delen van het document omvatten verschillende zinnen voor programmeer of analyse code. Dat kan een plot of een tabel zijn, maar ook berekeningen van de samenvattende statistiek, pakketten laden, etc.\n\n\nHet is makkelijk om:\n\nPlaatjes op te nemen.\n\nDe Markdown syntax te leren.\n\nLaTeX vergelijkingen op te nemen.\n\nInteractieve tabellen op te nemen.\nGebruik de versie via Git.\n\nDan is het makkelijker om te delen en samen te werken in analyses, projecten en publicaties!\n\nExterne linken toe te voegen - Rmarkdown begrijpt zelfs enige html codes!\n\nOm mooie documenten te maken.\n\nJe hoeft je geen zorgen te maken over pagina breuken of het plaatsen van de figuren.\n\nConsolideer jouw code en plaats het in een file:\n\nPowerpoint, PDFs, html documenten en word files"
  },
  {
    "objectID": "posts/Reproducable-research/index.html#eenvoudige-werkwijze",
    "href": "posts/Reproducable-research/index.html#eenvoudige-werkwijze",
    "title": "Dynamische documenten maken met RMarkdown en knitr",
    "section": "Eenvoudige werkwijze",
    "text": "Eenvoudige werkwijze\nIn het kort, om een rapport te maken:\n\nOpen een .Rmd file.\n\nMaak een YAML kop (meer hierover zo dadelijk!)\n\nSchrijf de inhoud met RMarkdown syntax.\n\nNeem mee de R code in code chunks of met een inline code.\n\nDraai de document output.\n\n\n\n\nWerkwijze om een rapport te maken\n\n\nOverzicht van de stappen die RMarkdown maakt om een ‘gerenderd’ document te krijgen:\n\nMaak een .Rmd rapport met ‘R code chunks’ en markdown verhalen (zoals hierboven in stappen beschreven).\n\nGeef de .Rmd-file aan knitr om de ‘R code chunks’ uit te voeren en een nieuwe .md file te maken.\n\nKnitr is een pakket binnen R die jou in staat stelt de code binnen RMarkdown documenten uit te voeren zoals HTML, latex, pdf, word en andere document types.\n\n\nGeef de .md file aan pandoc, die er een definitief document van maakt (b.v. html, Microsoft word, pdf, etc.).\n\nPandoc is een universeel gereedschap om documenten te converteren en zet het ene document type (in dit geval: .Rmd) om in een ander (in dit geval: HTML)\n\n\n\n\n\nHoe een Rmd document wordt omgezet\n\n\nHoewel dit mogelijk wat ingewikkeld lijkt, kunnen we op de “Knit” knop drukken boven aan de pagina die er zo uitziet:\n\nof we kunnen de volgende code runnen:\n\nrmarkdown::render(\"RMarkdown_Lesson.Rmd\", \"html_document\")"
  },
  {
    "objectID": "posts/Reproducable-research/index.html#maak-een-.rmd-file",
    "href": "posts/Reproducable-research/index.html#maak-een-.rmd-file",
    "title": "Dynamische documenten maken met RMarkdown en knitr",
    "section": "Maak een .Rmd file",
    "text": "Maak een .Rmd file\nHet wordt tijd! Laten we met RMarkdown gaan werken!\n\nIn de menu bar, klik je op File -> New File -> RMarkdown\n\nOf je klikt eenvoudig op het groene plus teken links boven in de hoek van RStudio.\n\n\n\n\nHet volgende zal omhoog komen.\n\n\n\nHierbinnen kies je het type output dat je wilt hebben. Opgelet: deze output kan later heel makkelijk worden aangepast!\n\n\n\nKlik OK"
  },
  {
    "objectID": "posts/Reproducable-research/index.html#yaml-koppen",
    "href": "posts/Reproducable-research/index.html#yaml-koppen",
    "title": "Dynamische documenten maken met RMarkdown en knitr",
    "section": "YAML koppen",
    "text": "YAML koppen\nYAML staat voor “YAML Ain’t Markup Language” en is eigenlijk een soort geklusterde structuur voor de metadata van het document. Het staat tussen twee regels van drie streepjes --- en wordt automatisch omgezet door RStudio. Een eenvoudig voorbeeld:\n---\ntitle:  \"Analyse Rapport\"  \nAuthor:  \"Harrie Jonkman\"  \ndate: \"1 Maart 2017\"  \noutput:  html_document\n---\nHet voorbeeld boven zal een HTML document maken. Echter, de volgende opties zijn ook beschikbaar.\n\nhtml_document\n\npdf_document\n\nword_document\n\nbeamer_presentation (pdf powerpoint)\n\nioslides_presentation (HTML powerpoint)\n\nen nog meer …\n\nVandaag leggen we de nadruk op HTML files. Echter voel je vrij als je hier wat mee wilt spelen door bv. word en pdf documenten te maken. Presentatie documenten kennen een wat andere syntax (bv. om aan te geven wanneer de ene dia eindigt en de andere begint) en dan is er nog wat markdown syntax specifiek voor presentaties maar die gaat voorbij het doel van deze workshop.\nIn deze workshops bouwen we verder voort op de details van YAML koppen."
  },
  {
    "objectID": "posts/Reproducable-research/index.html#markdown-basis",
    "href": "posts/Reproducable-research/index.html#markdown-basis",
    "title": "Dynamische documenten maken met RMarkdown en knitr",
    "section": "Markdown Basis",
    "text": "Markdown Basis\nKijk hiernaar RMarkdown Reference Guide\nHaal hier ook informatie vandaan RMarkdown Cheatsheet:\n\n\n\nMarkdown Basis van RStudio’s RMarkdown Cheatsheet\n\n\nHandige tips:\n\nEindig elke regel met twee spaties om een nieuwe paragraaf te beginnen.\n\nWoorden binnen een code moeten aan beide kanten zo’n kommateken kennen: `\n\nOm iets tot superscript te maken moet je een ^ aan beide zijden plaatsen. Superscript werd gevormd door Super^script^ te typen.\n\nVergelijkingen kunnen in een inline code worden geplaatst met $ en als blok gecentreerd binnen het document door $$. Bijvoorbeeld \\(E = mc^2\\) staat tussen de regels terwijl het volgende geblokt wordt opgenomen: \\[E = mc^2\\]\n\nOpgelet: Om met $ en $$ een superscript ^ te maken, is het nodig om voor elk aLFAnumeriEK dat superscript te gebruiken.\n\nAnder wiskundig materiaal:\n\nVierkantswortel: $\\sqrt{b}$ zal \\(\\sqrt{b}\\) maken\nBreuken: $\\frac{1}{2}$ = \\(\\frac{1}{2}\\)\n\n\nVergelijkingen met breuken: $f(x)=\\frac{P(x)}{Q(x)}$ = \\(f(x)=\\frac{P(x)}{Q(x)}\\)\n\n\n\nBinomiale Coefficienten: $\\binom{k}{n}$ = \\(\\binom{k}{n}\\)\n\nIntegralen: $$\\int_{a}^{b} x^2 dx$$ = \\[\\int_{a}^{b} x^2 dx\\]\n\nShareLaTeX is een prachtige bron voor LaTeX-codes.\n\n\nNog wat wiskundig materiaal:\n\n\n\n\n\n\n\n\nBeschrijving\nCode\nVoorbeelden\n\n\n\n\nGriekse letters\n$\\alpha$ $\\beta$ $\\gamma$ $\\rho$ $\\sigma$ $\\delta$ $\\epsilon$ $mu$\n\\(\\alpha\\) \\(\\beta\\) \\(\\gamma\\) \\(\\rho\\) \\(\\sigma\\) \\(\\delta\\) \\(\\epsilon\\) \\(\\mu\\)\n\n\nBinaire handelingen\n$\\times$ $\\otimes$ $\\oplus$ $\\cup$ $\\cap$\n\\(\\times\\) \\(\\otimes\\) \\(\\oplus\\) \\(\\cup\\) \\(\\cap\\) \\(\\times\\)\n\n\nRelationele handelingen\n$< >$ $\\subset$ $\\supset$ $\\subseteq$ $\\supseteq$\n\\(< >\\) \\(\\subset\\) \\(\\supset\\) \\(\\subseteq\\) \\(\\supseteq\\)\n\n\nVerder\n$\\int$ $\\oint$ $\\sum$ $\\prod$\n\\(\\int\\) \\(\\oint\\) \\(\\sum\\) \\(\\prod\\)\n\n\n\n\n\nUitdaging: Probeer de volgende output te maken:\n\n\nVandaag voel ik mij vet omdat ik RMarkdown leer.\nhoning is heel zoet.\nYAS!!!!!!\nR2 waarden zijn informatief!\n\\(R^{2}\\) beschrijft de variantie verklaard door het model.\nIk kende geen RMarkdown Vandaag heb ik RMarkdown geleerd\nRStudio link\nOutput van het volgende:\n\n\n# RMarkdown   \n## R   \n### Knitr   \n#### Pandoc  \n##### HTML  \n\n\\(\\sqrt{b^2 - 4ac}\\)\n\\[\\sqrt{b^2 - 4ac}\\]\n\\(X_{i,j}\\)\n\n\n\nVandaag maak ik een dynamisch document!\n\n\n\nHet volgende lijstje:\n\nChocolade Chips Kook Recept\n\nboter\nsuiker\n\nEen mengsel van bruine en witte suiker maakt het lekkerder\n\nmix dat met boter voordat je de eieren eraan toevoegt\n\n\neieren\nvanille\n\nMix wat droge ingredienten:\n\nmeel, zout, bak soda\n\n\nchocolade chips\n\n\nFijn feitje! De inhoudsopgave van deze website is gemaakt met koppen met 1-3 pond symbolen! (Daarover dadelijk meer)"
  },
  {
    "objectID": "posts/Reproducable-research/index.html#een-code-in-het-document",
    "href": "posts/Reproducable-research/index.html#een-code-in-het-document",
    "title": "Dynamische documenten maken met RMarkdown en knitr",
    "section": "Een Code in het document",
    "text": "Een Code in het document\nEr zijn twee manieren om een code in een RMarkdown document op te nemen.\n\nCode in het document: Korte code als een onderdeel van het geschreven document.\nCode Chunks: Delen van het document die verschillende programmeer of analyse codes omvatten. Daarmee kan een figuur of tabel worden gemaakt, statistieken worden berekend, pakketten worden geladen, etc.\n\n\nR Code in het document\nEen R code kan in het document wordt gemaakt door een komma hoog achterwaarts (`) en de letter r gevolgd door nog zo’n komma.\n\nBijvoorbeeld: 211 is 2048.\n\nStel dat je een p-waarde rapporteert en je wilt niet terug om de statistische test steeds weer uit te voeren. De p-waarde was eerder 0.0045.\nDit is echt handig als de resultaten op papier moeten worden gezet. Bijvoorbeeld, je hebt een aantal statistieken uitgevoerd voor jouw wetenschappelijke vragen is dit een manier waarop R die waarde in a variabele naam bewaart. Bijvoorbeeld: Wijkt het brandstofverbruik van de automaat significant af de auto met handtransmissie significant af binnen de mtcars data set?\n\nmpg_auto <- mtcars[mtcars$am == 0,]$mpg # automatic transmission mileage\nmpg_manual <- mtcars[mtcars$am == 1,]$mpg # manual transmission mileage\ntransmission_ttest <- t.test(mpg_auto, mpg_manual)\n\nOm de p-waarde vast te stellen kunnen we transmission_ttest$p.value als R code in het document gebruiken.\nDe p-waarde is dan 0.0013736.\n\n\nR Code Chunks\nR code chunks (nogmaals ik gebruik maar de Engelse benaming hier, sorry)kunnen worden gebruikt om de R output in het document te krijgen of om de code als illustratie zichtbaar te maken.\nDe anatomie van een code chunk:\nOm een R code chunk te plaatsen, kun je met de hand typen door ```{r} gevolgd door ``` op een volgende regel. Je kunt ook de Insert a new code chunk knop gebruiken of de ‘shortcut key’. Dat geeft dan de volgende code chunk:\n\n\n\nEen code chunk invoeren\n\n\n`\u000b``{r}\nn <- 10\nseq(n)\n```\nGeef de code chunk een betekenisvolle naam die samenhangt met wat het doet. Hieronder heb ik code chunk 10-random-numbers genoemd:\n`\u000b``{r 10-random-numbers}\nn <- 10\nseq(n)\n```\nDe code chunk input en output zien er dan als volgt uit:\n\nn = 10\nseq(n)\n\n [1]  1  2  3  4  5  6  7  8  9 10"
  },
  {
    "objectID": "posts/Reproducable-research/index.html#chunk-labels",
    "href": "posts/Reproducable-research/index.html#chunk-labels",
    "title": "Dynamische documenten maken met RMarkdown en knitr",
    "section": "Chunk Labels",
    "text": "Chunk Labels\nChunk labels krijgen unieke IDs in een document en zijn goed voor:\n\nOm externe files te genereren zoals plaatjes en ‘cached’ documenten.\n\nChunk labels zijn vaak output als fouten omhoog komen(vaker voor codes in het document).\n\nNavigeren door lange .Rmd documenten.\n\n\n\n\nEen methode om door lange .Rmd files te navigeren\n\n\nAls je de code chunk een naam geef, gebruik dan - of _ tussen woorden voor code chunks labels in plaats van ruimtes. Dat helpt jou en andere gebruikers bij het navigeren in het document.\nChunk labels moeten uniek zijn in het document - anders zal er een fout optreden!"
  },
  {
    "objectID": "posts/Reproducable-research/index.html#chunk-opties",
    "href": "posts/Reproducable-research/index.html#chunk-opties",
    "title": "Dynamische documenten maken met RMarkdown en knitr",
    "section": "Chunk Opties",
    "text": "Chunk Opties\nDruk tab als tussen de haakjes code chunk opties omhoog komen.\n\n\n\nEnkele Knitr Chunk Opties\n\n\n\nresults = \"asis\" staat voor “as is” en geeft de output van een niet geformateerde versie.\ncollapse is een andere chunk optie die handig kan zijn, zeker als een code chunk veel korte R uitdrukking heeft met wat output.\n\nEr zijn teveel chunk opties om hier te behandelen. Kijk na deze workshop nog eens wat rond voor deze opties.\nEen mooie website om dat op te doen is Knitr Chunk Options.\n\nUitdaging\nDraai de code chunk hieronder en speel wat met de volgende knitr code chunk opties:\n\n\n\neval = TRUE/FALSE\n\necho = TRUE/FALSE\n\ncollapse = TRUE/FALSE\n\nresults = \"asis\",\"markup en \"hide\n\n\n\nSla je resultaten op in markdown.\nOpgelet: Wees er zeker van dat je jouw chunks een naam geeft!\n\n\n1+1\n2*5\nseq(1, 21, by = 3)\nhead(mtcars)\n\nEnkele voorbeelden voortbouwend op de chunk hierboven\nResultaten van results=\"markup\", collapse = TRUE}:\n\n1+1\n## [1] 2\n2*5\n## [1] 10\nseq(1, 21, by = 3)\n## [1]  1  4  7 10 13 16 19\nhead(mtcars)\n##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nResultaten van results=\"asis\", collapse = TRUE}:\n1+1\n[1] 2\n2*5\n[1] 10\nseq(1, 21, by = 3)\n[1] 1 4 7 10 13 16 19\nhead(mtcars)\n               mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1"
  },
  {
    "objectID": "posts/Reproducable-research/index.html#globale-opties",
    "href": "posts/Reproducable-research/index.html#globale-opties",
    "title": "Dynamische documenten maken met RMarkdown en knitr",
    "section": "Globale opties",
    "text": "Globale opties\nHet kan zijn dat je dezelfde chunk settings wilt handhaven voor het gehele document. Het kan daarom handig zijn om de opties in een keer te typen in plaats van het iedere keer weer voor een chunk te moeten doen. Om dat te doen kun je de globale chunk opties bovenaan het document vaststellen.\nknitr::opts_chunk$set(echo = FALSE, \n                      eval = TRUE, \n                      message = FALSE,\n                      warning = FALSE, \n                      fig.path = \"Figures/\",\n                      fig.width = 12, \n                      fig.height = 8)\nAls je bijvoorbeeld met iemand samenwerkt die de code niet wil zien, kun je schrijven eval = TRUE en echo = FALSE gebruiken zodat de code wel gedraaid wordt maar niet getoond. In aanvulling wil je misschien message = FALSE en warning = FALSE gebruiken zodat jouw samenwerkingspartner geen enkele boodschap of waarschuwing van R ziet.\nAls je figuren wilt opslaan en bewaren in een subdirectory binnen het project, gebruik dan fig.path = \"Figures/\". Hier verwijst de \"Figures/\" naar een folder Figures binnen de huidige directory waar de figuur die gemaakt wordt in het document wordt opgeslagen.\nOpgelet: de figuren worden niet standaard opgeslagen.\nGlobale chunk opties zullen voor de rest van het documenten worden vastgezet. Als je wilt dat een bepaalde chunk afwijkt van de globale opties, maak dat aan het begin van die bepaalde chunk duidelijk."
  },
  {
    "objectID": "posts/Reproducable-research/index.html#figuren",
    "href": "posts/Reproducable-research/index.html#figuren",
    "title": "Dynamische documenten maken met RMarkdown en knitr",
    "section": "Figuren",
    "text": "Figuren\nKnitr maakt vrij eenvoudig figuren. Als een analyse code binnen een chunk een bepaald figuur moet produceren, dan zal hij dat in het document afdrukken.\nEnkele knitr chunk opties gerelateerd aan figuren:\n\nfig.width en fig.height\n\nStandaard: fig.width = 7, fig.height = 7\n\n\nfig.align: Hoe het figuur uit te lijnen\n\nOpties omvatten: \"left\", \"right\" en \"center\"\n\n\nfig.path: Een file pad naar de directory waar knitr de grafische output moet opslaan die er met de chunk wordt gemaakt.\n\nStandaard: 'figure/'\n\n\nEr is zelfs een fig.retina(alleen voor HTML output) voor hogere figuur resoluties met retina afdrukken.\n\n\n\n\nEen enkelvoudig figuur maken:\nMet fig.align = \"center\"\n\nggplot(mtcars, aes(x = mpg)) + xlab(\"Miles per Gallon\") +\n    geom_histogram(bins = 30, fill = \"cornflowerblue\", color = \"black\") +\n    geom_vline(xintercept=mean(mtcars$mpg), col=\"red\") \n\n\n\n\n\n\n\n\nMet fig.align = \"right\"\n\nggplot(mtcars, aes(x = mpg)) + xlab(\"Miles per Gallon\") +\n    geom_histogram(bins = 30, fill = \"cornflowerblue\", color = \"black\") +\n    geom_vline(xintercept=mean(mtcars$mpg), col=\"red\") \n\n\n\n\n\n\n\n\nMet fig.align = \"left\"\n\nggplot(mtcars, aes(x = mpg)) + xlab(\"Miles per Gallon\") +\n    geom_histogram(bins = 30, fill = \"cornflowerblue\", color = \"black\") +\n    geom_vline(xintercept=mean(mtcars$mpg), col=\"red\") \n\n\n\n\n\n\n\n\nMet fig.width = 2, fig.height = 2\n\nggplot(mtcars, aes(x = mpg)) + xlab(\"Miles per Gallon\") +\n    geom_histogram(bins = 30, fill = \"cornflowerblue\", color = \"black\") +\n    geom_vline(xintercept=mean(mtcars$mpg), col=\"red\") \n\n\n\n\n\n\n\n\nMet fig.width = 10, fig.height = 10\n\nggplot(mtcars, aes(x = mpg)) + xlab(\"Miles per Gallon\") +\n    geom_histogram(bins = 30, fill = \"cornflowerblue\", color = \"black\") +\n    geom_vline(xintercept=mean(mtcars$mpg), col=\"red\") \n\n\n\n\n\n\n\n\n\nmyplots <- list()  # new empty list\nfor(i in 1:ncol(mtcars)){\n  col <- names(mtcars)[i]\n  ggp <- ggplot(mtcars, aes_string(x = col)) +\n    geom_histogram(bins = 30, fill = \"cornflowerblue\", color = \"black\") +\n    geom_vline(xintercept = mean(mtcars[[col]]), col = \"red\") \n  myplots[[i]] <- ggp  # add each plot into plot list\n}\nmultiplot(plotlist = myplots, cols = 4) # must load in multiplot function from the Rcookbook see http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/"
  },
  {
    "objectID": "posts/Reproducable-research/index.html#tabellen",
    "href": "posts/Reproducable-research/index.html#tabellen",
    "title": "Dynamische documenten maken met RMarkdown en knitr",
    "section": "Tabellen",
    "text": "Tabellen\nTabellen kunnen in Markdown voor nogal wat hoofdpijn kosten. We gaan er hier verder niet op in. Als je meer wilt leren over Markdown-tabellen kijk naar documentation on tables op de RMarkdown website.\nEr zijn enkele tabeltypen die handig kunnen zijn. Hier zullen we ons vorig voorbeeld gebruiken van de mtcars data\nIn zijn Knitr in a Knutshell introduceert Dr. Karl Broman: kable, panderen xtable en vooral die eerste twee deden mij plezier:\n\nkable: Binnen het knitr pakket - niet veel opties maar het ziet er goed uit.\npander: Binnen het pander pakket - heeft veel opties en handigheden. Makkelijk voor het vetmaken van waarden (bv. waarden onder een bepaalde waarde).\n\nkable en pander tabellen zijn mooi en handig bij het maken van niet-interactieve tabellen:\n\nkable(head(mtcars, n = 4)) # kable table with 4 rows\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n\n\n# Pander table\n# install.packages(\"pander\") # install pander first\nlibrary(pander)\npander(head(mtcars, n = 4))\n\n\nTable continues below\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\n\n\n\n\nMazda RX4\n21\n6\n160\n110\n3.9\n2.62\n16.46\n0\n1\n\n\nMazda RX4 Wag\n21\n6\n160\n110\n3.9\n2.875\n17.02\n0\n1\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.32\n18.61\n1\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n\n\n\n\n\n\n\n\n\n\n\n \ngear\ncarb\n\n\n\n\nMazda RX4\n4\n4\n\n\nMazda RX4 Wag\n4\n4\n\n\nDatsun 710\n4\n1\n\n\nHornet 4 Drive\n3\n1"
  },
  {
    "objectID": "posts/Reproducable-research/index.html#html-widgets",
    "href": "posts/Reproducable-research/index.html#html-widgets",
    "title": "Dynamische documenten maken met RMarkdown en knitr",
    "section": "HTML Widgets",
    "text": "HTML Widgets\nMet de uitgave van de nieuwe RMarkdown v2 is het makkelijker dan ooit tevoren om HTML Widgets te gebruiken. Volg de link om uit te zoeken in welke widgets jij ge?nteresseerd bent!\nOnlangs ontdekte ik bijvoorbeeld het DT pakket waarmee tabellen interactief kunnen worden gemaakt in de HTML output. Daarbij levert Plotly for R echt mooie interactieve grafieken op, welke gebaseerd zijn op Plotly.\nCool, of niet?\n\n# DT table = interactive\n# install.packages(\"DT\") # install DT first\nlibrary(DT)\ndatatable(head(mtcars, n = nrow(mtcars)), options = list(pageLength = 5)) \n\n\n\n\n# plotly\n# install.packages(\"plotly\")\n#library(plotly)\n#mtcars$car <- row.names(mtcars)\n#plot_ly(mtcars, x = wt, y = mpg, \n#        text = paste(\"Car: \", car),\n#       mode = \"markers\", color = wt, size = wt)"
  },
  {
    "objectID": "posts/Reproducable-research/index.html#spelling-controleren",
    "href": "posts/Reproducable-research/index.html#spelling-controleren",
    "title": "Dynamische documenten maken met RMarkdown en knitr",
    "section": "Spelling controleren",
    "text": "Spelling controleren\nIn de spelling kunnen natuurlijk altijd fouten zitten en daarom kan het nodig zijn dat we onze spelling in het document willen controleren. Er zijn twee manieren om de spelling te controleren:\n\nDruk op de “ABC check mark”  links van de vergrootglasknop in RStudio.\nGebruik de aspell() functie van het utils pakket. Je kunt dan echter beter de code chunks overslaan. De aspell() functie kan een filter functie overnemen om bepaalde regels in de files over te slaan en kan worden gebruikt met de knit_filter() die ontworpen is om de code chunks in een file over te slaan."
  },
  {
    "objectID": "posts/Reproducable-research/index.html#knitr-themas",
    "href": "posts/Reproducable-research/index.html#knitr-themas",
    "title": "Dynamische documenten maken met RMarkdown en knitr",
    "section": "Knitr Thema’s",
    "text": "Knitr Thema’s\nHet knitr-syntax-thema kan worden aangepast of helemaal naar de hand worden gezet. Als je de standaardthema’s niet wilt, gebruik dan het knit_theme om het te veranderen. Er zijn 80 thema’s opgenomen binnen knitr en we kunnen de namen ervan zien door knit_theme$get().\nWat zijn de eerste 30 knitr thema’s?\n\nhead(knit_theme$get(), 30)\n\n [1] \"acid\"          \"aiseered\"      \"andes\"         \"anotherdark\"  \n [5] \"autumn\"        \"baycomb\"       \"bclear\"        \"biogoo\"       \n [9] \"bipolar\"       \"blacknblue\"    \"bluegreen\"     \"breeze\"       \n[13] \"bright\"        \"camo\"          \"candy\"         \"clarity\"      \n[17] \"dante\"         \"darkblue\"      \"darkbone\"      \"darkness\"     \n[21] \"darkslategray\" \"darkspectrum\"  \"default\"       \"denim\"        \n[25] \"dusk\"          \"earendel\"      \"easter\"        \"edit-anjuta\"  \n[29] \"edit-eclipse\"  \"edit-emacs\"   \n\n\nWij kunnen knit_theme$set() gebruiken om het thema vast te zetten. Om het thema op fruit vast te zetten, kunnen we de bijvoorbeeld de volgende code gebruiken:\n\nknit_theme$set(\"fruit\")\n\nHier is de link naar jouw favoriete thema 80 knitr highlight themes."
  },
  {
    "objectID": "posts/Reproducable-research/index.html#andere-programmeer-talen",
    "href": "posts/Reproducable-research/index.html#andere-programmeer-talen",
    "title": "Dynamische documenten maken met RMarkdown en knitr",
    "section": "Andere programmeer talen",
    "text": "Andere programmeer talen\nTerwijl knitr binnen een R omgeving moet draaien, ondersteunt het ook andere programmeertalen waaronder:\n\nPython\n\nRuby\n\nHaskell\n\nawk/gawk\n\nsed\n\nshell scripts\n\nPerl\n\nSAS\n\nTikZ\n\nGraphviz\n\nC++\n\nEn andere talen…\n\nWe moeten echter het corresponderende software pakket installeren om een taal te gebruiken.\nGebruik de engine functie in Knitr. Deze functie laat de gebruiker de taal van een chunk specificeren.\n\nengine = \"bash\" zal de boel in bash laden and stelt de gebruiker in staat de scripts zo binnen de code chunk te schrijven ."
  },
  {
    "objectID": "posts/Reproducable-research/index.html#code-folding",
    "href": "posts/Reproducable-research/index.html#code-folding",
    "title": "Dynamische documenten maken met RMarkdown en knitr",
    "section": "Code Folding",
    "text": "Code Folding\nZoals het je misschien is opgevallen heeft elk van de code chunks in dit document een interactieve  knop. Deze wordt gecontroleerd in de YAML kop and is nieuw in RMarkdown v2.\nWanneer de knitr code chunk optie echo = TRUE is gespecificeerd (default = TRUE) zal de R code in het output document verschijnen. Echter, er zijn momenten waarin de gebruiker de code helemaal niet wil laten zien (echo = FALSE).\n-code_folding:\n- code_folding: hide: Kan de R code meenemen maar deze is standaard verborgen.\n- code_folding: show: Laat de R code zien. De lezers kunnen dan op de  knop drukken om de chunk te verstoppen als ze dat willen.\noutput: html_document\n    code_folding: show"
  },
  {
    "objectID": "posts/Reproducable-research/index.html#inhoudsopgave",
    "href": "posts/Reproducable-research/index.html#inhoudsopgave",
    "title": "Dynamische documenten maken met RMarkdown en knitr",
    "section": "Inhoudsopgave",
    "text": "Inhoudsopgave\nEen inhoudsopgave kan aan het gerenderd document worden toegevoegd door de toc optie in de YAML kop te gebruiken.\nOpties hierbij:\n\ntoc: of de inhoudsopgave moeten worden meegenomen:\n\ntoc: true: hier wordt de inhoudsopgave meegenomen\n\nDefault:toc: false: Hier wordt de inhoudsopgave niet meegenomen\n\n\ntoc_depth:: Hoeveel niveau’s moeten in de inhoudsopgave worden worden meegenomen?\n\nDefault: doc_depth: 3 zal koppen tot en met ### meenemen.\n\n\nnumber_sections: Voegt sectienummers toe aan de koppen. Bijvoorbeeld, dit document heeft number_sections: true\n\nDefault: number_sections: false\n\nOpgelet: Met elk # zal er een decimaal punt worden toegevoegd aan alle koppen.\n\n\ntoc_float:\n\n2 andere mogelijke parameters binnen toc_float:\n\ncollapsed: Controleert of de inhoudsopgave alleen aan het begin verschijnt. Het zal met de cursor erover verschijnen.\n\nDefault: collapsed: TRUE\n\nsmooth_scroll: Controleert of de pagina scrolls werken wanneer op de onderdelen van de inhoudsopgave wordt geklikt.\n\nDefault: smooth_scroll: true\n\n\n\n\nBijvoorbeeld:\noutput:\n  html_document:\n    toc: true\n    toc_depth: 2\n---\n\nUitdaging: Maak de YAML kop voor een HTML document die het volgende inhoudt:\n\n\n\nInhoudsopgave\nLaat de inhoudsopgave vloeien\n\nSectie koppen met twee hashtags (##)\n\nGenummerde secties\n\nGeen makkelijke scrolling"
  },
  {
    "objectID": "posts/Reproducable-research/index.html#themas",
    "href": "posts/Reproducable-research/index.html#themas",
    "title": "Dynamische documenten maken met RMarkdown en knitr",
    "section": "Thema’s",
    "text": "Thema’s\nRMarkdown heeft verschillende opties die de pr?sance van de HTML documenten controleren. Enkele mogelijkheden waaruit kan worden gekozen, hier met de Engelse termen:\n\ntheme\n\nhighlight\n\nsmart\n\nDe HTML output thema’s komen van Bootswatch library. Valide HTML themes omvatten de volgende:\n\ncerulean, cosmo,flatly, journal, readable,spacelab en united.\n\nBijvoorbeeld, het thema van de pagina is readable.\n\nZet het op nul voor geen thema (in dit geval kun je de css parameter gebruiken om jouw eigen stijl te gebruiken).\n\nHighlight specificeert de wijze waarop de syntax stijl oplicht. Stijlen die mogelijk zijn omvatten de volgende:\n\ndefault, espresso, haddock, kate, monochrome, pygments, tango, textmate en zenburn.\n\nOok hier, plaats nul om syntax oplichting te voorkomen.\n\nSmart indiceert of de typografisch correcte output wordt weergegeven, zet rechte aanhalingstekens om in gekru, — rechte aanhalingstekens, – om in gekrulde aanhalingstekens en … in ellipsen. Smart is standaard ingesteld.\nBijvoorbeeld:\n---\noutput:\n  html_document:\n    theme: slate\n    highlight: tango\n---\nAls je wilt kun je ook jouw eigen stijl-thema produceren en gebruiken. Als je dat zou doen, zou de output sectie van jouw YAML kop er z’on beetje zo uitzien:\noutput:\n  html_document:\n    css: styles.css\nAls je nog wat verder wilt gaan en jouw eigen thema wilt schrijven in aanvulling op het oplichten, zou de YAML kop er beetje zo uitzien:\n---\noutput:\n  html_document:\n    theme: null\n    highlight: null\n    css: styles.css\n---\nHier is een link naar Pr?sance en Stijl in de HTML output."
  },
  {
    "objectID": "posts/Reproducable-research/index.html#chunk-afhankelijkheden",
    "href": "posts/Reproducable-research/index.html#chunk-afhankelijkheden",
    "title": "Dynamische documenten maken met RMarkdown en knitr",
    "section": "Chunk afhankelijkheden",
    "text": "Chunk afhankelijkheden\n\nMet de hand\nWe kunnen met de hand specificeren of chunks van elkaar afhankelijk zijn.\n\ndependson specificeert van welke chunk de huidige chunk afhankelijk is.\n\nVoorbeelden hiervan:\n\ndepesndson = 1: Chunk vertrouwt op de eerste Chunk\n\ndependson = c(6,8): Chunk vertrouwt op de 6de en 8ste chunks\n\ndependson = -1: Chunk vertrouwt op de vorige chunk.\ndependson = c(-1, -2): Chunk vertrouwt op de twee vorige chunks.\n\nOpgelet: Als dependson een bepaalde waarde aanneemt, kan het niet afhankelijk zijn van een latere chunk - alleen van vorige chunks. Daarom is het makkelijk om chunks namen te geven.\n\n\nVoorbeelden:\n\ndependson = c(\"Chunk-1\", \"Chunk-2\", \"Chunk-3\")\n\ndependson = c(\"data-generation\", \"data-transforamtion\")\n\n\n\nAls resultaat, elke keer als de verborgen chunks \"Chunk-1\", \"Chunk-2\" en \"Chunk-3\" opnieuw worden opgebouwd, zal de huidige chunk zijn verborgenheid verliezen en zal het opnieuw worden gedraaid!\n\n\n\nAutomatisch\nVoeg in: autodep chunk optie en de functie dep_auto()\n\nautodep en dep_auto() staan er voor dat de objecten in de huidige chunk door vorige chunks zijn gemaakt. Dus de huidige chunk hangt af van de vorige chunk.\n\nVoor een meer conservatieve benadering voeg dep_prev() in.\n\ndep_prev staat er voor dat een gevouwen chunk afhangt van al zijn vorige chunks. Dus als vorige chunks zijn ververst, zullen ook alle latere chunks worden ververst.\n\nKnitr indentificeert alleen veranderingen in de opgevouwen chunks, niet in de niet opgevouwen chunks! Gelukkig geeft knitr een waarschuwing wanneer het een afhankelijkheid ziet met een niet opgevouwen chunk.\n\n\nCache met de hand laden\nStel dat je aan het eind van een document een z berekent, maar je wilt de z in een eerdere chunk gebruiken. Dit is onmogelijk omdat knitr het document op een liniarie manier samenvoegt en het geen objecten kan gebruiken die in de toekomst worden gemaakt.\nVoeg in: load_cache, die de chunk label zoekt in de ‘cache’ database\nload_cache(label, object, notfound = \"NOT AVAILABLE\", \n  path = opts_chunk$get(\"cache.path\"), lazy = TRUE)\nAls jij dan een z in een ‘’inline R expressie’ gebruikt, geeft het NOT AVAILABLE terug en omdat je hebt gespecificeerd notfound = \"NOT AVAILALBE\" zal het naar het einde teruggaan en de waarde z verplaatsen.\nZo handig!"
  },
  {
    "objectID": "posts/Reproducable-research/index.html#zijeffecten",
    "href": "posts/Reproducable-research/index.html#zijeffecten",
    "title": "Dynamische documenten maken met RMarkdown en knitr",
    "section": "Zijeffecten",
    "text": "Zijeffecten\nEen zijeffect refereert naar een statusverandering die optreedt buiten een functie die de teruggeven waarde niet representeert.\n\npar() en options() zijn zijeffecten in de betekenis dat ze niet opgevouwen zijn.\n\nStel globale opties van de eerste chunk in en vouw deze chunk nooit op.\n\nWe moeten voorzichtig zijn met de chunk opties om er zeker van te zijn dat de resultaten van de opgevouwen chunks worden ververst.\nWe kunnen ook het sloom-laden afzetten met cache.lazy = FALSE."
  },
  {
    "objectID": "posts/Reproducable-research/index.html#plaatsen",
    "href": "posts/Reproducable-research/index.html#plaatsen",
    "title": "Dynamische documenten maken met RMarkdown en knitr",
    "section": "Plaatsen",
    "text": "Plaatsen\nDe bibliografie wordt automatisch aan het einde van het document geplaatst. Daarom moet je jouw .Rmd document met # Referenties eindigen zodat de bibliografie naar de kop voor bibliografie komt.\nlaatste woorden...\n\n# Referenties"
  },
  {
    "objectID": "posts/Reproducable-research/index.html#stylen-van-citeren",
    "href": "posts/Reproducable-research/index.html#stylen-van-citeren",
    "title": "Dynamische documenten maken met RMarkdown en knitr",
    "section": "Stylen van citeren",
    "text": "Stylen van citeren\nCitation Sylte Language (CSL) is een op XML-gebaseerde taal die het format van citaten en bibliografie?n vaststelt. Referentie management programma’s zoals Zotero, Mendeley en Papers gebruiken allemaal CSL.\nZoek jouw favcoriete tijdschrift en CSL in de Zotero Style Repository, waar nu meer dan 8,152 CSLs inzitten. Is er een stijl waar je naar zoekt en die er niet in zit?\noutput: html_document\nbibliography: bibliography.bib\ncsl: nature.csl\nIn de github repo voor deze workshop heb ik de nature.csl en the-isme-journal.csl toegevoegd om mee te spelen. Download anders een stijl van de Zotero Style Repository!"
  },
  {
    "objectID": "posts/Reproducable-research/index.html#citaten",
    "href": "posts/Reproducable-research/index.html#citaten",
    "title": "Dynamische documenten maken met RMarkdown en knitr",
    "section": "Citaten",
    "text": "Citaten\nCitaten gaan tussen vierkante haakjes [ ] en worden afgescheiden door punt-komma’s’ ;. Elk citaat moet een sleutel hebben, samen de @ + de citaat identificatie van de database vormen en die optioneel a prefix, a locator en a suffix hebben. Om te controleren wat de citaatsleutel is van een referentie, werp dan een blik op de .bib file. Hier in die file, kun je de sleutel voor elke referentie veranderen. Echter, wees er wel van bewust dat elke ID uniek is!\nHier zijn wat voorbeelden met bijpassende code in het Engels:\n\nMicrobes control Earth’s biogeochemical cycles [@Falkowski2008].\n\nCode: Microbes contorl Earth's biogeochemical cycles  [@Falkowski2008].\n\n\nI love making beautiful plots with ggplot2 [@R-ggplot2]\n\nCode: I love making beautiful plots with ggplot2 [@R-ggplot2]\n\n\nDr. Yuhui Xie’s book about Dynamic Documents [@Xie2015] inspired me to host this workshop.\n\nCode: Dr. Yuhui Xie's book about Dynamic Documents [@Xie2015] inspired me to host this workshop.\n\n\nA great article in Science regarding biogeography of microbes asks readers to imagine their Alice in Wonderland to shrink down to understand the microbial world [@Green2008].\n\nCode: A great article in *Science* regarding biogeography of microbes asks readers to imagine they are Alice in Wonderland to and shrink down to understand the microbial world [@Green2008].\n\n\nHet is cool dat de enige refenties die aan het document worden toegevoegd degene zijn die jijzelf citeert!"
  },
  {
    "objectID": "posts/Reproducable-research/index.html#rpubs-vernieuwen",
    "href": "posts/Reproducable-research/index.html#rpubs-vernieuwen",
    "title": "Dynamische documenten maken met RMarkdown en knitr",
    "section": "RPubs vernieuwen",
    "text": "RPubs vernieuwen\nAls je veranderingen in het document wilt aanbrengen is het makkelijk om de webpagina te vernieuwen. Als je een keer jouw aangepaste documentg hebt gerenderd klik je op de  knop rechtsboven in de hoek van de preview scherm. Het aangepaste document zal dezelfde URL hebben als het orginele document."
  },
  {
    "objectID": "posts/Rstudiormarkdown/index.html",
    "href": "posts/Rstudiormarkdown/index.html",
    "title": "RMarkdown",
    "section": "",
    "text": "Dat boekje vind je hier. http://www.harriejonkman.nl/wp-content/uploads/2018/01/rbasics.pdf"
  },
  {
    "objectID": "posts/Psm/index.html#impactevaluatie",
    "href": "posts/Psm/index.html#impactevaluatie",
    "title": "Propensity Score Matching",
    "section": "Impactevaluatie",
    "text": "Impactevaluatie\nOm precies het effect van een aanpak of politieke keuze vast te stellen is een ingewikkelde kwestie. Toch is er dat soort onderzoek nodig om de keuze voor programma’s te legitimeren. Tegenwoordig is er een heel spectrum van technieken om de impact van programma’s vast te stellen. Dit zijn technieken die kunnen worden gebruikt binnen hele verschillende soorten impactstudies. Het is goed daar kennis van te nemen, zeker nu steeds meer mogelijk is omdat er meer data beschikbaar zijn waarop deze evaluaties gebaseerd kunnen worden. Impactstudies worden uitgevoerd om vast te stellen of programma’s de effecten opleveren die ze nastreven, om te begrijpen of en waarom deze programma’s werken, om vast te stellen in hoeverre veranderingen zijn toe te schrijven aan de inzet van het programma en ook om vast te stellen of de gelden op een goede manier worden besteed. Op dit terrein is er natuurlijk een enorme hoeveelheid literatuur en enkele uitgaven geven ons hiervan een goed en up-to-date overzicht1. Experimentele studies kunnen natuurlijk goede impactstudies zijn, met sterke punten en beperkingen. Maar er zijn ook aanvullende methodes die in quasi-experimentele of observationele studies kunnen worden toegepast. Zo zijn er panel datamethodes die gebruikt kunnen worden, regressie discontinu?teit methodes en instrumentele variabelen methodes. Daarnaast zijn er verschillende matchingsmethodes die in impactstudies worden gebruikt. Hier stellen we zo’n matchingsmethode voor die goed gebruikt kan worden in verschillende soorten impactstudies en laten we zien hoe deze uitgevoerd kan worden."
  },
  {
    "objectID": "posts/Sem/index.html",
    "href": "posts/Sem/index.html",
    "title": "SEM",
    "section": "",
    "text": "De presentatie vind je hier"
  },
  {
    "objectID": "posts/Tufte/index.html",
    "href": "posts/Tufte/index.html",
    "title": "Tufte",
    "section": "",
    "text": "Hier vind je een document over het werken in de Tufte stijl"
  },
  {
    "objectID": "posts/Visualisatiegapminder/index.html",
    "href": "posts/Visualisatiegapminder/index.html",
    "title": "Gapmindervisualisatie",
    "section": "",
    "text": "In dit deel laten we zien hoe relatief eenvoudig de ggplot -code inzichtelijke en esthetisch aangename plots kan maken die ons helpen trends in de wereldgezondheid en economie beter te begrijpen. Later breiden we de code iets uit om de plots te perfectioneren en beschrijven we enkele algemene principes als leidraad voor datavisualisatie."
  },
  {
    "objectID": "posts/Visualisatiegapminder/index.html#voorbeeld-1-levensverwachting-en-vruchtbaarheidcijfers",
    "href": "posts/Visualisatiegapminder/index.html#voorbeeld-1-levensverwachting-en-vruchtbaarheidcijfers",
    "title": "Gapmindervisualisatie",
    "section": "Voorbeeld 1: Levensverwachting en vruchtbaarheidcijfers",
    "text": "Voorbeeld 1: Levensverwachting en vruchtbaarheidcijfers\nHans Rosling was medeoprichter van de Gapminder Foundation, een organisatie die het publiek wil stimuleren om gegevens te gebruiken om veelvoorkomende mythes over de zogenaamde ontwikkelingswereld te verdrijven. De organisatie gebruikt gegevens om aan te tonen hoe de daadwerkelijke tendensen in gezondheid en economie de verhalen tegenspreken die van de media komen en sensationeel berichten over catastrofes, tragedies en andere ongelukkige gebeurtenissen, zoals die op de website van de Gapminder Foundation staan.\n\n\n\nJournalisten en lobbyisten vertellen dramatische verhalen. Dat moeten ze want dat is hun taak. Ze vertellen verhalen over bijzondere gebeurtenissen en ongewone mensen. De stapels dramatische verhalen die een al te dramatisch wereldbeeld vormen en zorgen voor sterke negatieve stressgevoelens: “De wereld wordt erger”, “Wij vs. zij! , ,,Andere mensen zijn vreemd”, “De bevolking blijft maar groeien” en “Het maakt niemand wat uit”!\n\n\n\nHans Rosling bracht actuele data-gebaseerde trends op een eigen dramatische manier over aan de hand van effectieve datavisualisatie. Dit hoofdstuk is gebaseerd op twee gesprekken die deze benadering van onderwijs illustreren: Nieuwe inzichten in armoede en De beste Statistiek die je gezien hebt.\nIn deze paragraaf willen we aan de hand van gegevens een antwoord geven op de volgende twee vragen:\n\nIs het een eerlijke typering van de wereld van vandaag om te zeggen dat ze verdeeld is in westerse rijke naties en de ontwikkelingslanden in Afrika, Azi? en Latijns-Amerika?\nIs de inkomensongelijkheid tussen landen de afgelopen 40 jaar toegenomen?\n\nOm deze vraag te beantwoorden zullen we gebruik maken van de gapminder dataset in dslabs. Deze dataset is gemaakt met behulp van een aantal spreadsheets die beschikbaar zijn bij de Stichting [Gapminder] (http://www.gapminder.org/). U kunt op deze manier toegang krijgen tot de tafel:\n\nlibrary(dslabs)\n\nWarning: package 'dslabs' was built under R version 4.1.3\n\ndata(gapminder)\nhead(gapminder)\n\n              country year infant_mortality life_expectancy fertility\n1             Albania 1960           115.40           62.87      6.19\n2             Algeria 1960           148.20           47.50      7.65\n3              Angola 1960           208.00           35.98      7.32\n4 Antigua and Barbuda 1960               NA           62.97      4.43\n5           Argentina 1960            59.87           65.39      3.11\n6             Armenia 1960               NA           66.86      4.55\n  population          gdp continent          region\n1    1636054           NA    Europe Southern Europe\n2   11124892  13828152297    Africa Northern Africa\n3    5270844           NA    Africa   Middle Africa\n4      54681           NA  Americas       Caribbean\n5   20619075 108322326649  Americas   South America\n6    1867396           NA      Asia    Western Asia\n\n\n\nDe quiz van Hans Rosling\nZoals gedaan in de New Insights on Poverty video, beginnen we met het testen van onze kennis over verschillen in kindersterfte tussen verschillende landen.\nVoor elk van de zes onderstaande paren landenparen, willen we weten welk land volgens u de hoogste kindersterfte in 2015 had? Welke paren lijken volgens jou het meest op elkaar?\n\nSri Lanka of Turkije\nPolen of Zuid-Korea\nMaleisi? of Rusland\nPakistan of Vietnam\nThailand of Zuid-Afrika\n\nWanneer deze vragen zonder gegevens worden beantwoord, worden de niet-Europese landen doorgaans gekozen als landen met hogere sterftecijfers: Sri Lanka boven Turkije, Zuid-Korea boven Polen en Maleisi? boven Rusland. Ook in landen die als ontwikkelingslanden worden beschouwd, Pakistan, Vietnam, Thailand en Zuid-Afrika, is het sterftecijfer even hoog.\nOm deze vragen __ met data__ te beantwoorden kunnen we het R-pakket dplyr gebruiken. Voor de eerste vergelijking zien we bijvoorbeeld dat\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\ngapminder %>% \n  filter(year == 2015 & country %in% c(\"Sri Lanka\",\"Turkey\")) %>% \n  select(country, infant_mortality)\n\n    country infant_mortality\n1 Sri Lanka              8.4\n2    Turkey             11.6\n\n\nTurkije heeft een hogere score.\nWe kunnen deze code op alle vergelijkingen plakken en dan zien we het volgende:\n\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nPlease use `tibble()` instead.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\n\nNew names:\n* `country` -> `country...1`\n* `infant_mortality` -> `infant_mortality...2`\n* `country` -> `country...3`\n* `infant_mortality` -> `infant_mortality...4`\n\n\n\n\n\ncountry…1\ninfant_mortality…2\ncountry…3\ninfant_mortality…4\n\n\n\n\nSri Lanka\n8.4\nTurkey\n11.6\n\n\nPoland\n4.5\nSouth Korea\n2.9\n\n\nMalaysia\n6.0\nRussia\n8.2\n\n\nPakistan\n65.8\nVietnam\n17.3\n\n\nThailand\n10.5\nSouth Africa\n33.6\n\n\n\n\n\nWe zien dat de Europese landen hogere cijfers hebben: Polen heeft een hoger percentage dan Zuid-Korea en Rusland een hoger percentage dan Maleisi?. We zien ook dat Pakistan een veel hoger percentage heeft dan Vietnam en Zuid-Afrika een veel hoger percentage dan Thailand. Het blijkt dat de meeste mensen het slechter doen dan wanneer ze zouden raden. We lijken wel onwetend zijn en we zijn verkeerd ge?nformeerd.\n\n\nLevensverwachting en vruchtbaarheidscijfers\nDe reden hiervoor is het idee dat de wereld in twee groepen te verdelen is: de Westerse wereld (West-Europa en Noord-Amerika), met z’n lange levensduur en kleine gezinnen, tegenover de ontwikkelingslanden (Afrika, Azi? en Latijns-Amerika), gekenmerkt door een korte levensduur en grote gezinnen. Maar rechtvaardigen de gegevens deze dichotomie van twee groepen wel?\nDe nodige gegevens om deze vraag te beantwoorden zitten ook in onze gapminder tabel. Met behulp van onze nieuw aangeleerde vaardigheden om data te visualiseren, zullen we in staat zijn om deze vraag te beantwoorden.\nDe eerste plot die we maken om te zien wat de gegevens zeggen over dit wereldbeeld is een spreidingplot van levensverwachting versus vruchtbaarheidscijfers (gemiddeld aantal kinderen per vrouw). We kijken eerst naar gegevens van zo’n vijftig jaar geleden, toen dit standpunt misschien nog te rechtvaardigen was.\n\nds_theme_set()\n\nfilter(gapminder, year==1962) %>%\n  ggplot( aes(fertility, life_expectancy)) +\n  geom_point()\n\n\n\n\nDe meeste punten vallen in twee verschillende categorie?n uiteen:\n\nLevensverwachting rond 70 jaar en 3 of minder kinderen per gezin\nLevensverwachting lager dan 65 jaar en met meer dan 5 kinderen per gezin.\n\nOm te bevestigen dat de landen inderdaad afkomstig zijn uit de regio’s die wij verwachten, kunnen wij kleur gebruiken om het continent te vertegenwoordigen.\n\nfilter(gapminder, year==1962) %>%\n  ggplot( aes(fertility, life_expectancy, color = continent)) +\n  geom_point() \n\n\n\n\nDus de visie in 1962, “het westen versus de ontwikkelingslanden”, was gebaseerd op een of andere realiteit. Maar is dat 50 jaar later nog steeds het geval?\n\n\nFacetteren\nWe konden de gegevens voor 2012 gemakkelijk in kaart brengen op dezelfde manier als voor 1962. Maar om de gegevens te vergelijken, kunnen we de inzichten het beste naast elkaar zetten. In ggplot kunnen we dit doen met’ faceting variabelen’: we stratificeren de gegevens met een of andere variabele en maken dezelfde plot voor elke groep.\nOm te facetteren (ik weet niet of het een Nederlands woord is, maar goed) voegen we een laag toe met de functie facet_grid, die automatisch de groepen scheidt. Met deze functie kunt u maximaal twee variabelen facetteren met behulp van kolommen om de ene variabele weer te geven en rijen om de andere weer te geven. De functie verwacht de rij- en kolomvariabelen die door een ~ van elkaar zijn gescheiden. Hier is een voorbeeld van een verstrooiingsplot met een facet_grid als laatste laag toegevoegd:\n\nfilter(gapminder, year%in%c(1962, 2012)) %>%\n  ggplot(aes(fertility, life_expectancy, col = continent)) +\n  geom_point() +\n  facet_grid(continent~year)\n\n\n\n\nWe zien een plot voor elk continent/jaarpaar. Maar dit is slechts een voorbeeld en is al meer dan wat we willen, want dat is gewoon om 1962 en 2012 te vergelijken. In dit geval is er maar ??n variabele en gebruiken we . om de facet te laten weten dat we geen van de variabelen gebruiken:\n\nfilter(gapminder, year%in%c(1962, 2012)) %>%\n  ggplot(aes(fertility, life_expectancy, col = continent)) +\n  geom_point() +\n  facet_grid( . ~ year)\n\n\n\n\nDeze plot laat duidelijk zien dat de meerderheid van de landen zich heeft ontwikkeld van ontwikkelingslandcluster naar het ontwikkeldlandcluster. In 2012 heeft het oude perspectief geen zin meer. Dit wordt vooral duidelijk bij een vergelijking van Europa en Azi?, want vooral binnen dat laatste continent zijn er verschillende landen waarbinnen grote verbeteringen hebben doorgevoerd.\n\nfacet_wrap\nOm te onderzoeken hoe deze transformatie door de jaren heen is gegaan, kunnen we het perceel voor meerdere jaren maken. We kunnen bijvoorbeeld 1970, 1980, 1990, 2000 toevoegen. Als we dit doen, willen we niet dat alle percelen op dezelfde rij staan, het standaard gedrag van facet_grid, omdat ze te dun worden om de gegevens weer te geven. In plaats daarvan zullen we meerdere rijen en kolommen gebruiken. Dat kan met de functie facet_wrap , waarmee automatisch een reeks percelen onstaat met ded juiste afmetingen:\n\nyears <- c(1962, 1980, 1990, 2000, 2012)\ncontinents <- c(\"Europe\", \"Asia\")\ngapminder %>% \n  filter(year %in% years & continent %in% continents) %>%\n  ggplot( aes(fertility, life_expectancy, col = continent)) +\n  geom_point() +\n  facet_wrap(~year) \n\n\n\n\nDit plot toont duidelijk aan hoe de meeste Aziatische landen zich veel sneller hebben verbeterd dan de Europese.\n\n\n\nVaste schalen voor betere vergelijkingen\nMerk op dat de standaard keuze van het bereik van de assen een belangrijke is. Wanneer geen facet' wordt gebruikt, wordt het bereik bepaald door de gegevens die in de grafiek worden getoond. Bij gebruik vanfacet’ wordt dit bereik bepaald door de gegevens die op alle percelen worden weergegeven en wordt het dus voor alle percelen vastgehouden. Dit maakt vergelijkingen tussen percelen veel gemakkelijker. In bovenstaand perceel zien we bijvoorbeeld dat de levensverwachting in de meeste landen is gestegen en de vruchtbaarheid is afgenomen. We zien dit omdat de puntenwolk beweegt. Dit is niet het geval als we de schalen aanpassen:\n\n\n\n\n\nIn de plot hierboven moeten we speciale aandacht besteden aan het bereik om op te merken dat de rechter plot aan de rechter kant een grotere levensverwachting vertoont.\n\n\nAnimatie\nMet het gganimate pakket kunt u eenvoudig facetten omzetten in een animatie:\n\nwest <- c(\"Western Europe\",\"Northern Europe\",\"Southern Europe\",\n          \"Northern America\",\"Australia and New Zealand\")\n\ngapminder <- gapminder %>%\n  mutate(group = case_when(\n    .$region %in% west ~ \"The West\",\n    .$region %in% c(\"Eastern Asia\", \"South-Eastern Asia\") ~ \"East Asia\",\n    .$region %in% c(\"Caribbean\", \"Central America\", \"South America\") ~ \"Latin Am\nerica\",\n    .$continent == \"Africa\" & .$region != \"Northern Africa\" ~ \"Sub-Saharan Afric\na\",\n    TRUE ~ \"Others\"))\ngapminder <- gapminder %>%\n  mutate(group = factor(group, levels = rev(c(\"Others\", \"Latin America\", \"East A\nsia\",\"Sub-Saharan Africa\", \"The West\"))))\n\nlibrary(gganimate)\ntheme_set(theme_bw(base_size = 16))\nyears <- seq(1962, 2013)\np <- filter(gapminder, year%in%years & !is.na(group) &\n         !is.na(fertility) & !is.na(life_expectancy)) %>%\n  mutate(population_in_millions = population/10^6) %>%\n  ggplot( aes(fertility, y=life_expectancy, col = group, frame = year, size = population_in_millions)) +\n  geom_point(alpha = 0.8) +\n  guides(size=FALSE) +\n  theme(plot.title = element_blank(), legend.title = element_blank()) +\n  coord_cartesian(ylim = c(30, 85)) +\n  xlab(\"Fertility rate (births per woman)\") +\n  ylab(\"Life Expectancy\") +\n  geom_text(aes(x=7, y=82, label=year), cex=20, color=\"grey\")\n  #, legend.position = \"top\") +\n\ngganimate(p, filename = \"animation-example.html\", title_frame = FALSE)\n\n\n\nTijdreeksfiguren\nBovenstaande visualisaties laten duidelijk zien dat data het oude beeld van het westen tegenover ontwikkelingslanden niet meer ondersteunen. Als we deze figuren eenmaal hebben gezien, rijzen er nieuwe vragen. Welke landen verbeteren bijvoorbeeld meer en welke minder? Was de verbetering de afgelopen 50 jaar constant of was er in bepaalde perioden sprake van een versnelling? Om deze vraag beter te kunnen beantwoorden, gaan we dieper in op de tijdreeksfiguren.\nIn tijdreeksfiguren staat tijd op de x-as en een uitkomst of meting die van belang is op de y-as. Hier is bijvoorbeeld een trendfiguur voor de vruchtbaarheidscijfers van Nederland:\n\ngapminder %>% filter(country == \"Netherlands\") %>% \n  ggplot(aes(year,fertility)) +\n  geom_point()\n\n\n\n\nWe zien dat de trend helemaal niet lineair is. In plaats daarvan zien we een scherpe daling tijdens de jaren ’60 en ’70 naar onder de 2. Dan komt de trend terug op 2 en stabiliseert zich tijdens de jaren ’90.\nWanneer de punten regelmatig en dicht op elkaar liggen, zoals hier, maken we krommen door de punten als lijnen met elkaar te verbinden om aan te geven dat deze gegevens uit ??n land afkomstig zijn. Hiervoor gebruiken we de functie geom_line in plaats van geom_point.\n\ngapminder %>% filter(country == \"Netherlands\") %>% \n  ggplot(aes(year,fertility)) +\n  geom_line()\n\n\n\n\nDit is met name nuttig wanneer we naar twee landen kijken. Als we de gegevens zo onderverdelen in twee landen, ??n uit Europa en ??n uit Azi?, dan kopieer je de gegevens naar de bovenstaande code:\n\ncountries <- c(\"South Korea\",\"Netherlands\")\ngapminder %>% filter(country %in% countries) %>% \n  ggplot(aes(year,fertility)) +\n  geom_line()\n\n\n\n\nMerk op dat dit niet de figuur is die we willen. In plaats van een lijn voor elk land, worden de punten voor beide landen samengevoegd. Dit wordt eigenlijk verwacht omdat we ggplot niets hebben verteld over het willen hebben van twee aparte lijnen. Om ggplot te laten weten dat we twee afzonderlijke curven willen hebben, wijzen we elk punt toe aan een ‘groep’, ??n voor elk land:\n\ncountries <- c(\"South Korea\",\"Netherlands\")\ngapminder %>% filter(country %in% countries) %>% \n  ggplot(aes(year,fertility, group = country)) +\n  geom_line()\n\nWarning: Removed 2 row(s) containing missing values (geom_path).\n\n\n\n\n\nMaar welke lijn gaat over welk land? We kunnen kleuren toewijzen om dat onderscheid te maken. Een nuttig neveneffect van het gebruik van het ‘kleur’-argument om verschillende kleuren toe te wijzen aan de verschillende landen, is dat de gegevens automatisch worden gegroepeerd:\n\ncountries <- c(\"South Korea\",\"Netherlands\")\ngapminder %>% filter(country %in% countries) %>% \n  ggplot(aes(year,fertility, col = country)) +\n  geom_line()\n\nWarning: Removed 2 row(s) containing missing values (geom_path).\n\n\n\n\n\nUit het perceel blijkt duidelijk dat het vruchtbaarheidscijfer van Zuid-Korea in de jaren ’60 en ’70 drastisch is gedaald en in 1990 even hoog was als in Duitsland.\n\nDe voorkeur van labels boven legenda’s\nVoor trendplots raden we aan om de lijnen te labelen in plaats van legenda’s te gebruiken omdat de kijker snel kan zien welke lijn welk land is. Deze suggestie is eigenlijk van toepassing op de meeste figuren: labeling heeft meestal de voorkeur boven legenda’s.\nAan de hand van de gegevens over de levensverwachting laten we zien hoe we dit kunnen doen. We defini?ren een datatabel met de labellocaties en gebruiken dan een tweede mapping alleen voor deze labels:\n\nlabels <- data.frame(country = countries, x = c(1975,1965), y = c(60,72))\n\ngapminder %>% filter(country %in% countries) %>% \n  ggplot(aes(year, life_expectancy, col = country)) +\n  geom_line() +\n  geom_text(data = labels, aes(x, y, label = country), size = 5) + theme(legend.position = \"none\")\n\n\n\n\nDe figuur toont duidelijk aan hoe een verbetering van levensverwachting de dalingen in vruchtbaarheidscijfers volgde. Terwijl de Duitsers in 1960 meer dan 15 jaar meer Zuid-Koreanen woonden, is de kloof in 2010 volledig gedicht. Het is een voorbeeld van de verbetering die veel niet-westerse landen in de afgelopen veertig jaar hebben bereikt.\n\n\n\nVoorbeeld 2: Inkomensverdeling\nEen andere veelgehoorde opmerking is dat de verdeling van de welvaart over de wereld de laatste decennia is verslechterd. Wanneer het algemene publiek wordt gevraagd of arme landen armer zijn geworden en rijke landen rijker, antwoordt de meerderheid ja. Door gebruik te maken van stratificatie, histogrammen, vloeiende verdeling en boxplots zullen we in staat zijn om te begrijpen of dit inderdaad het geval is. We leren dan ook hoe transformaties soms kunnen helpen om meer informatieve samenvattingen en figuren aan te bieden.\n\n\nTransformaties\nDe `gapminder’-gegevenstabel bevat een kolom met het bruto binnenlands product (BBP) van de landen. Het BBP meet de marktwaarde van de goederen en diensten die een land in een jaar produceert. Het BBP per persoon wordt vaak gebruikt als een ruwe samenvatting van hoe rijk een land is. Hier delen we deze hoeveelheid door 365 om de meer interpreteerbare maat dollars per dag te krijgen. Wanneer we de huidige US-dollar als eenheid gebruiken, wordt een persoon die met een inkomen van minder dan $ 2 per dag overleeft, gedefinieerd als een persoon die in absloute armoede leeft. Deze variabele voegen we toe aan de datatabel:\n\ngapminder <- gapminder %>% \n  mutate(dollars_per_day = gdp/population/365)\n\nMerk op dat de BBP-waarden zijn gecorrigeerd voor inflatie en staan voor de huidige US-dollar. Dus deze waarden zijn bedoeld om over de jaren heen vergelijkbaar te zijn. Merk ook op dat het hier om landsgemiddelden gaat en dat er binnen elk land veel variatie is. Alle hieronder beschreven grafieken en inzichten hebben betrekking op landsgemiddelden en staan dus niet individuele personen.\n\nVerdeling van het landinkomen\nHier is een histogram van de inkomens per dag uit 1970:\n\npast_year <- 1970\ngapminder %>% \n  filter(year == past_year & !is.na(gdp)) %>%\n  ggplot(aes(dollars_per_day)) + \n  geom_histogram(binwidth = 1, color = \"black\")\n\n\n\n\nWe gebruiken het color = \"black\" om een grens te trekken en de bins (‘bakjes’) duidelijk van elkaar te onderscheiden.\nIn dit diagram zien we dat voor de meeste landen gemiddelden zijn onder $10 per dag. Het grootste deel van de x-as is echter gewijd aan de 35 landen met gemiddelden boven $10. De grafiek is dus niet erg informatief over landen met waarden onder $10 per dag.\nHet is misschien informatiever om snel te kunnen zien hoeveel landen een gemiddeld daginkomen hebben van ongeveer $1 (extreem arm), $2 (zeer arm), $4 (arm), $8 (midden), $16 (welgesteld), $32 (rijk), $64 (zeer rijk) per dag. Deze veranderingen zijn vermenigvuldigend en logtransformaties.\nHier is de verdeling als we een log2 transformatie toepassen:\n\ngapminder %>% \n  filter(year == past_year & !is.na(gdp)) %>%\n  ggplot(aes(log2(dollars_per_day))) + \n  geom_histogram(binwidth = 1, color = \"black\")\n\n\n\n\nIn zekere zin geeft dit een close up van de landen met een gemiddeld tot lager inkomen.\n\n\nWelke basis?\nIn het bovenstaande geval hebben we basis 2 gebruikt in de log-transformaties. Andere veelvoorkomende keuzes zijn basis \\(e\\) (de natuurlijke log) en basis 10.\nOver het algemeen raden wij het gebruik van het natuurlijke logboek voor het verkennen en visualiseren van gegevens aan.Dit is omdat \\(2^2, 2^3, 2^4, \\dots\\) or \\(10^1, 10^2, \\dots\\) makkelijk zijn te berekenen in ons hoofd. Hetzelfde geldt niet voor \\(\\mathrm{e}^2, \\mathrm{e}^3, \\dots\\).\nIn het voorbeeld dollars per dag gebruikten we basis 2 in plaats van basis 10 omdat het resulterende bereik gemakkelijker te interpreteren is. Het bereik van de waarden die worden uitgezet is 0.3269426, 48.8852142.\nIn basis 10 verandert dit in een bereik dat zeer weinig gehele getallen omvat: slechts 0 en 1. Met basis twee omvat ons assortiment -2, -1, 0, 1, 2, 3, 4 en 5. Het is gemakkelijker om \\(2^x\\) en \\(10^x\\) te berekenen wanneer \\(x\\) een geheel getal is en tussen -10 en 10 ligt. Dus geven we de voorkeur aan meer kleine gehele getallen in de schaal. Een ander gevolg van een beperkt bereik is dat het kiezen van de ‘bin’-breedte een grotere uitdaging is. Met log base 2 weten we dat een ‘bin’-breedte van 1 zal vertalen naar een bin met bereik van \\(x\\) tot \\(2x\\).\nAls voorbeeld waarbij basis 10 zinvoller is, overweeg dan de populatiegrootte. Een logbasis 10 is zinvoller omdat het bereik hiervoor ongeveer 1000 tot 10 miljard is. Hier is het histogram van de getransformeerde waarden:\n\ngapminder %>% filter(year == past_year) %>%\n  ggplot(aes(log10(population))) +\n  geom_histogram(binwidth = 0.5, color = \"black\")\n\n\n\n\nHier zien we al snel dat de bevolking van een land varieert tussen de tienduizend en tien miljard.\n\n\nTransform the values or the scale?\nEr zijn twee manieren waarop we log-transformaties in grafieken kunnen gebruiken. We kunnen de waarden loggen voordat we ze plotten of gebruik maken van logschalen in de assen. Beide benaderingen zijn nuttig en hebben verschillende sterke punten. Als we de gegevens loggen, kunnen we gemakkelijker tussenliggende waarden interpreteren in de schaal. Bijvoorbeeld als we zien\n\n\n—-1—-x—-2——–3—-\n\n\nvoor log getransformeerde gegevens weten we dat de waarde van \\(x\\) is ongeveer 1,5. Als de weegschalen gelogd zijn\n\n\n—-1—-x—-10——100—\n\n\nom x te bepalen, moeten we \\(10^{1.5}\\) berekenen. Dat is niet gemakkelijk te doen in onze hoofden. Het voordeel van het tonen van gelogde schalen is echter dat de originele waarden worden weergegeven in de plot, die gemakkelijker te interpreteren zijn. Bijvoorbeeld, we zouden “32 dollar per dag” zien in plaats van “5 log basis 2 dollar per dag”.\nZoals we eerder leerden, als we de as willen schalen met logs kunnen we de functie ‘schaal_x_ccontinue’ gebruiken. Dus in plaats van eerst de waarden te loggen, passen we deze laag toe:\n\ngapminder %>% \n  filter(year == past_year & !is.na(gdp)) %>%\n  ggplot(aes(dollars_per_day)) + \n  geom_histogram(binwidth = 1, color = \"black\") +\n  scale_x_continuous(trans = \"log2\")\n\n\n\n\nMerk op dat de log base 10 transformatie zijn eigen functie heeft: scale_x_log10(), maar momenteel base 2 niet. Hoewel we dit zelf gemakkelijk konden defini?ren.\nMerk op dat er andere transformatie beschikbaar zijn via het trans argument. Zoals we later leren, is bijvoorbeeld de vierkantsworteltransformatie (sqrt) nuttig bij het tellen. De logistieke transformatie (logit) is nuttig bij het plotten van proporties tussen 0 en 1. De omgekeerde transformatie is nuttig als we willen dat kleinere waarden rechts of bovenop staan.\n\n\n\nModus\nIn de statistieken wordt deze hobbel ook wel modus genoemd. De modus van een verdeling is de waarde met de hoogste frequentie. De modus van de normale verdeling is het gemiddelde. Wanneer een distributie, zoals de bovenstaande, niet eentonig afneemt van de modus, noemen we de locaties waar het op en neer gaat weer lokale modi en zeggen dat de distributie meerdere modi heeft.\nHet histogram hierboven suggereert dat de inkomensverdeling van het land in 1970 twee modi kent: ??n met ongeveer 2 dollar per dag (1 in de log 2 schaal) en ??n met ongeveer 32 dollar per dag (5 in de log 2 schaal). Deze bimodaliteit is consistent met een dichotome wereld die bestaat uit landen met een gemiddeld inkomen van minder dan $8 (3 in de log 2 schaal) per dag en landen daarboven.\n\n\nStratificeren en boxplot\nHet histogram liet zien dat de inkomensverdelingswaarden een tweedeling vertonen. Het histogram laat echter niet zien of de twee groepen landen west tegenover de ontwikkelings wereld zijn.\nOm de verdeling naar geografische regio te zien, stratificeren we eerst de gegevens naar regio’s en onderzoeken we vervolgens de verdeling voor elke regio.\n\nlength(levels(gapminder$region))\n\n[1] 22\n\n\nVanwege het aantal regio’s zijn histogrammen of gladde dichtheden voor elk niet nuttig. In plaats daarvan kunnen we boxplots naast elkaar stapelen:\n\np <- gapminder %>% \n  filter(year == past_year & !is.na(gdp)) %>%\n  ggplot(aes(region, dollars_per_day)) \np + geom_boxplot() \n\n\n\n\nMerk op dat we de regionamen niet kunnen lezen omdat het standaard ggplot-gedrag is om de labels horizontaal te schrijven en hier lopen we dan de ruimte uit. We kunnen dit eenvoudig repareren door de etiketten te draaien. In de sheetuitleg vinden we dat we de namen kunnen roteren door het thema te veranderen via element_text. Het just=1 zorgt ervoor dat deze zich naast de as bevindt.\n\np + geom_boxplot() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\nWe zien nu dat er inderdaad een tweedeling is tussen het westen en de rest.\n\nOrden niet alfabetisch\nEr zijn nog een paar aanpassingen die we kunnen maken om in de grafiek deze realiteit beter bloot te leggen. Ten eerste helpt het om de regio’s in de boxplots te ordenen van arm naar rijk in plaats van alfabetisch. Dit kan worden gedaan met behulp van de reorder functie. Deze functie laat ons de orde van de niveaus van een factorvariabele op basis van een samenvatting veranderen die op een numerieke vector wordt berekend. Een karaktervector wordt in een factor gedwongen:\nHieronder staat een voorbeeld. Merk op hoe de volgorde van de niveaus verandert:\n\nfac <- factor(c(\"Asia\", \"Asia\", \"West\", \"West\", \"West\"))\nlevels(fac)\n\n[1] \"Asia\" \"West\"\n\nvalue <- c(10, 11, 12, 6, 4)\nfac <- reorder(fac, value, FUN = mean)\nlevels(fac)\n\n[1] \"West\" \"Asia\"\n\n\nTen tweede kunnen we kleur gebruiken om de verschillende continenten te onderscheiden, een visuele kleurschakering die helpt om specifieke regio’s te vinden. Hier is de code:\n\np <- gapminder %>% \n  filter(year == past_year & !is.na(gdp)) %>%\n  mutate(region = reorder(region, dollars_per_day, FUN = median)) %>%\n  ggplot(aes(region, dollars_per_day, fill = continent)) +\n  geom_boxplot() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  xlab(\"\")\np\n\n\n\n\nDit figuur toont twee duidelijke groepen, met de rijke groep bestaande uit Noord-Amerika, Noord- en West-Europa, Nieuw-Zeeland en Australi?. Net als met het histogram, als we de plot herschikken met behulp van een logschaal zijn we in staat om verschillen binnen de deconcentratiewereld beter te zien.\n\np + scale_y_continuous(trans = \"log2\")\n\n\n\n\n\n\nDe data tonen\nIn veel gevallen tonen we de gegevens niet omdat het rommel aan het figuur toevoegt en het bericht vertroebelt. In bovenstaand voorbeeld hebben we niet zoveel punten. Dan kunnen deze laag toevoegen met behulp van geom_point() en door punten toe te voegen kunnen we eigenlijk alle gegevens zien\n\np + scale_y_continuous(trans = \"log2\") + geom_point(show.legend = FALSE) \n\n\n\n\n\n\n\nVerdelingen vergelijken\nDe bovenstaande verkennende gegevensanalyse heeft twee kenmerken van de gemiddelde inkomensverdeling in 1970 aan het licht gebracht. Aan de hand van een histogram vonden we een bimodale verdeling met de modi voor arme en rijke landen. Door in het onderzoek stratificatie per regio toe te passen zagen we met boxplots dat de rijke landen meestal in Europa, Noord-Amerika, Australi? en Nieuw-Zeelandm lagen. Met deze regio’s defini?ren we een vector:\n\nwest <- c(\"Western Europe\",\"Northern Europe\",\"Southern Europe\",\n          \"Northern America\",\"Australia and New Zealand\")\n\nNu willen we ons richten op het vergelijken van de verschillen in verdelingen in de tijd.\nWe bevestigen eerst de bimodaliteit die we in 1970 waarnamen en dat deze wordt verklaard door de westelijk tegenover de ontwikkelingswereld- dichotomie. Dit doen we door histogrammen te maken voor de groepen die we hebben ge?dentificeerd. Merk op dat we de twee groepen maken met ifelse binnen een mutaat en dat we facet_grid gebruiken om een histogram te maken voor elke groep:\n\ngapminder %>% \n  filter(year == past_year & !is.na(gdp)) %>%\n  mutate(group = ifelse(region%in%west, \"West\", \"Developing\")) %>%\n  ggplot(aes(dollars_per_day)) +\n  geom_histogram(binwidth = 1, color = \"black\") +\n  scale_x_continuous(trans = \"log2\") + \n  facet_grid(. ~ group)\n\n\n\n\nNu kunnen we kijken of de scheiding vandaag de dag slechter is dan veertig jaar geleden. Dit doen we door zowel per regio als per jaar te facetteren:\n\npast_year <- 1970\npresent_year <- 2010\ngapminder %>% \n  filter(year %in% c(past_year, present_year) & !is.na(gdp)) %>%\n  mutate(group = ifelse(region%in%west, \"West\", \"Developing\")) %>%\n  ggplot(aes(dollars_per_day)) +\n  geom_histogram(binwidth = 1, color = \"black\") +\n  scale_x_continuous(trans = \"log2\") + \n  facet_grid(year ~ group)\n\n\n\n\nVoordat we de bevindingen van deze plot interpreteren, stellen we vast dat er meer landen vertegenwoordigd zijn in de histogrammen van 2010 dan in 1970: het aantal tellingen is groter. Een van de redenen hiervoor is dat verschillende landen na 1970 zijn opgericht. De Sovjet-Unie is in de jaren negentig is veranderd in verschillende landen, waaronder Rusland en Oekra?ne. Een andere reden is dat in 2010 voor meer landen gegevens beschikbaar zijn.\nWe hebben de figuren opnieuw gemaakt met behulp van alleen landen met gegevens die beschikbaar zijn voor beide jaren. In het hoofdstuk over datawisselingen leren we tidyverse tools waarmee we hiervoor effici?nte codes kunnen schrijven, maar hier een eenvoudige code met behulp van de kruispunt functie:\n\ncountry_list_1 <- gapminder %>% \n  filter(year == past_year & !is.na(dollars_per_day)) %>% .$country\n\ncountry_list_2 <- gapminder %>% \n  filter(year == present_year & !is.na(dollars_per_day)) %>% .$country\n      \ncountry_list <- intersect(country_list_1, country_list_2)\n\nThese 108 account for 86 % of the world population, so this subset should be representative.\nLaten we de plot opnieuw maken, maar alleen voor deze subset door simpelweg land % in% country_list aan de filterfunctie toe te voegen:\n\n\n\n\n\nWe zien nu dat terwijl de rijke landen procentueel wat rijker zijn geworden, de arme landen meer lijken te zijn verbeterd. We zien vooral dat het aandeel van ontwikkelingslanden waar mensen meer dan $16 per dag verdienen aanzienlijk toeneemt.\nOm te zien welke specifieke regio’s het meest verbeterden, kunnen we de boxplots die we hierboven hebben gemaakt met nu 2010 toegevoegd, opnieuw maken\n\np <- gapminder %>% \n  filter(year %in% c(past_year, present_year) & country %in% country_list) %>%\n  mutate(region = reorder(region, dollars_per_day, FUN = median)) %>%\n  ggplot() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  xlab(\"\") +\n  scale_y_continuous(trans = \"log2\")  \n\nen dan met behulp van facet om de twee jaar te vergelijken:\n\np + geom_boxplot(aes(region, dollars_per_day, fill = continent)) +\n  facet_grid(year~.)\n\n\n\n\nHier pauzeren we om nog een krachtige ggplot2-functie te introduceren. Omdat we elke regio voor en na willen vergelijken, zou het handig zijn om de 1970 boxplot naast de 2010 boxplot voor elke regio te hebben. Over het algemeen zijn vergelijkingen gemakkelijker wanneer gegevens naast elkaar worden uitgezet.\nDus in plaats van facetteren houden we de gegevens van elk jaar bij elkaar, maar vragen we ggplot om ze afhankelijk van het jaar te kleuren (of te vullen). Ggplot scheidt ze automatisch en plaatst de twee boxplots naast elkaar. Omdat jaar een getal is, maken we er een factor van omdat ggplot automatisch een kleur toewijst aan elke categorie van een factor:\n\np + geom_boxplot(aes(region, dollars_per_day, fill = factor(year)))\n\n\n\n\nTot slot wijzen we erop dat als wat we het meest ge?nteresseerd zijn in het vergelijken van voor en na waarden, kan het zinvoller zijn om de verhoudingen, of verschil in de log schaal plot te zetten. We zijn nog steeds niet klaar om te leren om dit te leren coderen, maar hier hoe het figuur eruit zou zien:\n\n\n\n\n\n\n\nSubtiele verdelingplots\nMet behulp van dataverkenning hebben we ontdekt dat de inkomenskloof tussen rijke en arme landen de afgelopen veertig jaar aanzienlijk is gedicht. We gebruikten een reeks histogrammen en boxplots om dit te laten zien. Hier suggereren we een beknopte manier om deze boodschap over te brengen met slechts een plot. We zullen hiervoor subtiele verdelingplots gebruiken.\nLaten we beginnen met op te merken dat verdelingplots voor inkomensverdeling in 1970 en 2010 de boodschap afgeven dat de kloof aan het dichten is:\n\ngapminder %>% \n  filter(year %in% c(past_year, present_year) & country %in% country_list) %>%\n  ggplot(aes(dollars_per_day)) +\n  geom_density(fill = \"grey\") + \n  scale_x_continuous(trans = \"log2\") + \n  facet_grid(year~.)\n\n\n\n\nIn de 1970 plot zien we twee heldere modi, arme en rijke landen. In 2010 lijkt het erop dat sommige arme landen naar rechts zijn verschoven, dat aangeeft dat de kloof is gedicht.\nDe volgende boodschap die we moeten uitdragen is dat de reden voor deze verandering in de verdeling is dat arme landen rijker werden in plaats van dat sommige rijke landen armer werden. Om dit te doen hoeven we alleen een kleur toe te wijzen aan de groepen die we hebben ge?dentificeerd tijdens de dataverkenning.\nVoordat we dit echter kunnen doen, moeten we leren hoe we deze vlotte dichtheden zo kunnen maken dat de informatie over het aantal landen in elke groep behouden blijft. Om te begrijpen waarom we dit nodig hebben, let dan op de discrepantie in de grootte van elke groep:\n\n\n\n\n\ngroup\nn\n\n\n\n\nDeveloping\n87\n\n\nWest\n21\n\n\n\n\n\nmaar als twee verdelingen overlappen, is de standaardinstelling dat het gebied dat door elke distributie wordt weergegeven tot 1 wordt opgeteld, ongeacht de grootte van elke groep:\n\ngapminder %>% \n  filter(year %in% c(past_year, present_year) & country %in% country_list) %>%\n  mutate(group = ifelse(region %in% west, \"West\", \"Developing\")) %>%\n  ggplot(aes(dollars_per_day, fill = group)) +\n  scale_x_continuous(trans = \"log2\") +\n  geom_density(alpha = 0.2) + \n  facet_grid(year ~ .)\n\n\n\n\nwaardoor het lijkt alsof er in elke groep evenveel landen zitten. Om dit te veranderen, zullen we moeten leren om berekende variabelen met de geom_density functie te benaderen.\n\n\nToegang tot computervariabelen\nOm de oppervlakten van deze verdelingen evenredig te laten zijn met de grootte van de groepen, kunnen we de y-aswaarden eenvoudig vermenigvuldigen met de grootte van de groep. Vanuit de geom_density help-file zien we dat de functie, die een variabele genaamd count berekenen, precies dit doet. We willen dat deze variabele op de y-as komt te staan.\nIn ggplot krijgen we toegang tot deze variabelen door ze te omringen met de naam ... Daarom zullen we de volgende mapping gebruiken:\n\naes(x = dollars_per_day, y = ..count..)\n\nNu kunnen we de gewenste plot maken door simpelweg de mapping in het vorige codebrok te veranderen:\n\np <- gapminder %>% \n  filter(year %in% c(past_year, present_year) & country %in% country_list) %>%\n  mutate(group = ifelse(region %in% west, \"West\", \"Developing\")) %>%\n  ggplot(aes(dollars_per_day, y = ..count.., fill = group)) +\n  scale_x_continuous(trans = \"log2\")\n\np + geom_density(alpha = 0.2) + facet_grid(year ~ .)\n\n\n\n\nAls we willen dat de verdelingen wordt subtieler worden, gebruiken we het ‘bw’-argument. We probeerden er een paar en kozen voor 0,75:\n\np + geom_density(alpha = 0.2, bw = 0.75) + facet_grid(year ~ .)\n\n\n\n\nDee plot laat nu heel duidelijk zien wat er aan de hand is. De verdeling in de ontwikkelingslanden is aan het veranderen. Een derde modus lijkt te bestaan en bestaat uit de landen die de kloof het meest hebben gedicht.\n\n\ncase_when\nWe kunnen dit cijfer zelfs iets informatiever maken. Uit de verkennende gegevensanalyse merkten we dat veel van de landen die het meest verbeterden, afkomstig waren uit Azi?. We kunnen de plot gemakkelijk wijzigen door belangrijke regio’s afzonderlijk weer te geven.\nWe introduceren de case_when functie die handig is voor het defini?ren van groepen. Het heeft momenteel geen data-argument (dit kan veranderen), dus we moeten de onderdelen van onze gegevenstabel benaderen met behulp van de stip plaatsbewerker:\n\ngapminder <- gapminder %>% \n  mutate(group = case_when(\n    .$region %in% west ~ \"West\",\n    .$region %in% c(\"Eastern Asia\", \"South-Eastern Asia\") ~ \"East Asia\",\n    .$region %in% c(\"Caribbean\", \"Central America\", \"South America\") ~ \"Latin America\",\n    .$continent == \"Africa\" & .$region != \"Northern Africa\" ~ \"Sub-Saharan Africa\",\n    TRUE ~ \"Others\"))\n\nWe maken van deze ‘groep’-variabele een factor om de volgorde van de niveaus te bepalen:\n\ngapminder <- gapminder %>% \n  mutate(group = factor(group, levels = c(\"Others\", \"Latin America\", \"East Asia\",\"Sub-Saharan Africa\", \"West\")))\n\nWe kiezen deze specifieke volgorde vanwege een reden die later duidelijk wordt.\nNu kunnen we eenvoudig de verdeling voor elk in kaart brengen. We gebruiken kleur' engrootte’ om de toppen duidelijk te laten zien:\n\np <- gapminder %>% \n    filter(year %in% c(past_year, present_year) & country %in% country_list) %>%\n  ggplot(aes(dollars_per_day, y = ..count.., fill = group, color = group)) +\n  scale_x_continuous(trans = \"log2\")\n\np + geom_density(alpha = 0.2, bw = 0.75, size = 2) + facet_grid(year ~ .)\n\n\n\n\nDe plot is rommelig en wat moeilijk te lezen. Soms krijg je een duidelijker beeld door de dichtheden op elkaar te stapelen:\n\np + geom_density(alpha = 0.2, bw = 0.75, position = \"stack\") + facet_grid(year ~ .)\n\n\n\n\nHier zien we duidelijk dat de verdelingen voor Oost-Azi?, Latijns-Amerika e.a. duidelijk naar rechts verschuiven. Terwijl Afrika bezuiden de Sahara blijft stagneren.\nMerk op dat we de niveaus van de groep zo ordenen dat de West verdeling eerst wordt uitgezet, dan Sub-Sahara Afrika. Als we eerst de twee uitersten in kaart brengen, zien we de resterende bimodaliteit beter.\n\nGewogen verdelingen\nTot slot merken we op dat deze uitkeringen in alle landen hetzelfde wegen. Dus als het grootste deel van de bevolking zich verbetert, maar in een heel groot land woont, zoals China, zullen we dit misschien niet op prijs stellen. We kunnen de vloeiende verdelingen eigenlijk wegen met behulp van het gewicht in kaart brengen argument. Het plot ziet er dan als volgt uit:\n\n\n\n\n\nDeze specifieke figuur laat heel duidelijk zien hoe de inkomenskloof wordt gedicht, waarbij de meeste armen in Afrika bezuiden de Sahara blijven."
  },
  {
    "objectID": "posts/Visualisatiegapminder/index.html#ecologische-denkfout",
    "href": "posts/Visualisatiegapminder/index.html#ecologische-denkfout",
    "title": "Gapmindervisualisatie",
    "section": "Ecologische denkfout",
    "text": "Ecologische denkfout\nIn deze sectie hebben we regio’s van de wereld met elkaar vergeleken. We hebben gezien dat sommige regio’s het gemiddeld beter doen dan andere. Hier richten we ons op het beschrijven van het belang van verschillen binnen de groepen.\nHier richten we ons op de relatie tussen de overlevingskans van kinderen in een land en het gemiddelde inkomen. We beginnen met het vergelijken van deze hoeveelheden tussen regio’s. We defini?ren nog een paar regio’s:\n\ngapminder <- gapminder %>% \n  mutate(group = case_when(\n    .$region %in% west ~ \"The West\",\n    .$region %in% \"Northern Africa\" ~ \"Northern Africa\",\n    .$region %in% c(\"Eastern Asia\", \"South-Eastern Asia\") ~ \"East Asia\",\n    .$region == \"Southern Asia\"~ \"Southern Asia\",\n    .$region %in% c(\"Central America\", \"South America\", \"Caribbean\") ~ \"Latin America\",\n    .$continent == \"Africa\" & .$region != \"Northern Africa\" ~ \"Sub-Saharan Africa\",\n    .$region %in% c(\"Melanesia\", \"Micronesia\", \"Polynesia\") ~ \"Pacific Islands\"))\n\nVervolgens berekenen we deze hoeveelheden per regio.\n\nsurv_income <- gapminder %>% \n  filter(year %in% present_year & !is.na(gdp) & !is.na(infant_mortality) & !is.na(group)) %>%\n  group_by(group) %>%\n  summarize(income = sum(gdp)/sum(population)/365,\n            infant_survival_rate = 1-sum(infant_mortality/1000*population)/sum(population)) \n\nsurv_income %>% arrange(income)\n\n# A tibble: 7 x 3\n  group              income infant_survival_rate\n  <chr>               <dbl>                <dbl>\n1 Sub-Saharan Africa   1.76                0.936\n2 Southern Asia        2.07                0.952\n3 Pacific Islands      2.70                0.956\n4 Northern Africa      4.94                0.970\n5 Latin America       13.2                 0.983\n6 East Asia           13.4                 0.985\n7 The West            77.1                 0.995\n\n\nDit laat een dramatisch verschil zien. Terwijl in het westen minder dan 0,5 procent van de kinderen sterft, is dat in Afrika bezuiden de Sahara meer dan 6 procent! De relatie tussen deze twee variabelen is bijna perfect lineair\n\nsurv_income %>% ggplot(aes(income, infant_survival_rate, label = group, color = group)) +\n  scale_x_continuous(trans = \"log2\", limits = c(0.25, 150)) +\n  scale_y_continuous(trans = \"logit\", limit = c(0.875, .9981), \n                     breaks = c(.85,.90,.95,.99,.995,.998)) +\n  geom_label(size = 3, show.legend = FALSE)\n\n\n\n\nIn deze plot introduceren we het gebruik van het limit argument dat laat het bereik van de assen veranderen. We maken het bereik groter dan de gegevensbehoeften omdat we dit plot later zullen vergelijken met een plot met meer variabiliteit; we willen dat de bereiken hetzelfde zijn. We introduceren ook het `breaks’ argument, waarmee we de locatie van de aslabels kunnen instellen. Eindelijk introduceren we een nieuwe transformatie, de logistieke transformatie.\n\nLogistische transformatie\nDe logistische of logistieke transformatie voor een deel of een koers van \\(p\\) wordt gedefinieerd als\n\\[f(p) = \\log \\left( \\frac{p}{1-p} \\right)\\]\nWanneer \\(p\\) een proportie of waarschijnlijkheid is, wordt de hoeveelheid die wordt gelogd, \\(p/(1-p)\\) de odds genoemd. In het geval \\(p\\) is het aandeel van een kind dat overleefde. De kansen vertellen ons hoeveel meer kinderen worden uitgedreven om te overleven dan om te sterven. De logtransformatie maakt dit symmetrisch. Als de snelheden gelijk zijn, dan is de log odds 0. Bij toe- of afname verandert het in positieve en negatieve stappen.\nDeze schaal is handig als we verschillen in de buurt van 0 of 1 willen markeren. Voor overlevingskansen is dit van belang omdat een overlevingsgraad van 90% onaanvaardbaar is, terwijl een overlevingsgraad van 99% relatief goed is. We zouden veel liever een overlevingskans hebben die dichter bij 99,9 procent ligt. We willen dat onze schaal dit verschil benadrukt en de logit doet dit. Merk op dat 99.9/0.1 ongeveer 10 keer groter is dan 99/1, wat ongeveer 10 keer groter is dan 90/10. En door gebruik te maken van de log worden deze wissels steeds groter.\n\n\nToon de gegevens\nNu, terug naar onze plot. Kan nu geconcludeerd worden dat op basis van het bovenstaande een land met een laag inkomen een lage overlevingskans zal hebben? Kunnen we concluderen dat alle overlevingskansen in Afrika bezuiden de Sahara lager zijn dan in Zuid-Azi?, dat op zijn beurt lager is dan op de eilanden in de Stille Oceaan, enzovoort?\nDe conclusie gebaseerd op een plot dat weer wordt gebaseerd op gemiddelden wordt ecologische denkfout genoemd. De bijna perfecte relatie tussen overlevingskansen en inkomen zien we alleen bij de gemiddelden op regionaal niveau. Als we alle gegevens eenmaal hebben getoond, krijgen we een wat ingewikkelder verhaal:\n\nlibrary(ggrepel)\nhighlight <- c(\"Sierra Leone\", \"Mauritius\",  \"Sudan\", \"Botswana\", \"Tunisia\",\n               \"Cambodia\",\"Singapore\",\"Chile\", \"Haiti\", \"Bolivia\",\n               \"United States\",\"Sweden\", \"Angola\", \"Serbia\")\n\ngapminder %>% filter(year %in% present_year & !is.na(gdp) & !is.na(infant_mortality) & !is.na(group) ) %>%\n  ggplot(aes(dollars_per_day, 1 - infant_mortality/1000, col = group, label = country)) +\n  scale_x_continuous(trans = \"log2\", limits=c(0.25, 150)) +\n  scale_y_continuous(trans = \"logit\",limit=c(0.875, .9981),\n                     breaks=c(.85,.90,.95,.99,.995,.998)) + \n  geom_point(alpha = 0.5, size = 3) +\n  geom_text_repel(size = 4, show.legend = FALSE,\n    data = filter(gapminder, year %in% present_year & country %in% highlight))\n\n\n\n\nWe zien we een grote mate van variabiliteit. We zien dat landen uit dezelfde regio’s heel verschillend kunnen zijn en dat landen met hetzelfde inkomen verschillende overlevingskansen kunnen hebben. Terwijl Afrika bezuiden de Sahara bijvoorbeeld gemiddeld de slechtste gezondheids- en economische resultaten had, is er binnen die groep sprake van grote variabiliteit. Merk bijvoorbeeld op dat Mauritius en Botswana het beter doen dan Angola en Sierra Leone en Mauritius zelfs vergelijkbaar is met Westerse landen."
  },
  {
    "objectID": "posts/Communiceren_met_rmarkdown/index.html",
    "href": "posts/Communiceren_met_rmarkdown/index.html",
    "title": "Communiceren met RMarkdown",
    "section": "",
    "text": "Eerder schreef ik een snelle introductie op enkele algemene kenmerken van LaTeX, een opensource software syteem om verschillende soorten documenten te zetten. Dit programmma wordt vooral gebruikt voor maken van wetenschappelijke documenten. Die introductie is hier  te vinden. Daarin laat ik zien hoe \\(\\Latex\\) werkt en welke verschillende soorten documenten je ermee kunt maken (waaronder artikel, boek, rapport, een poster, een proefschrift, een presentatie). Verder kun je hier \\(\\Latex\\)-tutorial ook informatie vinden en hier Snel overzicht.\n\\(\\Latex\\) werkt met veel verschillende commando’s en je moet de tijd nemen dit te leren. Op internet is overigens wel goede informatie te vinden en de meeste problemen kun je zelf oplossen. De laatste jaren vinden er binnen het programma R veel ontwikkelingen plaats die het maken van wetenschappelijke documenten vergemakkelijken. De ontwikkelingen vallen onder de term RMarkdown dat in 2015 met de introductie van het knitr-pakket werd geïntroduceerd. Hier wordt binne R gebruik gemaakt van de Markdown taal waarmee technische documenten betrekkelijk eenvoudig te maken zijn. Het pakket maakt het mogelijk om verschillende soorten documenten te maken (waaronder pdf, html en word). Tot slot maakt RMarkdown het mogelijk om tekst (inclusief bv. grafieken, tabellen en referentie) en analyses in een keer te draaien. Met RMarkdown, knitr en alles wat hier aan vastzit kun je onder anderen:\n- Een goed uitziend wetenschappelijk document maken in verschillende formats tegelijkertijd (pdf, html en word);\n- Kun je met notebooks werken waarin naast tekst analyses zijn opgenomen;\n- Kun je nieuwe vormen van presentaties voorbereiden;\n- Kun je dashboards maken waarin informatie overzichtelijk, aantrekkelijk, flexibel en interactief wordt gepresenteerd;\n- Kun je interactieve toepassingen maken bijvoorbeeld door de inzet van Shiny; - Kun je wetenschappelijke artikelen maken; - Kun je boeken zelf maken; - KUn je een blog en website maken.\nEen uitgebreide gids hierover vind je hier RMarkdown: The Definitive Guide. En een handleiding vind je hier Cheat sheet RMarkdown.\n\n\nOm met onderstaande te kunnen werken moet je in ieder geval R installeren R alsmede RStudio RStudio. Om pdf te draaien moet je ook Latex op je computer installeren. Heb je dat niet kun je ook binnen R/RStudio het pakket tinytex. Als je R en RStudio hebt binnengehaald moet je de pakketten knitr en rmarkdown binnenhalen. Voor achtergrondinformatie over R, RStudio en RMarkdown vind je hier RRStudioRMarkdowninformatie\nEerder schreef ik al uitgebreid over Reproducable Research en de workshop die Schmidt op 11 Mei 2016 en er waren wat aanvullende materialen beschikbaar zie post Reproducable Research.Delen daarvan breng ik hier nog eens onder de aandacht en ik update de informatie.\n\n\n\nVoordat je aan het werk gaat, zorg ervoor dat je het volgende hebt gedaan:\n\nOpen RStudio.\n\nInstalleer en download het devtools R pakket door het volgende commando te runnen.\n\n\ninstall.packages(\"devtools\") # Nodig voor deze sessie\nlibrary(\"devtools\")          # Nodig voor deze sessie   \n\n\nCheck of je de goede versie hebt van R en RStudio door devtools::session_info() in de R console te draaien.\nHier geeft devtools:: aan om de session_info() functie in R te gebruiken ipv het devtools pakket en de sessionInfo() functie binnen het utils pakket. Het runnen van devtools::session_info() stelt ons in staat de versie van R en RStudio vast te stellen.\n\nHeb je de volgende versie van R en RStudio?\n\nR: Versie 3.3.0 (2016-05-03)\n\nRStudio: 0.99.1172\n\nZo ja dan kun je van start gaan!\n\nZo nee dan heb je nieuwe versies van R en RStudio nodig, volg dan Setup in dit document.\n\n\n\nInstalleer vervolgens enkele R pakketten die je nodig hebt.\n\n\n## Installeer de goede pakketten\n#install.packages(\"rmarkdown\")  # Dit zorgt voor koele dynamische documenten\n#install.packages(\"knitr\")      # Hier kun je R code Chunks mee runnen\n#install.packages(\"ggplot2\")    # Voor het plotten van mooie figuren\n#install.packages(\"DT\")         # Om interactieve HTML tabellen te maken\n\n\n## Deze pakketten ook laden om er zeker van te zijn dat je de goede pakketten hebt\nlibrary(\"rmarkdown\")           # Dit zorgt voor koele dynamische documenten\nlibrary(\"knitr\")               # Hier kun je R code Chunks mee runnen\nlibrary(\"ggplot2\")             # Voor het plotten van mooie figuren\nlibrary(\"DT\")                  # Om interactieve HTML tabellen te maken\n\n\nAls je de pakketten zonder fouten hebt geladen, kun je beginnen!"
  },
  {
    "objectID": "posts/Communiceren_met_rmarkdown/index.html#waarom-r-markdown",
    "href": "posts/Communiceren_met_rmarkdown/index.html#waarom-r-markdown",
    "title": "Communiceren met RMarkdown",
    "section": "Waarom R Markdown?",
    "text": "Waarom R Markdown?\nEen aantrekkelijk gereedschap voor reproduceerbare en dynamische rapporten!\n\nTerwijl het was gemaakt voor R, accepteert het veel programmeertalen. Om het eenvoudig te houden, werken we vandaag alleen met R.\n\nEen code kan op een aantal manieren worden uitgevoerd:\n\nInline Code: Een korte code die in de geschreven tekst van het document wordt uitgevoerd.\n\nCode Chunks: Delen van het document omvatten verschillende zinnen analyse code. Dat kan een plot of een tabel zijn, maar ook berekeningen van de samenvattende statistiek, pakketten laden, etc.\n\n\nHet is makkelijk om:\n\nPlaatjes op te nemen.\n\nDe Markdown syntax te leren.\n\n\\(\\LaTeX\\) elementen op te nemen.\n\nInteractieve tabellen op te nemen.\n\nGebruik de versie via Git.\n\nDan is het makkelijker om te delen en samen te werken in analyses, projecten en publicaties!\n\n\nExterne linken toe te voegen - Rmarkdown begrijpt zelfs enige html codes!\nOm mooie documenten te maken.\n\n\nJe hoeft je geen zorgen te maken over pagina breuken of het plaatsen van de figuren.\n\nConsolideer jouw code en plaats het in een file:\n\nPowerpoint, PDFs, html documenten en word files"
  },
  {
    "objectID": "posts/Communiceren_met_rmarkdown/index.html#eenvoudige-werkwijze",
    "href": "posts/Communiceren_met_rmarkdown/index.html#eenvoudige-werkwijze",
    "title": "Communiceren met RMarkdown",
    "section": "Eenvoudige werkwijze",
    "text": "Eenvoudige werkwijze\nIn het kort, om een rapport te maken:\n\nOpen een .Rmd file.\n\nMaak een YAML kop (meer hierover zo dadelijk!)\n\n\nSchrijf de inhoud met RMarkdown syntax.\n\nNeem mee de R code in code chunks of met een inline code.\n\nDraai de document output.\n\nOverzicht van de stappen die RMarkdown maakt om een ‘gerenderd’ document te krijgen:\n\nMaak een .Rmd rapport met ‘R code chunks’ en markdown verhalen (zoals hierboven in stappen beschreven).\n\nGeef de .Rmd-file aan knitr om de ‘R code chunks’ uit te voeren en een nieuwe .md file te maken.\n\nKnitr is een pakket binnen R die jou in staat stelt de code binnen RMarkdown documenten uit te voeren zoals HTML, latex, pdf, word en andere document types.\n\n\nGeef de .md file aan pandoc, die er een definitief document van maakt (b.v. html, Microsoft word, pdf, etc.).\n\nPandoc is een universeel gereedschap om documenten te converteren en zet het ene document type (in dit geval: .Rmd) om in een ander (in dit geval: HTML)\n\n\nHoewel dit mogelijk wat ingewikkeld lijkt, kunnen we op de Knit knop drukken boven aan de pagina."
  },
  {
    "objectID": "posts/Communiceren_met_rmarkdown/index.html#maak-een-.rmd-file",
    "href": "posts/Communiceren_met_rmarkdown/index.html#maak-een-.rmd-file",
    "title": "Communiceren met RMarkdown",
    "section": "Maak een .Rmd file",
    "text": "Maak een .Rmd file\nLaten we eens met een RMarkdown gaan werken!\n\nIn de menu bar, klik je op File -> New File -> RMarkdown\nDan krijg je het volgende te zien\n\n\n\nHierbinnen kies je het type output dat je wilt hebben. Opgelet: deze output kan later heel makkelijk worden aangepast!\n\n\nKlik OK"
  },
  {
    "objectID": "posts/Communiceren_met_rmarkdown/index.html#yaml-koppen",
    "href": "posts/Communiceren_met_rmarkdown/index.html#yaml-koppen",
    "title": "Communiceren met RMarkdown",
    "section": "YAML koppen",
    "text": "YAML koppen\nYAML staat voor “YAML Ain’t Markup Language” en is eigenlijk de structuur voor de metadata van het document. Het staat tussen twee regels van drie streepjes --- en wordt automatisch omgezet door RStudio. Een eenvoudig voorbeeld:\n---\ntitle:  \"Analyse Rapport\"  \nAuthor:  \"Harrie Jonkman\"  \ndate: \"1 Maart 2017\"  \noutput:  html_document\n---\nHet voorbeeld boven zal een HTML document maken. Echter, de volgende opties zijn ook beschikbaar.\n\nhtml_document\n\npdf_document\n\nword_document\n\nbeamer_presentation (pdf powerpoint)\n\nioslides_presentation (HTML powerpoint)\n\nen nog meer …\n\nHier ligt de nadruk op HTML files. Echter voel je vrij als je hier wat mee wilt spelen door bv. word en pdf documenten te maken. Presentatie-documenten kennen een wat andere syntax (bv. om aan te geven wanneer de ene dia eindigt en de andere begint) en dan is er nog wat markdown syntax specifiek voor presentaties maar die gaat voorbij het doel van deze workshop."
  },
  {
    "objectID": "posts/Communiceren_met_rmarkdown/index.html#markdown-basis",
    "href": "posts/Communiceren_met_rmarkdown/index.html#markdown-basis",
    "title": "Communiceren met RMarkdown",
    "section": "Markdown Basis",
    "text": "Markdown Basis\nKijk hiernaar RMarkdown Reference Guide\nHaal hier ook informatie vandaan RMarkdown Cheatsheet.\nHandige tips:\n\nEindig elke regel met drie spaties om een nieuwe regel te beginnen.\n\nWoorden binnen een code moeten aan beide kanten zo’n kommateken kennen: `\n\nOm iets tot superscript te maken moet je een ^ aan beide zijden plaatsen. Superscript werd gevormd door Super^script^ te typen.\n\nVergelijkingen kunnen in een inline code worden geplaatst met $ en als blok gecentreerd binnen het document door $$. Bijvoorbeeld \\(E = mc^2\\) staat tussen de regels terwijl het volgende geblokt wordt opgenomen: \\[E = mc^2\\]\n\nAnder wiskundig materiaal:\n- Vierkantswortel: $\\sqrt{b}$ zal \\(\\sqrt{b}\\) maken - Breuken: $\\frac{1}{2}$ = \\(\\frac{1}{2}\\)\n- - Vergelijkingen met breuken: $f(x)=\\frac{P(x)}{Q(x)}$ = \\(f(x)=\\frac{P(x)}{Q(x)}\\)\n- Binomiale Coefficienten: $\\binom{k}{n}$ = \\(\\binom{k}{n}\\)\n- Integralen: $$\\int_{a}^{b} x^2 dx$$ = \\[\\int_{a}^{b} x^2 dx\\]\n\nShareLaTeX is een prachtige bron voor LaTeX-codes.\n\n\nNog wat wiskundig materiaal:\n\n\n\n\n\n\n\n\nBeschrijving\nCode\nVoorbeelden\n\n\n\n\nGriekse letters\n$\\alpha$ $\\beta$ $\\gamma$ $\\rho$ $\\sigma$ $\\delta$ $\\epsilon$ $mu$\n\\(\\alpha\\) \\(\\beta\\) \\(\\gamma\\) \\(\\rho\\) \\(\\sigma\\) \\(\\delta\\) \\(\\epsilon\\) \\(\\mu\\)\n\n\nBinaire handelingen\n$\\times$ $\\otimes$ $\\oplus$ $\\cup$ $\\cap$\n\\(\\times\\) \\(\\otimes\\) \\(\\oplus\\) \\(\\cup\\) \\(\\cap\\) \\(\\times\\)\n\n\nRelationele handelingen\n$< >$ $\\subset$ $\\supset$ $\\subseteq$ $\\supseteq$\n\\(< >\\) \\(\\subset\\) \\(\\supset\\) \\(\\subseteq\\) \\(\\supseteq\\)\n\n\nVerder\n$\\int$ $\\oint$ $\\sum$ $\\prod$\n\\(\\int\\) \\(\\oint\\) \\(\\sum\\) \\(\\prod\\)\n\n\n\n\nUitdaging: Probeer eens de volgende output te maken:\n\n\nVandaag voel ik mij vet omdat ik RMarkdown leer.\nhoning is heel zoet.\nYAS!!!!!!\nR2 waarden zijn informatief!\n\\(R^{2}\\) beschrijft de variantie verklaard door het model.\nIk kende geen RMarkdown Vandaag heb ik RMarkdown geleerd\nRStudio link\nOutput van het volgende:\n\n\n# RMarkdown   \n## R   \n### Knitr   \n#### Pandoc  \n##### HTML  \n\n\\(\\sqrt{b^2 - 4ac}\\)\n\\[\\sqrt{b^2 - 4ac}\\]\n\\(X_{i,j}\\)\n\n\n\nVandaag maak ik een dynamisch document!\n\n\n\nHet volgende lijstje:\n\nChocolade Chips Kook Recept\n\nboter\nsuiker\n\nEen mengsel van bruine en witte suiker maakt het lekkerder\n\nmix dat met boter voordat je de eieren eraan toevoegt\n\n\neieren\nvanille\n\nMix wat droge ingredienten:\n\nmeel, zout, bak soda\n\n\nchocolade chips"
  },
  {
    "objectID": "posts/Communiceren_met_rmarkdown/index.html#een-code-in-het-document",
    "href": "posts/Communiceren_met_rmarkdown/index.html#een-code-in-het-document",
    "title": "Communiceren met RMarkdown",
    "section": "Een Code in het document",
    "text": "Een Code in het document\nEr zijn twee manieren om een code in een RMarkdown document op te nemen.\n\nCode in het document: Korte code als een onderdeel van het geschreven document.\nCode Chunks: Delen van het document die verschillende programmeer of analyse codes omvatten. Daarmee kan een figuur of tabel worden gemaakt, statistieken worden berekend, pakketten worden geladen, etc.\n\n\nR Code in het document\nEen R code kan in het document wordt gemaakt door een komma hoog achterwaarts (`) en de letter r gevolgd door nog zo’n komma.\n\nBijvoorbeeld: 211 is 2048.\n\nStel dat je een p-waarde rapporteert en je wilt niet terug om de statistische test steeds weer uit te voeren. De p-waarde was eerder 0.0045.\nDit is echt handig als de resultaten op papier moeten worden gezet. Bijvoorbeeld, je hebt een aantal statistieken uitgevoerd voor jouw wetenschappelijke vragen is dit een manier waarop R die waarde in a variabele naam bewaart. Bijvoorbeeld: Wijkt het brandstofverbruik van de automaat significant af de auto met handtransmissie significant af binnen de mtcars data set?\n\nmpg_auto <- mtcars[mtcars$am == 0,]$mpg # automatic transmission mileage\nmpg_manual <- mtcars[mtcars$am == 1,]$mpg # manual transmission mileage\ntransmission_ttest <- t.test(mpg_auto, mpg_manual)\n\nOm de p-waarde vast te stellen kunnen we transmission_ttest$p.value als R code in het document gebruiken.\nDe p-waarde is dan 0.0013736.\n\n\nR Code Chunks\nR code chunks (nogmaals ik gebruik maar de Engelse benaming hier, sorry)kunnen worden gebruikt om de R output in het document te krijgen of om de code als illustratie zichtbaar te maken.\nDe anatomie van een code chunk:\nOm een R code chunk te plaatsen, kun je met de hand typen door ```{r} gevolgd door ``` op een volgende regel. Je kunt ook de Insert a new code chunk knop gebruiken of de ‘shortcut key’. Dat geeft dan de volgende code chunk:\n`\u000b``{r}\nn <- 10\nseq(n)\n```\nGeef de code chunk een betekenisvolle naam die samenhangt met wat het doet. Hieronder heb ik code chunk 10-random-numbers genoemd:\n`\u000b``{r 10-random-numbers}\nn <- 10\nseq(n)\n```\nDe code chunk input en output zien er dan als volgt uit:\n\nn = 10\nseq(n)\n\n [1]  1  2  3  4  5  6  7  8  9 10"
  },
  {
    "objectID": "posts/Communiceren_met_rmarkdown/index.html#chunk-labels",
    "href": "posts/Communiceren_met_rmarkdown/index.html#chunk-labels",
    "title": "Communiceren met RMarkdown",
    "section": "Chunk Labels",
    "text": "Chunk Labels\nChunk labels krijgen unieke IDs in een document en zijn goed voor:\n\nOm externe files te genereren zoals plaatjes en ‘cached’ documenten.\n\nChunk labels zijn vaak output als fouten omhoog komen(vaker voor codes in het document).\n\nAls je de code chunk een naam geef, gebruik dan - of _ tussen woorden voor code chunks labels in plaats van ruimtes. Dat helpt jou en andere gebruikers bij het navigeren in het document.\nChunk labels moeten uniek zijn in het document - anders zal er een fout optreden!"
  },
  {
    "objectID": "posts/Communiceren_met_rmarkdown/index.html#chunk-opties",
    "href": "posts/Communiceren_met_rmarkdown/index.html#chunk-opties",
    "title": "Communiceren met RMarkdown",
    "section": "Chunk Opties",
    "text": "Chunk Opties\nDruk tab als tussen de haakjes code chunk opties omhoog komen.\n\nresults = \"asis\" staat voor “as is” en geeft de output van een niet geformateerde versie.\ncollapse is een andere chunk optie die handig kan zijn, zeker als een code chunk veel korte R uitdrukking heeft met wat output.\n\nEr zijn teveel chunk opties om hier te behandelen. Kijk na deze workshop nog eens wat rond voor deze opties.\nEen mooie website om dat op te doen is Knitr Chunk Options.\n\nUitdaging\nDraai de code chunk hieronder en speel wat met de volgende knitr code chunk opties:\n\n\n\neval = TRUE/FALSE\n\necho = TRUE/FALSE\n\ncollapse = TRUE/FALSE\n\nresults = \"asis\",\"markup en \"hide\n\n\n\nSla je resultaten op in markdown.\nOpgelet: Wees er zeker van dat je jouw chunks een naam geeft!\n\n\n1+1\n2*5\nseq(1, 21, by = 3)\nhead(mtcars)\n\nEnkele voorbeelden voortbouwend op de chunk hierboven\nResultaten van results=\"markup\", collapse = TRUE}:\n\n1+1\n## [1] 2\n2*5\n## [1] 10\nseq(1, 21, by = 3)\n## [1]  1  4  7 10 13 16 19\nhead(mtcars)\n##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nResultaten van results=\"asis\", collapse = TRUE}:\n1+1\n[1] 2\n2*5\n[1] 10\nseq(1, 21, by = 3)\n[1] 1 4 7 10 13 16 19\nhead(mtcars)\n               mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1"
  },
  {
    "objectID": "posts/Communiceren_met_rmarkdown/index.html#globale-opties",
    "href": "posts/Communiceren_met_rmarkdown/index.html#globale-opties",
    "title": "Communiceren met RMarkdown",
    "section": "Globale opties",
    "text": "Globale opties\nHet kan zijn dat je dezelfde chunk settings wilt handhaven voor het gehele document. Het kan daarom handig zijn om de opties in een keer te typen in plaats van het iedere keer weer voor een chunk te moeten doen. Om dat te doen kun je de globale chunk opties bovenaan het document vaststellen.\nknitr::opts_chunk$set(echo = FALSE, \n                      eval = TRUE, \n                      message = FALSE,\n                      warning = FALSE, \n                      fig.path = \"Figures/\",\n                      fig.width = 12, \n                      fig.height = 8)\nAls je bijvoorbeeld met iemand samenwerkt die de code niet wil zien, kun je schrijven eval = TRUE en echo = FALSE gebruiken zodat de code wel gedraaid wordt maar niet getoond. In aanvulling wil je misschien message = FALSE en warning = FALSE gebruiken zodat jouw samenwerkingspartner geen enkele boodschap of waarschuwing van R ziet.\nAls je figuren wilt opslaan en bewaren in een subdirectory binnen het project, gebruik dan fig.path = \"Figures/\". Hier verwijst de \"Figures/\" naar een folder Figures binnen de huidige directory waar de figuur die gemaakt wordt in het document wordt opgeslagen.\nOpgelet: de figuren worden niet standaard opgeslagen.\nGlobale chunk opties zullen voor de rest van het documenten worden vastgezet. Als je wilt dat een bepaalde chunk afwijkt van de globale opties, maak dat aan het begin van die bepaalde chunk duidelijk."
  },
  {
    "objectID": "posts/Communiceren_met_rmarkdown/index.html#figuren",
    "href": "posts/Communiceren_met_rmarkdown/index.html#figuren",
    "title": "Communiceren met RMarkdown",
    "section": "Figuren",
    "text": "Figuren\nKnitr maakt vrij eenvoudig figuren. Als een analyse code binnen een chunk een bepaald figuur moet produceren, dan zal hij dat in het document afdrukken.\nEnkele knitr chunk opties gerelateerd aan figuren:\n\nfig.width en fig.height\n\nStandaard: fig.width = 7, fig.height = 7\n\n\nfig.align: Hoe het figuur uit te lijnen\n\nOpties omvatten: \"left\", \"right\" en \"center\"\n\n\nfig.path: Een file pad naar de directory waar knitr de grafische output moet opslaan die er met de chunk wordt gemaakt.\n\nStandaard: 'figure/'\n\n\nEr is zelfs een fig.retina(alleen voor HTML output) voor hogere figuur resoluties met retina afdrukken.\n\n\n\n\nEen enkelvoudig figuur maken:\nMet fig.align = \"center\"\n\nggplot(mtcars, aes(x = mpg)) + xlab(\"Miles per Gallon\") +\n    geom_histogram(bins = 30, fill = \"cornflowerblue\", color = \"black\") +\n    geom_vline(xintercept=mean(mtcars$mpg), col=\"red\") \n\n\n\n\n\n\n\n\nMet fig.align = \"right\"\n\nggplot(mtcars, aes(x = mpg)) + xlab(\"Miles per Gallon\") +\n    geom_histogram(bins = 30, fill = \"cornflowerblue\", color = \"black\") +\n    geom_vline(xintercept=mean(mtcars$mpg), col=\"red\") \n\n\n\n\n\n\n\n\nMet fig.align = \"left\"\n\nggplot(mtcars, aes(x = mpg)) + xlab(\"Miles per Gallon\") +\n    geom_histogram(bins = 30, fill = \"cornflowerblue\", color = \"black\") +\n    geom_vline(xintercept=mean(mtcars$mpg), col=\"red\")"
  },
  {
    "objectID": "posts/Communiceren_met_rmarkdown/index.html#tabellen",
    "href": "posts/Communiceren_met_rmarkdown/index.html#tabellen",
    "title": "Communiceren met RMarkdown",
    "section": "Tabellen",
    "text": "Tabellen\nTabellen kunnen in Markdown voor nogal wat hoofdpijn kosten. We gaan er hier verder niet op in. Als je meer wilt leren over Markdown-tabellen kijk naar documentation on tables op de RMarkdown website.\nEr zijn enkele tabeltypen die handig kunnen zijn. Hier zullen we ons vorig voorbeeld gebruiken van de mtcars data\nIn zijn Knitr in a Knutshell introduceert Dr. Karl Broman: kable, panderen xtable en vooral die eerste twee deden mij plezier:\n\nkable: Binnen het knitr pakket - niet veel opties maar het ziet er goed uit.\npander: Binnen het pander pakket - heeft veel opties en handigheden. Makkelijk voor het vetmaken van waarden (bv. waarden onder een bepaalde waarde).\n\nkable en pander tabellen zijn mooi en handig bij het maken van niet-interactieve tabellen:\n\nkable(head(mtcars, n = 4)) # kable table with 4 rows\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n\n\n# Pander table\n# install.packages(\"pander\") # install pander first\nlibrary(pander)\npander(head(mtcars, n = 4))\n\n\nTable continues below\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\n\n\n\n\nMazda RX4\n21\n6\n160\n110\n3.9\n2.62\n16.46\n0\n1\n\n\nMazda RX4 Wag\n21\n6\n160\n110\n3.9\n2.875\n17.02\n0\n1\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.32\n18.61\n1\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n\n\n\n\n\n\n\n\n\n\n\n \ngear\ncarb\n\n\n\n\nMazda RX4\n4\n4\n\n\nMazda RX4 Wag\n4\n4\n\n\nDatsun 710\n4\n1\n\n\nHornet 4 Drive\n3\n1\n\n\n\n\n\nZie ook hoe je mooie tabellen kunt maken hierKable"
  },
  {
    "objectID": "posts/Communiceren_met_rmarkdown/index.html#html-widgets",
    "href": "posts/Communiceren_met_rmarkdown/index.html#html-widgets",
    "title": "Communiceren met RMarkdown",
    "section": "HTML Widgets",
    "text": "HTML Widgets\nMet de uitgave van de nieuwe RMarkdown v2 is het makkelijker dan ooit tevoren om HTML Widgets te gebruiken. Volg de link om uit te zoeken in welke widgets jij ge?nteresseerd bent!"
  },
  {
    "objectID": "posts/Communiceren_met_rmarkdown/index.html#spelling-controleren",
    "href": "posts/Communiceren_met_rmarkdown/index.html#spelling-controleren",
    "title": "Communiceren met RMarkdown",
    "section": "Spelling controleren",
    "text": "Spelling controleren\nIn de spelling kunnen natuurlijk altijd fouten zitten en daarom kan het nodig zijn dat we onze spelling in het document willen controleren. Er zijn twee manieren om de spelling te controleren:\n\nDruk op de “ABC check mark”  links van de vergrootglasknop in RStudio.\nGebruik de aspell() functie van het utils pakket. Je kunt dan echter beter de code chunks overslaan. De aspell() functie kan een filter functie overnemen om bepaalde regels in de files over te slaan en kan worden gebruikt met de knit_filter() die ontworpen is om de code chunks in een file over te slaan."
  },
  {
    "objectID": "posts/Communiceren_met_rmarkdown/index.html#knitr-themas",
    "href": "posts/Communiceren_met_rmarkdown/index.html#knitr-themas",
    "title": "Communiceren met RMarkdown",
    "section": "Knitr Thema’s",
    "text": "Knitr Thema’s\nHet knitr-syntax-thema kan worden aangepast of helemaal naar de hand worden gezet. Als je de standaardthema’s niet wilt, gebruik dan het knit_theme om het te veranderen. Er zijn 80 thema’s opgenomen binnen knitr en we kunnen de namen ervan zien door knit_theme$get().\nHier is de link naar jouw favoriete thema 80 knitr highlight themes."
  },
  {
    "objectID": "posts/Communiceren_met_rmarkdown/index.html#inhoudsopgave",
    "href": "posts/Communiceren_met_rmarkdown/index.html#inhoudsopgave",
    "title": "Communiceren met RMarkdown",
    "section": "Inhoudsopgave",
    "text": "Inhoudsopgave\nEen inhoudsopgave kan aan het gerenderd document worden toegevoegd door de toc optie in de YAML kop te gebruiken.\nOpties hierbij:\n\ntoc: of de inhoudsopgave moeten worden meegenomen:\n\ntoc: true: hier wordt de inhoudsopgave meegenomen\n\nDefault:toc: false: Hier wordt de inhoudsopgave niet meegenomen\n\n\ntoc_depth:: Hoeveel niveau’s moeten in de inhoudsopgave worden worden meegenomen?\n\nDefault: doc_depth: 3 zal koppen tot en met ### meenemen.\n\n\nnumber_sections: Voegt sectienummers toe aan de koppen. Bijvoorbeeld, dit document heeft number_sections: true\n\nDefault: number_sections: false\n\nOpgelet: Met elk # zal er een decimaal punt worden toegevoegd aan alle koppen.\n\n\ntoc_float:\n\n2 andere mogelijke parameters binnen toc_float:\n\ncollapsed: Controleert of de inhoudsopgave alleen aan het begin verschijnt. Het zal met de cursor erover verschijnen.\n\nDefault: collapsed: TRUE\n\nsmooth_scroll: Controleert of de pagina scrolls werken wanneer op de onderdelen van de inhoudsopgave wordt geklikt.\n\nDefault: smooth_scroll: true\n\n\n\n\nBijvoorbeeld:\noutput:\n  html_document:\n    toc: true\n    toc_depth: 2\n---\n\nUitdaging: Maak de YAML kop voor een HTML document die het volgende inhoudt:\n\n\n\nInhoudsopgave\nLaat de inhoudsopgave vloeien\n\nSectie koppen met twee hashtags (##)\n\nGenummerde secties\n\nGeen makkelijke scrolling"
  },
  {
    "objectID": "posts/Communiceren_met_rmarkdown/index.html#themas",
    "href": "posts/Communiceren_met_rmarkdown/index.html#themas",
    "title": "Communiceren met RMarkdown",
    "section": "Thema’s",
    "text": "Thema’s\nRMarkdown heeft verschillende opties voor de HTML documenten. Enkele mogelijkheden waaruit kan worden gekozen, hier met de Engelse termen:\n\ntheme\n\nhighlight\n\nsmart\n\nDe HTML output thema’s komen van Bootswatch library. Valide HTML themes omvatten de volgende:\n\ncerulean, cosmo,flatly, journal, readable,spacelab en united.\n\nBijvoorbeeld, het thema van de pagina is readable.\n\nZet het op nul voor geen thema (in dit geval kun je de css parameter gebruiken om jouw eigen stijl te gebruiken).\n\nHighlight specificeert de wijze waarop de syntax stijl oplicht. Stijlen die mogelijk zijn omvatten de volgende:\n\ndefault, espresso, haddock, kate, monochrome, pygments, tango, textmate en zenburn.\n\nOok hier, plaats nul om syntax oplichting te voorkomen.\n\nSmart indiceert of de typografisch correcte output wordt weergegeven, zet rechte aanhalingstekens om in gekru, — rechte aanhalingstekens, – om in gekrulde aanhalingstekens en … in ellipsen. Smart is standaard ingesteld.\nBijvoorbeeld:\n---\noutput:\n  html_document:\n    theme: slate\n    highlight: tango\n---\nAls je wilt kun je ook jouw eigen stijl-thema produceren en gebruiken. Als je dat zou doen, zou de output sectie van jouw YAML kop er z’on beetje zo uitzien:\noutput:\n  html_document:\n    css: styles.css\nAls je nog wat verder wilt gaan en jouw eigen thema wilt schrijven in aanvulling op het oplichten, zou de YAML kop er beetje zo uitzien:\n---\noutput:\n  html_document:\n    theme: null\n    highlight: null\n    css: styles.css\n---\nHier is een link naar Pr?sance en Stijl in de HTML output."
  },
  {
    "objectID": "posts/Communiceren_met_rmarkdown/index.html#plaatsen",
    "href": "posts/Communiceren_met_rmarkdown/index.html#plaatsen",
    "title": "Communiceren met RMarkdown",
    "section": "Plaatsen",
    "text": "Plaatsen\nDe bibliografie wordt automatisch aan het einde van het document geplaatst. Daarom moet je jouw .Rmd document met # Referenties eindigen zodat de bibliografie naar de kop voor bibliografie komt."
  },
  {
    "objectID": "posts/Communiceren_met_rmarkdown/index.html#stylen",
    "href": "posts/Communiceren_met_rmarkdown/index.html#stylen",
    "title": "Communiceren met RMarkdown",
    "section": "Stylen",
    "text": "Stylen\nCitation Sylte Language (CSL) is een op XML-gebaseerde taal die het format van citaten en bibliografie?n vaststelt. Referentie management programma’s zoals Zotero, Mendeley en Papers gebruiken allemaal CSL.\nZoek jouw favoriete tijdschrift en CSL in de Zotero Style Repository, waar nu meer dan 8,152 CSLs inzitten. Is er een stijl waar je naar zoekt en die er niet in zit?\noutput: html_document\nbibliography: bibliography.bib\ncsl: nature.csl\nIn de github repo voor deze workshop heb ik de nature.csl en the-isme-journal.csl toegevoegd om mee te spelen. Download anders een stijl van de Zotero Style Repository!"
  },
  {
    "objectID": "posts/Communiceren_met_rmarkdown/index.html#citaten",
    "href": "posts/Communiceren_met_rmarkdown/index.html#citaten",
    "title": "Communiceren met RMarkdown",
    "section": "Citaten",
    "text": "Citaten\nCitaten gaan tussen vierkante haakjes [ ] en worden afgescheiden door punt-komma’s’ ;. Elk citaat moet een sleutel hebben, samen de @ + de citaat identificatie van de database vormen en die optioneel a prefix, a locator en a suffix hebben. Om te controleren wat de citaatsleutel is van een referentie, werp dan een blik op de .bib file. Hier in die file, kun je de sleutel voor elke referentie veranderen. Echter, wees er wel van bewust dat elke ID uniek is!\nHier zijn wat voorbeelden met bijpassende code in het Engels:\n\nMicrobes control Earth’s biogeochemical cycles [@Falkowski2008].\n\nCode: Microbes contorl Earth's biogeochemical cycles  [@Falkowski2008].\n\n\nI love making beautiful plots with ggplot2 [@R-ggplot2]\n\nCode: I love making beautiful plots with ggplot2 [@R-ggplot2]\n\n\nDr. Yuhui Xie’s book about Dynamic Documents [@Xie2015] inspired me to host this workshop.\n\nCode: Dr. Yuhui Xie's book about Dynamic Documents [@Xie2015] inspired me to host this workshop.\n\n\nA great article in Science regarding biogeography of microbes asks readers to imagine their Alice in Wonderland to shrink down to understand the microbial world [@Green2008].\n\nCode: A great article in *Science* regarding biogeography of microbes asks readers to imagine they are Alice in Wonderland to and shrink down to understand the microbial world [@Green2008].\n\n\nHet is cool dat de enige refenties die aan het document worden toegevoegd degene zijn die jijzelf citeert!"
  },
  {
    "objectID": "posts/Communiceren_met_rmarkdown/index.html#rpubs-vernieuwen",
    "href": "posts/Communiceren_met_rmarkdown/index.html#rpubs-vernieuwen",
    "title": "Communiceren met RMarkdown",
    "section": "RPubs vernieuwen",
    "text": "RPubs vernieuwen\nAls je veranderingen in het document wilt aanbrengen is het makkelijk om de webpagina te vernieuwen. Als je een keer jouw aangepaste documentg hebt gerenderd klik je op de  knop rechtsboven in de hoek van de preview scherm. Het aangepaste document zal dezelfde URL hebben als het orginele document."
  },
  {
    "objectID": "posts/Communiceren_met_rmarkdown/index.html#artikel",
    "href": "posts/Communiceren_met_rmarkdown/index.html#artikel",
    "title": "Communiceren met RMarkdown",
    "section": "Artikel",
    "text": "Artikel\nIedereen die wel eens een wetenschappelijk artikel maakt, weet dat hij of zij soms tegen ingewikkelde zaken aanloopt. De consistente opbouw bijvoorbeeld, het toevoegen van een tabel of een figuur, de referenties goed plaatsen. Als je in Word werkt is het soms lastig als je van volgorde verandert. Bij RMarkdown kun je het pakket rticlesbinnenhalen. Dan krijg je enkele templates waaruit je een keuze kunt maken. Hier JSS-voorbeeld het artikel voor Journal of Statistical Software."
  },
  {
    "objectID": "posts/Communiceren_met_rmarkdown/index.html#boek",
    "href": "posts/Communiceren_met_rmarkdown/index.html#boek",
    "title": "Communiceren met RMarkdown",
    "section": "Boek",
    "text": "Boek\nEen boek bestaat uit verschillende onderdelen, bijvoorbeeld uit een een inhoudsopgave, de hoofdstukken en de literatuur. Hier wordt een stramien voor de opbouw van een boek aangeboden dat eenvoudig is aan te passen. Van het rapprt heb ik een klein boekje gemaakt, dat vind je hier DenHaagboekhtml\nHet is dan ook makkelijk om van hetzelfde een eboek te maken. Dat eboek vind je hier EboekDenHaag"
  },
  {
    "objectID": "posts/Communiceren_met_rmarkdown/index.html#rapport",
    "href": "posts/Communiceren_met_rmarkdown/index.html#rapport",
    "title": "Communiceren met RMarkdown",
    "section": "Rapport",
    "text": "Rapport\nVan hetzelfde onderzoek heb ik ook een fraai vormgegeven rapport gemaakt in de Tufte-stijl. Dat staat hier DenHaagRapport"
  },
  {
    "objectID": "posts/Communiceren_met_rmarkdown/index.html#presentatie",
    "href": "posts/Communiceren_met_rmarkdown/index.html#presentatie",
    "title": "Communiceren met RMarkdown",
    "section": "Presentatie",
    "text": "Presentatie\nDe presentaties kunnen er ook net wat strakker uitzien. Hier geef ik twee voorbeelden Beamer en ook in een andere stijl kun je het zelf draaien ioslides"
  },
  {
    "objectID": "posts/Communiceren_met_rmarkdown/index.html#een-blog-en-website",
    "href": "posts/Communiceren_met_rmarkdown/index.html#een-blog-en-website",
    "title": "Communiceren met RMarkdown",
    "section": "Een blog en website",
    "text": "Een blog en website\nDit blog is met het pakket Radix gemaakt. Van Den Haag heb ik ook een blog gemaakt en dat vind je hier.Blog"
  },
  {
    "objectID": "posts/Communiceren_met_rmarkdown/index.html#tutorial",
    "href": "posts/Communiceren_met_rmarkdown/index.html#tutorial",
    "title": "Communiceren met RMarkdown",
    "section": "Tutorial",
    "text": "Tutorial\nOver RMarkdown heb ik eerder deze tutorial van Schmidt bewerkt, zie Dynamische documenten"
  },
  {
    "objectID": "posts/Communiceren_met_rmarkdown/index.html#een-dashboard",
    "href": "posts/Communiceren_met_rmarkdown/index.html#een-dashboard",
    "title": "Communiceren met RMarkdown",
    "section": "Een dashboard",
    "text": "Een dashboard\nEen dashboard is een fraaie manier om kort maar krachtig enkele resultaten te presenteren. Zie hier Dashboard"
  },
  {
    "objectID": "posts/Communiceren_met_rmarkdown/index.html#tot-slot",
    "href": "posts/Communiceren_met_rmarkdown/index.html#tot-slot",
    "title": "Communiceren met RMarkdown",
    "section": "Tot slot",
    "text": "Tot slot\nHartelijke dank Marian Schmidt voor het opzetten van de workshop RMarkdown waar ik heel veel van heb geleerd en in dit document gebruik van heb gemaakt. Hartelijk dank natuurlijk ook naar Yihui Xie die zoveel hieromtrent heeft ontwikkeld en dit in alle openbaarheid met anderen deelt."
  },
  {
    "objectID": "posts/2022-01-03-latente-groei-modeling/latente-groei-modeling.html",
    "href": "posts/2022-01-03-latente-groei-modeling/latente-groei-modeling.html",
    "title": "Latente Groei Modeling",
    "section": "",
    "text": "Longitudinale gegevens zijn zo interessant omdat ze ons in staat stellen te kijken naar verandering in de tijd, je krijgt er een beter inzicht mee in causale verbanden en je kunt gebeurtenissen en hun timing ermee verklaren. Om gebruik te maken van dit soort gegevens, moeten we verder gaan dan de klassieke statistische methoden, zoals OLS regressie en ANOVA. Dan moeten we gebruik maken van modellen die de extra complexiteit van de gegevens ook echt aan kunnen. Alexandru Cernat schreef ook hier een duidelijke blog over dat ik heb bewerkt en waarbij ik ook de alcoholdata van Singer en Willet heb gebruik.\nEen populair model voor de analyse van longitudinale gegevens is het Latente Groei Model (Latent Growth Model, LGM). Hiermee kan de verandering in de tijd worden geschat, terwijl rekening wordt gehouden met de hiërarchische aard van de gegevens (meerdere punten in de tijd die genest zijn binnen individuen). Het is vergelijkbaar met het multilevel model van verandering, maar hier wordt de schatting gedaan met behulp van het Structural Equation Modeling (SEM)-raamwerk. Dit raamwerk maakt gebruik van gegevens in het brede formaat (elke rij is een individu en de diverse metingen in de tijd verschijnen als verschillende kolommen).\nMeer in het bijzonder kan het LGM helpen\n- te begrijpen hoe verandering in de tijd verloopt;\n- verandering verklaren met behulp van tijdvariërende en tijdconstante voorspellers;\n- variantie ontleden in tussen- en binnenvariatie;\n- en het model kan makkelijk worden uitgebreid naar andere analysemodellen.\nHieronder volgt een korte inleiding op LGM, hoe de uitkomsten zijn te schatten en hoe de schattingen van verandering zijn te visualiseren.\nLaten we eerst de benodigde pakketten eens laden. We zullen tidyverse gebruiken voor het opschonen en visualiseren van de gegevens en lavaan voor het uitvoeren van de LGM in R.\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv ggplot2 3.3.6     v purrr   0.3.4\nv tibble  3.1.7     v dplyr   1.0.9\nv tidyr   1.2.0     v stringr 1.4.1\nv readr   2.1.2     v forcats 0.5.1\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'stringr' was built under R version 4.1.3\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(lavaan)\n\nWarning: package 'lavaan' was built under R version 4.1.3\n\n\nThis is lavaan 0.6-11\nlavaan is FREE software! Please report any bugs.\n\n\nLaten we, voordat we aan de LGM beginnen, eens kijken naar het soort gegevens dat we zouden willen analyseren. Hier gebruik ik alcoholdata van jongeren met de drie metingen van Singer en Willet voor die vrij toegankelijk zijn op internet.\nStel dat we geïnteresseerd zijn in hoe alcoholscore in de tijd verandert. Om het preciezer te formuleren willen laten zien hoe alcoholgebruik onder jongeren gemiddeld verandert, en tegelijk willen we een onderscheid maken tussen variatie, hoe jongeren veranderen ten opzichte van anderen. Maar tegelijk willen we ook iets zeggen over binnenvariatie en hoe jongeren veranderen ten opzichte van hun eigen gemiddelde/trend.\nLaten we eerst eens kijken hoe de gegevens eruit zien. Laten we eens kijken naar de brede gegevens, dit zijn de gegevens die gebruikt worden om LGM uit te voeren en laten we ook maar meteen het lange bestand bekijken:\n\nalcohol1 <- read.table(\"https://stats.idre.ucla.edu/stat/r/examples/alda/data/alcohol1_pp.txt\", header=T, sep=\",\")\nattach(alcohol1)\n\nWe beginnen met het lange formaat, waar elke rij een combinatie is van individu en tijd. Dit is het formaat dat we nodig hebben voor visualisatie met ggplot2, en voor andere modellen (zoals het multilevel model voor verandering).\n\nhead(alcohol1)\n\n  id age coa male age_14   alcuse      peer      cpeer  ccoa\n1  1  14   1    0      0 1.732051 1.2649111  0.2469111 0.549\n2  1  15   1    0      1 2.000000 1.2649111  0.2469111 0.549\n3  1  16   1    0      2 2.000000 1.2649111  0.2469111 0.549\n4  2  14   1    1      0 0.000000 0.8944272 -0.1235728 0.549\n5  2  15   1    1      1 0.000000 0.8944272 -0.1235728 0.549\n6  2  16   1    1      2 1.000000 0.8944272 -0.1235728 0.549\n\n\nOm een idee te krijgen van wat we gaan modelleren, maken we een eenvoudige grafiek met de gemiddelde verandering in de tijd en de trend voor elk individu.\n\nggplot(alcohol1, aes(age_14, alcuse, group = id)) + \n  geom_line(alpha = 0.1) + # add individual line with transparency\n  stat_summary( # add average line\n    aes(group = 1),\n    fun = mean,\n    geom = \"line\",\n    size = 1.5,\n    color = \"red\"\n  ) +\n  theme_bw() + # nice theme\n  labs(x = \"Leeftijd\", y = \"Alcoholgebruik\") # nice labels\n\n\n\n\nWe zien hier een gemiddelde verandering in de tijd. Tegelijk willen we ook zichtbaar maken wat variatie is in de manier waarop mensen veranderen. LGM is in staat beide tegelijk te schatten!"
  },
  {
    "objectID": "posts/2022-01-03-latente-groei-modeling/latente-groei-modeling.html#wat-is-latente-groei-modellering",
    "href": "posts/2022-01-03-latente-groei-modeling/latente-groei-modeling.html#wat-is-latente-groei-modellering",
    "title": "Latente Groei Modeling",
    "section": "Wat is Latente Groei Modellering?",
    "text": "Wat is Latente Groei Modellering?\nNu we een idee hebben van de gegevens en het soort onderzoeksvragen dat we zouden kunnen hebben, kunnen we overgaan tot de uitvoering van LGM. De formule voor het LGM is eigenlijk zeer gelijkaardig aan die voor het multilevel model van verandering:\n\\(Y_j=\\alpha_0 + \\alpha_1*\\gamma_j + \\zeta_{00} + \\zeta_{11}*\\gamma_j + \\epsilon_j\\)\nWaarbij:\n\n\\(Y_j\\) is de variabele van belang (alcuse, alchoholgebruik van jongeren) die verandert in tijd, j.\n\n\\(\\alpha_0\\) is de gemiddelde waarde bij het begin van de gegevensverzameling (het beginpunt van de rode lijn hierboven).\n\n\\(\\alpha_1*\\gamma_j\\) is de gemiddelde snelheid van verandering in de tijd (de helling van de rode lijn in de grafiek hierboven). Hier is \\(gamma_j\\) gewoon een maat voor de tijd.\n\n\\(\\zeta_{00}\\) is de tussenvariatie aan het begin van de gegevens. Het vat samen hoe verschillend de individuele startpunten zijn ten opzichte van het gemiddelde startpunt.\n\n\\(\\zeta_{11}\\gamma_j\\) is de tussen variatie in de snelheid van verandering. Samenvattend hoe verschillend de individuele veranderingsversnellingen zijn ten opzichte van de gemiddelde verandering (rode lijn hierboven).\n\nDe \\(\\epsilon_j\\) is de binnenvariatie of hoeveel individuen variëren rond hun voorspelde trend. Met de onderstaande grafiek kunnen we een beter idee krijgen van de verschillende variatiebronnen:\n\n\nalcohol1 %>% \n  filter(id %in% 1:2) %>% # selecteer twee individuen\n  ggplot(aes(age_14, alcuse, color = id)) +\n  geom_point() + # punten voor VeerkrachtTotaal\n  geom_smooth(method = lm, se = FALSE) + # liniaire lijn\n  theme_bw() + # mooi thema\n  labs(x = \"Leeftijd\", y = \"Alcoholgebruik\") # nice labels\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nDe interne variatie wordt weergegeven door de afstand tussen de lijn en de punten. Dit wordt voor elk individu afzonderlijk gedaan (door de kleur in de grafiek). De tussenvariatie verwijst naar hoe verschillend de lijnen zijn. Dit kan zowel het beginpunt als de helling zijn.\nOmdat deze techniek met brede databestanden werkt, zetten we de data over van lang naar wijd\n\nlibrary(tidyr)\nlibrary(dplyr)\n\nalcwide<-alcohol1 %>%\n  select(id, alcuse, age_14) %>%\n  pivot_wider(names_from = age_14, \n              values_from = alcuse) %>%\n  rename(\n    \"age_14\"=\"0\",\n    \"age_15\"=\"1\",         \n    \"age_16\"=\"2\"\n    )\n\n i =~ 1*Meting.0 +  1*Meting.1 +  1*Meting.2 \n  s =~ 0*Meting.0 +  1*Meting.1 +  2*Meting.2 \n  i ~~ s\nStructural Equation Modeling heeft zijn eigen manier om deze statistische relaties weer te geven. Hieronder is afgebeeld hoe we ons het hierboven beschreven model zouden moeten voorstellen:\n\n\n\nFig.1, LGM grafisch verbeeld\n\n\nIn de figuur worden de latente variabelen voorgesteld door cirkels (de twee \\(\\eta\\)-variabelen, intercept en slope), terwijl de waargenomen variabelen worden voorgesteld door vierkanten (de vier y-variabelen). Wij krijgen ook de residuen (kleine cirkels die \\(\\epsilon\\) voorstellen). Voor de latente variabelen hebben wij gemiddelden (\\(\\alpha\\)) en varianties (\\(\\zeta\\)). Deze zijn geschat en hebben de hierboven beschreven interpretatie. De pijlen tussen de latente en de geobserveerde variabelen (die gewoon regressiehellingen of ladingen zijn) liggen van tevoren vast. Voor de latente interceptvariabele (weergegeven door \\(\\eta_0\\)) zijn de ladingen vastgesteld op 1 (daarom is er in bovenstaande formule niets vermenigvuldigd met \\(\\alpha_0\\) en \\(\\eta_{00}\\)). De ladingen voor de hellende latente variabele (weergegeven door \\(\\eta_1\\)) worden vastgesteld naar gelang van de verandering in tijd (\\(\\gamma_j\\) in bovenstaande formule). In dit geval gaat het eenvoudig van 0 naar 2. Er is ook een correlatie tussen het beginpunt en de verandering in tijd, weergegeven door de dubbele pijl \\(\\zeta_{01}\\). Dit wordt niet vaak geïnterpreteerd, maar het geeft je in feite een idee of mensen convergeren (of meer op elkaar gaan lijken in de tijd) of divergeren (meer van elkaar gaan verschillen).\nNu het technische deel duidelijk is gemaakt, kunnen we wat modelleren en meer grafieken maken!\n\n# first LGM \nmodel <- 'i =~ 1*age_14 + 1*age_15 + 1*age_16\n          s =~ 0*age_14 + 1*age_15 + 2*age_16 '\n                  \n\nfit1 <- growth(model, data = alcwide)\n\nsummary(fit1, standardized = TRUE)\n\nlavaan 0.6-11 ended normally after 22 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         8\n                                                      \n  Number of observations                            82\n                                                      \nModel Test User Model:\n                                                      \n  Test statistic                                 0.636\n  Degrees of freedom                                 1\n  P-value (Chi-square)                           0.425\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  i =~                                                                  \n    age_14            1.000                               0.898    0.963\n    age_15            1.000                               0.898    0.862\n    age_16            1.000                               0.898    0.796\n  s =~                                                                  \n    age_14            0.000                               0.000    0.000\n    age_15            1.000                               0.484    0.464\n    age_16            2.000                               0.967    0.857\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  i ~~                                                                  \n    s                -0.187    0.102   -1.841    0.066   -0.431   -0.431\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .age_14            0.000                               0.000    0.000\n   .age_15            0.000                               0.000    0.000\n   .age_16            0.000                               0.000    0.000\n    i                 0.634    0.103    6.163    0.000    0.706    0.706\n    s                 0.277    0.062    4.481    0.000    0.573    0.573\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .age_14            0.064    0.147    0.436    0.663    0.064    0.073\n   .age_15            0.420    0.094    4.463    0.000    0.420    0.387\n   .age_16            0.280    0.180    1.556    0.120    0.280    0.220\n    i                 0.807    0.193    4.177    0.000    1.000    1.000\n    s                 0.234    0.083    2.803    0.005    1.000    1.000\n\n\nEr zijn in principe zes soorten coëfficiënten die hier interessant zijn:\n\nintercept i: de waarde 0.634 staat voor de gemiddelde verwachte alcoholgebruik aan het begin van het onderzoek voor alle respondenten wanneer ze 14 jaar zijn.\n\nslope s: de waarde 0.277 vertegenwoordigt de gemiddelde verandering voor alle respondenten. Dus bij elke meting stijgt het gebruik van alcohol met met 0.277.\n\nvariantie i: de waarde 0.807 vertegenwoordigt de tussenvariatie aan het begin van het onderzoek. Dus hoe verschillend zijn mensen vergeleken met het gemiddelde.\n\nvariantie s: de waarde 0.234 staat voor de tussenvariatie in de veranderingssnelheid. Het laat zien hoe verschillend veranderingshellingen zijn voor verschillende mensen.\n\nVariantie alcoholgebruik: de waarden tussen 0.064 en 0.420 geven de interne variatie op elk punt in de tijd weer.\n\ncorrelatie tussen i en s: de waarde -0.187 laat zien dat het alcohol niet in de tijd convergeert (althans niet significant).\n\n\nHoe kunnen we de verandering visualiseren?\nEen goede manier om te begrijpen wat je modelleert, is de voorspelde scores van het model visualiseren. We zullen het predict() commando gebruiken om een nieuw object op te slaan met de voorspelde scores op individueel niveau voor het intercept en de helling.\n\n# voorspellen van twee latente variabelen\npred_lgm <- predict(fit1) \n\nDit heeft de voorspelde score voor het intercept en de helling voor elk individu:\n\nhead(pred_lgm)\n\n              i          s\n[1,] 1.67339200 0.15061147\n[2,] 0.01577357 0.41987762\n[3,] 1.03717373 0.89157072\n[4,] 0.15472328 0.80763226\n[5,] 0.01182452 0.09588422\n[6,] 2.85697940 0.05794456\n\n\nDeze zijn gebaseerd op ons model. Wij zouden dus bijvoorbeeld het gemiddelde van deze variabelen kunnen schatten en dat gemiddelde voor intercept en slope zou achtereenvolgens dezelfde resultaten moeten geven als hierboven:\n\n# gemiddelde van het intercept (eerste kolom)\nmean(pred_lgm[, 1]) \n\n[1] 0.634417\n\n\n\n# gemiddelde slope (tweede kolom)\nmean(pred_lgm[, 2])\n\n[1] 0.2773233\n\n\nOm de resultaten te plotten, willen wij deze gegevens (intercept (\\(\\zeta_0\\)) en helling (\\(\\zeta_1\\))) omzetten in verwachte scores bij elke meting (\\(\\gamma_j\\)j). We kunnen deze transformatie doen op basis van het padmodel dat we hierboven hebben gezien:\n\\(Y_1=\\eta_0 + \\eta_1\\gamma_j\\)\nVoor de eerste meting (time=0) is de verwachte waarde dus alleen het intercept (\\(\\eta_0\\)) omdat \\(\\gamma_j\\) gelijk is aan 0. Voor de meting zou de verwachte waarde het intercept (\\(\\eta_0\\)) en de helling (\\(\\eta_1\\)) zijn. Voor meting drie zou het intercept + 2*helling zijn, enzovoort.\nIn R zouden we al deze metingen met de hand kunnen berekenen of we zouden het automatisch kunnen doen met behulp van functioneel programmeren. Op basis van de bovenstaande formule kunnen we een tegenhanger in R maken:\npred_lgm[, 1] + x*pred_lgm[, 2]\nVoor de eerste meting (alc_14=0) krijgen we deze scores\n\npred_lgm[, 1] + 0*pred_lgm[, 2]\n\n [1] 1.67339200 0.01577357 1.03717373 0.15472328 0.01182452 2.85697940\n [7] 1.70002148 0.01182452 0.09353383 0.95999586 0.02431252 0.89485737\n[13] 0.01972262 2.74652531 1.69122663 0.98981031 0.07985392 1.88598973\n[19] 0.01182452 3.16706021 1.01268775 2.68643747 0.96288677 0.07985392\n[25] 2.71888080 0.08543872 2.64724315 0.01182452 0.01577357 1.03091617\n[31] 1.77626546 1.62959225 0.08543872 0.08380297 0.95604681 0.96394491\n[37] 1.09292161 0.13755300 0.01182452 0.01182452 1.25884647 0.01182452\n[43] 0.01182452 0.11987977 2.13501694 0.01740932 0.07985392 0.01182452\n[49] 0.01182452 0.01182452 0.01182452 0.07985392 0.01182452 0.01577357\n[55] 0.01182452 0.11848083 0.01182452 0.99389867 0.02227273 0.08543872\n[61] 1.92422677 0.01182452 0.01182452 0.15671367 0.08543872 1.49845628\n[67] 0.88801741 0.07985392 0.01182452 1.83223971 0.01182452 0.01182452\n[73] 0.01182452 0.13848524 0.01182452 0.01182452 0.11920222 0.01182452\n[79] 1.04572058 0.96288677 0.01182452 0.11198167\n\n\nVoor de tweede meting (alc_14=1) ziet het er zo uit\n\npred_lgm[, 1] + 1*pred_lgm[, 2]\n\n [1] 1.8240035 0.4356512 1.9287445 0.9623555 0.1077087 2.9149240 1.5604805\n [8] 0.1077087 1.3870516 1.0257079 1.1447539 1.1224615 0.7635937 2.8155774\n[15] 2.0052428 1.2209099 0.2510256 1.5773629 0.1077087 2.4340560 1.3706937\n[22] 2.1045835 1.2657784 0.2510256 2.4588992 0.7148063 2.0159411 0.1077087\n[29] 0.4356512 1.4090953 2.3055104 1.4121079 0.7148063 0.5789681 0.6977654\n[36] 1.3536504 1.9968551 1.0118258 0.1077087 0.1077087 1.3953791 0.1077087\n[43] 0.1077087 1.2942168 2.4109430 0.5714894 0.2510256 0.1077087 0.1077087\n[50] 0.1077087 0.1077087 0.2510256 0.1077087 0.4356512 0.1077087 1.1780436\n[57] 0.1077087 1.5604210 0.9753629 0.7148063 1.9775396 0.1077087 0.1077087\n[64] 1.1276442 0.7148063 2.2197859 0.5544485 0.2510256 0.1077087 1.1445052\n[71] 0.1077087 0.1077087 0.1077087 1.0892425 0.1077087 0.1077087 1.2379508\n[78] 0.1077087 1.3386955 1.2657784 0.1077087 0.6383319\n\n\nBij de derde meting ziet het alcoholgebruik er zo uit.\n\npred_lgm[, 1] + 2*pred_lgm[, 2]\n\n [1] 1.9746149 0.8555288 2.8203152 1.7699878 0.2035930 2.9728685 1.4209395\n [8] 0.2035930 2.6805694 1.0914199 2.2651952 1.3500657 1.5074647 2.8846296\n[15] 2.3192590 1.4520095 0.4221974 1.2687361 0.2035930 1.7010517 1.7286996\n[22] 1.5227296 1.5686701 0.4221974 2.1989176 1.3441739 1.3846391 0.2035930\n[29] 0.8555288 1.7872745 2.8347553 1.1946236 1.3441739 1.0741332 0.4394841\n[36] 1.7433558 2.9007885 1.8860986 0.2035930 0.2035930 1.5319118 0.2035930\n[43] 0.2035930 2.4685539 2.6868691 1.1255695 0.4221974 0.2035930 0.2035930\n[50] 0.2035930 0.2035930 0.4221974 0.2035930 0.8555288 0.2035930 2.2376064\n[57] 0.2035930 2.1269432 1.9284531 1.3441739 2.0308524 0.2035930 0.2035930\n[64] 2.0985747 1.3441739 2.9411155 0.2208797 0.4221974 0.2035930 0.4567708\n[71] 0.2035930 0.2035930 0.2035930 2.0399998 0.2035930 0.2035930 2.3566993\n[78] 0.2035930 1.6316705 1.5686701 0.2035930 1.1646821\n\n\nwaarbij x onze codering van tijd voorstelt (of \\(\\gamma_j\\)). We kunnen deze functie meerdere keren toepassen met het map() commando. De onderstaande syntaxis past deze formule toe voor de getallen 0, 1, 2 (onze codering van meting (variabele alc_14).\n\nmap(0:2, # loop over, in ons geval 1,2,3\n    function(x) pred_lgm[, 1] + x * pred_lgm[, 2]) # formule die gebruikt wordt\n\n[[1]]\n [1] 1.67339200 0.01577357 1.03717373 0.15472328 0.01182452 2.85697940\n [7] 1.70002148 0.01182452 0.09353383 0.95999586 0.02431252 0.89485737\n[13] 0.01972262 2.74652531 1.69122663 0.98981031 0.07985392 1.88598973\n[19] 0.01182452 3.16706021 1.01268775 2.68643747 0.96288677 0.07985392\n[25] 2.71888080 0.08543872 2.64724315 0.01182452 0.01577357 1.03091617\n[31] 1.77626546 1.62959225 0.08543872 0.08380297 0.95604681 0.96394491\n[37] 1.09292161 0.13755300 0.01182452 0.01182452 1.25884647 0.01182452\n[43] 0.01182452 0.11987977 2.13501694 0.01740932 0.07985392 0.01182452\n[49] 0.01182452 0.01182452 0.01182452 0.07985392 0.01182452 0.01577357\n[55] 0.01182452 0.11848083 0.01182452 0.99389867 0.02227273 0.08543872\n[61] 1.92422677 0.01182452 0.01182452 0.15671367 0.08543872 1.49845628\n[67] 0.88801741 0.07985392 0.01182452 1.83223971 0.01182452 0.01182452\n[73] 0.01182452 0.13848524 0.01182452 0.01182452 0.11920222 0.01182452\n[79] 1.04572058 0.96288677 0.01182452 0.11198167\n\n[[2]]\n [1] 1.8240035 0.4356512 1.9287445 0.9623555 0.1077087 2.9149240 1.5604805\n [8] 0.1077087 1.3870516 1.0257079 1.1447539 1.1224615 0.7635937 2.8155774\n[15] 2.0052428 1.2209099 0.2510256 1.5773629 0.1077087 2.4340560 1.3706937\n[22] 2.1045835 1.2657784 0.2510256 2.4588992 0.7148063 2.0159411 0.1077087\n[29] 0.4356512 1.4090953 2.3055104 1.4121079 0.7148063 0.5789681 0.6977654\n[36] 1.3536504 1.9968551 1.0118258 0.1077087 0.1077087 1.3953791 0.1077087\n[43] 0.1077087 1.2942168 2.4109430 0.5714894 0.2510256 0.1077087 0.1077087\n[50] 0.1077087 0.1077087 0.2510256 0.1077087 0.4356512 0.1077087 1.1780436\n[57] 0.1077087 1.5604210 0.9753629 0.7148063 1.9775396 0.1077087 0.1077087\n[64] 1.1276442 0.7148063 2.2197859 0.5544485 0.2510256 0.1077087 1.1445052\n[71] 0.1077087 0.1077087 0.1077087 1.0892425 0.1077087 0.1077087 1.2379508\n[78] 0.1077087 1.3386955 1.2657784 0.1077087 0.6383319\n\n[[3]]\n [1] 1.9746149 0.8555288 2.8203152 1.7699878 0.2035930 2.9728685 1.4209395\n [8] 0.2035930 2.6805694 1.0914199 2.2651952 1.3500657 1.5074647 2.8846296\n[15] 2.3192590 1.4520095 0.4221974 1.2687361 0.2035930 1.7010517 1.7286996\n[22] 1.5227296 1.5686701 0.4221974 2.1989176 1.3441739 1.3846391 0.2035930\n[29] 0.8555288 1.7872745 2.8347553 1.1946236 1.3441739 1.0741332 0.4394841\n[36] 1.7433558 2.9007885 1.8860986 0.2035930 0.2035930 1.5319118 0.2035930\n[43] 0.2035930 2.4685539 2.6868691 1.1255695 0.4221974 0.2035930 0.2035930\n[50] 0.2035930 0.2035930 0.4221974 0.2035930 0.8555288 0.2035930 2.2376064\n[57] 0.2035930 2.1269432 1.9284531 1.3441739 2.0308524 0.2035930 0.2035930\n[64] 2.0985747 1.3441739 2.9411155 0.2208797 0.4221974 0.2035930 0.4567708\n[71] 0.2035930 0.2035930 0.2035930 2.0399998 0.2035930 0.2035930 2.3566993\n[78] 0.2035930 1.6316705 1.5686701 0.2035930 1.1646821\n\n\n\n\nConclusies\nHopelijk geeft dit je een idee over wat LGM is, hoe je het kan schatten in R en hoe je deze verandering kan visualiseren.\n[Met dank aan Alexandru Cernat}(https://www.alexcernat.com/estimating-and-visualizing-change-in-time-using-latent-growth-models-with-r/)"
  },
  {
    "objectID": "posts/2022-01-03-multilevel-modeling/multilevel-modeling.html",
    "href": "posts/2022-01-03-multilevel-modeling/multilevel-modeling.html",
    "title": "Multilevel modeling",
    "section": "",
    "text": "Longitudinale gegevens zijn heel boeiend omdat je ermee kunt kijken naar verandering in de tijd, een beter begrip krijgt van causale verbanden en gebeurtenissen en hun timing ermee kunt verklaren. Om dit te kunnen doen, moeten we verder gaan dan de klassieke statistische methoden, zoals OLS regressie en ANOVA, en modellen gebruiken die beter kunnen omgaan met complexiteit van de gegevens. Alexander Cernat schreef er een blog over hier die ik hier in het Nederlands overzet en waarbij ik alcoholdata van Willet en Singer gebruik/\nEen populair model voor de analyse van longitudinale gegevens is het Multilevel Model voor Verandering; MultiLevel Model for Change (MLMC). Dit model maakt de schatting van verandering in de tijd mogelijk, terwijl rekening wordt gehouden met de hiërarchische aard van de gegevens (meerdere punten in de tijd genest binnen individuen). Het is vergelijkbaar met het Latente Groei Model; Latent Growth Model zie deze post, maar hier wordt geschat met behulp van het multilevel model raamwerk (ook bekend als hiërarchische modellering of random effecten). Deze techniek maakt gebruik van het lange dataformaat (elke rij is is een rij gegevens op een specifiek tijdstip voor een individu).\nMeer in het bijzonder kan het MLMC helpen:\n- te begrijpen hoe de individuele en geaggregeerde verandering in de tijd verlopen;\n- verandering te verklaren met behulp van tijdsvariërende (bv. tijd) en tijdsconstante (b.v. geslacht) voorspellers;\n- variantie te ontleden in tussen- en binnen- variatie;\n- gemakkelijk om te gaan met continue tijd, onevenwichtige gegevens (niet alle individuen zijn op alle tijdstippen aanwezig) en verschillende timings (niet iedereen geeft gegevens op precies hetzelfde moment).\nHier volgt een korte inleiding op MLMC, hoe hiermee te werken in R en hoe veranderingen zijn te visualiseren.\nLaten we eerst de R-pakketten laden (deze moeten dus wel geïnstalleerd zijn). We zullen tidyverse gebruiken voor het opschonen en visualiseren van de data, lme4 voor het uitvoeren van de MLMC in R en sjstats voor het schatten van intra class correlation (icc)\nJe kunt pakketten installeren met het install.packages() commando.\nLaten we vervolgens de pakketten binnenhalen die wij bij deze analyse zullen gebruiken.\n\n# pakket voor dataopschonen en visualiseren\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv ggplot2 3.3.6     v purrr   0.3.4\nv tibble  3.1.7     v dplyr   1.0.9\nv tidyr   1.2.0     v stringr 1.4.1\nv readr   2.1.2     v forcats 0.5.1\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'stringr' was built under R version 4.1.3\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\n# pakket voor multilevel-modeleren\nlibrary(lme4)\n\nWarning: package 'lme4' was built under R version 4.1.3\n\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n# pakket om intra class correlation makkelijk vast te stellen\nlibrary(sjstats)\n\nLaten we, voordat we aan de MLMC beginnen, eerst kijken naar de data die we willen analyseren. Hier gebruik ik alcuse (alcoholgebruik) met drie metingen. Dit is een longitudinale dataset van 82 jongeren.\nWe willen weten hoe alcoholgebruik in de tijd verandert en we willen die verandering begrijpen. Hierbij maken we een onderscheid tussen tussenvariatie (hoe de jongeren ten opzichte van elkaar veranderen) en binnenvariatie (hoe jongeren veranderen ten opzichte van hun eigen gemiddelde/trend).\nLaten we deze gegevens eens onderzoeken. We kijken hiervoor naar de gegevens in lang formaat, die we zullen gebruiken voor de modellering en de grafieken. Hieronder zie je de eerste tien gegevens:\n\nalcohol1 <- read.table(\"https://stats.idre.ucla.edu/stat/r/examples/alda/data/alcohol1_pp.txt\", header=T, sep=\",\")\nattach(alcohol1)\nhead(alcohol1, n=10)\n\n   id age coa male age_14   alcuse      peer      cpeer  ccoa\n1   1  14   1    0      0 1.732051 1.2649111  0.2469111 0.549\n2   1  15   1    0      1 2.000000 1.2649111  0.2469111 0.549\n3   1  16   1    0      2 2.000000 1.2649111  0.2469111 0.549\n4   2  14   1    1      0 0.000000 0.8944272 -0.1235728 0.549\n5   2  15   1    1      1 0.000000 0.8944272 -0.1235728 0.549\n6   2  16   1    1      2 1.000000 0.8944272 -0.1235728 0.549\n7   3  14   1    1      0 1.000000 0.8944272 -0.1235728 0.549\n8   3  15   1    1      1 2.000000 0.8944272 -0.1235728 0.549\n9   3  16   1    1      2 3.316625 0.8944272 -0.1235728 0.549\n10  4  14   1    1      0 0.000000 1.7888544  0.7708544 0.549\n\n\nWe zien dat elke rij een combinatie is van een jongere (variabele id (van 1 tot en met 4, die we in de analyse gebruiken)) en tijd (variabele age_14, alcoholgebruik op 14-jarige leeftijd). Dit is ook het formaat dat we nodig hebben voor een visualisatie met ggplot2.\nOm te zien wat we gaan modelleren, kunnen we een eenvoudige grafiek maken met een gemiddelde veranderingslijn in tijd voor de hele dataset en een wirwar van lijnen voor de verandering van elk individu:\n\nggplot(alcohol1, aes(age, alcuse, group = id)) +\n  geom_line(alpha = 0.1) + # voeg individuele lijn met transparantie toe\n  stat_summary( # voeg gemiddelde lijn toe\n    aes(group = 1),\n    fun = mean,\n    geom = \"line\",\n    size = 1.0,\n    color = \"red\"\n  ) +\n  theme_bw() + # goed theme voor visualisatie\n  labs(x = \"Leeftijd\", y = \"Alcoholgebrui\") # de labels\n\n\n\n\nWe zien dus een hele lichte positieve en constante verandering in de tijd, maar vooral ook heel wat variatie in de manier waarop jongeren alcohol gebruiken (grijze lijnen). MLMC is in staat om beide dingen (het structurele en individuele) tegelijk te schatten!"
  },
  {
    "objectID": "posts/2022-01-03-multilevel-modeling/multilevel-modeling.html#wat-is-multilevel-modellering",
    "href": "posts/2022-01-03-multilevel-modeling/multilevel-modeling.html#wat-is-multilevel-modellering",
    "title": "Multilevel modeling",
    "section": "Wat is multilevel modellering?",
    "text": "Wat is multilevel modellering?\nMultilevel modellering is een uitbreiding van regressie modellering (‘just regression’) waarin we als het ware verschillende bronnen van variatie uit elkaar trekken. Waarom is dit belangrijk?\nTraditioneel gaat OLS-regressie er vanuit dat alle gevallen onafhankelijk zijn. Dit impliceert dat er geen correlatie is tussen de geobserveerde metingen als gevolg van zaken als clustering. Dit is vaak niet waar bij sociaal-wetenschappelijke gegevens. Individuen zijn bijvoorbeeld genest in huishoudens, klassen, buurten, regio’s en landen. Studenten zijn genest in klassen, scholen, regio’s en landen. Deze geneste structuur zorgt ervoor dat individuen op elkaar lijken. Zo zullen de gezondheidsuitkomsten waarschijnlijk vergelijkbaar zijn voor mensen die in dezelfde buurt wonen, vanwege bijvoorbeeld zelfselectie (vergelijkbaar inkomen en opleiding), kwaliteit van de gezondheidszorg of luchtkwaliteit. Als dit waar is, dan kunnen we mensen die uit dezelfde buurt komen niet als onafhankelijk behandelen.\nMultilevel modellering lost dit probleem op door willekeurige effecten (d.w.z. variatie) te schatten voor de verschillende niveaus die in de gegevens aanwezig zijn. Op die manier worden de regressiecoëfficiënten (zoals standaardfouten) gecorrigeerd voor deze geneste structuur. Bovendien kan met dit soort modellen worden geschat hoeveel variatie van elk niveau afkomstig is. Dit kan zeer informatief zijn. Door bijvoorbeeld de variatie van de leerlingresultaten uit elkaar te halen op leerlingniveau, klasniveau en schoolniveau, kunnen we begrijpen wat de belangrijkste factoren zijn die de uitkomsten van de leerlingen beïnvloeden. Dit kan informatie opleveren voor theorie en beleid.\n\nMultilevel modellering en longitudinale data\nLongitudinale data zijn ook genest. Metingen op verschillende tijdstippen zijn genest binnen een individu. Deze waarnemingen zijn niet onafhankelijk, omdat stabiele individuele kenmerken (zoals genen, persoonlijkheid of context) leiden tot consistente uitkomsten binnen de individuen en over de tijd. Multilevel modellering kan dus helpen corrigeren voor dit geklusterd karakter, maar helpt ons ook om twee belangrijke bronnen van variatie in longitudinale gegevens uit elkaar te halen: tussen-variatie (tussen individuen) en binnen-variatie (binnen het individu).\nTussen-variatie heeft betrekking op de manier waarop individuen van elkaar verschillen wat de belangrijke uitkomst betreft (bv. in dit geval dat het ene individu gemiddeld een hoger alcuse score heeft dan het andere individu). Terwijl binnen-variatie, de tweede vorm van variatie, betrekking heeft op de manier waarop alcuse op een bepaalde meting verschilt van het individuele gemiddelde (bv. hebben zij een lager of hoger score op alcoholgebruik in vergelijking met hun normale inkomen).\nLaten we, om deze vormen van variatie beter te begrijpen, eens naar de onderstaande grafieken kijken (een voor individu met idn=1 en de ander met idn=10).\n\nggplot(alcohol1, aes(age_14, alcuse)) +\n  stat_summary( # voeg gemiddelde lijn toe\n    aes(id = 1),\n    fun = mean,\n    geom = \"line\",\n    size = 1.0,\n    color = \"red\"\n  ) +\n  theme_bw() + # nice theme\n  labs(x = \"Leeftijd\", y = \"Alcoholgebruik_14\") # nice labels\n\nWarning: Ignoring unknown aesthetics: id\n\n\n\n\n\n\nggplot(alcohol1, aes(age_14, alcuse)) +\n  aes(id=10) + \n  stat_summary( # voeg gemiddelde lijn toe, Harrie: deze klopt nog niet\n    fun = mean,\n    geom = \"line\",\n    size = 1.0,\n    color = \"blue\"\n  ) +\n  theme_bw() + # nice theme\n  labs(x = \"Leeftijd\", y = \"Alcoholgebruik_14\") # nice labels\n\n\n\n\nHier zien we de twee alcuse-scores van twee individuen en de individuele trends over de tijd (drie metingen). We kunnen de variatie tussen de lijnen zien als het verschil tussen de lijnen. Dit vertelt ons of de trend voor het ene individu anders is dan voor het andere. Als deze bron van variatie 0 zou zijn, dan zouden alle lijnen gelijk zijn. Als individuen zeer verschillende lijnen hebben, dan zal dit een belangrijke bron van variatie zijn. Met interne variatie wordt het verschil bedoeld tussen de individuele trend en de bijbehorende waargenomen scores. Als deze bron van variatie 0 zou zijn, zouden we alcuse bij elke meting perfect voorspellen en zou er geen interne variatie zijn. variatie zijn. Hoe groter deze bron van variatie, hoe meer de individuele waarden buiten hun eigen trend schommelen.\n\n\nHet onvoorwaardelijke verandermodel (ook wel random effects genoemd)\nNu we wat basiskennis hebben van multilevel modellering en longitudinale gegevens kunnen we het uitproberen. Wij gaan gewoonlijk uit van een eenvoudig model dat enkel de binnen- en tussenvariatie wil scheiden. We kunnen het model definiëren als:\n\\(Y_ij=\\gamma_{00}+u_{01}+e_{ij}\\)\nWaarbij:\n\n\\(Y_{ij}\\) is de variabele van belang (bijvoorbeeld alcuse) en varieert per individu (i) en tijd (j)\n\n\\(\\gamma_{00}\\) is het intercept in de regressie. We kunnen dit interpreteren als het grote gemiddelde of het gemiddelde van de uitkomst over alle individuen en tijdstippen.\n\n\\(u_{0i}\\) is de tussenvariatie en vertelt ons hoe verschillend jongeren van elkaar zijn in hun alcoholgebruik-score. Als iedereen hetzelfde scoort op alcuse, zou dit 0 zijn. Hoe groter de verschillen, hoe groter deze coëfficiënt zal zijn en dus de variatie is.\n\n\\(e_{ij}\\) dit is het residu, maar heeft ook hier de interpretatie van binnenvariatie en vertelt ons hoeveel elk individu varieert rond zijn eigen gemiddelde. Hoe groter deze coëfficiënt, hoe meer individuen in hun uitkomsten op alcoholgebruik schommelen.\n\nNu we weten wat we willen modelleren, laten we eens kijken hoe we dat kunnen doen in R en met het pakket lme4. Voor het schatten van multilevel modellen zullen we het lmer()commando gebruiken. We moeten de data en de formule opgeven. Onze uitkomst is alcuse, dus dat staat aan de linkerkant van ~. Aan de rechterkant hebben we “1”, dat staat voor het intercept. Tussen haakjes definiëren we de willekeurige effecten. Hier zeggen we dat we het intercept (“1”) willen laten variëren per individu (| id). Dit alles leidt tot deze syntaxis:\n\n# random intercept model \nm0 <- lmer(data = alcohol1, alcuse ~ 1 + (1 | id))\n\n# resultaten checken\nsummary(m0)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: alcuse ~ 1 + (1 | id)\n   Data: alcohol1\n\nREML criterion at convergence: 673\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.8892 -0.3079 -0.3029  0.6111  2.8562 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n id       (Intercept) 0.5731   0.7571  \n Residual             0.5617   0.7495  \nNumber of obs: 246, groups:  id, 82\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   0.9220     0.0963   9.574\n\n\nLaten we de belangrijkste coëfficienten eens interpreteren:\n\nonder “Fixed effects” hebben we het “(Intercept)”, dat het grote gemiddelde is (\\(\\gamma_{00}\\) en ons vertelt dat over alle tijdstippen en individuen de gemiddelde score 0.922 is.\n\n“Random effects” vertegenwoordigt alles dat varieert met id, de tussen-variatie. In dit geval is de tussenvariatie voor het intercept (\\(\\beta_{0i}\\)) 0.573\n\nonder “Random effects” vertegenwoordigt de “Residual”-coëfficiënt de binnen-variatie. In dit geval is de binnen variatie voor het intercept \\(e_{ij}\\) 0.562\n\nOm de tussen en binnen-variantie beter te begrijpen, berekenen we InterClass Coefficient (ICC), dat is de tussen-variatie gedeeld door de totale variatie (tussen en binnen-variatie opgeteld). Dit berekenen we met het sjstats-pakket waarmee we het percentage tussenvariatie berekenen van het m0-model (51%), het percentage variatie dat door de groep kan worden verklaard.\n\nicc(m0)\n\nWarning: 'icc' is deprecated.\nUse 'performance::icc()' instead.\nSee help(\"Deprecated\")\n\n\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.505\n  Unadjusted ICC: 0.505\n\n\nDit is dus gewoon een verhouding van de tussen-variatie op de totale variatie en vertelt ons welk deel van de variatie tussen individuen is. In ons geval blijkt dat ongeveer 51% van de variatie in alcuse tussen jongeren komt, terwijl de resterende (~ 49%) binnen jongeren ligt. In wezen zou dit erop wijzen dat de verschillen op alcoholgebruik tussen jongeren net iets belangrijker zijn dan de verschillen op alcoholgebruik binnen jongeren.\nOm beter te begrijpen wat het model doet, kunnen we de scores voorspellen en een grafiek maken met de voorspelde individuele score (lijnen) en de waargenomen scores (punten) voor vijf individuen:\n\n# basis voor voorspelling\nalcohol1$pred_m0 <- predict(m0)\n\n\nalcohol1 %>% \n  filter(id %in% 1:5) %>% # haal er vijf jongeren uit, kunnen ook andere jongeren zijn\n  ggplot(aes(age, pred_m0, color = id)) +\n  geom_point(aes(age, alcuse)) + # punten voor de geobserveerde alcgebruik-Totaal-scores\n  geom_smooth(method = lm, se = FALSE) + # liniaire lijn voor voorspellen\n  theme_bw() + # mooi thema\n  labs(x = \"Leeftijd\", y = \"Alcoholgebruik\") + # Labels toevoegen\n  theme(legend.position = \"none\") # geen legenda\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\n\nHet model van onvoorwaardelijke verandering\nHet vorige model is nuttig om ons een idee te geven van hoeveel variatie we op elk niveau hebben, maar we willen ook kijken naar verandering in de tijd! Laten we het model dus uitbreiden:\n\\(Y_{ij}=\\gamma_{00}+ \\gamma_{10}*Meting_{ij}+ u_{0i}+e_{ij}\\)\nIn dit model voegen we tijd (metingen van veertienjarige leeftijd gemeten) toe als voorspeller. Nu stelt \\(\\gamma_{00}\\) de gemiddelde score op alcoholgebruik voor wanneer de tijd 0 (alcoholgebruik op veertien jarige leeftijd) is, terwijl \\(\\gamma_{10}\\) het veranderingspercentage van alcoholgebruik voorstelt wanneer de tijd met 1 toeneemt.\nOm de zaken gemakkelijker te interpreteren, is het belangrijk te beginnen bij 0 zodat de \\(\\gamma_{00}\\) de mooie interpretatie heeft van verwacht alcoholgebruik aan het begin van de studie. De tijdsvariabele ‘age_14’ (alcoholgebruik op 14-jarige leeftijd) kent al de waarden 0, 1 en 2\nNu kunnen we ons model uitvoeren. We voegen gewoon de nieuwe tijdsvariabele toe als een fixed effect:\n\n# unconditionele veranderings model nu (a.k.a. MLMC)\nm1 <- lmer(data = alcohol1, alcuse ~ 1 + age_14 + (1 | id))\nsummary(m1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: alcuse ~ 1 + age_14 + (1 | id)\n   Data: alcohol1\n\nREML criterion at convergence: 654.1\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.19816 -0.66940  0.03001  0.44728  2.66167 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n id       (Intercept) 0.5966   0.7724  \n Residual             0.4915   0.7011  \nNumber of obs: 246, groups:  id, 82\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  0.65130    0.11077   5.880\nage_14       0.27065    0.05474   4.944\n\nCorrelation of Fixed Effects:\n       (Intr)\nage_14 -0.494\n\n\nEen belangrijk verschil met het vorig model zit in het fixed effects-deel. Nu interpreteren wij het intercept (0.651) als het verwachte alcoholgebruik-score aan het begin van de studie (wanneer ze veertien jaar zijn, age_14=0). Het effect van een jaar extra, 0.271, vertelt ons de gemiddelde veranderingssnelheid bij ieder van de twee metingen. De score op alcoholgebruik neemt dus elk jaar met 0.271 toe.\nLaten we de resultaten opnieuw visualiseren op basis van het nieuwe model:\n\n# De basis voor voorspelling van dit model\nalcohol1$pred_m1 <- predict(m1)\n\n\nalcohol1 %>% \n  filter(id %in% 1:5) %>% # selecteer weer 5 individuen\n  ggplot(aes(age_14, pred_m1, color = id)) +\n  geom_point(aes(age_14, alcuse)) + # observatiepunten voor VeerkrachtTotaal\n  geom_smooth(method = lm, se = FALSE) + # lineaire lijn gebaseerd op voorspelling\n  theme_bw() + # mooi thema\n  labs(x = \"Leeftijd\", y = \"Alcoholgebruik\") + # goede labels\n  theme(legend.position = \"none\") # legenda verstoppen\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nNu zien we dat we twee vormen van tussenvariatie hebben. De coëfficiënt \\(u_{0i}\\) vertegenwoordigt de tussenvariatie aan het begin van het onderzoek terwijl \\(u_{1i}\\) de tussenvariatie in het tempo van de verandering vertegenwoordigt. Dit betekent dat we toestaan dat individuen verschillende alcoholgebruik-scores aan het begin hebben (age_14=0), maar ook verschillende trends laten zien.\nDit model willen we met lme4 uitvoeren. We kunnen eenvoudigweg “meting” of in dit geval “jaartal” toevoegen aan het willekeurige deel van het model:\nWe zien nu dus een positieve trend die te wijten is aan de tijdscoëfficiënt. De individuele lijnen lopen, algemeen gezegd, hier parallel aan. We nemen nu aan dat de verandering in de tijd voor alle individuen gelijk is. In meer technische termen nemen we aan dat er geen tussenvariatie is in de snelheid van verandering. Dat is een vrij sterke veronderstelling. In ons geval zouden we dat niet verwachten gezien de eerste grafiek die we hebben gemaakt (en die individuele lijnen die verschillende kanten opgingen). Laten we het model dus uitbreiden met de tussenvariatie in de veranderingssnelheid:\n\\(Y_{ij}=\\gamma_{00}+\\gamma_{10}*Meting_{ij}+u_{0i}+u_{1i}*Meting_{ij}+e{ij}\\)\nNu zien we dat we twee bronnen van tussenvariatie hebben. De coëfficiënt \\(u_0i\\) vertegenwoordigt de tussenvariatie aan het begin van de studie terwijl \\(u_1i\\) de tussenvariatie in het tempo van verandering vertegenwoordigt. Dit betekent dat we toestaan dat individuen verschillen in alcoholgebruik aan het begin, maar ook verschillende trends kunnen kunnen laten zien.\nOm een dergelijk model in lme4 uit te voeren. We kunnen eenvoudigweg “tijd” (age_14) toevoegen aan het willekeurige deel van het model:\n\n# unconditional change model (a.k.a. MLMC) with re for change\nm2 <- lmer(data = alcohol1, alcuse ~ 1 + age_14 + (1 + age_14 | id))\nsummary(m2)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: alcuse ~ 1 + age_14 + (1 + age_14 | id)\n   Data: alcohol1\n\nREML criterion at convergence: 643.2\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.48287 -0.37933 -0.07858  0.38876  2.49284 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n id       (Intercept) 0.6355   0.7972        \n          age_14      0.1552   0.3939   -0.23\n Residual             0.3373   0.5808        \nNumber of obs: 246, groups:  id, 82\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  0.65130    0.10573   6.160\nage_14       0.27065    0.06284   4.307\n\nCorrelation of Fixed Effects:\n       (Intr)\nage_14 -0.441\n\n\nIn de resultaten zien we dat het random deel van het model nu twee coëfficiënten heeft die variëren naar id. Het “(Intercept)”, 0.65130, staat voor de variatie tussen het beginpunt van het onderzoek (\\(\\gamma_{0i})\\), terwijl de coëfficiënt voor age_14, 0.271, staat voor de variatie tussen de veranderingssnelheden ( \\(\\gamma_{1i}\\) ).\nAls wij nu de voorspellingen onderzoeken, zien wij dat individuen zowel verschillende beginpunten als verschillende trends mogen hebben:\n\n# laten we basis voor voorspelling definieren\nalcohol1$pred_m2 <- predict(m2)\n\n\nalcohol1 %>% \n  filter(id %in% 1:5) %>% # selecteer enkele individuen\n  ggplot(aes(age_14, pred_m2, color = id)) +\n  geom_point(aes(age_14, alcuse)) + # punten van geobserveerde VeerkrachtTotaal\n  geom_smooth(method = lm, se = FALSE) + # liniaire lijn voor voorspelling\n  theme_bw() + # mooi thema\n  labs(x = \"Leeftijd\", y = \"Alcoholgebruik\") + # goede labels\n  theme(legend.position = \"none\") # legenda verbergen\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nHoe groter de \\(e_{0i}\\)-coëfficiënt, hoe groter het verschil tussen de mensen aan het begin van het onderzoek, terwijl een grotere \\(e_{1i}\\) op meer uiteenlopende veranderingssnelheden wijst.\n\n\nConclusies\nDit geeft een idee wat multilevel model voor verandering ons in onderzoek kan bieden, hoe je het kunt schatten in R en hoe je deze verandering kunt visualiseren. Dit model is vergelijkbaar met het Latent Growth Model (Latent Groei Model) en daar zal ik ook een stukje over schrijven.\nMet dank aan Alexander Cernat"
  },
  {
    "objectID": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#data-voorbeeld",
    "href": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#data-voorbeeld",
    "title": "Multilevel modeling met STAN",
    "section": "Data voorbeeld",
    "text": "Data voorbeeld\nZe analyseren de Gcsemv dataset (Rasbash et al. 2000) uit het mlmRev pakket in R. De gegevens omvatten de GCSE-examenscores (General Certificate of Secondary Education) van 1.905 leerlingen van 73 scholen in Engeland op een natuurwetenschappelijk vak. De Gcsemv-dataset bestaat uit de volgende 5 variabelen:\n\nschool: schoolidentificatiecode\n\nstudent: identificatiecode student\n\ngender: geslacht van een leerling (M: Man, F: Vrouw)\n\nwritten: totaalscore op schriftelijk werkstuk\ncourse: totaalscore op schriftelijk werkstuk\n\n\n# Use example dataset from mlmRev package: GCSE exam score\ndata(Gcsemv, package = \"mlmRev\")\nsummary(Gcsemv)\n\n     school        student     gender      written          course      \n 68137  : 104   77     :  14   F:1128   Min.   : 0.60   Min.   :  9.25  \n 68411  :  84   83     :  14   M: 777   1st Qu.:37.00   1st Qu.: 62.90  \n 68107  :  79   53     :  13            Median :46.00   Median : 75.90  \n 68809  :  73   66     :  13            Mean   :46.37   Mean   : 73.39  \n 22520  :  65   27     :  12            3rd Qu.:55.00   3rd Qu.: 86.10  \n 60457  :  54   110    :  12            Max.   :90.00   Max.   :100.00  \n (Other):1446   (Other):1827            NA's   :202     NA's   :180     \n\n\nTwee onderdelen van het examen werden geregistreerd als uitkomstvariabelen: schriftelijk werkstuk (written) en cursuswerkstuk (course). In deze tutorial wordt alleen de totaalscore op het cursuswerkstuk (course) geanalyseerd. Zoals hierboven te zien is, ontbreken er bij sommige waarnemingen waarden voor bepaalde covariaten. Hoewel we de data niet subsetten om alleen volledige gevallen op te nemen om aan te tonen dat rstanarm deze waarnemingen automatisch laat vallen, is het over het algemeen een goed gebruik om dit handmatig te doen indien nodig.\n\n# Maak Male dereferentiecategorie en hernoem de variabele\nGcsemv$female <- relevel(Gcsemv$gender, \"M\")\n\n\n# Gebruik alleen de totaalscore op course \nGCSE <- subset(x = Gcsemv, \n               select = c(school, student, female, course))\n\n# Tel unieke scholen en studenten\nJ <- length(unique(GCSE$school))\nN <- nrow(GCSE)\n\nHet pakket rstanarm automatiseert verschillende stappen van datavoorbewerking, waardoor het gebruik ervan sterk lijkt op dat van lme4 en wel op de volgende manier.\n\nInput - rstanarm kan een dataframe als input nemen2.\nOntbrekende gegevens - rstanarm verwijdert automatisch waarnemingen met NA waarden voor elke variabele gebruikt in het model3.\nIdentifiers - rstanarm vereist niet dat identifiers opeenvolgend zijn4. We stellen voor dat het een goede gewoonte is om alle cluster- en unit-identifiers, evenals categorische variabelen als factoren op te slaan. Dit geldt evenzeer voor lme4 als voor rstanarm. Men kan de structuur van de variabelen controleren met behulp van de str() functie.\n\n\n# Check structure of data frame\nstr(GCSE)\n\n'data.frame':   1905 obs. of  4 variables:\n $ school : Factor w/ 73 levels \"20920\",\"22520\",..: 1 1 1 1 1 1 1 1 1 2 ...\n $ student: Factor w/ 649 levels \"1\",\"2\",\"3\",\"4\",..: 16 25 27 31 42 62 101 113 146 1 ...\n $ female : Factor w/ 2 levels \"M\",\"F\": 1 2 2 2 1 2 2 1 1 2 ...\n $ course : num  NA 71.2 76.8 87.9 44.4 NA 89.8 17.5 32.4 84.2 ..."
  },
  {
    "objectID": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#model-1-variërend-intercept-model-zonder-voorspellers-variantiecomponentenmodel",
    "href": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#model-1-variërend-intercept-model-zonder-voorspellers-variantiecomponentenmodel",
    "title": "Multilevel modeling met STAN",
    "section": "Model 1: Variërend intercept model zonder voorspellers (Variantiecomponentenmodel)",
    "text": "Model 1: Variërend intercept model zonder voorspellers (Variantiecomponentenmodel)\nBeschouw het eenvoudigste multilevel model voor leerlingen \\(i=1,...,n\\) genest binnen scholen \\(j=1,...,J\\) en voor wie we examenresultaten als respons hebben. We kunnen een variabel interceptmodel met twee niveaus zonder voorspellers schrijven met de gebruikelijke tweedelige formulering als\n\\[Y_{ij}=\\alpha_{j} + \\epsilon_{ij}, \\text{ where } \\epsilon{ij} \\sim N(0,\\sigma^2_{y}) \\] \\[\\alpha{_j}=u_{\\alpha} + u_{j}, \\text { where } \\epsilon{ij} \\sim N(0,\\sigma^2_{y})\\]\nwaarin \\(yij\\) de examenscore is voor de \\(ith\\) leerling op de \\(jth\\) school, \\(\\alpha_{j}\\) de variërende intercept voor de jde school, en \\(u_{a}\\) het algemene gemiddelde voor alle scholen. Als alternatief kan het model in verkorte vorm worden uitgedrukt als\n\\[y_{ij}=u_{a} + u_{j} = \\epsilon_{ij}\\] Als we verder aannemen dat de fouten op leerlingniveau \\(\\epsilon_{ij}\\) normaal verdeeld zijn met gemiddelde 0 en variantie \\(\\sigma^2_{y}\\), en dat de variërende intercepten op schoolniveau \\(\\alpha_j\\) normaal verdeeld zijn met gemiddelde \\(u_{a}\\) en variantie \\(\\sigma^2_{a}\\), dan kan het model worden uitgedrukt als\n\\[y_{ij}∼ N(\\alpha_{j},\\sigma^2_{y})\\]\n\\[a_{j}∼ N(u_{a},\\sigma^2_{a})\\]\nDit model kan dan worden aangepast met lmer(). We specificeren een intercept (de voorspeller “1”) en laten deze variëren met de niveau-2-identifier (school). We specificeren ook de REML = FALSE optie om maximum likelihood (ML) schattingen te krijgen in plaats van de standaard restricted maximum likelihood (REML) schattingen.\n\nM1 <- lmer(formula = course ~ 1 + (1 | school), \n           data = GCSE, \n           REML = FALSE)\nsummary(M1)\n\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: course ~ 1 + (1 | school)\n   Data: GCSE\n\n     AIC      BIC   logLik deviance df.resid \n 14111.4  14127.7  -7052.7  14105.4     1722 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.9693 -0.5101  0.1116  0.6741  2.7613 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n school   (Intercept)  75.24    8.674  \n Residual             190.77   13.812  \nNumber of obs: 1725, groups:  school, 73\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)    73.72       1.11    66.4\n\n\n\n\n\n\n\n\nOnder Fixed effects zien we dat het intercept \\(u_{a}\\), gemiddeld over de populatie van scholen, wordt geschat op 73,72. Onder Random-effects zien we dat de standaardafwijking tussen de scholen \\(\\sigma_{a}\\) wordt geschat op \\(8.67\\) en de standaardafwijking binnen de scholen \\(\\sigma_{y}\\) op \\(13.81\\)."
  },
  {
    "objectID": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#model-2-variërend-intercept-model-met-een-enkele-voorspeller",
    "href": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#model-2-variërend-intercept-model-met-een-enkele-voorspeller",
    "title": "Multilevel modeling met STAN",
    "section": "Model 2: Variërend intercept model met een enkele voorspeller",
    "text": "Model 2: Variërend intercept model met een enkele voorspeller\nHet variërende interceptmodel5 met een indicatorvariabele voor het vrouwzijn xij kan worden geschreven als\n\\[Y_{ij}∼N(a_{j}+\\beta x{ij}, \\sigma^2_{y}),\\]\n\\[a_{j}∼N(u_{a}, \\sigma^2_{a})\\].\nDe vergelijking van de gemiddelde regressielijn voor alle scholen is \\(u_{ij}=u_{α}+βxij\\). De regressielijnen voor specifieke scholen zullen evenwijdig zijn aan de gemiddelde regressielijn (met dezelfde helling β), maar verschillen wat betreft het intercept \\(a_{j}\\). Dit model kan geschat worden door vrouwelijk toe te voegen aan de formule in de lmer() functie, waardoor alleen het intercept per school varieert en de “helling” voor vrouwelijkheid constant blijft voor alle scholen\n\nM2 <- lmer(formula = course ~ 1 + female + (1 | school), \n           data = GCSE, \n           REML = FALSE)\nsummary(M2)\n\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: course ~ 1 + female + (1 | school)\n   Data: GCSE\n\n     AIC      BIC   logLik deviance df.resid \n 14017.4  14039.2  -7004.7  14009.4     1721 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.7809 -0.5401  0.1259  0.6795  2.6753 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n school   (Intercept)  76.65    8.755  \n Residual             179.96   13.415  \nNumber of obs: 1725, groups:  school, 73\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   69.730      1.185   58.87\nfemaleF        6.739      0.678    9.94\n\nCorrelation of Fixed Effects:\n        (Intr)\nfemaleF -0.338\n\n\n\n\n\nDe gemiddelde regressielijn over de scholen wordt dus geschat als \\(\\hat{\\mu}_{ij}=69.73+ 6.74 x_{ij}\\), waarbij \\(\\sigma_\\_alpha\\) en \\(\\sigma_y\\) worden geschat als respectievelijk \\(8.76\\) en \\(13.41\\). Als we deze schattingen van \\(\\mu_alpha\\), \\(\\beta\\), \\(\\sigma^2_{y}\\), en \\(\\sigma^2_{alpha}\\) als de ware parameterwaarden beschouwen, kunnen we de **Best Linear Unbiased Predictions (BLUPs) voor de fouten op schoolniveau \\(\\hat{u}_j = \\hat{\\alpha}_{j} - \\hat{\\mu}_{\\alpha}\\) verkrijgen.\nDe BLUPs zijn equivalent met de zogenaamde Empirical Bayes (EB)-voorspelling, die het gemiddelde is van de posterieure verdeling van \\(u_{j}\\) gegeven alle geschatte parameters, alsmede de willekeurige variabelen \\(y_{ij}\\) en \\(x_{ij}\\) voor het cluster. Deze voorspellingen worden “Bayes” genoemd omdat ze gebruik maken van de vooraf gespecificeerde prioriteitsverdeling1 \\(u_j \\sim N(\\mu_\\alpha, \\sigma^2_\\alpha)\\), en bij uitbreiding \\(u_j \\sim N(0, \\sigma^2_\\alpha)\\), en “Empirisch” genoemd omdat de parameters van deze prior, \\(\\mu_\\alpha\\) en \\(\\sigma^2_{\\alpha}\\), naast \\(\\beta\\) en \\(\\sigma^2_{y}\\), geschat worden uit de data.\nIn vergelijking met de ML-benadering (Maximum Likelihood - maximale waarschijnlijkheid), waarbij waarden voor \\(u_j\\) worden voorspeld door alleen de geschatte parameters en gegevens van cluster \\(j\\) te gebruiken, houdt de EB-benadering bovendien rekening met de voorafgaande verdeling van \\(u_{j}\\), en levert zij voorspelde waarden op die dichter bij \\(0\\) liggen (een verschijnsel dat wordt beschreven als shrinkage of partial pooling). Om te zien waarom dit verschijnsel shrinkage wordt genoemd, drukken we de uit EB voorspelling verkregen schattingen voor \\(u_j\\) gewoonlijk uit als \\(\\hat{u}_j^{text{EB}} = \\hat{R}_j\\hat{u}_j^{\\text{ML}}\\) waarbij \\(\\hat{u}_j^{\\text{ML}}\\) de ML schattingen zijn, en \\(\\hat{R}_j = \\frac{\\sigma_alpha^2}{n_j}\\) de zogenaamde Shrinkage factor is.\n\nhead(ranef(M2)$school)\n\n      (Intercept)\n20920 -10.1702110\n22520 -17.0578149\n22710   7.8007260\n22738   0.4871012\n22908  -8.1940346\n23208   4.4304453\n\n\nDeze waarden schatten hoeveel het intercept naar boven of beneden verschoven is in bepaalde scholen. Bijvoorbeeld, in de eerste school in de dataset is het geschatte intercept ongeveer 10.17 lager dan gemiddeld, zodat de schoolspecifieke regressielijn \\((69.73 - 10.17) + 6.74 x_{ij}\\) is.\nGelman&Hill (2006) karakteriseren multilevel modellering als partial pooling (ook wel shrinkage genoemd), wat een compromis is tussen twee uitersten: complete pooling, waarbij de clustering helemaal niet in het model wordt meegenomen, en no pooling, waarbij voor elke school aparte intercepten worden geschat als coëfficiënten van dummy-variabelen. De geschatte schoolspecifieke regressielijnen in het bovenstaande model zijn gebaseerd op partial pooling schattingen. Om dit aan te tonen, schatten we eerst de intercept en de helling voor elke school op drie manieren:\n\n# Complete-pooling regression\npooled <- lm(formula = course ~ female,\n             data = GCSE)\na_pooled <- coef(pooled)[1]   # complete-pooling intercept\nb_pooled <- coef(pooled)[2]   # complete-pooling slope\n\n# No-pooling regression\nnopooled <- lm(formula = course ~ 0 + school + female,\n               data = GCSE)\na_nopooled <- coef(nopooled)[1:J]   # 73 no-pooling intercepts              \nb_nopooled <- coef(nopooled)[J+1]\n\n# Partial pooling (multilevel) regression\na_part_pooled <- coef(M2)$school[, 1]\nb_part_pooled <- coef(M2)$school[, 2]\n\nVervolgens plotten we de gegevens en schoolspecifieke regressielijnen voor een selectie van acht scholen met behulp van de volgende commando’s.2:\n\n# (0) Assen plaatsen & scholen kiezen\ny <- GCSE$course\nx <- as.numeric(GCSE$female) - 1 + runif(N, -.05, .05)\nschid <- GCSE$school\nsel.sch <- c(\"65385\",\n             \"68207\",\n             \"60729\",\n             \"67051\",\n             \"50631\",\n             \"60427\",\n             \"64321\",\n             \"68137\")\n\n# (1) Subset 8 van de scholen; genereer data frame\ndf <- data.frame(y, x, schid)\ndf8 <- subset(df, schid %in% sel.sch)\n\n# (2) Aangeven van schattingen van volledige-pooling, geen-pooling, gedeeltelijke pooling \ndf8$a_pooled <- a_pooled \ndf8$b_pooled <- b_pooled\ndf8$a_nopooled <- a_nopooled[df8$schid]\ndf8$b_nopooled <- b_nopooled\ndf8$a_part_pooled <- a_part_pooled[df8$schid]\ndf8$b_part_pooled <- b_part_pooled[df8$schid]\n\n# (3) Plot van hoe regressie fit voor de 8 scholen\nggplot(data = df8, \n       aes(x = x, y = y)) + \n  facet_wrap(facets = ~ schid, \n             ncol = 4) + \n  theme_bw() +\n  geom_jitter(position = position_jitter(width = .05, \n                                         height = 0)) +\n  geom_abline(aes(intercept = a_pooled, \n                  slope = b_pooled), \n              linetype = \"solid\", \n              color = \"blue\", \n              size = 0.5) +\n  geom_abline(aes(intercept = a_nopooled, \n                  slope = b_nopooled), \n              linetype = \"longdash\", \n              color = \"red\", \n              size = 0.5) + \n  geom_abline(aes(intercept = a_part_pooled, \n                  slope = b_part_pooled), \n              linetype = \"dotted\", \n              color = \"purple\", \n              size = 0.7) + \n  scale_x_continuous(breaks = c(0, 1), \n                     labels = c(\"male\", \"female\")) + \n  labs(title = \"Schatting van Volledige-pooling, Geen-pooling en Gedeeltelijke pooling\",\n       x = \"\", \n       y = \"Totale score op cursuswerk paper\")+theme_bw( base_family = \"serif\")\n\n\n\n\nDe blauw-gestreepte, rood-gestreepte en paars-gestippelde lijnen geven respectievelijk de volledige pooling, no-pooling en gedeeltelijke pooling schattingen weer. We zien dat de geschatte schoolspecifieke regressielijn van de gedeeltelijke pooling-schattingen tussen de volledige pooling- en de geenpooling-regressielijn ligt. Er is meer pooling (paarse stippellijn dichter bij blauwe ononderbroken lijn) op scholen met een kleine steekproefomvang."
  },
  {
    "objectID": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#model-3-variërend-intercept-en-slope-model-met-een-enkele-voorspeller",
    "href": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#model-3-variërend-intercept-en-slope-model-met-een-enkele-voorspeller",
    "title": "Multilevel modeling met STAN",
    "section": "Model 3: Variërend intercept en slope model met een enkele voorspeller",
    "text": "Model 3: Variërend intercept en slope model met een enkele voorspeller\nWe breiden nu het variërende interceptmodel met één voorspeller uit om zowel het intercept als de helling willekeurig te laten variëren tussen scholen met behulp van het volgende model \\[y_{ij} = \\alpha_j + \\beta_j x_{ij} +\\epsilon_{ij},\\] \\[\\alpha_j = \\mu_\\alpha + u_j,\\] \\[\\beta_j = \\mu_\\beta + v_j,\\] or in a reduced form as \\[y_{ij} = \\mu_\\alpha + \\mu_\\beta x_{ij} + u_j + v_j x_{ij} + \\epsilon_{ij}\\] where \\(\\epsilon_{ij} \\sim N(0, \\sigma_{y}^{2})\\) and \\(\\left( \\begin{matrix} u_j \\\\ v_j \\end{matrix} \\right) \\sim N\\left( \\left( \\begin{matrix} 0 \\\\ 0 \\end{matrix} \\right) ,\\left( \\begin{matrix} { \\sigma }_{ \\alpha }^{ 2 } & \\rho { \\sigma }_{ \\alpha }{ \\sigma }_{ \\beta } \\\\ \\rho { \\sigma }_{ \\alpha }{ \\sigma }_{ \\beta } & { \\sigma }_{ \\beta }^{ 2 } \\end{matrix} \\right) \\right)\\).]:\n\\[y_{ij}\\sim N(\\alpha_{j}+\\beta_{j}x_{ij} , \\sigma_y ^2 ),\\] \\[\\left( \\begin{matrix} \\alpha _{ j } \\\\ \\beta _{ j } \\end{matrix} \\right) \\sim N\\left( \\left( \\begin{matrix} { \\mu  }_{ \\alpha  } \\\\ { \\mu  }_{ \\beta  } \\end{matrix} \\right) , \\left( \\begin{matrix} { \\sigma  }_{ \\alpha  }^{ 2 } & \\rho { \\sigma  }_{ \\alpha  }{ \\sigma  }_{ \\beta  } \\\\ \\rho { \\sigma  }_{ \\alpha  }{ \\sigma  }_{ \\beta  } & { \\sigma  }_{ \\beta  }^{ 2 } \\end{matrix} \\right)  \\right).\\]\nMerk op dat we nu variatie hebben in de \\(\\alpha_{j}\\)’s en de \\(\\beta_{j}\\)’s, en ook een correlatie parameter \\(\\rho\\) tussen \\(\\alpha_{j}\\) en \\(\\beta_{j}\\). Dit model kan gefit worden met gebruik van lmer() en wel als volgt:\n\nM3 <- lmer(formula = course ~ 1 + female + (1 + female | school), \n           data = GCSE, \n           REML = FALSE)\nsummary(M3) \n\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: course ~ 1 + female + (1 + female | school)\n   Data: GCSE\n\n     AIC      BIC   logLik deviance df.resid \n 13983.4  14016.2  -6985.7  13971.4     1719 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.6886 -0.5222  0.1261  0.6529  2.6729 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n school   (Intercept) 102.93   10.146        \n          femaleF      47.94    6.924   -0.52\n Residual             169.79   13.030        \nNumber of obs: 1725, groups:  school, 73\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   69.425      1.352  51.338\nfemaleF        7.128      1.131   6.302\n\nCorrelation of Fixed Effects:\n        (Intr)\nfemaleF -0.574\n\n\n\n\n\nIn dit model wordt de residuele standaardafwijking binnen de school geschat als \\(\\hat{\\sigma}_{y}=\\) 13.03. De geschatte standaardafwijkingen van de schoolintercepten en de schoolhellingen zijn respectievelijk \\(\\hat{\\sigma}_{\\alpha}= 10.15\\) en \\(\\hat{\\sigma}_{\\beta}= 6.92\\). De geschatte correlatie tussen variërende intercepts en hellingen is \\(\\hat{\\rho} = -0.52\\). We kunnen een soortgelijke code als die in paragraaf 2.2 gebruiken om de gegevens en de schoolspecifieke regressielijnen voor een selectie van acht scholen te plotten."
  },
  {
    "objectID": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#gebruik-van-het-rstanarm-pakket",
    "href": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#gebruik-van-het-rstanarm-pakket",
    "title": "Multilevel modeling met STAN",
    "section": "Gebruik van het rstanarm pakket",
    "text": "Gebruik van het rstanarm pakket\nVeel relatief eenvoudige modellen kunnen worden aangepast met behulp van het rstanarm pakket zonder enige code te schrijven in de Stan taal. Het rstanarm pakket is een “wrapper” voor het rstan pakket waarmee de meest gebruikte regressiemodellen kunnen worden geschat met behulp van Markov Chain Monte Carlo (MCMC) en toch kunnen worden gespecificeerd met de gebruikelijke R modelleersyntaxis. Onderwijs onderzoekers kunnen Bayesiaanse schatting gebruiken voor multilevel modellen met slechts minimale veranderingen in hun bestaande code met lmer().\nBijvoorbeeld, Model 1 met standaard prior verdelingen voor \\(\\mu_{\\alpha}\\), \\(\\sigma_{\\alpha}\\), en \\(\\sigma_{y}\\) kan worden gespecificeerd met het rstanarm pakket door stan_ toe te voegen aan de lmer aanroep:\n\nM1_stanlmer <- stan_lmer(formula = course ~ 1 + (1 | school), \n                         data = GCSE,\n                         seed = 349)\n\nDeze stan_lmer() functie is qua syntax gelijk aan lmer(), maar in plaats van een maximum likelihood schatting uit te voeren, wordt een Bayesiaanse schatting uitgevoerd via MCMC. Omdat elke stap in de MCMC schatting random trekkingen uit de parameter ruimte inhoudt, voegen we een seed optie toe om ervoor te zorgen dat stan_lmer elke keer dat de code wordt uitgevoerd, dezelfde resultaten geeft."
  },
  {
    "objectID": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#prior-distributies",
    "href": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#prior-distributies",
    "title": "Multilevel modeling met STAN",
    "section": "Prior distributies",
    "text": "Prior distributies\nModel 1 is een variabel interceptiemodel met normaal verdeelde leerlingresiduen en intercepten op schoolniveau: \\(y_{ij} \\sim N(\\alpha_{j}, \\sigma_{y}^{2}),\\) en \\(\\alpha_{j},\\sim N(\\mu_{alpha}, \\sigma_{alpha}^{2})\\). De normale verdeling voor de \\(\\alpha{j}\\)’s kan worden beschouwd als een prioriteitsverdeling voor deze variërende intercepten. De parameters van deze prior verdeling, \\(\\mu_{\\alpha}\\) en \\(\\sigma_{\\alpha}\\), worden geschat uit de gegevens bij gebruik van maximum likelihood schatting. Bij volledige Bayesiaanse inferentie hebben alle hyperparameters (\\(\\mu_{\\alpha}\\) en \\(\\sigma_{\\alpha}\\)), samen met de andere niet-gemodelleerde parameters (in dit geval, \\(\\sigma_{y}\\)) ook een priorverdeling nodig.\nHier gebruiken we de standaard prior verdelingen voor de hyperparameters in stan_lmer door geen prior opties op te geven in stan_lmer() functie. De standaard priors zijn bedoeld als zwak informatief in de zin dat ze gematigde regularisatie bieden [Regularisatie kan worden beschouwd als een techniek om ervoor te zorgen dat schattingen binnen een acceptabel bereik van waarden worden begrensd] en helpen bij het stabiliseren van de berekening. Opgemerkt moet worden dat de auteurs van rstanarm suggereren om niet te vertrouwen op rstanarm om de standaard prior voor een model te specificeren, maar eerder om de priors expliciet te specificeren, zelfs als ze inderdaad de huidige standaard zijn, aangezien updates van het pakket andere defaults mee kunnen krijgen.\nTen eerste wordt, alvorens rekening te houden met de schaal van de variabelen, \\(\\mu_{alpha}\\) een normale priorverdeling gegeven met gemiddelde 0 en standaardafwijking 10. Dat wil zeggen, \\(mu_{alpha} \\sim N(0, 10^2)\\). De standaardafwijking van deze prioriteitsverdeling, 10, is vijf keer zo groot als de standaardafwijking van de respons indien deze gestandaardiseerd zou zijn. Dit zou een dichte benadering moeten zijn van een niet-informatieve prior over het door de waarschijnlijkheid ondersteunde bereik, die in gevolgtrekkingen zou moeten geven die vergelijkbaar zijn met die verkregen met maximale waarschijnlijkheidsmethoden indien even zwakke priors worden gebruikt voor de andere parameters.\nTen tweede wordt de (ongeschaalde) prior voor \\(\\sigma_{y}\\) ingesteld op een exponentiële verdeling met de ‘rate’-parameter op 1.\nTen derde, om een prior voor de varianties en covarianties van de variërende (of “willekeurige”) effecten te specificeren, zal rstanarm deze matrix ontbinden in een correlatiematrix van de variërende effecten en een functie van hun varianties. Omdat er in dit voorbeeld slechts één variërend effect is, reduceert de standaard (ongeschaalde) prior voor \\(\\sigma_{\\alpha}\\) die rstanarm gebruikt tot een exponentiële verdeling met de rate parameter op 1.\nOok moet worden opgemerkt dat rstanarm de priors zal schalen tenzij de autoscale = FALSE optie wordt gebruikt. Na het fitten van een model met stan_lmer, kunnen we de gebruikte priors controleren door de prior_summary() functie op te roepen.\n\n# Een samenvatting krijgen van de priors die gebruikt worden \nprior_summary(object = M1_stanlmer)\n\nPriors for model 'M1_stanlmer' \n------\nIntercept (after predictors centered)\n  Specified prior:\n    ~ normal(location = 73, scale = 2.5)\n  Adjusted prior:\n    ~ normal(location = 73, scale = 41)\n\nAuxiliary (sigma)\n  Specified prior:\n    ~ exponential(rate = 1)\n  Adjusted prior:\n    ~ exponential(rate = 0.061)\n\nCovariance\n ~ decov(reg. = 1, conc. = 1, shape = 1, scale = 1)\n------\nSee help('prior_summary.stanreg') for more details\n\n\n\n# Hiermee krijgen we de SD van de uitkomst \nsd(GCSE$course, na.rm = TRUE)\n\n[1] 16.32096\n\n\nZoals hierboven te zien is, worden de schalen van de priors voor \\(\\mu_{\\alpha}\\) en \\(\\sigma_y\\) op respectievelijk \\(163,21\\) en \\(16,32\\) gezet na herschaling. Aangezien de standaard prior voor het intercept normaal is met een schaalparameter van \\(10\\), is de herschaalde prior ook normaal maar met een schaalparameter van \\(\\text{scale} \\times \\text{SD}(y) = 10 \\times 16.321= 163.21\\). Aangezien de standaard prior voor \\(\\sigma_y\\) exponentieel is met een snelheidsparameter van \\(1\\) (of gelijkwaardig, de schaalparameter \\(\\text{scale} = \\frac{1}{text{rate} = 1\\)), is de herschaalde prior eveneens exponentieel met een schaalparameter van \\(\\text{scale} \\maal \\text{SD}(y) = 1 maal 16,321= 16,32\\)."
  },
  {
    "objectID": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#directe-output-van-stan_lmer",
    "href": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#directe-output-van-stan_lmer",
    "title": "Multilevel modeling met STAN",
    "section": "Directe output van stan_lmer",
    "text": "Directe output van stan_lmer\n\nPosterior medianen en posterior mediaan absolute deviaties\nWe kunnen een snelle samenvatting van de fit van Model 1 weergeven door de print methode op de volgende manier te gebruiken:\n\nprint(M1_stanlmer, digits = 2)\n\nstan_lmer\n family:       gaussian [identity]\n formula:      course ~ 1 + (1 | school)\n observations: 1725\n------\n            Median MAD_SD\n(Intercept) 73.68   1.14 \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 13.82   0.24 \n\nError terms:\n Groups   Name        Std.Dev.\n school   (Intercept)  8.87   \n Residual             13.82   \nNum. levels: school 73 \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\n\n\nHier is de puntschatting van \\(\\mu_{\\alpha}\\) uit stan_lmer \\(73.75\\) en dit komt overeen met de mediaan van de posterior trekkingen. Dit is vergelijkbaar met de ML schatting uit lmer. De puntschatting voor \\(`sigma_{_alpha}\\) van stan_lmer is \\(8.87\\), die groter is dan de ML schatting (\\(8.67\\)). Dit verschil kan deels komen doordat de ML benadering in lmer() geen rekening houdt met de onzekerheid in \\(\\mu_{\\alpha}\\) bij het schatten van \\(\\sigma_{\\alpha}\\). De REML benadering (\\(8.75\\)) in lmer() houdt, zoals eerder vermeld, wel rekening met deze onzekerheid.\nBij gebruik van stan_lmer worden standaardfouten verkregen door de mediaan absolute afwijking (MAD) van elke trekking ten opzichte van de mediaan van die trekkingen te beschouwen. Het is bekend dat ML de neiging heeft om onzekerheden te onderschatten, omdat het gebaseerd is op puntschattingen van hyperparameters. Full Bayes daarentegen propageert de onzekerheid in de hyperparameters over alle niveaus van het model en levert adequatere onzekerheidsschattingen op. Zie ook Brown e.a. (2006) voor verdere discussie.\n\n\nPosterior gemiddelden, posterior standaard deviaties, 95% credible interval en Monte Carlo fouten\n\nsummary(M1_stanlmer, \n        pars = c(\"(Intercept)\", \"sigma\", \"Sigma[school:(Intercept),(Intercept)]\"),\n        probs = c(0.025, 0.975),\n        digits = 2)\n\n\nModel Info:\n function:     stan_lmer\n family:       gaussian [identity]\n formula:      course ~ 1 + (1 | school)\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 1725\n groups:       school (73)\n\nEstimates:\n                                        mean   sd     2.5%   97.5%\n(Intercept)                            73.67   1.12  71.49  75.92 \nsigma                                  13.82   0.24  13.36  14.30 \nSigma[school:(Intercept),(Intercept)]  78.67  15.55  53.46 114.10 \n\nMCMC diagnostics\n                                      mcse Rhat n_eff\n(Intercept)                           0.05 1.01  572 \nsigma                                 0.00 1.00 4579 \nSigma[school:(Intercept),(Intercept)] 0.61 1.00  641 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\nHet is de moeite waard om op te merken dat bij gebruik van de summary methode, de schatting voor de standaardafwijking \\(sigma_y\\) het gemiddelde is van de posterior trekkingen van de parameter. Dit in tegenstelling tot de mediaan van de posterior trekkingen die we krijgen bij gebruik van de print methode. Een voordeel van het gebruik van de mediaan is dat de schatting voor \\(\\sigma_y^2\\) gewoon het kwadraat is van de schatting voor \\(\\sigma_y\\) als het aantal steekproeven oneven is. Dit is niet het geval bij gebruik van het gemiddelde. In dit geval, en meer algemeen wanneer we andere functies van de parameters moeten evalueren, moeten we de posterior trekkingen rechtstreeks benaderen. Dit wordt beschreven in het volgende deel.\nOnder Diagnostics, verwijzen we de lezer naar Paragraaf 5 voor meer informatie over Rhat en n_eff. De waarden onder mcse vertegenwoordigen schattingen voor de Monte Carlo standaardfouten, die de willekeurigheid vertegenwoordigen die geassocieerd is met elke MCMC schattingsrun. Dat wil zeggen, met dezelfde dataset, herhaaldelijk gebruik van een MCMC benadering om een parameter te schatten levert schattingen op met een standaardafwijking gelijk aan de Monte Carlo standaardfout."
  },
  {
    "objectID": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#andere-output-van-stan_lmer",
    "href": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#andere-output-van-stan_lmer",
    "title": "Multilevel modeling met STAN",
    "section": "Andere output van stan_lmer",
    "text": "Andere output van stan_lmer\nZoals gezegd, kunnen gebruikers er de voorkeur aan geven om direct met de posterior trekkingen te werken om schattingen van meer complexe parameters te verkrijgen. Om dit te doen, moeten gebruikers ze handmatig benaderen vanuit het stan_lmer object. We laten zien hoe dit moet in de context van het maken van vergelijkingen tussen individuele scholen.\n\nToegang tot de simulaties en samenvattende resultaten\nGebaseerd op de standaard instellingen, genereert stan_lmer 4 MCMC-ketens van 2.000 iteraties elk. De helft van deze iteraties in elke keten wordt gebruikt als warming-up/burn-in (om de keten te laten convergeren naar de posterior verdeling), en daarom gebruiken we slechts 1.000 steekproeven per keten. Deze door MCMC gegenereerde steekproeven worden geacht getrokken te zijn uit de posterior verdelingen van de parameters in het model. Wij kunnen deze steekproeven gebruiken voor voorspellingen, om de onzekerheid samen te vatten en ‘çredible intervals’ (geloofwaardige intervallen) te schatten voor elke functie van de parameters.\nOm toegang te krijgen tot de posterior trekkingen voor alle parameters, passen we de methode as.matrix() toe op het stanreg object M1_stanlmer. Dit geeft een \\(S\\) bij \\(P\\) matrix, waarbij \\(S\\) de grootte is van de posterior steekproef (of gelijkwaardig, het aantal MCMC iteraties na warm-up) en \\(P\\) het aantal parameters/kwantiteiten. Door deze matrix te manipuleren kunnen we een matrix genereren voor de variërende intercepts \\(\\alpha_{j}\\) en vectoren met de trekkingen voor de within standaardafwijking en de between variantie. Merk op dat om de juiste kolommen voor de parameter van belang te selecteren, het nuttig is om de kolomnamen van de matrix sims te onderzoeken.\nEen meer directe benadering voor het verkrijgen van de posterior trekkingen voor specifieke parameters is gebruik te maken van de ingebouwde functionaliteit van de as.matrix methode voor stanreg objecten. Wanneer de as.matrix methode wordt toegepast op een stanreg object, kan de gebruiker ofwel een optionele karaktervector van parameternamen specificeren, of een optionele karaktervector van reguliere expressies3 om de posterior trekkingen van alleen de parameters waarin ze geïnteresseerd zijn te extraheren. Bijvoorbeeld, omdat de parameter die het totale gemiddelde representeert is gelabeld met (Intercept), kunnen we de posterior trekkingen van alleen deze parameter extraheren door de optie pars = \"(Intercept)\" op te nemen. En omdat de parameters die de 73 schoolfouten representeren allemaal de string b[(Intercept) school: bevatten, kunnen we alle parameters die deze string bevatten extraheren door de optie regex_pars = \"b[(Intercept) school:” te gebruiken.\n\n# Extraheren van de posterior trekkingen voor alle parameters\nsims <- as.matrix(M1_stanlmer)\ndim(sims)\n\n[1] 4000   76\n\npara_name <- colnames(sims)\npara_name\n\n [1] \"(Intercept)\"                          \n [2] \"b[(Intercept) school:20920]\"          \n [3] \"b[(Intercept) school:22520]\"          \n [4] \"b[(Intercept) school:22710]\"          \n [5] \"b[(Intercept) school:22738]\"          \n [6] \"b[(Intercept) school:22908]\"          \n [7] \"b[(Intercept) school:23208]\"          \n [8] \"b[(Intercept) school:25241]\"          \n [9] \"b[(Intercept) school:30474]\"          \n[10] \"b[(Intercept) school:35270]\"          \n[11] \"b[(Intercept) school:37224]\"          \n[12] \"b[(Intercept) school:47627]\"          \n[13] \"b[(Intercept) school:50627]\"          \n[14] \"b[(Intercept) school:50631]\"          \n[15] \"b[(Intercept) school:60421]\"          \n[16] \"b[(Intercept) school:60427]\"          \n[17] \"b[(Intercept) school:60437]\"          \n[18] \"b[(Intercept) school:60439]\"          \n[19] \"b[(Intercept) school:60441]\"          \n[20] \"b[(Intercept) school:60455]\"          \n[21] \"b[(Intercept) school:60457]\"          \n[22] \"b[(Intercept) school:60501]\"          \n[23] \"b[(Intercept) school:60729]\"          \n[24] \"b[(Intercept) school:60741]\"          \n[25] \"b[(Intercept) school:63619]\"          \n[26] \"b[(Intercept) school:63833]\"          \n[27] \"b[(Intercept) school:64251]\"          \n[28] \"b[(Intercept) school:64321]\"          \n[29] \"b[(Intercept) school:64327]\"          \n[30] \"b[(Intercept) school:64343]\"          \n[31] \"b[(Intercept) school:64359]\"          \n[32] \"b[(Intercept) school:64428]\"          \n[33] \"b[(Intercept) school:65385]\"          \n[34] \"b[(Intercept) school:66365]\"          \n[35] \"b[(Intercept) school:67051]\"          \n[36] \"b[(Intercept) school:67105]\"          \n[37] \"b[(Intercept) school:67311]\"          \n[38] \"b[(Intercept) school:68107]\"          \n[39] \"b[(Intercept) school:68111]\"          \n[40] \"b[(Intercept) school:68121]\"          \n[41] \"b[(Intercept) school:68125]\"          \n[42] \"b[(Intercept) school:68133]\"          \n[43] \"b[(Intercept) school:68137]\"          \n[44] \"b[(Intercept) school:68201]\"          \n[45] \"b[(Intercept) school:68207]\"          \n[46] \"b[(Intercept) school:68217]\"          \n[47] \"b[(Intercept) school:68227]\"          \n[48] \"b[(Intercept) school:68233]\"          \n[49] \"b[(Intercept) school:68237]\"          \n[50] \"b[(Intercept) school:68241]\"          \n[51] \"b[(Intercept) school:68255]\"          \n[52] \"b[(Intercept) school:68271]\"          \n[53] \"b[(Intercept) school:68303]\"          \n[54] \"b[(Intercept) school:68321]\"          \n[55] \"b[(Intercept) school:68329]\"          \n[56] \"b[(Intercept) school:68405]\"          \n[57] \"b[(Intercept) school:68411]\"          \n[58] \"b[(Intercept) school:68417]\"          \n[59] \"b[(Intercept) school:68531]\"          \n[60] \"b[(Intercept) school:68611]\"          \n[61] \"b[(Intercept) school:68629]\"          \n[62] \"b[(Intercept) school:68711]\"          \n[63] \"b[(Intercept) school:68723]\"          \n[64] \"b[(Intercept) school:68805]\"          \n[65] \"b[(Intercept) school:68809]\"          \n[66] \"b[(Intercept) school:71927]\"          \n[67] \"b[(Intercept) school:74330]\"          \n[68] \"b[(Intercept) school:74862]\"          \n[69] \"b[(Intercept) school:74874]\"          \n[70] \"b[(Intercept) school:76531]\"          \n[71] \"b[(Intercept) school:76631]\"          \n[72] \"b[(Intercept) school:77207]\"          \n[73] \"b[(Intercept) school:84707]\"          \n[74] \"b[(Intercept) school:84772]\"          \n[75] \"sigma\"                                \n[76] \"Sigma[school:(Intercept),(Intercept)]\"\n\n# Verkrijgen van school-niveau varyiërende intercept a_j\n# trekking voor het algemeen gemiddelde\nmu_a_sims <- as.matrix(M1_stanlmer, \n                       pars = \"(Intercept)\")\n# trekkingen voor 73 scholen van de school-niveua fouten \nu_sims <- as.matrix(M1_stanlmer, \n                    regex_pars = \"b\\\\[\\\\(Intercept\\\\) school\\\\:\")\n# trekkingen van alle 73 school variërende intercepten               \na_sims <- as.numeric(mu_a_sims) + u_sims          \n\n# Verkrijgen van sigma_y en sigma_alpha^2\n# trekkingen van sigma_y\ns_y_sims <- as.matrix(M1_stanlmer, \n                       pars = \"sigma\")\n# trekkingen van sigma_alpha^2\ns__alpha_sims <- as.matrix(M1_stanlmer, \n                       pars = \"Sigma[school:(Intercept),(Intercept)]\")\n\n\n\nVerkrijgen van gemiddelden, standaard deviaties, medianen en 95% geloofwaardigheids intervallen\nIn a_sims hebben we 4.000 posterior trekkingen (van alle 4 ketens) voor de variërende intercepten \\(\\alpha_{j}\\) van de 73 scholen opgeslagen. De eerste kolom van de matrix van 4.000 bij 73 is bijvoorbeeld een vector van 4.000 posterior simulatietrekkingen voor de variërende intercept van de eerste school (School 20920). Een kwantitatieve manier om de posterior kansverdeling van deze 4.000 schattingen voor \\(1,1pha_{1}\\) samen te vatten is het onderzoeken van hun quantielen.\n\n# Computeer gemiddelde, SD, mediaan en 95% geloofwaardigheids interval van de varyiërende intercepten\n\n# Posterior gemiddelde en SD van elke alpha\na_mean <- apply(X = a_sims,     # posterior gemiddelde\n                MARGIN = 2,\n                FUN = mean)\na_sd <- apply(X = a_sims,       # posterior SD\n              MARGIN = 2,\n              FUN = sd)\n\n# Posterior mediaan en 95% geloofwaardigheids interval\na_quant <- apply(X = a_sims, \n                 MARGIN = 2, \n                 FUN = quantile, \n                 probs = c(0.025, 0.50, 0.975))\na_quant <- data.frame(t(a_quant))\nnames(a_quant) <- c(\"Q2.5\", \"Q50\", \"Q97.5\")\n\n# Combineer samenvattende statistieken van posterior simulatie trekkingen\na_df <- data.frame(a_mean, a_sd, a_quant)\nround(head(a_df), 2)\n\n                            a_mean a_sd  Q2.5   Q50 Q97.5\nb[(Intercept) school:20920]  63.63 4.40 54.97 63.57 72.17\nb[(Intercept) school:22520]  57.15 1.79 53.72 57.15 60.60\nb[(Intercept) school:22710]  81.64 3.15 75.47 81.64 87.78\nb[(Intercept) school:22738]  73.03 4.27 64.48 73.11 81.50\nb[(Intercept) school:22908]  66.51 5.28 56.47 66.47 76.70\nb[(Intercept) school:23208]  79.23 2.81 73.60 79.22 85.02\n\n\nWij kunnen een rupsplot maken om de volledige Bayes-schattingen voor de schoolafhankelijke intercepts in rangorde te tonen, samen met hun 95% credible intervallen.\n\n# Sorteer dataframe die een geschatte alfa gemiddelde en sd voor elke school omvatten\na_df <- a_df[order(a_df$a_mean), ]\na_df$a_rank <- c(1 : dim(a_df)[1])  # een vector van de schoolranking \n\n# Plot school-niveau alfas posterior gemiddelde en 95% credible interval\nggplot(data = a_df, \n       aes(x = a_rank, \n           y = a_mean)) +\n  geom_pointrange(aes(ymin = Q2.5, \n                      ymax = Q97.5),\n                  position = position_jitter(width = 0.1, \n                                             height = 0)) + \n  geom_hline(yintercept = mean(a_df$a_mean), \n             size = 0.5, \n             col = \"red\") + \n  scale_x_continuous(\"Rank\", \n                     breaks = seq(from = 0, \n                                  to = 80, \n                                  by = 5)) + \n  scale_y_continuous(expression(paste(\"varying intercept, \", alpha[j]))) + \n  theme_bw( base_family = \"serif\")\n\n\n\n\nDezelfde aanpak kan natuurlijk worden gevolgd om 95% geloofwaardige intervallen te genereren voor \\(\\sigma_y\\) en \\(\\sigma_\\alpha\\).\n\n\nVergelijkingen maken tussen individuele scholen\nHet hebben van steekproeven van alle parameters en variërende intercepten uit hun gezamenlijke posterior verdeling maakt het gemakkelijk om inferenties te trekken over functies van deze parameters.\nIn onderwijsonderzoek en in de onderwijspraktijk is het vaak interessant om de scholen in de data met elkaar te vergelijken. Relevante vragen zijn bijvoorbeeld (1) wat is het verschil tussen de gemiddelden van school A en school B, (2) presteert school A beter dan school B en (3) wat is de rangorde van deze scholen binnen de steekproef. Wanneer niet-Bayesiaanse methoden worden gebruikt, kunnen wij proberen dergelijke vergelijkingen te maken op basis van empirische Bayes- (of Best Linear Unbiased-) voorspellingen van de variërende intercepten. Maar het zal in het algemeen onmogelijk zijn om de onzekerheid uit te drukken voor niet-lineaire functies zoals rangschikkingen. Zie ook Goldstein en Spiegelhalter (2006) voor verdere discussie.\nHier zullen we twee scholen vergelijken als voorbeeld: Scholen 60501 (de 21ste school) en 68271 (de 51ste school). We hebben al 4.000 posterior simulatietrekkingen voor beide scholen. Om conclusies te trekken over het verschil tussen de gemiddelde scores van de twee scholen, kunnen we eenvoudigweg het verschil nemen tussen de twee vectoren van trekkingen \\(\\alpha_{51} - \\alpha_{21}\\).\n\n# Het verschil tussen de twee schoolgemiddelden (school #21 en #51)\nschool_diff <- a_sims[, 21] - a_sims[, 51]\n\nWij kunnen de posterior verdeling van het verschil als volgt onderzoeken met beschrijvende statistieken en een histogram:\n\n# Onderzoek verschillen van twee distributies \nmean <- mean(school_diff)\nsd <- sd(school_diff)\nquantile <- quantile(school_diff, probs = c(0.025, 0.50, 0.975))\nquantile <- data.frame(t(quantile))\nnames(quantile) <- c(\"Q2.5\", \"Q50\", \"Q97.5\")\ndiff_df <- data.frame(mean, sd, quantile)\nround(diff_df, 2)\n\n  mean   sd  Q2.5  Q50 Q97.5\n1 5.12 4.48 -3.53 5.12 14.02\n\n\n\n# Histogram van de verschillen \nggplot(data = data.frame(school_diff), \n       aes(x = school_diff)) + \n  geom_histogram(color = \"black\", \n                 fill = \"gray\", \n                 binwidth = 0.75) + \n  scale_x_continuous(\"Score verschil tussen twee scholen: #21, #51\",\n                     breaks = seq(from = -20, \n                                  to = 20, \n                                  by = 10)) + \n  geom_vline(xintercept = c(mean(school_diff),\n                            quantile(school_diff, \n                                     probs = c(0.025, 0.975))),\n             colour = \"red\", \n             linetype = \"longdash\") + \n  geom_text(aes(5.11, 20, label = \"mean = 5.11\"), \n            color = \"red\", \n            size = 4) + \n  geom_text(aes(9, 50, label = \"SD = 4.46\"), \n            color = \"blue\", \n            size = 4) + \n  theme_bw( base_family = \"serif\") \n\n\n\n\n\nprop.table(table(a_sims[, 21] > a_sims[, 51]))\n\n\n  FALSE    TRUE \n0.12425 0.87575 \n\n\nHet verwachte verschil komt uit op 5,11 met een standaardafwijking van 4,46 en een grote bandbreedte van onzekerheid. Het 95% geloofwaardigheidsinterval is [-3.64, 13.66], dus we zijn er 95% zeker van dat de ware waarde van het verschil tussen de twee scholen binnen het bereik ligt, gegeven de gegevens.\nWe kunnen ook het deel van de tijd bepalen dat School 60501 een hoger gemiddelde heeft dan School 68271:\n\nprop.table(table(a_sims[, 21] > a_sims[, 51]))\n\n\n  FALSE    TRUE \n0.12425 0.87575 \n\n\n\n\n\nDit betekent dat de posterior waarschijnlijkheid dat School 60501 beter is dan School 68271 87.6% is. Elk paar scholen binnen de steekproef van scholen kan op deze manier vergeleken worden."
  },
  {
    "objectID": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#model-2-een-voorspeller-op-studentenniveau-toevoegen",
    "href": "posts/2022-02-11-multilevelrstan/multilevelrstan.html#model-2-een-voorspeller-op-studentenniveau-toevoegen",
    "title": "Multilevel modeling met STAN",
    "section": "Model 2: Een voorspeller op studentenniveau toevoegen",
    "text": "Model 2: Een voorspeller op studentenniveau toevoegen\nOnderzoekers zouden de variërende interceptmodellen kunnen uitbreiden met waargenomen verklarende variabelen op het niveau van de leerling \\(x_{ij}\\), in dit voorbeeld een indicatorvariabele voor vrouw. Een eenvoudig variërend interceptmodel met één voorspeller op het niveau van de leerling kan worden geschreven als \\(y_{ij} \\N(\\alpha_{j} + \\beta x_{ij}, \\sigma_{y}^{2})\\) en \\(\\alpha_{j} \\N(\\mu_{alpha}, \\sigma_{alpha}^{2})\\). We gebruiken niet-informatieve prioriteitsverdelingen voor de hyperparameters (\\(\\mu_{\\alpha}\\) en \\(\\sigma_{\\alpha}\\)) zoals gespecificeerd in het variërende interceptmodel zonder voorspellers. Bovendien krijgt de regressiecoëfficiënt \\(\\beta\\) een normale prioriteitsverdeling met gemiddelde 0 en standaardafwijking 100. Dit betekent, ruwweg, dat we verwachten dat deze coëfficiënt in het bereik \\((-100, 100)\\) ligt, en als de ML schatting in dit bereik ligt, geeft de prior verdeling zeer weinig informatie voor de inferentie.\nHet bovenstaande model kan als volgt worden gefit met de stan_lmer() functie in het rstanarm pakket:\n\nM2_stanlmer <- stan_lmer(formula = course ~ female + (1 | school), \n                         data = GCSE, \n                         prior = normal(location = 0, \n                                        scale = 100,\n                                        autoscale = FALSE),\n                         prior_intercept = normal(location = 0, \n                                                  scale = 100, \n                                                  autoscale = FALSE),\n                         seed = 349)\n\n\nprior_summary(object = M2_stanlmer)\n\nPriors for model 'M2_stanlmer' \n------\nIntercept (after predictors centered)\n ~ normal(location = 0, scale = 100)\n\nCoefficients\n ~ normal(location = 0, scale = 100)\n\nAuxiliary (sigma)\n  Specified prior:\n    ~ exponential(rate = 1)\n  Adjusted prior:\n    ~ exponential(rate = 0.061)\n\nCovariance\n ~ decov(reg. = 1, conc. = 1, shape = 1, scale = 1)\n------\nSee help('prior_summary.stanreg') for more details\n\n\n\nM2_stanlmer\n\nstan_lmer\n family:       gaussian [identity]\n formula:      course ~ female + (1 | school)\n observations: 1725\n------\n            Median MAD_SD\n(Intercept) 69.7    1.3  \nfemaleF      6.7    0.7  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 13.4    0.2  \n\nError terms:\n Groups   Name        Std.Dev.\n school   (Intercept)  9      \n Residual             13      \nNum. levels: school 73 \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nMerk op dat in plaats van de standaard priors in stan_lmer, \\(\\mu_{\\alpha}\\) en \\(\\beta\\) normale prior verdelingen krijgen met gemiddelde 0 en standaardafwijking 100 door de argumenten prior en prior_intercept op te geven als normal(location = 0, scale = 100, autoscale = FALSE). Om te voorkomen dat stan_lmer de prior schaalt, moeten we ervoor zorgen dat het argument autoscale = FALSE wordt toegevoegd.\nDe puntschattingen van \\(_mu_{alpha}\\), \\(\\beta\\), en \\(\\sigma_{y}\\) zijn bijna identiek aan de ML-schattingen van de lmer() fit. Echter, deels omdat ML de onzekerheid over \\(\\mu_{alpha}\\) negeert bij het schatten van \\(\\sigma_{alpha}\\), is de Bayesiaanse schatting voor \\(\\sigma_{alpha}\\) (\\(9,0\\)) groter dan de ML-schatting (\\(8,8\\)), net als bij model 1.\n\nModel 3: Variërende slopes over scholen toevoegen\nWe also use stan_lmer to fit Model 3 using the command below. Note that here, we use the default priors which are mostly similar to what was done in Model 1. Additionally, we are also required to specify a prior for the covariance matrix \\(\\Sigma\\) for \\(\\alpha_j\\) and \\(\\beta_j\\) in this Model. stan_lmer decomposes this covariance matrix (up to a factor of \\(\\sigma_y\\)) into (i) a correlation matrix \\(R\\) and (ii) a matrix of variances \\(V\\), and assigns them separate priors as shown below.\n$$\n\\[\\begin{aligned}\n\\Sigma &=\n\\left(\\begin{matrix}\n\\sigma_\\alpha^2 & \\rho\\sigma_\\alpha \\sigma_\\beta \\\\\n\\rho\\sigma_\\alpha\\sigma_\\beta&\\sigma_\\beta^2\n\\end{matrix} \\right)\\\\ &=\n\\sigma_y^2\\left(\\begin{matrix}\n\\sigma_\\alpha^2/\\sigma_y^2 & \\rho\\sigma_\\alpha \\sigma_\\beta/\\sigma_y^2 \\\\\n\\rho\\sigma_\\alpha\\sigma_\\beta/\\sigma_y^2 & \\sigma_\\beta^2/\\sigma_y^2\n\\end{matrix} \\right)\\\\ &=\n\\sigma_y^2\\left(\\begin{matrix}\n\\sigma_\\alpha/\\sigma_y & 0 \\\\\n0&\\sigma_\\beta/\\sigma_y\n\\end{matrix} \\right)\n\\left(\\begin{matrix}\n1 & \\rho\\\\\n\\rho&1\n\\end{matrix} \\right)\n\\left(\\begin{matrix}\n\\sigma_\\alpha/\\sigma_y & 0 \\\\\n0&\\sigma_\\beta/\\sigma_y\n\\end{matrix} \\right)\\\\\n&= \\sigma_y^2VRV.\n\\end{aligned}\\]\n$$\nDe correlatiematrix \\(R\\) is een 2 bij 2 matrix met 1-en op de diagonaal en \\(rho\\)’s op de off-diagonaal. stan_lmer kent er een LKJ^[Voor meer details over de LKJ verdeling, zie hier en hier prior aan toe, met regularisatieparameter 1 (Lewandowski et all., 2009). Dit komt overeen met het toekennen van een uniforme prior voor \\(rho\\). Hoe groter de regularisatieparameter is dan 1, hoe meer de verdeling voor \\(\\rho\\) de waarde 0 aanneemt.\nDe matrix van (geschaalde) varianties \\(V\\) kan eerst worden samengevat in een vector van (geschaalde) varianties, en vervolgens ontleed in drie delen, \\(J\\), \\(\\tau^2\\) en \\(\\pi\\) zoals hieronder getoond. $$\n(\n\\[\\begin{matrix}\n\\sigma_\\alpha^2/\\sigma_y^2 \\\\\n\\sigma_\\beta^2/\\sigma_y^2\n\\end{matrix}\\]\n) = 2()(\n\\[\\begin{matrix}\n\\frac{\\sigma_\\alpha^2/\\sigma_y^2}{\\sigma_\\alpha^2/\\sigma_y^2 + \\sigma_\\beta^2/\\sigma_y^2} \\\\\n\\frac{\\sigma_\\beta^2/\\sigma_y^2}{\\sigma_\\alpha^2/\\sigma_y^2 + \\sigma_\\beta^2/\\sigma_y^2}\n\\end{matrix}\\]\n)= J^2 .\n$$\nIn deze formulering is \\(J\\) het aantal variërende effecten in het model (hier \\(J=2\\)), kan \\(Jtau^2\\) worden beschouwd als een gemiddelde (geschaalde) variantie over de variërende effecten \\(Jalpha_j\\) en \\(Jbeta_j\\), en is \\(Jpi\\) een niet-negatieve vector die sommeert tot 1 (een zogenaamde simplex/probabiliteitsvector). Een symmetrische Dirichlet4 verdeling met concentratieparameter ingesteld op 1 wordt dan gebruikt als de prior voor \\(\\pi\\). Standaard impliceert dit een gezamenlijk uniforme prior over alle simplexvectoren van dezelfde grootte. Een schaalinvariante Gamma-voorrang met vorm- en schaalparameters beide op 1 wordt dan toegekend voor \\(\\tau\\). Dit komt overeen met het toekennen van de exponentiële verdeling met de snelheidsparameter op 1 die consistent is met de prioriteit toegekend aan \\(\\sigma_y\\) als prioriteit.\n\nM3_stanlmer <- stan_lmer(formula = course ~ female + (1 + female | school), \n                         data = GCSE,\n                         seed = 349)\nprior_summary(object = M3_stanlmer)\n\nPriors for model 'M3_stanlmer' \n------\nIntercept (after predictors centered)\n  Specified prior:\n    ~ normal(location = 73, scale = 2.5)\n  Adjusted prior:\n    ~ normal(location = 73, scale = 41)\n\nCoefficients\n  Specified prior:\n    ~ normal(location = 0, scale = 2.5)\n  Adjusted prior:\n    ~ normal(location = 0, scale = 83)\n\nAuxiliary (sigma)\n  Specified prior:\n    ~ exponential(rate = 1)\n  Adjusted prior:\n    ~ exponential(rate = 0.061)\n\nCovariance\n ~ decov(reg. = 1, conc. = 1, shape = 1, scale = 1)\n------\nSee help('prior_summary.stanreg') for more details\n\nM3_stanlmer\n\nstan_lmer\n family:       gaussian [identity]\n formula:      course ~ female + (1 + female | school)\n observations: 1725\n------\n            Median MAD_SD\n(Intercept) 69.4    1.3  \nfemaleF      7.1    1.1  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 13.0    0.2  \n\nError terms:\n Groups   Name        Std.Dev. Corr \n school   (Intercept) 10.3          \n          femaleF      7.2     -0.49\n Residual             13.0          \nNum. levels: school 73 \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\n\n\nHier zien we dat de puntschattingen voor \\(\\mu_{\\alpha}\\) en \\(\\sigma_{y}\\) identiek zijn aan de ML-schattingen uit lmer() fit. De puntschatting voor $\\(_bèta\\) is iets anders in dit model (7.14 vergeleken met 7.13). Verder is, net als bij de vorige twee modellen, de Bayesiaanse schatting voor \\(\\sigma_{\\alpha}\\) (10.3) groter dan de ML schatting (10.15). Daarnaast zijn de Bayesiaanse schattingen voor \\(\\sigma_{\\beta}\\) (7.2) en \\(\\rho\\) (-0.49) groter dan de overeenkomstige ML schattingen (respectievelijk 6.92 en -0.52)."
  },
  {
    "objectID": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html",
    "href": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html",
    "title": "Regressie en nog zo iets",
    "section": "",
    "text": "Vijftien jaar geleden schreven Gelman en Hill Data analysis using regression and multilevel/hierarchical models, een klassieker over moderne data-analyse. Ze gebruikte R en WinBugs voor lineaire en logistische, hierarchische regressieanalyse en causale inferentie. Ze lieten zien hoe je dat op de frequentistische en Bayesiaanse manier kunt doen. Het boek werd voor mij een naslagwerk dat ik steeds maar weer uit de kast trok. Vorig jaar dacht ik, laat ik eens zien of Gelman al weer iets nieuws heeft geschreven en toen zag ik dat Regression and other stories hiernet uit was is. Dat heeft Andrew Gelman weer met Jennifer Hill geschreven maar nu ook met de Fin Aki Vehtari. Ik was er nog niet aan toe gekomen om het te lezen. Dat heb ik deze maand gedaan. Ook dit boek zal ik vaker uit de kast trekken. Dit boek gaat over allerlei aspecten van regressie. Het is een theoretisch én praktisch boek. Je leert, wat ze noemen, voorspellende modellen beter begrijpen, toepassen in verschillende praktische problemen en je leert het simuleren. Je leert het opbouwen vanaf de basis en daarna kun je het in verschillende situaties toepassen. Het wil kritisch zijn, zonder nihilistisch te worden en vooral laten zien dat je van statistische analyse kunt leren. Ook dit boek staat op twee benen: frequentisch en Bayesiaans en laat zien hoe informatie wordt gebruikt in het schattingsproces, de assumpties die eraan ten grondslag liggen en hoe schattingen en voorspellingen kunnen worden geïnterpreteerd in beide raamwerken. Beide kunnen worden gebruikt, maar het is ook duidelijk dat de voorkeur bij Bayesiaanse benadering ligt. Dan kun je ook andere informatie gebruiken om te schatten of te voorspellen. En omdat je simuleert (het model duizenden keren draait) kunt je met de Bayesiaanse techniek meer zeggen over onzekerheid. Dat maakt deze techniek zeer geschikt voor regressieanalyses zoals in dit boek gepresenteerd. Wat ik zelf van dit boek heb geleerd zijn de mogelijkheden om op basis van gegevens te voorspellen. Vooral hoofdstuk 9 (Voorspellen en Bayesiaanse inferentie) vond ik interessant. Maar het boek zit vol informatie en kennis en laat zich amper samenvatten. Het lijkt erop dat het een eerste deel is en ik verwacht dat er later nog een tweede deel komt dat de nadruk legt op multilevel analyse. We zullen zien Bij het boek zit ook nog een website met data en scripts om zelf uit te proberen, prachtig onderwijsmateriaal opgesteld door Aki Vehtari hier."
  },
  {
    "objectID": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#interssante-blog",
    "href": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#interssante-blog",
    "title": "Regressie en nog zo iets",
    "section": "Interssante blog",
    "text": "Interssante blog\nToen ik het boek uit had kwam ik een een blog tegen op R-bloggers. Het verscheen op 1 september 2021 hier, maar ik kon niet zien van wie het is (Mister X, sorry. Hij of zij schreef het nadat deze persoon Regression and other stories had gelezen. Het vat heel goed samen hoe moderne regressieanalyse werkt en daarom heb ik het voor hier vertaald.\n\n# Eerst de pakketten inladen die we nodig hebben\nlibrary(plyr); library(dplyr)\n\nWarning: package 'plyr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:plyr':\n\n    arrange, count, desc, failwith, id, mutate, rename, summarise,\n    summarize\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(rstanarm)\n\nWarning: package 'rstanarm' was built under R version 4.1.3\n\n\nLoading required package: Rcpp\n\n\nWarning: package 'Rcpp' was built under R version 4.1.3\n\n\nThis is rstanarm version 2.21.3\n\n\n- See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n\n\n- Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n\n\n- For execution on a local, multicore CPU with excess RAM we recommend calling\n\n\n  options(mc.cores = parallel::detectCores())\n\nlibrary(bayesplot)\n\nWarning: package 'bayesplot' was built under R version 4.1.3\n\n\nThis is bayesplot version 1.9.0\n\n\n- Online documentation and vignettes at mc-stan.org/bayesplot\n\n\n- bayesplot theme set to bayesplot::theme_default()\n\n\n   * Does _not_ affect other ggplot2 plots\n\n\n   * See ?bayesplot_theme_set for details on theme setting\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\nlibrary(readr)\n\nWarning: package 'readr' was built under R version 4.1.3"
  },
  {
    "objectID": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#bayesiaanse-regressieanalyse-met-rstanarm",
    "href": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#bayesiaanse-regressieanalyse-met-rstanarm",
    "title": "Regressie en nog zo iets",
    "section": "Bayesiaanse regressieanalyse met Rstanarm",
    "text": "Bayesiaanse regressieanalyse met Rstanarm\nIn deze post zullen we een eenvoudig voorbeeld van Bayesiaanse regressieanalyse doornemen met het rstanarm pakket in R. Ik heb Gelman, Hill en Vehtari’s recente boek Regression and Other Stories” gelezen, en deze blog post is mijn poging om enkele van de dingen die ik heb geleerd toe te passen. Ik heb de afgelopen jaren stukjes en beetjes van de Bayesiaanse benadering opgevangen, en ik vind het een heel interessante manier om over gegevensanalyse na te denken en ze uit te voeren. Ik heb met veel plezier het nieuwe boek van Gelman en collega’s doorgewerkt en geëxperimenteerd met deze technieken, en ik ben blij dat ik hier iets kan delen van wat ik heb geleerd.\nJe kunt de gegevens en alle code van deze blogpost hier op Github vinden."
  },
  {
    "objectID": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#de-data",
    "href": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#de-data",
    "title": "Regressie en nog zo iets",
    "section": "De data",
    "text": "De data\nDe gegevens die we in deze blog zullen onderzoeken bestaan uit de dagelijkse totale stappentellingen van verschillende fitnesstrackers die ik de afgelopen 6 jaar heb gehad. De eerste waarneming werd geregistreerd op 2015-03-04 en de laatste op 2021-03-15. Gedurende deze periode bevat de dataset de dagelijkse totale stappentellingen voor 2.181 dagen.\nNaast de dagelijkse totale stappentelling bevat de dataset informatie over de dag van de week (bijv. maandag, dinsdag, etc.), het apparaat dat is gebruikt om de stappentelling vast te leggen (door de jaren heen heb ik er 3 gehad - Accupedo, Fitbit en Mi-Band), en het weer voor elke datum (de gemiddelde dagelijkse temperatuur in graden Celsius en de totale dagelijkse neerslag in millimeters, verkregen via het GSODR pakket in R).\nDe dataset (genaamd steps_weather) ziet er als volgt uit:\n\n# Data inladen\nlibrary(readr)\nsteps_weather <- read_csv(\"C:/FilesHarrie/Stanexample/Rstanarmexample/bayesian_regression_rstanarm/Data/steps_weather.csv\")\n\nRows: 2181 Columns: 7\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr  (3): dow, week_weekend, device\ndbl  (3): daily_total, temp, prcp\ndate (1): date\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(steps_weather)\n\n# A tibble: 6 x 7\n  date       daily_total dow   week_weekend device    temp  prcp\n  <date>           <dbl> <chr> <chr>        <chr>    <dbl> <dbl>\n1 2015-03-04       14136 Wed   Weekday      Accupedo   4.3   1.3\n2 2015-03-05       11248 Thu   Weekday      Accupedo   4.7   0  \n3 2015-03-06       12803 Fri   Weekday      Accupedo   5.4   0  \n4 2015-03-07       15011 Sat   Weekend      Accupedo   7.9   0  \n5 2015-03-08        9222 Sun   Weekend      Accupedo  10.2   0  \n6 2015-03-09       21452 Mon   Weekday      Accupedo   8.8   0  \n\n\nHieronder zie je de eerste en laatste data.\n\nmin(steps_weather$date)\n\n[1] \"2015-03-04\"\n\nmax(steps_weather$date)\n\n[1] \"2021-03-15\"\n\n\nHieronder zie je histogrammen van twee variabelen, namelijk dagelijks totale aantal stappen en de gemiddelde temperatuur over deze periode.\n\nhist(steps_weather$daily_total, breaks = 50)\n\n\n\nhist(steps_weather$temp, breaks = 50)"
  },
  {
    "objectID": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#regressie-analyse",
    "href": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#regressie-analyse",
    "title": "Regressie en nog zo iets",
    "section": "Regressie Analyse",
    "text": "Regressie Analyse\nHet doel van deze blogbijdrage is Bayesiaanse regressiemodellering te verkennen met behulp van het rstanarm pakket. Daarom zullen we de gegevens gebruiken om een zeer eenvoudig model te maken en ons te concentreren op het begrijpen van de modelfit en verschillende regressiediagnoses.\nOns model hier is een lineair regressiemodel dat de gemiddelde temperatuur in graden Celsius gebruikt om het totale dagelijkse aantal stappen te voorspellen. We gebruiken het stan_glm commando om de regressie-analyse uit te voeren. We kunnen het model uitvoeren en een samenvatting van de resultaten zien die de volgende tabel oplevert.\nHet draait 4.000 iteraties en daarna worden de resultaten gepresenteerd.\n\nfit_1 <- stan_glm(daily_total ~ temp, data = steps_weather) \n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.473 seconds (Warm-up)\nChain 1:                0.184 seconds (Sampling)\nChain 1:                0.657 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.277 seconds (Warm-up)\nChain 2:                0.174 seconds (Sampling)\nChain 2:                0.451 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.584 seconds (Warm-up)\nChain 3:                0.168 seconds (Sampling)\nChain 3:                0.752 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.884 seconds (Warm-up)\nChain 4:                0.174 seconds (Sampling)\nChain 4:                1.058 seconds (Total)\nChain 4: \n\nsummary(fit_1)\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      daily_total ~ temp\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 2181\n predictors:   2\n\nEstimates:\n              mean    sd      10%     50%     90%  \n(Intercept) 16218.9   276.0 15869.3 16215.8 16580.9\ntemp           26.6    21.0    -0.9    26.4    53.5\nsigma        6199.8    96.4  6082.2  6198.2  6322.6\n\nFit Diagnostics:\n           mean    sd      10%     50%     90%  \nmean_PPD 16519.2   190.8 16279.1 16518.9 16767.0\n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   4.3  1.0  4169 \ntemp          0.3  1.0  4436 \nsigma         1.7  1.0  3317 \nmean_PPD      3.1  1.0  3896 \nlog-posterior 0.0  1.0  1716 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\nDeze tabel bevat de volgende variabelen:\n\nIntercept: Dit cijfer geeft het verwachte dagelijkse aantal stappen weer wanneer de gemiddelde dagtemperatuur 0 is. Met andere woorden, het model voorspelt dat, wanneer de gemiddelde dagtemperatuur 0 graden Celsius is, ik op die dag 16211,7 stappen zal lopen.\ntemp: Dit is de geschatte toename van het dagelijkse aantal stappen per 1 eenheid stijging van de gemiddelde dagelijkse temperatuur in graden Celsius. Met andere woorden, het model voorspelt dat voor elke 1 graad stijging van de gemiddelde dagtemperatuur, ik die dag 26,8 extra stappen zal zetten.\nsigma: Dit is de geschatte standaardafwijking van de residuen van het regressiemodel. (Het residu is het verschil tussen de modelvoorspelling en de waargenomen waarde voor het dagelijkse totale aantal stappen). De verdeling van de residuele waarden heeft een standaardafwijking van 6195,1.\ngeman_PPD: De mean_ppd is de gemiddelde posterior predictive distributie van de door het model geïmpliceerde uitkomstvariabele (we zullen dit verder bespreken in het gedeelte over posterior predictive checques hieronder).\n\nDe uitvoer in de samenvattende tabel hierboven lijkt vrij veel op de uitvoer van een standaard gewone kleinste kwadratenregressie. In de Bayesiaanse benadering van regressiemodellering krijgen we echter niet gewoon puntschattingen van coëfficiënten, maar eerder volledige verdelingen van simulaties die mogelijke waarden van de coëfficiënten vertegenwoordigen, gegeven het model. Met andere woorden, de getallen in de bovenstaande tabel zijn gewoon samenvattingen van verdelingen van coëfficiënten die de relatie tussen de voorspellers en de uitkomstvariabele beschrijven.\nStandaard geven de rstanarm regressiemodellen 4.000 simulaties van de posterior verdeling voor elke modelparameter. We kunnen de simulaties uit het modelobject halen en ze als volgt bekijken:\n\n# extraheer de simulaties van het modelobject\nsims <- as.matrix(fit_1)\n\nEn dat geeft 4000 posterior simulaties van de parameters intercept, temp en sd. Deze simulaties drukken de onzekerheid uit van de modeloutput die je hierboven vindt\n\nhead(sims)\n\n          parameters\niterations (Intercept)      temp    sigma\n      [1,]    16172.03 34.679624 6116.416\n      [2,]    16380.96 22.963727 6173.921\n      [3,]    16054.89 29.856779 6223.247\n      [4,]    16107.86 28.293847 6290.878\n      [5,]    16487.89 18.927277 6172.173\n      [6,]    16692.51  6.882619 6199.710\n\n\nDe gemiddelde waarden van deze verdelingen van simulaties worden weergegeven in de hierboven afgebeelde tabel met regressiesamenvattingsuitvoer.\n\n# gemiddelde en intercept - matchen met de tabel \nmean(sims[,1])\n\n[1] 16218.87\n\nsd(sims[,1])\n\n[1] 275.9854\n\n# gemiddelde en sd temp - matchen met de tabel \nmean(sims[,2])\n\n[1] 26.62242\n\nmedian(sims[,2])\n\n[1] 26.44791\n\nsd(sims[,2])\n\n[1] 21.03643\n\n# gemiddelde en sd sigma - matchen met de tabel \nmean(sims[,3])\n\n[1] 6199.79\n\nsd(sims[,3])\n\n[1] 96.39904"
  },
  {
    "objectID": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#visualiseren-van-de-posterior-distributies-met-bayesplot",
    "href": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#visualiseren-van-de-posterior-distributies-met-bayesplot",
    "title": "Regressie en nog zo iets",
    "section": "Visualiseren van de posterior distributies met bayesplot",
    "text": "Visualiseren van de posterior distributies met bayesplot\nHet uitstekende bayesplot-pakket bevat een aantal handige functies om de posterior distributies van onze coëfficiënten te visualiseren. Laten we de mcmc_areas-functie gebruiken om de 90% geloofwaardige intervallen voor de modelcoëfficiënten weer te geven. die de volgende plot oplevert.\n\ncolor_scheme_set(\"red\")\nplot_title <- ggtitle(\"Posterior distributies\",\n                      \"met medianen en 90% intervallen\")\nmcmc_areas(sims, prob = 0.90) + plot_title\n\n\n\n\nDeze grafiek is zeer interessant en toont ons de posterior distributie van de simulaties van het model dat we hierboven hebben getoond. De plot geeft ons een idee van de variatie van alle parameters, maar de coëfficiënten liggen op zulke verschillende schalen dat de details verloren gaan door ze allemaal samen weer te geven.\nLaten we ons concentreren op de temperatuurcoëfficiënt en een gebiedsplot maken met alleen deze parameter:\n\n# area plot van temperatuur parameter\nmcmc_areas(sims,\n          pars = c(\"temp\"),\n          prob = 0.90) + plot_title\n\n\n\n\nDeze grafiek toont de mediaan van de verdeling (26,69, die zeer dicht bij ons gemiddelde van 26,83 ligt). We kunnen de grenzen van het hierboven getoonde gearceerde gebied bepalen met de posterior_interval functie, of rechtstreeks uit de simulaties zelf:\n\nposterior_interval(fit_1, pars = \"temp\", prob=.9)\n\n           5%      95%\ntemp -8.21517 60.75168\n\n# of rechtstreeks van de posterior distribution\nquantile(sims[,2], probs = c(.05,.95))  \n\n      5%      95% \n-8.21517 60.75168 \n\n\nBeide methoden geven hetzelfde resultaat."
  },
  {
    "objectID": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#visualiseren-van-slopes-van-de-posterior-distributie",
    "href": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#visualiseren-van-slopes-van-de-posterior-distributie",
    "title": "Regressie en nog zo iets",
    "section": "Visualiseren van slopes van de posterior distributie",
    "text": "Visualiseren van slopes van de posterior distributie\nEen andere interessante manier om de verschillende coëfficiënten uit de posterior verdeling te visualiseren is door de regressielijnen van vele simulaties uit de posterior distributie gelijktijdig uit te zetten tegen de ruwe data. Deze visualisatietechniek wordt veel gebruikt in zowel Richard McElreath’s Statistical Rethinking als in Gelmans Regression and Other Stories. Beide boeken maken dit soort tekeningen met behulp van plotfuncties in basis-R. Ik was erg blij deze blog post te vinden met een voorbeeld van hoe je deze plots kan maken met behulp van ggplot2! Ik heb de code lichtjes aangepast om de onderstaande figuur te maken.\nDe eerste stap is het extraheren van de basisinformatie om elke regressielijn te tekenen. We doen dit met de volgende code, waarbij we in essentie ons model-object doorgeven aan een dataframe, en dan enkel het intercept en de temperatuur-hellingen voor elk van onze 4.000 simulaties uit de posterior distributie houden.\nDit geeft het volgende dataframe terug:\n\n# Dit is een data-frame van posterior samples \n# Een rij per sample.\nfits <- fit_1 %>% \n  as_tibble() %>% \n  rename(intercept = `(Intercept)`) %>% \n  select(-sigma)\n# hoe ziet dat dataframe eruit?\nhead(fits)\n\n# A tibble: 6 x 2\n  intercept  temp\n      <dbl> <dbl>\n1    16172. 34.7 \n2    16381. 23.0 \n3    16055. 29.9 \n4    16108. 28.3 \n5    16488. 18.9 \n6    16693.  6.88\n\n\nDit dataframe heeft 4.000 rijen, één voor elke simulatie uit de posterior verdeling in onze originele sims matrix.\nWe stellen dan enkele “esthetische regelaars” in, die specificeren hoeveel lijnen van de posterior verdeling we willen plotten, hun transparantie (de alpha parameter), en de kleuren voor de individuele posterior lijnen en het algemene gemiddelde van de posterior schattingen. De ggplot2 code stelt dan de assen in met het originele data frame (steps_weather), plot een steekproef van regressielijnen uit de posterior verdeling in licht grijs, en plot dan de gemiddelde helling van alle posterior simulaties in blauw.\nDat levert de volgende plot op:\n\n# Eerst de opmaak instellen\nn_draws <- 500\nalpha_level <- .15\ncolor_draw <- \"grey60\"\ncolor_mean <-  \"#3366FF\"\n\n# plot maken\nggplot(steps_weather) + \n  # eerst - eerst de assen van de originele data bepalen\n  aes(x = temp, y = daily_total ) + \n  # restrictie opleggen aan y-as om de focus te leggen op de verschillende slopes in het\n  # centrum van de data\n  coord_cartesian(ylim = c(15000, 18000)) +\n  # Plot een random sample van rijen van  de simulatie\n  # als grijze semi-transparante lijnen\n  geom_abline(\n    aes(intercept = intercept, slope = temp), \n    data = sample_n(fits, n_draws), \n    color = color_draw, \n    alpha = alpha_level\n  ) + \n  # Plot de gemiddelde waarden van onze parameters in blauw\n  # dit correspondeert met de coefficienten die we terugkregen van onze \n  # modelsamenvatting\n  geom_abline(\n    intercept = mean(fits$intercept), \n    slope = mean(fits$temp), \n    size = 1, \n    color = color_mean\n  ) +\n  geom_point() + \n  # definieer de aslabels en de titel van de plot\n  labs(x = 'Gemiddelde dagelijkse temperatuur (Graden Celsius)', \n       y = 'Dagelijkse totale aantal stappen' , \n       title = 'Visualisatie of 500 regressie lijnnen van de posterior eistributie')\n\n\n\n\nDe gemiddelde helling (weergegeven in de grafiek en ook terug te vinden in de modelsamenvatting hierboven) van de temperatuur is 26,8. Maar het plotten van samples uit de posterior verdeling maakt duidelijk dat er nogal wat onzekerheid is over de grootte van deze relatie! Sommige van de hellingen uit de verdeling zijn negatief - zoals we zagen in onze berekening van de onzekerheidsintervallen hierboven. In essentie is er een “gemiddelde” coëfficiëntschatting, maar wat het Bayesiaanse raamwerk heel goed doet (via de posterior verdelingen) is extra informatie verschaffen over de onzekerheid van onze schattingen."
  },
  {
    "objectID": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#posterior-voorspellingscontroles",
    "href": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#posterior-voorspellingscontroles",
    "title": "Regressie en nog zo iets",
    "section": "Posterior voorspellingscontroles",
    "text": "Posterior voorspellingscontroles\nEen laatste manier om grafieken te gebruiken om ons model te begrijpen is door gebruik te maken van posterior predictive checks. Ik hou van deze intuïtieve manier om de logica achter deze reeks technieken uit te leggen: “Het idee achter posterior predictive checking is simpel: als een model een goede fit is, moeten we het kunnen gebruiken om gegevens te genereren die veel lijken op de gegevens die we hebben waargenomen. De gegenereerde gegevens worden de posterior predictive distributie genoemd, dat is de verdeling van de uitkomstvariabele (dagelijks totaal aantal stappen in ons geval) die wordt geïmpliceerd door een model (het regressiemodel dat hierboven is gedefinieerd). Het gemiddelde van deze verdeling wordt weergegeven in de bovenstaande uitvoer van het regressieoverzicht, met de naam mean_PPD.\nEr zijn vele soorten visualisaties die men kan maken om posterieure voorspellende controles uit te voeren. Wij zullen één zo’n analyse uitvoeren (voor meer informatie over dit onderwerp), die het gemiddelde van onze uitkomstvariabele (dagelijks totaal aantal stappen) in onze oorspronkelijke dataset en de posterior predictive distributie van ons regressie model.\nDe code is rechttoe rechtaan.\n\n# posterior predictive checks\n# https://mc-stan.org/bayesplot/reference/pp_check.html\n# http://mc-stan.org/rstanarm/reference/pp_check.stanreg.html\n\n# Het idee achter posterior predictive checking is eenvoudig: als een model een goede  \n# fit heeft dan moeten we data kunnen genereren die erg lijken op de data die we hebben geobserveerd.\n#  Om die data te genereren voor posterior predictive checks (PPCs), simuleren we die van de posterior predictive distributie. \n\n# posterior predictive check - voor meer informatie zie:\n# https://mc-stan.org/bayesplot/reference/pp_check.html\n# http://mc-stan.org/rstanarm/reference/pp_check.stanreg.html\npp_check(fit_1, \"stat\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nWe kunnen zien dat het gemiddelde van onze dagelijkse stappen variabele in de originele dataset in principe in het midden van de posterior voorspellende distributie valt. Volgens deze analyse “genereert ons regressiemodel gegevens die veel lijken op de gegevens die we hebben geobserveerd!”"
  },
  {
    "objectID": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#het-model-gebruiken-om-voorspellingen-te-doen-met-nieuwe-gegevens",
    "href": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#het-model-gebruiken-om-voorspellingen-te-doen-met-nieuwe-gegevens",
    "title": "Regressie en nog zo iets",
    "section": "Het model gebruiken om voorspellingen te doen met nieuwe gegevens",
    "text": "Het model gebruiken om voorspellingen te doen met nieuwe gegevens\nTenslotte zullen we het model gebruiken om voorspellingen te doen over het aantal stappen per dag, gebaseerd op een specifieke waarde van de gemiddelde dagtemperatuur. In Regression and Other Stories bespreken de auteurs in hoofdstuk 9 hoe een Bayesiaans regressiemodel kan worden gebruikt om voorspellingen te doen op een aantal verschillende manieren, waarbij telkens verschillende niveaus van onzekerheid in de voorspellingen worden opgenomen. Wij zullen elk van deze methoden achtereenvolgens toepassen.\nVoor elk van de onderstaande methoden zullen we het gemiddelde aantal stappen per dag voorspellen wanneer de temperatuur 10 graden Celsius bedraagt. We kunnen een nieuw dataframe opzetten dat we zullen gebruiken om de modelvoorspellingen te verkrijgen:\n\n# Gebruik het model om voorspellingen te doen\n# defineer nieuwe data van waaruit we deze voorspellingen maken \n# we doen voorspellingen voor het geval de gemiddelde dagtemperatuur 10 graden Celsius is for when the average daily temperature \nnew <- data.frame(temp = 10)"
  },
  {
    "objectID": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#puntvoorspellingen-met-behulp-van-de-samenvattingen-van-de-afzonderlijke-waardecoëfficiënten-van-de-posterieure-verdelingen",
    "href": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#puntvoorspellingen-met-behulp-van-de-samenvattingen-van-de-afzonderlijke-waardecoëfficiënten-van-de-posterieure-verdelingen",
    "title": "Regressie en nog zo iets",
    "section": "Puntvoorspellingen met behulp van de samenvattingen van de afzonderlijke waardecoëfficiënten van de posterieure verdelingen",
    "text": "Puntvoorspellingen met behulp van de samenvattingen van de afzonderlijke waardecoëfficiënten van de posterieure verdelingen\nDe eerste benadering komt overeen met die welke we zouden gebruiken bij een klassieke regressieanalyse. We gebruiken gewoon de puntschattingen uit de modelsamenvatting, voegen de nieuwe temperatuur in waarvoor we een voorspelling willen, en produceren onze voorspelling in de vorm van een enkel getal. We kunnen dit doen met de predict functie in R, of door de coëfficiënten uit onze modelsamenvatting te vermenigvuldigen. Beide methoden leveren dezelfde voorspelling op:\n\n# gebruik simpel de puntsamenvatting van de posterio distributie \n# voor de modelcoefficienten (van de modelsamenvatting van hierboven)\ny_point_est <- predict(fit_1, newdata = new)\n# zelfde predictie \"met de hand\"\ny_point_est_2 <- mean(sims[,1]) + mean(sims[,2])*new\n\nBeide leveren een puntvoorspelling van 16483.71.\n\n# ze zijn hetzelfde \npredict(fit_1, newdata = new)\n\n      1 \n16485.1 \n\nmean(sims[,1]) + mean(sims[,2])*new\n\n     temp\n1 16485.1"
  },
  {
    "objectID": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#lineaire-voorspellingen-met-onzekerheid-in-de-interceptie-temperatuurcoëfficiënten",
    "href": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#lineaire-voorspellingen-met-onzekerheid-in-de-interceptie-temperatuurcoëfficiënten",
    "title": "Regressie en nog zo iets",
    "section": "Lineaire voorspellingen met onzekerheid (in de interceptie + temperatuurcoëfficiënten)",
    "text": "Lineaire voorspellingen met onzekerheid (in de interceptie + temperatuurcoëfficiënten)\nWe kunnen echter genuanceerder zijn in onze voorspelling van het dagelijkse totale aantal stappen. Het hierboven berekende regressiemodel geeft 4.000 simulaties voor drie parameters - de intercept, de temperatuurcoëfficiënt, en sigma (de standaardafwijking van de residuen).\nDe volgende methode is geïmplementeerd in rstanarm met de posterior_linpred functie, en we kunnen deze gebruiken om de voorspellingen direct te berekenen. We kunnen hetzelfde resultaat ook “met de hand” berekenen met behulp van de matrix van simulaties uit de posterior verdeling van onze coëfficiëntschattingen. Bij deze aanpak wordt gewoon de temperatuur ingevoerd waarvoor wij voorspellingen willen (10 graden Celsius) en wordt voor elk van de simulaties het intercept opgeteld bij de temperatuurcoëfficiënt maal 10. Beide methoden leveren dezelfde vector van 4.000 voorspellingen op:\n\n# lineaire predictor met onzekerheid met gebruikmaking van posterior_linpred\n\ny_linpred <- posterior_linpred(fit_1, newdata = new)\n# uitrekenen \"met de hand\" \n# we gebruiken de sims matrix die we hierboven definieerden \n# sims <- as.matrix(fit_1)\ny_linpred_2 <- sims[,1] + (sims[,2]*10)  \n\nDat geeft dezelfde resultaten.\n\n# check - ze zijn hetzelfde!\nplot(y_linpred,y_linpred_2)\n\n\n\ncor.test(y_linpred, y_linpred_2)\n\n\n    Pearson's product-moment correlation\n\ndata:  y_linpred and y_linpred_2\nt = Inf, df = 3998, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 1 1\nsample estimates:\ncor \n  1"
  },
  {
    "objectID": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#posterior-predictive-distributies-met-de-onzekerheid-in-de-coëfficiëntschattingen-en-in-sigma",
    "href": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#posterior-predictive-distributies-met-de-onzekerheid-in-de-coëfficiëntschattingen-en-in-sigma",
    "title": "Regressie en nog zo iets",
    "section": "Posterior Predictive Distributies met de onzekerheid in de coëfficiëntschattingen en in sigma",
    "text": "Posterior Predictive Distributies met de onzekerheid in de coëfficiëntschattingen en in sigma\nDe laatste voorspellingsmethode voegt nog een extra laag van onzekerheid toe aan onze voorspellingen, door de posterior verdelingen voor sigma mee te nemen in de berekeningen. Deze methode is beschikbaar via de functie posterior_predict, en we gebruiken opnieuw onze matrix van 4.000 simulaties om een vector van 4.000 voorspellingen te berekenen.\nDe posterior predict methode volgt de aanpak van de posterior_linpred functie hierboven, maar voegt een extra foutterm toe gebaseerd op onze schattingen van sigma, de standaardafwijking van de residuen. De berekening zoals getoond in het “met de hand” gedeelte van de code hieronder maakt. Het maakt duidelijk waar de willekeurigheid in het spel komt, en vanwege deze willekeurigheid zullen de resultaten van de posterior_predict functie en de “met de hand” berekening niet overeenkomen tenzij we dezelfde seed instellen voordat we elke berekening uitvoeren. Beide methoden leveren een vector van 4.000 voorspellingen op.\n\n# predictive distributie voor een nieuwe observatie met gebruik van posterior_predict\n\nset.seed(1)\ny_post_pred <- posterior_predict(fit_1, newdata = new)\n\nOf\n\n# uitrekenen \"met de hand\"\nn_sims <- nrow(sims)\nsigma <- sims[,3]\nset.seed(1)\ny_post_pred_2 <- as.numeric(sims[,1] + sims[,2]*10) + rnorm(n_sims, 0, sigma)\n\nDan zien we dezelfde resultaten:\n\n# check - ze zijn hetzelfde!\nplot(y_post_pred, y_post_pred_2)\n\n\n\ncor.test(y_post_pred, y_post_pred_2)\n\n\n    Pearson's product-moment correlation\n\ndata:  y_post_pred and y_post_pred_2\nt = Inf, df = 3998, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 1 1\nsample estimates:\ncor \n  1"
  },
  {
    "objectID": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#visualisatie-van-de-drie-soorten-voorspellingen",
    "href": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#visualisatie-van-de-drie-soorten-voorspellingen",
    "title": "Regressie en nog zo iets",
    "section": "Visualisatie van de drie soorten voorspellingen",
    "text": "Visualisatie van de drie soorten voorspellingen\nLaten we een visualisatie maken die de resultaten weergeeft van de voorspellingen die we hierboven deden. We kunnen een enkele cirkel gebruiken om de puntvoorspelling van de regressiecoëfficiënten weer te geven in de modelsamenvatting, en histogrammen om de posterior verdelingen weer te geven die geproduceerd zijn door de lineaire voorspelling met onzekerheid (posterior_linpred) en posterior predictive distribution (posterior_predict) methoden die hierboven beschreven zijn.\nWe zetten eerst de vectoren van posterior verdelingen die we hierboven hebben gemaakt in een dataframe. We maken ook een dataframe met de enkele puntvoorspelling van onze lineaire voorspelling. Vervolgens stellen we ons kleurenpalet in (afkomstig uit het NineteenEightyR pakket) en maken dan de plot:\n\n# creeer een dataframe die de waarden van de posterior distributies omvat \n# van de voorspellingen van de totaal aantal dagelijkse stappen bij 10 grade Celcius\npost_dists <- as.data.frame(rbind(y_linpred, y_post_pred)) %>% \n      setNames(c('prediction'))\npost_dists$pred_type <- c(rep('posterior_linpred', 4000),\n                          rep('posterior_predict', 4000))\ny_point_est_df = as.data.frame(y_point_est)\n\nDat geeft de volgende plot:\n\n# 70 en meer kleuren - via NineteenEightyR pakket\n# https://github.com/m-clark/NineteenEightyR\npal <- c('#FEDF37', '#FCA811', '#D25117', '#8A4C19', '#573420')\n\nggplot(data = post_dists, aes(x = prediction, fill = pred_type)) + \n  geom_histogram(alpha = .75, position=\"identity\") + \n  geom_point(data = y_point_est_df,\n             aes(x = y_point_est,\n                 y = 100,\n                 fill = 'Linear Point Estimate'),\n             color =  pal[2],\n             size = 4,\n             # alpha = .75,\n             show.legend = F) +\n  scale_fill_manual(name = \"Prediction Method\",\n                    values = c(pal[c(2,3,5)]),\n                    labels = c(bquote(paste(\"Lineaire Punt Schatting \", italic(\"(predict)\"))),\n                               bquote(paste(\"Lineaire Voorspelling met Onzekerheid \" , italic(\"(posterior_linpred)\"))),\n                               bquote(paste(\"Posterior Predictive Distributie \",  italic(\"(posterior_predict)\"))))) +\n  # set the plot labels and title\n  labs(x = \"Predicted Daily Total Step Count\", \n       y = \"Aantal\", \n       title = 'Onzekerheid in Posterior Predictie Methode')   +\n  theme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nDeze grafiek is zeer informatief en maakt duidelijk hoe groot de onzekerheid is die we voor elk van onze voorspellingsmethoden krijgen. Hoewel alle drie voorspellingsmethoden op dezelfde plaats op de x-as gecentreerd zijn, verschillen zij sterk wat betreft de onzekerheid rond de voorspellingsramingen.\nDe puntvoorspelling is een enkele waarde en bevat als zodanig geen onzekerheid. De lineaire voorspelling met onzekerheid, die rekening houdt met de posterior verdeling van onze interceptie- en temperatuurcoëfficiënten, heeft een zeer scherpe piek, waarbij de modelschattingen binnen een relatief klein bereik variëren. De posterior predictive distributie varieert veel meer, met het laagste bereik van de verdeling onder nul, en het hoogste bereik van de verdeling boven 40.000!"
  },
  {
    "objectID": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#samenvatting-en-conclusie",
    "href": "posts/2022-02-11-regressie-en-zo/regressie-en-zo.html#samenvatting-en-conclusie",
    "title": "Regressie en nog zo iets",
    "section": "Samenvatting en conclusie",
    "text": "Samenvatting en conclusie\nIn dit artikel hebben we een eenvoudig model gemaakt met behulp van het rstanarm pakket in R, om te leren over Bayesiaanse regressie analyse. We gebruikten een dataset bestaande uit mijn geschiedenis van dagelijkse totale stappen, en bouwden een regressie model om het dagelijkse aantal stappen te voorspellen uit de dagelijkse gemiddelde temperatuur in graden Celsius. In tegenstelling tot de gewone kleinste kwadraten benadering die puntschattingen van modelcoëfficiënten oplevert, geeft de Bayesiaanse regressie posterior verdelingen van de coëfficiëntschattingen. Wij hebben een aantal verschillende samenvattingen en visualisaties van deze posterior verdelingen gemaakt om de coëfficiënten en de Bayesiaanse benadering in het algemeen te begrijpen - A) het gebruik van het bayesplot pakket om de posterior verdelingen van onze coëfficiënten te visualiseren\nB) het plotten van 500 hellingen van de posterior verdeling, en\nC) het uitvoeren van een controle van de posterior predictive distributie."
  },
  {
    "objectID": "posts/2022-03-22-bayesiaanse-principes/bayes-principes.html",
    "href": "posts/2022-03-22-bayesiaanse-principes/bayes-principes.html",
    "title": "Bayes’ principes",
    "section": "",
    "text": "Bayes Rules!"
  },
  {
    "objectID": "posts/2022-03-22-bayesiaanse-principes/bayes-principes.html#inleiding",
    "href": "posts/2022-03-22-bayesiaanse-principes/bayes-principes.html#inleiding",
    "title": "Bayes’ principes",
    "section": "Inleiding",
    "text": "Inleiding\nOnlangs verscheen een prachtig boek van Alicia A. Johnson, Miles Q. Ot en Mine Dogucu onder de titel Bayes Rules! An Introduction to Applied Bayesian Modeling en het verscheen bij CRC Press (2022). Eerdere versies kon je al via bookdown bekijken (https://www.bayesrulesbook.com/) en vanaf de eerste keer dat ik het zag, was ik hier heel enthousiast over. Het boek heb ik direct besteld en vorige week kon ik het ophalen.\nHet boek bestaat uit vier duidelijke delen. Het eerste deel gaat in op de fundamenten van het Bayesiaanse perspectief. Het leert je denken als een Bayesiaan en het gaat in op die belangrijke Bayesiaanse regel \\(posterior=\\frac{prior.likelihood}{normaliserende constante}\\). Aan de hand van enkele voorbeelden gaan Johnson e.a. in op hoe het in de praktijk werkt. Daarna gaat het in op hoe kennis en data op elkaar inwerken en het laat enkele basisanalyses zien en hoe dat in deze vorm van statistiek werkt (normaal, binair en poisson). Het tweede deel is een meer technisch hoofdstuk en laat je ook onder de moterkap van deze techniek kijken. Het gaat ook op de wetenschappelijke principes van de benadering, waar je hier op moet letten, hoe je hiermee hypothesen kunt testen (niet alleen tov van een nulhypothese, maar hoeveel beter de ene hypothese is ten opzichte van de andere hyposthese) en hoe je hiermee ook kunt voorspellen. De twee volgende delen (Deel drie en vier) zijn praktische delen. Deel drie gaat in op regressieanalyses voor continue variabelen en classificatieanalyses voor binaire variabelen. Het vierde deel, ten slotte, gaat in op geclusterde datasets en hoe je hierarchische Bayesiaanse regressie en classificatieanalyses uitvoert.\nNatuurlijk, er zijn onderhand al verschillende interessante boeken te krijgen die je laten zien hoe Bayesiaanse denken in de praktijk kan werken. De boeken van Gelman, McElreath, Spiegelhalter en Kruschke verschenen de afgelopen tien/vijftien jaar en leren je dit. Maar Bayes Rules! vind ik op dit moment als introductieboek mogelijk wel het beste.\nNu het boek bij mij op het bureau ligt, kan ik er binnenkort een keer een korte recensie over schrijven. Voor nu heb ik uit elk deel een hoofdstuk genomen en het vertaald en bewerkt. Hieronder zie je een bewerking van het het vierde hoofdstuk van het eerste deel (Balance and Sequentiallity in Bayesian Analysis).Deze hoofdstukken zo overzetten is voor mij niet alleen een goede manier om het mij eigen te maken, maar ook een manier om het boek anderen aan te raden. Dus lezen en gebruiken deze Bayes Rules! An Introduction to Applied Bayesian Modeling"
  },
  {
    "objectID": "posts/2022-03-22-bayesiaanse-principes/bayes-principes.html#balans-en-opeenvolging-in-bayesiaanse-analyses",
    "href": "posts/2022-03-22-bayesiaanse-principes/bayes-principes.html#balans-en-opeenvolging-in-bayesiaanse-analyses",
    "title": "Bayes’ principes",
    "section": "Balans en opeenvolging in Bayesiaanse Analyses",
    "text": "Balans en opeenvolging in Bayesiaanse Analyses\nIn Alison Bechdels stripverhaal The Rule uit 1985 zegt een personage dat ze alleen naar een film gaan als die aan de volgende drie regels voldoet (Bechdel 1986):\n\ner moeten minstens twee vrouwen in de film voorkomen;\n\ndeze twee vrouwen praten met elkaar; en\n\nze praten over iets anders dan een man.\nDeze criteria vormen de Bechdel-test voor de vertegenwoordiging van vrouwen in films. Als je aan films denkt die je hebt gezien, welk percentage van alle recente films slaagt dan volgens jou voor de Bechdel-test? Ligt dat dichter bij 10%, 50%, 80%, of 100%?\n\nLaat \\(\\pi\\), een willekeurige waarde tussen 0 en 1, het onbekende aandeel van recente films zijn die de Bechdel test doorstaan. Drie vrienden - de feminist, de onwetende, en de optimist - hebben enkele vooroordelen over \\(\\pi\\). Nadenkend over films die zij in het verleden heeft gezien, begrijpt de feminist dat het in de meeste films ontbreekt aan sterke vrouwelijke personages. De onwetende herinnert zich niet echt de films die hij gezien heeft, en weet dus niet zeker of het halen van de Bechdel test gebruikelijk of ongewoon is. De optimist ten slotte denkt dat de Bechdel-test een erg lage lat is voor de vertegenwoordiging van vrouwen in films, en gaat er dus van uit dat bijna alle films de test doorstaan. Dit alles om te zeggen dat drie vrienden drie verschillende voorafgaande modellen hebben van \\(\\pi\\). Geen probleem! Een Beta kan voorafgaand aan het opstellen van een model worden afgestemd op iemands voorkennis (zie de figuur hieronder).\nDoor de grootste aannemelijkheid vooraf te geven aan de waarden van \\(\\pi\\) die lager zijn dan 0,5, weerspiegelt de Beta(5,11) prior het inzicht van de feminist: de meerderheid van de films doorstaat de Bechdel test niet. De Beta(14,1) daarentegen plaatst een grotere aannemelijkheid vooraf op waarden van \\(\\pi\\) in de buurt van 1, en komt dus overeen met de vooronderstelling van de optimist. Dan blijft de Beta(1,1) of Unif(0,1) over die, door een gelijke aannemelijkheid te geven aan alle waarden van \\(\\pi\\) tussen 0 en 1, dat overeenkomt met het figuurlijke schouderophalen van de onwetende - het enige dat zij weet is dat \\(\\pi\\) een proportie is, en dus ergens tussen 0 en 1 ligt.\nDe drie analisten komen overeen een steekproef te nemen van \\(n\\) recente films en noteren \\(Y\\), het aantal dat de Bechdel test doorstaat. Herkenning van \\(Y\\) als het aantal “successen” in een vast aantal onafhankelijke proeven, specificeren zij de afhankelijkheid van \\(Y\\) van \\(\\pi\\) met behulp van een binomiaal model. Elke analist heeft dus een uniek Beta-Binomiaal model van \\(\\pi\\) met verschillende voorafgaande hyperparameters\n\\(\\alpha\\) en \\(\\beta\\):\n\\[Y|\\pi \\sim (Bin(n, \\pi) \\\\\n  \\pi \\sim Beta(\\alpha, \\beta)\\]\nWe weten dat elke analist een uniek posterior model heeft van \\(\\pi\\) dat afhangt van zijn of haar unieke prior (via \\(\\alpha\\) en \\(\\beta\\)) en de gemeenschappelijke waargenomen gegevens (via \\(Y\\) en \\(n\\))\n\\[\\pi|(Y=y) \\sim Beta (\\alpha + y, \\beta +n - Y)\\]\nAls je denkt “Kan iedereen zijn eigen voorkeuren hebben?! Zal dit altijd zo subjectief zijn?!” dan stel je de juiste vragen! En de vragen houden daar niet op. In hoeverre zouden hun verschillende priors van de analisten kunnen leiden tot drie verschillende posterior conclusies over de Bechdel test? Hoe zou dit kunnen afhangen van de steekproefgrootte en de uitkomsten van de filmgegevens die ze verzamelen? In hoeverre zullen de posterior inzichten van de analisten evolueren naarmate ze meer en meer gegevens verzamelen? Zullen ze het ooit eens worden over de vertegenwoordiging van vrouwen in film? We zullen deze fundamentele vragen hier onderzoeken en wij kijken naar ons vermogen om het Bayesiaanse denken verder te ontwikkelen.\nDus:\n- We onderzoek de evenwichtige invloed van de prior en data op de posterior.\n- We voeren verschillende achtereenvolgende Bayesiaanse analyses uit.\n\n# Laten we eerst de pakketten openen die we in dit hoofstuk zullen gebruiken (wel eerst binnenhalen)\nlibrary(bayesrules)\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\n-- Attaching packages --------------------------------------- tidyverse 1.3.1 --\n\n\nv ggplot2 3.3.6     v purrr   0.3.4\nv tibble  3.1.7     v dplyr   1.0.9\nv tidyr   1.2.0     v stringr 1.4.1\nv readr   2.1.2     v forcats 0.5.1\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'stringr' was built under R version 4.1.3\n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\n\n\nVerschillende priors, verschillende posteriors\nDe voorafgaande modellen van \\(\\pi\\) (het percentage recente films dat de Bechdel test doorstaat) zijn afgestemd op de onwetende, de feministe, en de optimist. Niet alleen weerspiegelen de verschillende prior gemiddelden onenigheid over de vraag of \\(\\pi\\) dichter bij 0 of 1 ligt, maar ook weerspiegelen hun verschillende niveaus van variabiliteit het feit dat de drie analisten niet allemaal even zeker zijn van hun kennis en informatie. Hoe zekerder ze daarover zijn, hoe kleiner de voorafgaande variabiliteit. Maar ook, hoe vager de prior-informatie, hoe groter de prior-variabiliteit. De priors van de optimist en de onwetende vertegenwoordigen hier twee uitersten. Een Beta(14,1) prior vertoont de kleinste variabiliteit en daarmee is de optimist het meest zeker in zijn prioriteitsbegrip van \\(\\pi\\) (of om duidelijker te zijn, dat bijna alle films de Bechdel test zullen doorstaan). Dergelijke priors noemen we informatief.\n\nEen informatieve prior reflecteert specifieke informatie over de onbekende variabele met grote zekerheid, bv. lage variabiliteit.\n\nMet de grootste prioriteitsvariabiliteit is de onwetende het minst zeker over \\(\\pi\\). In feite kent deze Beta(1,1) prior evenveel prior plausibiliteit toe aan elke waarde van \\(\\pi\\) tussen 0 en 1. Dit type prior model van “schouderophalen” (van “ik weet het echt niet”) heeft een officiële naam: het is een vage prior.\n\nEen vage of diffuse prior reflecteert weinig specifieke informatie over de onbekende variabele. Een vlakke prior die gelijke plausibiliteit toekent aan alle mogelijke waarden van de variabele, is een speciaal geval.\n\nDe volgende voor de hand liggende vraag is dan: hoe zullen hun verschillende priors de posterior conclusies van de feminist, de onwetende en de optimist beïnvloeden? Om deze vraag te beantwoorden, hebben we gegevens nodig. Onze analisten besluiten een willekeurige steekproef te nemen van \\(n=20\\) recente films te bekijken, gebruikmakend van gegevens die verzameld zijn voor het FiveThirtyEight-artikel over de Bechdel-test Het bayesrules-pakket bevat een gedeeltelijke versie van deze dataset, genaamd bechdel. Een volledige versie wordt geleverd door het fivethirtyeight R-pakket (Kim, Ismay, and Chunn 2020). Samen met de titel en het jaar van elke film in deze dataset, geeft de binaire variabele aan of de film slaagde of niet voor de Bechdel test:\n\n# Importeer data\ndata(bechdel, package = \"bayesrules\")\n\n# Neem een sample van 20 films\nset.seed(84735)\nbechdel_20 <- bechdel %>% \n  sample_n(20)\n\nbechdel_20 %>% \n  head(3)\n\n# A tibble: 3 x 3\n   year title      binary\n  <dbl> <chr>      <chr> \n1  2005 King Kong  FAIL  \n2  1983 Flashdance PASS  \n3  2013 The Purge  FAIL  \n\n\nVan de 20 films in deze steekproef slaagden er slechts 9 (45%) voor de test:\n\nbechdel_20 %>% \n  tabyl(binary) %>% \n  adorn_totals(\"row\")\n\n binary  n percent\n   FAIL 11    0.55\n   PASS  9    0.45\n  Total 20    1.00\n\n\nDe posterior modellen van de drie analisten voor\\(\\pi\\) die volgen uit de formule \\(\\pi|(Y=y) \\sim Beta(\\alpha+y, \\beta+n-y)\\) op hun unieke voorgaande modellen en gemeenschappelijke filmgegevens, zijn samengevat in tabel hieronder en de figuur hierboven. Bijvoorbeeld, de posterior parameters van de feminist worden berekend door \\(\\alpha+y=5+9=14\\) en \\(\\beta+n-y=11+20-9=22\\)\n\n\n\nAnalyst\nPrior\nPosterior\n\n\n\n\nfeminist\nBeta(5,11)\nBeta(14,22)\n\n\nonwetende\nBeta(1,1)\nBeta(10,12)\n\n\noptimist\nBeta(14,1)\nBeta(23,12)\n\n\n\nHad je instinct gelijk? Herinner je dat de optimist begon met het vasthoudend optimistisme vooraf over \\(\\pi\\) - zijn prior model had een hoog gemiddelde met lage variabiliteit. Het is dan ook niet verwonderlijk dat zijn posterior model niet zo synchroon loopt met de gegevens als de posteriors van de andere analisten. De trieste gegevens waarbij slechts 45% van de 20 films de test doorstonden was niet genoeg om hem ervan te overtuigen dat er een probleem is in Hollywood - hij denkt nog steeds dat de waarden van \\(\\pi\\) boven 0,5 het meest plausibel is. Aan het andere uiterste staat de onwetende die begon met een vlak, vaag model van \\(\\pi\\). Zonder enige voorinformatie weerspiegelt zijn posterior model direct de inzichten die zijn verkregen uit de waargenomen filmgegevens. In feite is zijn posterior niet te onderscheiden van de geschaalde likelihood functie.\n\n\n\nPosterior models"
  },
  {
    "objectID": "posts/2022-03-22-bayesiaanse-principes/bayes-principes.html#verschillende-data-verschillende-posteriors",
    "href": "posts/2022-03-22-bayesiaanse-principes/bayes-principes.html#verschillende-data-verschillende-posteriors",
    "title": "Bayes’ principes",
    "section": "Verschillende data, verschillende posteriors",
    "text": "Verschillende data, verschillende posteriors\nAls jij je zorgen maakt over het feit dat onze drie analisten achteraf verschillende opvattingen hebben over \\(\\pi\\), de proportie van recente films die de Bechdel halen, wanhoop dan nog niet. Vergeet niet dat data ook een rol spelen in een Bayesiaanse analyse. Om deze dynamiek te onderzoeken, beschouwen we drie nieuwe analisten - Morteza, Nadide, en Ursula - die allen de optimistische Beta(14,1) prior voor \\(\\pi\\) delen maar elk toegang hebben tot andere gegevens. Morteza beoordeelt \\(n=13\\) films uit het jaar 1991, waarvan \\(Y=6\\) (ongeveer 46%) door de Bechdel komen:\n\nbechdel %>% \n  filter(year == 1991) %>% \n  tabyl(binary) %>% \n  adorn_totals(\"row\")\n\n binary  n   percent\n   FAIL  7 0.5384615\n   PASS  6 0.4615385\n  Total 13 1.0000000\n\n\nNadide beoordeelt \\(n=63\\) films uit 2000, waaronder \\(Y=29\\) (ongeveer 46%) door de Bechdel komen:\n\nbechdel %>% \n  filter(year == 2000) %>% \n  tabyl(binary) %>% \n  adorn_totals(\"row\")\n\n binary  n   percent\n   FAIL 34 0.5396825\n   PASS 29 0.4603175\n  Total 63 1.0000000\n\n\nTot slot, beoordeelt Ursula \\(n=99\\) films uit 2013, waarvan er \\(Y=46\\) (ongeveer 46%) door de Bechdel komen:\n\nbechdel %>% \n  filter(year == 2013) %>% \n  tabyl(binary) %>% \n  adorn_totals(\"row\")\n\n binary  n   percent\n   FAIL 53 0.5353535\n   PASS 46 0.4646465\n  Total 99 1.0000000\n\n\nWat een toeval! Hoewel Morteza, Nadide, en Ursula verschillende gegevens hebben verzameld, constateren ze elk een Bechdel slaagpercentage van ongeveer 46%. Toch is hun steekproefomvang \\(n\\) verschillend - Morteza bekeek slechts 13 films terwijl Ursula er 99 bekeek.\nDe posterior modellen van de drie analisten voor \\(\\pi\\) die volgen uit de toepassing van hun gegevens op hun gemeenschappelijke Beta(14,1) voormodel en unieke filmgegevens, zijn samengevat in de figuur en tabel hieronder. Merk op dat hoe groter de steekproefgrootte \\(n\\) hoe “indringender” de likelihoodfunctie. Bijvoorbeeld, de waarschijnlijkheidsfunctie die het slagingspercentage van 46% in Morteza’s kleine steekproef van 13 films weergeeft is vrij breed - deze gegevens zijn relatief plausibel voor elke \\(\\pi\\) tussen 15% en 75%. Ursula’s waarschijnlijkheidsfunctie daarentegen, met het slagingspercentage van 46% in een veel grotere steekproef van 99 films weergeeft, is smal - haar gegevens zijn onwaarschijnlijk voor \\(\\pi\\) waarden buiten het bereik van 35% tot 55%. We zien dat hoe groter de waarschijnlijkheid, hoe meer invloed de gegevens hebben op de posterior. Morteza blijft het minst overtuigd door het lage Bechdel-slaagpercentage dat in zijn kleine steekproef wordt waargenomen, terwijl Ursula het meest overtuigd is. Haar vroege optimisme evolueerde naar een posterior begrip dat \\(\\p\\) waarschijnlijk tussen 40% en 55% ligt.\n\n\n\nPosterior models\n\n\nTabel: De prior en posterior modellen voor \\(\\pi\\) zijn geconstrueerd in het licht van een gemeenschappelijke Beta(14,1)-prior en verschillende gegevens.\n\n\n\nAnalyst\nData\nPosterior\n\n\n\n\nMorteza\n\\(Y=6;n=13\\)\nBeta(20,8)\n\n\nNadide\n\\(Y=26;n=63\\)\nBeta(43,35)\n\n\nUrsula\n\\(Y=46;n=99\\)\nBeta(60,54)"
  },
  {
    "objectID": "posts/2022-03-22-bayesiaanse-principes/bayes-principes.html#het-vinden-van-de-balans-tussen-de-prior-en-de-data",
    "href": "posts/2022-03-22-bayesiaanse-principes/bayes-principes.html#het-vinden-van-de-balans-tussen-de-prior-en-de-data",
    "title": "Bayes’ principes",
    "section": "Het vinden van de balans tussen de prior en de data",
    "text": "Het vinden van de balans tussen de prior en de data\n\nObservaties aan concepten verbinden\nWe hebben gekeken naar de invloed die verschillende priors en verschillende gegevens kunnen hebben op ons posterior begrip van een onbekende variabele. De posterior is echter een meer genuanceerd touwtrekken tussen deze twee kanten. De figuren hieronder illustreren het evenwicht dat het posterior model vindt tussen de prior en de gegevens. Elke rij komt overeen met een uniek prior model en elke kolom met een unieke reeks gegevens.\nVan links naar rechts neemt de steekproefgrootte toe van \\(n=13\\) tot \\(n=99\\) films met behoud van het aandeel films dat de Bechdel-test doorstaat: \\(Y/n\\sim0.46\\) . De waarschijnlijkheid en, dienovereenkomstig, de invloed van de gegevens op de posterior nemen toe met de steekproefgrootte \\(n\\). Dit betekent ook dat de invloed van onze voorkennis afneemt naarmate we nieuwe gegevens vergaren. Verder hangt de snelheid waarmee de posterior balans doorslaat in het voordeel van de gegevens af van de prior. Van boven naar beneden over het rooster gaan de priors van informatief (Beta(14,1)) naar vaag (Beta(1,1)). Het spreekt vanzelf dat hoe informatiever de prior is, hoe groter zijn invloed op de posterior is.\nDoor deze waarnemingen te combineren levert de laatste kolom in het rooster een zeer belangrijke Bayesiaanse clou op: ongeacht de sterkte van en discrepanties tussen hun prioriteitsbegrip van \\(\\pi\\) zullen drie analisten tot een gemeenschappelijke posterior interpretatie komen in het licht van sterke gegevens. Deze vaststelling is een opluchting. Als Bayesiaanse modellen niets zouden betekenen in het licht van steeds meer gegevens, zouden we een probleem hebben.\nMet dit soort gegevens kun je spelen en kijken naar de rol die de prior en de data spelen in een posterior analyse, gebruik je de plot_beta_binomial() en summarize_beta_binomial() functies in het bayesrules pakket om het Beta-Binomial posterior model van \\(\\pi\\) onder verschillende combinaties van voorafgaande Beta(\\(\\alpha, \\beta\\)) modellen en waargenomen gegevens, \\(Y\\) successen in \\(n\\) proeven:\n# Plot the Beta-Binomial model\nplot_beta_binomial(alpha = ___, beta = ___, y = ___, n = ___)\n\n# Obtain numerical summaries of the Beta-Binomial model\nsummarize_beta_binomial(alpha = ___, beta = ___, y = ___, n = ___)\n\n\nConcepten met theorie verbinden\nDe patronen die we hebben waargenomen in het posterior evenwicht tussen de prior en de data zijn intuïtief. Ze worden ook ondersteund door een elegant wiskundig resultaat. Daar moet je het boek maar voor lezen."
  },
  {
    "objectID": "posts/2022-03-22-bayesiaanse-principes/bayes-principes.html#sequentiële-analyse-evolueren-met-dat",
    "href": "posts/2022-03-22-bayesiaanse-principes/bayes-principes.html#sequentiële-analyse-evolueren-met-dat",
    "title": "Bayes’ principes",
    "section": "Sequentiële analyse: Evolueren met dat",
    "text": "Sequentiële analyse: Evolueren met dat\nWe hebben de toenemende invloed van de gegevens en de afnemende invloed van de prior op de posterior onderzocht naarmate er meer en meer gegevens binnenkomen. Overweeg de nuances van dit concept. De uitdrukking “naarmate er meer gegevens binnenkomen” roept het idee op dat het verzamelen van gegevens, en daarmee de evolutie in ons begrip van de posterior, stapsgewijs gebeurt. Zo is het inzicht van wetenschappers in de klimaatverandering in de loop van tientallen jaren geëvolueerd naarmate zij nieuwe informatie kregen. Het inzicht van presidentskandidaten in hun kansen om een verkiezing te winnen evolueert in de loop van maanden naarmate nieuwe opiniepeilingen beschikbaar komen. Het bieden van een formeel kader voor deze evolutie is een van de meest krachtige eigenschappen van Bayesiaanse statistiek!\nIn een eerder hoofdstuk gingen ze in op Milgram’s gedragsstudie over gehoorzaamheid. Daar gaat \\(\\pi\\) om het aandeel van de mensen die het gezag zullen gehoorzamen zelfs als dit betekent dat ze anderen schade toebrengen. In de studie van Milgram betekende gehoorzamen aan het gezag het toedienen van een zware elektrische schok aan een andere deelnemer (wat in feite een list was). Voorafgaand aan de experimenten van Milgram verwachtte een fictieve psycholoog dat weinig mensen gezag zouden gehoorzamen als ze een ander zouden schaden:\n\\(\\pi\\sim Beta(1,10)\\). In het onderzoek stelden we vast dat 26 van de 40 deelnemers aan het onderzoek een schok toebrachten die zij opvatten als een zware schok.\nStel nu dat de psycholoog deze gegevens stapsgewijs, dag na dag, over een periode van drie dagen verzamelde. Elke dag, evalueerde ze \\(n\\) proefpersonen en registreerde ze \\(Y\\), het aantal dat de zwaarste schok kreeg (dus \\(Y|\\pi \\sim Bin(n,\\pi)\\)Y. Van de \\(n=10\\) dag-één deelnemers, alleen \\(Y=1\\) de zwaarste schok. Dus aan het eind van de eerste dag is het begrip van de psycholoog van \\(\\pi\\) al geëvolueerd en blijkt\n\\[\\pi|Y=1) \\sim Beta(2,19)\\]\nDag twee was veel drukker en de resultaten grimmiger: onder \\(n=20\\) deelnemers, gaf \\(Y=17\\) de zwaarste schok. Aan het eind van dag twee was het begrip van de psycholoog van \\(\\pi\\) opnieuw geëvolueerd - \\(\\pi\\) was waarschijnlijk groter dan zij hadden verwacht. Dus, het posterior model van \\(\\pi\\) aan het eind van dag twee is \\(Beta(19,22)\\). Op dag drie heeft \\(Y=8\\) van \\(n=10\\) deelnemers de zwaarste schok toegediend, en dus is het model geëvolueerd van een \\(Beta(19,22)\\) prior naar een \\(Beta(27,24)\\) posterior. De volledige evolutie van de oorspronkelijke \\(Beta(1,10)\\) prior van de psycholoog naar een \\(Beta(27,24)\\) posterior aan het einde van de driedaagse studie is samengevat in de tabel hieronder.\n\n\n\nDag\nData\nModel\n\n\n\n\n0\nNA\nBeta(1,10)\n\n\n1\n\\(Y=1;n=10\\)\nBeta(2,19)\n\n\n2\n\\(Y=17;n=20\\)\nBeta(19,22)\n\n\n3\n\\(Y=8;n=10\\)\nBeta(27,24)\n\n\n\nHet proces dat we zojuist hebben doorlopen, het incrementeel bijwerken van het posterior model van de psycholoog wordt meer algemeen een sequentiële Bayesiaanse analyse of Bayesiaans leren genoemd.\nDe mogelijkheid om te evolueren naarmate nieuwe gegevens binnenkomen, is een van de krachtigste kenmerken van het Bayesiaanse raamwerk. Dit soort sequentiële analyses heeft ook twee fundamentele en gemeenschappelijke sensitieve eigenschappen. Ten eerste is het uiteindelijke posterior model invariant t.a.v. data volgorde: het wordt niet beïnvloed door de volgorde waarin we de data observeren. Bijvoorbeeld, stel dat de psycholoog de studiegegevens van Milgram in omgekeerde volgorde had geobserveerd: \\(Y=8\\) van\n\\(n=10\\) op dag één, \\(Y=17\\) van \\(n=20\\) op dag twee, en \\(Y=1\\) van \\(n=10\\) op dag drie. De resulterende evolutie in hun begrip van \\(\\pi\\) is samengevat in de tabel hieronder. Het evoluerende begrip van de psycholoog van \\(\\pi\\) verloopt een ander pad. Het eindigt echter nog steeds op dezelfde plaats - de Beta(27,24) posterior.\n\n\n\nDag\nData\nModel\n\n\n\n\n0\nNA\nBeta(1,10)\n\n\n1\n\\(Y=8;n=10\\)\nBeta(9,12)\n\n\n2\n\\(Y=17;n=20\\)\nBeta(26,15)\n\n\n3\n\\(Y=1;n=10\\)\nBeta(27,24)\n\n\n\nHet tweede fundamentele kenmerk van een sequentiële analyse is dat de uiteindelijke posterior alleen afhangt van de cumulatieve gegevens. Bijvoorbeeld, in de gecombineerde drie dagen van Milgram’s experiment, waren er \\(n=10+20+20=40\\) deelnemers die \\(Y=1+17+8=26\\) de zwaarste schok opleverde. In paragraaf 3.6 evalueerden wij deze gegevens in één keer, niet stapsgewijs. Daarbij sprongen wij rechtstreeks van het oorspronkelijke Beta(1,10) model van de psycholoog naar het Beta(27,24) posterior model van \\(\\pi\\). Dat wil zeggen, of we de gegevens nu incrementeel of in één keer evalueren, we komen op dezelfde plaats uit."
  },
  {
    "objectID": "posts/2022-03-22-bayesiaanse-principes/bayes-principes.html#bewijs-van-invariantie-van-gegevensvolgorde",
    "href": "posts/2022-03-22-bayesiaanse-principes/bayes-principes.html#bewijs-van-invariantie-van-gegevensvolgorde",
    "title": "Bayes’ principes",
    "section": "Bewijs van invariantie van gegevensvolgorde",
    "text": "Bewijs van invariantie van gegevensvolgorde\nIn de vorige sectie zag je het bewijs van datavolgorde invariantie in actie. Hier zullen we bewijzen dat alle Bayesiaanse modellen van deze eigenschap genieten. Dit deel is leuk, maar hoef je niet echt te weten voor je toekomstige werk.Hier gaan we daar niet verder op in (ook voor dit meer technische deel moet je het boek lezen)."
  },
  {
    "objectID": "posts/2022-03-22-bayesiaanse-principes/bayes-principes.html#een-opmerking-over-subjectiviteit",
    "href": "posts/2022-03-22-bayesiaanse-principes/bayes-principes.html#een-opmerking-over-subjectiviteit",
    "title": "Bayes’ principes",
    "section": "Een opmerking over subjectiviteit",
    "text": "Een opmerking over subjectiviteit\nWe zinspeelden eerder op een veelgehoorde kritiek op Bayesiaanse statistiek - het is te subjectief. In het bijzonder zijn sommigen bezorgd dat het “subjectief” afstemmen van een prior model een Bayesiaanse analist in staat stelt om tot elke conclusie te komen die hij wil. In het licht van wat we hier hebben geleerd, kunnen we deze kritiek meer rigoureus bestrijden. Voordat we dat doen, willen we eerst een aantal concepten nog eens nader bekijken en uitbreiden.\nBevestigd is dat een Bayesiaan inderdaad een prior kan bouwen op basis van “subjectieve” ervaring. Heel zelden is dit een slechte zaak, en heel vaak is het een goede zaak! In het beste geval kan een subjectieve prioriteit een schat aan ervaringen uit het verleden weerspiegelen die in onze analyse moeten worden opgenomen - het zou jammer zijn dat niet te doen. Zelfs als een subjectieve prior ingaat tegen het feitelijk waargenomen bewijs, verdwijnt zijn invloed op de posterior naarmate dit bewijs zich opstapelt. We hebben één uitzondering gezien in het ergste geval. En het was te voorkomen. Als een subjectieve prioriteit koppig genoeg is om de waarschijnlijkheid van 0 toe te kennen aan een mogelijke parameterwaarde, zal geen hoeveelheid tegenbewijs genoeg zijn om het te veranderen.\nTenslotte, hoewel we je aanmoedigen om kritisch te zijn in je toepassing van Bayesiaanse methoden, maak je alsjeblieft geen zorgen dat ze subjectiever zijn dan frequentistische methoden. Geen mens is in staat om alle subjectiviteit uit een analyse te halen. De levenservaringen en kennis die we met ons meedragen bepalen alles, van de onderzoeksvragen die we stellen tot de data die we verzamelen. Het is belangrijk om zowel bij Bayesiaanse als bij frequentistische analyses rekening te houden met de mogelijke implicaties van deze subjectiviteit."
  },
  {
    "objectID": "posts/2022-03-22-bayesiaanse-principes/bayes-principes.html#samenvatting",
    "href": "posts/2022-03-22-bayesiaanse-principes/bayes-principes.html#samenvatting",
    "title": "Bayes’ principes",
    "section": "Samenvatting",
    "text": "Samenvatting\nWij hebben het evenwicht onderzocht dat een posterior model aanbrengt tussen een prior model en de gegevens. In het algemeen zagen wij de volgende tendensen:\n\nInvloed van de prior\nHoe minder vaag en hoe informatiever de prior, d.w.z. hoe groter onze priorzekerheid, hoe meer invloed de prior op de posterior heeft.\nInvloed van de gegevens\nHoe meer gegevens we hebben, hoe meer invloed de gegevens hebben op de posterior. Dus, als twee onderzoekers voldoende gegevens hebben, zullen ze met verschillende priors vergelijkbare posteriors hebben.\n\nVerder hebben we gezien dat we in een sequentiële Bayesiaanse analyse ons posterior model incrementeel bijwerken naarmate er meer en meer gegevens binnenkomen. De uiteindelijke bestemming van deze posterior wordt niet beïnvloed door de volgorde waarin we deze gegevens observeren (d.w.z. de posterior is datavolgorde invariant) of door de vraag of we de gegevens in één keer of opbouwend observeren."
  }
]